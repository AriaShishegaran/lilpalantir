{"docstore/metadata": {"2c2ec5ed-64e5-4db4-8b41-392dfddc7774": {"doc_hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2"}, "ce0b9e05-f4ba-4c97-a8e3-1ecf42aac77f": {"doc_hash": "13e757fcb30c376d6cb07471c55b33a4b47f48caa62d044ca3f1f97f34078fb9", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "75d4fb0b-08fb-40eb-b50d-b1260a6592f1": {"doc_hash": "a93d268fc4eb2296ec5d6cdd30c56ffdbbe24f6f9949caca5c4857be24d738cd", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "6ed7accd-39b3-47c9-9f7a-dc8ceb9e62bc": {"doc_hash": "20f1815c8b2189c42846072ede13f9ce614569a3f013d395057d4fc36c872ea3", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "0724a624-7669-4eb7-bf18-7591e9632c85": {"doc_hash": "8de2d930e51f2d45073b9a57a9d09afaef9811d7d9b38b7632b4f4a11a274b2b", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "d7f6502a-55e0-40ad-937f-c05a86f9eacd": {"doc_hash": "38fbd9661dbc7cc76d105c65c785c744cf4ca84eefebea2159346d008f85df96", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "b466d2f2-0f85-47f9-89d7-27a9d64af5f5": {"doc_hash": "3d05cb152b83790be6a0b18c745b993461a6305d6f38ead07c86063fcf86c703", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "d41a7326-c21f-46f0-8d47-9dd42dfaef09": {"doc_hash": "59e01320292b73f9f9339700b7121b40da7d86df7e5d37bb8b40e665644c9163", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "99b6da0f-83f7-4222-a681-9d783aeff388": {"doc_hash": "9a65c08b56f0eeb6ed4a242433c8b1f017fc7a83b645443e48d2ae5403e1e383", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "ab4675ba-d576-4f01-a59d-e12f9a7439ec": {"doc_hash": "9c14e31351c590f000272c4d614dfffdfc69a3704e0527631b59117d09c03eff", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "a133bbfb-864e-42cf-93c4-692cc1a8b49a": {"doc_hash": "bec39f22d50643886a39edcad632c1b5e20b3dd83718c70a70a5e5534ca9ce84", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "f82c496f-4376-4a62-b2de-0b496e690569": {"doc_hash": "9ebc6588d20cc0d1dc6eb2ecd4d95e1ba162bb2784afc22f103a9e31d4576f0f", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "423d1d2f-8018-4f56-888f-bc984448017c": {"doc_hash": "766af295130e6e03d37acf755e4859e42c8633372ef689f8ac27602c7838e668", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "5c218b3c-c3ac-443d-8ff5-38b7f8aad1b8": {"doc_hash": "ad0a8931fec7f3ac1353bf43ce8ac2b6627ddb4e8829545757854bb1f46bee9f", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "3b40620f-c940-49b5-8d5d-7267ee87459c": {"doc_hash": "9f758013df32e0757ffe85bb7d366d71f90763ece4b2e004d118dd4e22fce3f3", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "48c29261-138b-4f7e-ae82-e0d4871ad9e7": {"doc_hash": "551f60b84e07e1af078c23c35e6ea0f46e201ea037fb091112566ee8078e6182", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "f9815be4-bcae-4703-8b18-69079213728e": {"doc_hash": "b9dfe8c6dd73bef74b69cc9e33bd6ad527784d5339be45131802ddf4983ea32a", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "d70c0638-6e54-4009-a63d-5d033d34e276": {"doc_hash": "e606b1d721ee4f9595c9dba350bbb97501272e0bc5b9b92f2092e52095cbe50e", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "a191cbfb-4c98-4e78-975f-89cfec9078fd": {"doc_hash": "14517740e41a20df982ae312fc11f62e5d7b925a95d798ebedb99be076c23668", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "c4eb8364-7b9d-4ccf-a75e-5de32553ee2d": {"doc_hash": "9e12bff790db2e0bf55915c5ff7b7695ec44e4aa6a810f778e27fb1a646ab4d6", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "c6620f44-b87c-4e9e-b972-a6ce35447511": {"doc_hash": "3e44ffd25de986a73dcd94320b8db8cf04c61e62b8b61d3ae4fbcd888fa86f5d", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "c545670a-68aa-4696-801a-f6fe38eb4075": {"doc_hash": "b1a2f4ba32c66ff0055eb315dc79704302a54f2b22bad9f268cd6e1285e2da16", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "60f7b2bf-4ef4-480e-83a1-3c5a44c04f43": {"doc_hash": "2b2a1375a78473e279e98afe35785439b9962559e41b7890ee3693c1d2741bd4", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "59473d92-bda3-4a69-bbcf-5e5ff1f154c4": {"doc_hash": "ce6a96d3935d838d3186bf7ae4381d3c8746b7ceee5dba298f2403fcc0744219", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "5642e195-1c2e-4a22-8de9-389dbfea0e1f": {"doc_hash": "111dc09cc3548c578d307e6e449e8fce8e72f050f355d69a9bb339ff8f55f4b5", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "8afa1e4c-4311-43c7-903c-49de079b1671": {"doc_hash": "9dce77e68266c8f78f8a2c4ce45bcbc7a1e3914d3b5d08dfdd1b5fa68851fc72", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "d031bc19-0a0b-4a01-a903-5a706e12bfce": {"doc_hash": "86fcc583f4fa1c317bf6fff0adf5d0024a89d4cb93a74205f02633d28cf6caa4", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "0f33a452-5b20-49d4-a598-ae18401fc514": {"doc_hash": "dd59e3584f94183f5c47e32bfbbb2238ae88e963ec97cbfef50a11a388957776", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "5daa7103-08f4-468b-bcc3-59fb4d83307f": {"doc_hash": "9f5f37a6ea1e9088c1f3241764dad25cd52ccbcf8ca54dfd0cd9adeca55f5dab", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "700aebbd-f2ee-48f1-bd3c-2a6f4dba2e6f": {"doc_hash": "ec5cf84423c118a3a71b161cb9f6c61989d68daaad93b33469601a36eb1923c4", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "0463471e-72fe-4443-bb7a-edcc7115419b": {"doc_hash": "8d9fc97a15158dd66876c0b25e85517da684bbfd3ab3a2e674d4874bfc4f2088", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "4fa797d7-de8f-47a7-98c9-88ecb074fbc1": {"doc_hash": "bed0f039248ef336d861d2b39fc75bb09743230618a6ff7424e04ab6f888e4c5", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "4f5fcfa4-ca5b-46a6-85a9-8e527600a09e": {"doc_hash": "359465c3d057023e8aea62fab6cd7944746801f637f2b32ee989c908cb92f59d", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "a4efa5c2-b683-46cd-b9dd-b0167b13e735": {"doc_hash": "56c3c809dfbaea3cebc84938e7305a32d35ed9bcaca152525a4dd343cba980a2", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "0c9dbe0c-27f1-4b65-9822-836a0c505fae": {"doc_hash": "954d626dc44207de20760a1fcf5fbc706eb55c41e20ed4c56d53cca468bbd02e", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "02735595-f141-4f9b-86dc-363c92f33548": {"doc_hash": "aa09e398ccf3cbf623f312188a8dae04f1bf03c11032149d2115c198dff7401f", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "20d76e62-0b68-4554-8dd6-67cd64ee446e": {"doc_hash": "03d767be09628ea560549cdfc32513f05a25f030a454d67c968de47cb3ee05a7", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "7752d95b-e048-4ea9-abae-9126d0c3e226": {"doc_hash": "f4f75af869695f1b7ef88178ce7b6ff75d7fe7a190547bd26d08955b02b88801", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "5faf170a-4714-4769-a44b-73a42b2af98d": {"doc_hash": "885d28347d812badd69bd50c85deb98bd60dbeeb4131a4b30e1ef98fc94e93bd", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "8df08d6c-1ec8-464b-8902-fede8a6e9662": {"doc_hash": "c249b5e761e55048e28ca32502f435a4b142486671d5e2245cc0ad448cec06de", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "aa84c709-a13e-4950-bfa0-a6fc091396eb": {"doc_hash": "5b3dfa1d810ea41642b565387242c7f91810f6d147148b44f5e140e8266f914f", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "4ac1f787-e014-40cf-b7fc-221d16d412d9": {"doc_hash": "57b80c53ab26e082bc5300b57afdbd407af171bcc86c3f13964c2d1a91533ad6", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "219d9685-fedd-49fb-ae25-91b470521aa2": {"doc_hash": "03ad9d24e77ba38f1ce84a87dcd8bf0f1f84c046407df5d8dc7aaf4906127cef", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "5df39e9a-5fdd-4eee-92a9-bbf3fe9802b2": {"doc_hash": "4c3a3febd4d029916432166a433011fb117a37d284a4083cef3fc74222bb17d4", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "fb24882b-de0f-47d5-a14a-89930bfb4952": {"doc_hash": "2cbe895377dd7ea1097fb1800a42bede58ee04cc15e67183583b1a770c9c23b3", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "c758939b-e72c-41b7-a48e-63e7efdee069": {"doc_hash": "304ab9bac6f83cfd6e7eba7627d4d378acfe0158688e5cee0a585ca32969a3cf", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "6121ac62-6174-4244-825e-4f6c29e43a00": {"doc_hash": "ac6e7af74363068e52d9594500ba90f14c07f5386c7c9bfa4f0c358a655145c3", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "732f037c-2f31-4f4b-911a-6aece2dd565d": {"doc_hash": "698be9c12f53e9b2a0626b03594b8e0edfdff927a6f844f942d3df573aa049c6", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "b5e7e4a3-93e3-4d4b-a375-555a6edef2df": {"doc_hash": "31fc357c6631440cc2d35ede1d5db5ad56c0bb19554f940fda88eede04b91c21", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "60271cf2-42b6-4201-8fa2-5900ba62713b": {"doc_hash": "97d1248dd2cc56e79d0eb95263099b15f879c5e21c7652f5a09e11da80a1fa4d", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "4b02307a-749a-448b-9fee-a287c51518f9": {"doc_hash": "8cd7ae7af906f59c3a3ea032bb78663f4a39d8d374c4e4ebd00e7f3010d580d2", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "24b2c9d8-9a7b-439c-9e3d-0881b3d444f5": {"doc_hash": "a4dbc5a0770efb73d198aa36ac548b2809a5e1ea1ffb5a6a7e66ab8fb4aab78f", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "a09e5836-dc3d-4176-9c21-778a391a9e6f": {"doc_hash": "62ec3535905c26fcf61f2ecdf50c4347732d16145c4ded4532a3c48d006de17d", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "dd6ea1ce-514a-451a-8d45-df160c663ea3": {"doc_hash": "520cd6d6c2577ccaa0b274d410cb7fb7f97631b4b456e70498c87a367d69d74f", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "528afe42-98b6-4296-ab83-b61d8db1d54e": {"doc_hash": "7dc039d26f21f65ae5150e4c46eee7ffa7f0adca0727da374b2a44d086c0fbd7", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "a1122825-051d-422c-b717-4b077a1677fb": {"doc_hash": "557d7a694af1c8371bc03a728c48f07ee93e3dc7dc6141b9ef990c367642e2f6", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "535d0953-a27d-4542-a114-6c06ac41d09e": {"doc_hash": "d22a3ec3768a0e29a2fa5a3ebf45a1692ca29c920bc11bd65a383f52d2946772", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "6791e156-fcca-482f-bbe3-fbd41a9e2804": {"doc_hash": "f357f4799ef17d102bce4707c12417be5b8da558122caabb0db2a461964b4b3b", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "53073738-ab94-4899-b213-31eb2e3023bc": {"doc_hash": "923e2d2e4801838d406ef6ee7855dc562ac863041d43ec97b209e6b508ff1ef2", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "925f9454-708c-499d-86b4-96d54525be28": {"doc_hash": "a49e650084dbe2c932b0891ca7b74f35605092f203d5ceeb292a5e1f125f59b8", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "e667b938-5533-4206-999c-036a1610e504": {"doc_hash": "3236a1dee3578c89f03598466c285a9b0fdbd7f8e65d6463c22abb40d367c7b2", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "d72652b4-ad63-4c20-8710-4ee8a46e01b6": {"doc_hash": "5c6563fd794067001a49db47b6227683af53aebdd1ebf978e552e77e684a17a7", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "e88ea21d-5360-467f-892d-14ef3b4ff6db": {"doc_hash": "8b1ec89c6ae5d4cc80c6b62bbac0f7feef9cbccfe523de972f21af73f24ea52f", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "ca46d90a-6a0a-4054-b8bd-b909a3001165": {"doc_hash": "3a027c52c42ec086d3b462cbb4627425cd951a0f64c758ce314039bbfdc1266b", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "15890fde-4b03-4090-9ed1-7191f72f00c3": {"doc_hash": "4766563bd96357808d8158f3eb1a3906e387b07b4db19232713539c19b354c8a", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "7b13915e-5495-4ff4-904a-49ef7b277270": {"doc_hash": "6d946f85da8af850894681d0b280464d52df5cd99163d2e66792926a4cb2ed11", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "781e5e69-16ec-459e-aaac-5752fa3a22f4": {"doc_hash": "f294f50c1b7deebd74784d3b0087817f4774ef41e7c4cea96ac50401b9c012c4", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "84822d53-e37d-4927-ae72-dee3c757a1f8": {"doc_hash": "caa89b4649f426e9c03f2b79e9105f4e588c4e91df1882fe4e2d37e13c40871c", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "bf61401b-767e-46c8-a081-d3eef3139e70": {"doc_hash": "6e7ee39a88f76223106984e0dc938707af212542d7afae6e8649aec797e832de", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "0076e169-797d-4bf7-820b-b71ddd3cc632": {"doc_hash": "46adc2e427f3c97b051f6c0d35594f65834d8b6578bae30fc83dbcc3bd0df974", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "344ec229-3569-4665-a445-eb81a1513958": {"doc_hash": "3be566ebe398cfbd905808e7e26649e474389c134442f79ffc230dff49c27675", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "a2cd35ce-3af4-468c-af3d-b58d6778b03a": {"doc_hash": "57db6c8a9fd9993bc41d508a565ad86a368f8689f951ea7e4ab8ea557872bdad", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "b8f116ff-73b1-43df-a4a2-230fa30df2b7": {"doc_hash": "318190849db807d7e56d33df238c51e8d53209c1892a36b0d2bc01bd8a5cfa14", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "e6eb8953-5820-4348-92fc-6b2374b0eba3": {"doc_hash": "804f206815c1b9b7edfba1aeaa8ac401efe3274e66457b70d4b2124f758227a0", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "13e765d8-be27-4c9e-976d-ba30c50de3a7": {"doc_hash": "2759a61332c4833af55eba368c10120b9b98d60301f9853c95a3ba8a18b6802b", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "2797a072-f1ee-4430-90d2-be5174db837a": {"doc_hash": "b23f973442601604bb6ab0d7db26e1c834fdbb91d8e44e77db19d7dcfd00eff5", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "80576584-c57c-4e47-895e-33f0c5f9e477": {"doc_hash": "00d27fe148c12e591cf3ac8e2bef7375d55a45605bcae43c1292a3d7678098a2", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "ddc99327-8512-4467-8f63-1f1df4c7e861": {"doc_hash": "311c0c73627476d2f65f3fa4043ebaf4b077b39aab69118e4df3dd6ec2ddb857", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "b0f39855-071a-49bc-b7f8-48e96c142da8": {"doc_hash": "bd5eef27e89c6016baed154d13e76d51c70dfb75dbd135b289784d4ca523d806", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "668ce8c6-232e-4c86-8eb6-45ce8ec0a1a4": {"doc_hash": "86092da5308a61e8e7bb9e96a13b1316cd6e4ea1a41d0610d08f1ef45d909081", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "d319b514-73cb-4d7c-b438-4f376d791c26": {"doc_hash": "8819c0c1c01165060fe7600f8f77f20b723941a57ab227b833671049cabb95fd", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "05178036-8e53-443a-b2d0-9d58ed583caa": {"doc_hash": "0716203615f64f7acc910e18032424407290ca853adf046450770204d25c9737", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "5501ae58-814e-4f3e-9e74-1be94f455d12": {"doc_hash": "08e26acad63c152ee8adf97844cc6d4cfc26b64d08e63ab80ae831695dbfe095", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "77a515e5-0eb4-4703-ac62-7ce0f899bb9d": {"doc_hash": "e059842cfd463730871190963bbfd77c2185482386484d2b9b4ef93661e80f13", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "32043cd8-4f1b-40aa-a915-b20b24cc58b7": {"doc_hash": "2a35fd85d4a12361c1f1ec478ba55c8808c984228e610782f6f9f68af4781eda", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "cc5dba57-4dba-4185-9e02-e133ce92c8ba": {"doc_hash": "3c9c42d0f08a9f9063f81159a1792b00538b9acd12d18dabc3861e5b15de3b7c", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "331462d8-74f8-42ee-a2d4-c82b8b5f6eb8": {"doc_hash": "6e58a049a53b2e24881809dc9a1b0dc4d8d6eeba888f2d1e3ec700b95d6f491f", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "379795ad-cd6e-4919-ae94-e1e957728d85": {"doc_hash": "ccb6c2ca68ea0a57883def0065662eb5cefaa396a50936e0ff821a050c20be19", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "723d2a3a-1b1e-4e99-a1b6-6225edfe33bf": {"doc_hash": "d8556968af08de84eb99ccab300e52b4ea9776c9e4e9c51a894c7fab1b80d5a0", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "c94e6ca9-20cf-4a22-abd0-227220f8ba47": {"doc_hash": "f8df0352ea0c071cda80d5740ccf09157b0fe4c4107483540f9a8351604fbd7e", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "521cdbc0-f093-4182-9fb7-675d51d8bbe8": {"doc_hash": "a3fdc7d8c74c56e172e38b25249168642767845ac3d3813c3727eace4ca5cb00", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "fee58545-512c-4ca6-aeac-c39e423a6d64": {"doc_hash": "92872574562c0b71572d29af36ef63e0025537e2e0bf414b3b4a1fac762d6614", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "96229776-9ca8-4693-8b5c-f7a5eafcadef": {"doc_hash": "f9513356d95f27630bde967d43933730054af74b2118dfcc031b067e07aadf4d", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "35e62314-07b0-4bd4-ac68-c6a728a21e58": {"doc_hash": "fb78e0c8ceb82ed916bcb25d831458e8bcfa5d1471711baa289bfd56253e7e46", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "632c0d18-5b4c-461d-9d75-73d9a50db254": {"doc_hash": "87771d447172d7e30f68c93eb49a292cfddd6fddf9e7a2044f3c3b036f2f72c7", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "f48a3feb-0a05-4043-9d55-fe9e834590c8": {"doc_hash": "4a9faf8e819bc9f418f5c32cbbe5ee26b49d92891cc8816656919b9b12c2a98a", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "5ac5d598-6a86-4d48-98da-d1eb787a8069": {"doc_hash": "e88ad571b1aed8de903eeb7351c3d37d24847880470a4cb26cc4e8618edb96d4", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "74c9dd41-6daa-4924-bf7a-0b5abcd0a46f": {"doc_hash": "62eb4d13947f889430caac4f779e17931277ca0caec008fe6340f88b08b1311e", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "1f1080d4-7827-47b9-97ac-cc0014658a6a": {"doc_hash": "5286d7a36d5c46c7fc0ad3457b71fada26cf9f9ff005fc1b3db32d7d1979522e", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "ab11e74c-f3ec-4c74-b401-5c065c3ad6a1": {"doc_hash": "329dc0ee68604000d4a804e1e4eafce9c8b31d664e983c01a8d2cafcce1170f1", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "cbfcb61f-bd1b-480c-9294-75cd9ae2720b": {"doc_hash": "104c9c6de2958576c36d4bc8f64bce1adb8f3f5bd6ff7825dec1d21c9d1f18a9", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "b1c2e3c2-53e3-4f2b-8b70-5a2c96ff3ea2": {"doc_hash": "8e0267d1c27de4a24f57134b7ae584ee3491e1f0390f253d64c51d7ab5d2ae6a", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "a1a8a867-b916-4d1c-8344-83fc7f136d9f": {"doc_hash": "2e267ca5307d88cff6bd53c7d3facf1032cc8f11cca33e914c4a95e8e0dc71b1", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "0f4bca3b-8a2d-4b46-9e66-0ab40c3892e2": {"doc_hash": "f67fa49a5963c4469e150da7e7d4017550f3fa91a626f2055f56d284e0fc987b", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "b030ab3e-69b3-4c2e-9f0a-39541bf80cf0": {"doc_hash": "1808b3a041209555e8042b3cded0ecff601fd390cdbfb0aedb801a846a0bdcf5", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "1f4b4260-f0a6-44f2-8ec5-796514392f74": {"doc_hash": "1454b31f865de2854ecea836006853e0c26e13a3456b4e7da4de447e7ae010bf", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "98ec463e-00aa-4b21-acec-c61ca61a2547": {"doc_hash": "c2c6a53b4b91e191233652143f105478a349d15007e10e673eef03d1732bcfb1", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "542e3e7e-e3ca-4632-8700-4e602e99d875": {"doc_hash": "6c01396c652b640f7356aac40c1c4373a1c607463348990f63b36f67ddfe2031", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "290e2dc9-d928-4cb4-a952-a5717c1a52f9": {"doc_hash": "f0894c2296583fb3692e9ca843bd4f28614544035970022abd7a6f76adfdb9d8", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "3d09852a-b4f6-41e0-a385-15ab77b3aa46": {"doc_hash": "25a9eb3b0d4928f2afad0f35aa8f02d532a07bd58b63dd244baa2530880bcf63", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "471a4722-e519-4537-8b54-73e79cab831e": {"doc_hash": "1ca4cce76195a3c041218157e1d7f7d31313d353b0138e1c4c6bec2f5539c7ec", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "3a5fdbae-d8f0-49ed-ad88-651a98d525d1": {"doc_hash": "14ceda01cc50189e4bc188986d8b1ed96fe76484a70306eabaac7976535180cb", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "1b19fce5-054a-4767-be9c-c74e8662ee2b": {"doc_hash": "f0bbe651fca1086351852b05e21e11e8344e1aeda96e120642ded4a70b999412", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "9a8933ee-8098-405b-ae91-184e3ba43068": {"doc_hash": "53b2e65c517e5946b46c2dffd7d23e2f421b62f2722163dc98b6a70a4088d917", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "630f713f-df93-4c00-9741-54633ff17775": {"doc_hash": "275f608b627fabee65c8dc0578fff977b6a83c08b3b12a0c6252ffd00d0ce37d", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "2d607e9a-32d4-4498-a209-81f17a445317": {"doc_hash": "c3a0c13858d2d798a8528d00ee05ac5945620b054bc6d4eb715201bc4ce9bed2", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "2a325d46-66cf-4c0e-bc5b-d24307d91452": {"doc_hash": "2a8b3b0b85464be19cefd49e631443821045678bd477fe6d4c54bee582be4a87", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "cb765245-f3ad-4357-9964-f2f027ba17f4": {"doc_hash": "de980e51641f7683f89c8630bd0539a1e98e1845c26bb7323c029dbb435bd59f", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "b3dd270c-7cb5-43ae-98e5-689dbb7629b6": {"doc_hash": "69d0ecf644b3c732cee28bd5bb54c5adca907ad9937e0053ae1ac8fba4d63149", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "a8d803e0-c547-4d0c-ac84-b50dcbd3721d": {"doc_hash": "be03fde7a651d172bae779d6a9ff9a697c8be11b64af1c5e3b1785f20ddae149", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "48bb1a59-e55c-44ea-b0a0-d0e02c94b641": {"doc_hash": "91b052d0b70539bce6ff08bea5eadf593b88e7ae2c303511d065e0e8ba8f2b5a", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "63a4bd6d-e2f5-4fb9-933b-39662c7c812c": {"doc_hash": "8a557d39a10c704bfa33e37ed142a0b4ab7e2ee3b050b6c0303bc7902a60de0f", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "6aec1e8b-3df6-4662-92ea-3828d287524e": {"doc_hash": "e7f81a2dea54e88f1c8547e7bd3e62e198e55e45d206ce77c11d4fda5444bf85", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "cbc92233-4a9c-4d7d-813f-dd94a2fe8f04": {"doc_hash": "7e7d8770ac7095e603fbcb28d71257dab1ccca292eb41e3b7cd08f00371fcb98", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "dc6b2c27-0fde-4321-8ceb-45f8d5221ab8": {"doc_hash": "8c1774665141439f5185fdbd07560adb4615ca117509306b1a10918c9cb6a725", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "8dd7916e-c09d-4190-a57e-8dd1f2eeef37": {"doc_hash": "00b93056e3bb6b1c7863a59bec6534be6b5a4f3c34db498ccae5571560a02fe3", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "5b9692f0-02f5-4fd7-8ec0-6321c3b7f203": {"doc_hash": "15a7b67ec6978451a653f3b5ecdfd54e63b8162827abe51841e221a7fe3d196c", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "828b251c-7fc7-4b90-9ccd-d9e0900f2061": {"doc_hash": "c84f994b1cf6a720d04b74a532817ed5bf868ec16c3a67763daf1f3b9b1e49dd", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "1a068b35-98d5-43e2-9717-a01bfd64426c": {"doc_hash": "3a054464d24b33346ff22c4506d997deee9d210277165d40862b8de8647e2d71", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "965acd25-5015-4688-a3e9-d37f06a5a82a": {"doc_hash": "6e6b98a8400eeff1e6e1d17dd2da24d1897aaa44ce0892a118025a9b4c618832", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "77ad28f1-2c3c-4dd4-b3ac-b673719a7a46": {"doc_hash": "639dbc653f836e477093cfed16dee10bdfe5447628e311a6f83b915a3f0d4a3f", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "641a4acd-647f-4461-9c3b-665b15f08f74": {"doc_hash": "b8b5d72187e4aad4ecd7a7f7435789fcbecd5c1c9098d8979e1df5e05c0308f8", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "5d2ddf35-1b06-46dc-8939-020e2683c7ea": {"doc_hash": "ecc75bdf663b210c4e3e051053c3c1939dcdb7c1df98282a28ed0a371dac5a68", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "de394b31-815e-41b6-866b-4254de6f1fcb": {"doc_hash": "08e105e548eb383382fb5cc222bc0402ec8603c66f6dcbad76c63dfa45b06d03", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "8c4c6450-2aad-447c-aa48-7db5816d05fc": {"doc_hash": "d167d187ab6e59c44fcbd04f56040ad6437f088d2fee1b1b107fda71af50f96a", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "f54b124a-b23b-48c8-bc72-74d818994e78": {"doc_hash": "5fc7637f05fd82b846d173cf523774ccc8d13589135c0ece2d997dd1f04d27f6", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "3d951d46-71ee-4450-81a8-249a3d1d379c": {"doc_hash": "30020a89967eebbc80adcf9e12b17c2c91f9df6b5896ab995e57256da09bc0b2", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "4c10f578-9617-4ff9-a27e-0c9f487a398c": {"doc_hash": "2a2cbf831bfacbc3d54fe7e33de2e415362fc11bddd4ab43eac4b88714f6e54b", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "0417ac08-df82-4822-81bc-4c0b75fcb71b": {"doc_hash": "603720836885381ef94ebf9e4ee91561d809eea468f4c611a01f2e346e66ba80", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "5c34d5d6-e083-4f0a-bd38-a4f1a55402bd": {"doc_hash": "b46f6200dbbb5fb731f59c6c75324e5fc3b72c028d7bb1fb127eb32533ac2d88", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "3f296ff5-5e78-4b0c-85fe-d2f0aa9dfd36": {"doc_hash": "a29e0b74954f0eb497349132e08e5dc0a9e9a929dc00e24167ec37332730a558", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "f5b45c0c-780a-4c07-9329-57e41c1e3700": {"doc_hash": "c2518e6953b6c7a8858520d9a99ed16d7ceef9e9f2232b496f19ab84c4078b4e", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "80e5d739-3b00-4259-9797-6d2278fbfb14": {"doc_hash": "30be05431a5cae6f6c0f77a21b98060ea2dc97ff416612873545106e8a3f923d", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "e9a8004a-abac-48c4-b129-a20461b0a3e8": {"doc_hash": "6929717d2e85bfedac61b4aa8f6a654acd699cdc87cf9e01ac57d450d0988f3c", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "63e2e665-c350-4422-a3ef-cc5fc2712a93": {"doc_hash": "170a5d6b5069c314690c3a38898c7924362a8d4e508c8a94156d49eee0ed1170", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "9ec573f0-4669-4a2c-acac-0a589bb21e2a": {"doc_hash": "4e0f1a0bc8867eccc55361c02b88005920839f8e0eece753d51b8d646d0db95f", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "e6a8a432-83e6-41f8-9083-1f5431eff637": {"doc_hash": "73de9289b8bc72fdaeb3848394731388e2f231c55fe659d49373bfd3861ad104", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "40eb9039-c2f3-4d30-9b7d-7a8ef047469d": {"doc_hash": "1736f8927f6c2249c30a9f348d96c3ab601e086a62a7f178cb0dd09dd5a4f8bd", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "c35795ee-c5d2-473e-8726-ccc51ee9d9eb": {"doc_hash": "b46af15972205e3440847b2e651e321af83abb56f2e541f701d1b4acc33a83d4", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "7c4e0ddc-12fe-4500-b878-b6b87efed231": {"doc_hash": "d936b6f2f57480f657a76ca11a11831ee573357b94ce6f1b17bcf92a1b7cb493", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "e3a8ed41-5133-4ae7-923d-a565af99fa7e": {"doc_hash": "fa651f04b3ddebe1d488b1eb242b1dd99ecc31f0d1cdf0cd2c217cc038a0676e", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "6a4d1058-b028-41b4-98ad-72370fe1093b": {"doc_hash": "8e43abe89deb0aea20f904e52ec1eaa15cfa1d26c675e698b8e500f1e51270e6", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "b740b58a-71cd-4b92-bd5a-b07e2b25dc70": {"doc_hash": "ebe11657f36ffd4b857a4bb71bb2592123b958d4ba9ea4808ddace3f14c095c1", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "12bba127-6227-417f-a8a8-0b71e26447df": {"doc_hash": "e22230ef541ee3f17c50490a85ac4742d4dc19912918e38e695ef71236c4d782", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "9d09f8ab-0693-484b-9233-805f358333f9": {"doc_hash": "a25974ba9dbc6cc01b75894c7e308dfd17b8634d4b93e7a53977b3ecd1d2b8d9", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "2792e8e0-2019-403f-9636-67e2cc02a0cc": {"doc_hash": "552549b39481dddd97005535cf190fb8b43dc89b8594d16562c64f4265e1c091", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "1fbf18af-77b0-48ad-a125-081ab07d30ce": {"doc_hash": "78543d133bf9313ac8f573d75e22e1084750cb3510d4c158ed79bf2e07106350", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "623f002c-a899-46c5-9873-74a19770c592": {"doc_hash": "446523457480f2072ae02491c02e4de22dff72490f5591d2894a37837daf91e4", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "ffda6ceb-fd17-4d46-a30c-fd937a1d9d9d": {"doc_hash": "84bc7e46be319f66cf0e6df21aab064f0959967c32f23135ece1d07de63591f3", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "fbcf7042-6fcb-4267-8a07-f3e784a867af": {"doc_hash": "261cce08ebf8277ac4a7ea335c1c3aa213f63853a7d2e67e02fee4b163b40850", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "a5b7195c-ce65-4f18-9596-aa26e8a7dadf": {"doc_hash": "2cd292d4102ecec48e090d846f042f3abc3f78cde757feee27fd9d89c6633ce7", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "fb6798d7-574f-47f4-a964-fbe2cf227d40": {"doc_hash": "4a2c2106698004b3c1568347bb7b3f5d785d2413dbdd153ccb67323611c18766", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "e1942889-c301-4630-a8e0-b56f5bacfa05": {"doc_hash": "f38dd483b453309a71045a65cf6ba2686dd925d0f922be42d31a8315fb4afd88", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "43a14e95-5be9-433c-be93-d04baad41ab6": {"doc_hash": "bb9b8515e26e4a6c0e7ae98feafc61883946f04d305a1e69addf2b43a5e2987a", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "21d6d0ae-27af-43d5-946e-648883050065": {"doc_hash": "985aab847f4e757ef8e181a29d7ecac0a01f6f9407082dfb9a7c8089b7a9742c", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "0c4d99ed-5036-40e9-af5b-37117135d1ba": {"doc_hash": "d1af6d3160af450a3dd19b129b0175c1ef6bc20752700203782bde65f6ee85be", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "2f5f9482-81fa-496d-aa5d-a4b501471495": {"doc_hash": "aaa6d1bbf6a7ec2804aff496588134d1cef8663ecebabbe1b033fe5bc47179f0", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "64827f10-25d6-4752-9e3e-9ffbffd3dfca": {"doc_hash": "68ca75897ad6c82d00f59877be3d74d4d6bff49f2184838618cbb2c1feec9b39", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "30820258-a51a-4ed7-9749-4f0e55a08c77": {"doc_hash": "5117c6481a881a993be55c0301b898278ee577237def3af9471c05201c30f4e3", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "90a10781-a82b-4a14-a197-24cc7a5bcd76": {"doc_hash": "1be0f476fd331550faffef227231e0fdd08ff7eab2a877e0073820ef08d04211", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "2738c963-3b32-45a6-b316-65230187332d": {"doc_hash": "11f750b78a36b66981b38760a19668c160dac72522ee6dd3444d86ae248d740b", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "72d52d59-bb01-48dc-8a54-3dfbc699be5d": {"doc_hash": "3dd730aeb4d4846bff6331addd55a3bccadd00db348b14e907e6c22804361008", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "0a3484a5-3d67-485f-b143-70f5f1aaa459": {"doc_hash": "f72549613d70c86bf6c734cf8d60bb4229d2c4114d6c3f9f078b48b1b7a8823c", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "26ffa5ee-477a-45b9-81e6-397321ab72ad": {"doc_hash": "180428aee136c91183b01c1952fb0d2d8449912bddc0c217f64836185e96dfc8", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "ebcec406-cfa4-4318-bf26-b41ff5f72485": {"doc_hash": "5ca7c59a0a69b8a2d5a1c9506290a46c2fa96274bf7849a5ae0bcf9f49e7c630", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "4f90492d-4f3d-453b-ba4b-d46faef3e44c": {"doc_hash": "ace3d295cfd89bc86b0ad7ecb19fcc480563654f9d5a552bfcffa19e544a2854", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "b367f9e5-a1b5-4174-9aa3-b7fce1d27b22": {"doc_hash": "e167f456e9b82aca384efeaf155ebf2b2b4cce18931c422dbe142a89c35175c6", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "f5036856-90ce-4a0e-a95b-e49576288d61": {"doc_hash": "9199a75cdc66cfb6dfc59120d2529608b36dae951bf2e5bfc8049470eff9f5a1", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "b3411dae-27c7-4320-b6e1-9e4139c1c9eb": {"doc_hash": "6649493e5af683d41430c36b476763e8153df24002237595fdb47b01193f77dd", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "f6c17a3f-c81a-47d8-b715-5e99fe7593fc": {"doc_hash": "ddc5768b2625f10a7f0846459dc53356f776177740c56a5381318eb1550081fa", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "917ce3d2-de34-4533-82ae-de43e0475dee": {"doc_hash": "8a0bc86474354506f24a0c68312675ed8d2023faa5942efaab50876710b80f48", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "b9b0b7a8-cf13-4756-8a80-5fa4be811099": {"doc_hash": "99cf2d007498627f2188fccb83ccf8227f55a9a7e3a79a425ca30d9648b2b731", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "ba741414-e089-4eaa-af02-1bf9ebf40169": {"doc_hash": "fc29b73c109dfe2329e09e8b84128d1f5819cc639e533d064cc21c15df188dd2", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "b566f661-0271-4a9e-a219-ec657392410b": {"doc_hash": "edb0a54e4afed7fc3aaa917d53ecf0628420a6205dcd25b6c800de25e6cc8deb", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "ef3464ca-117d-46a4-bf59-338ef3972f22": {"doc_hash": "b1f21629c2e2d68c47313cc8b12f2347fd1c8016ddc22a3b2c9a8f6e4fd05daf", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "25384309-fcc4-4142-ba43-67d35e18ace8": {"doc_hash": "09fca7778fd4203ecfaaa253f093eb1df0df2bff927fcfb80e9d4f4328f333b7", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "772737cd-f6f7-48b7-b804-c01878343b78": {"doc_hash": "8528b62801eec55d07c5ef9e9ab1862b3ab1e90695a515a3b686e66893c5f863", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "d6c2b407-585b-40b9-af57-89741193073e": {"doc_hash": "ae618d8b5bfdc9146dd9397ded2d0ab16235890ab6b36df0e83604230295b737", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "840bfb23-2e93-4441-ab43-3d430280a18f": {"doc_hash": "00b59f1395e44b4903bee08c78a153b038e9d452a74b885ed82e278fa6fa3f2a", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "e73ae97e-5eb8-48f0-9863-d29a7b4bd638": {"doc_hash": "3966141ffdb6dbc771c15ae744b3349115535368ebf9e4b4e0a219a530f60fdb", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "071796b2-c690-4e5e-8f66-52dc4a24a140": {"doc_hash": "1c0f98e2e56505f4fc6b4b34a493e1a6712f84e1b7e12b44ee8c052307a80fb1", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "9c0fb3e8-b37d-4716-b3e7-e26f964ebf7b": {"doc_hash": "6abe0e5271af878b53736c1a6a1efa0bfe38d615bc31987b4cbf6659d997b464", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "c508c36d-07c2-433a-a0ad-8027f870e8bf": {"doc_hash": "0f1446f78d07fe0607da143dd5e1b8760faf8081958e7f969fe5ede829c35b4c", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "c1887ceb-8bfb-49e5-a4a1-70c459e1746c": {"doc_hash": "16383155f282d3903707cac013db05fcad61c0aca34f2dd46ad42837adbbce4e", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "96a04c58-6f55-4692-8422-a08b421f9430": {"doc_hash": "39c118467ebbeb4ea50c0faeb60be2d7d34a4bf21bd5c5b6394ebd13a284b6b0", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "6fd7a78e-6f18-42f2-9984-401de4bf5d57": {"doc_hash": "924c2771c6825f55951b1988b323040f1f930b21fbc4e9d3f73843f0e4b0d101", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "f59c3cd3-9d2a-4bed-a8a8-a22915fca3f3": {"doc_hash": "029932f8b29a1b6beeffa5502274b492b0d41f522952850a492f2825912e7ee8", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "fb922417-c0b2-469f-bcc9-8efcdb34f34e": {"doc_hash": "501b2234b84bf74870885309a48e48d141ad9c01441e875bafcd7fff17fd51b0", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "baa8a43a-847e-4883-bdfe-56562826cd5b": {"doc_hash": "a0b4a019a84536d3591dc50bbb47947c8d9b05622b189d421a3f038753f6dad4", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "19d154ae-7244-4933-a2f3-9899d84f5c54": {"doc_hash": "bac18e237b5d82a33a15e817d974313d16d3ad8013eb318a3f64b13b404b769f", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "9417db7d-1e88-4534-a6bf-e395abd7b616": {"doc_hash": "490bae60032f23fcdeb514d5c4ccdd716e77a2f00ffe07573de3b5d88d8d6eba", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "60b11cc4-41a6-4d86-99a4-afa86401c63e": {"doc_hash": "37a4b3e1cde0439aff355dbb958ad3e89a71588ee7263e2e9e5dd773b96d3fd1", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "076e0f0d-2a58-4075-8205-64d34161176d": {"doc_hash": "3d8c7c212c7165c4c8599dacabede64adca5c6c961531c8afb3211c7697a9276", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "2e49791e-be80-48bc-9abe-94fb78056f86": {"doc_hash": "037f62b945938a3d32eacd5bc0fe0a27b21d49ce4672181ddf6de5eb84c76670", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "d22bbedb-a982-43e1-b839-29ea388c95c4": {"doc_hash": "5fe404ee5462edc28dde6592bf4c5dad555d6230f1af12560682b669f80ee5c1", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "840a3c47-d0d1-4be7-9aa3-3601eb0ba40a": {"doc_hash": "83ece32235e37cb19a3d8e516e5f297e85b264a1fda80e816d88d25c56508520", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "b76ef6ff-ef7e-4a70-9a93-53418a22ce7d": {"doc_hash": "0830d80a7a6b9819feb30b8b8a21b5493246ceb2d4888a8333786adef13d8231", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "b091f2f0-8541-4b80-80bf-f5fceb61406c": {"doc_hash": "aceb20332e183ea9a1cccc7aa45386abe7d7b7e4427719bc5bdb48605fdd9787", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "aaa04e9b-1b9f-43aa-a323-56f1acbf036c": {"doc_hash": "373d5f2f09104a59ebfac933723f46ffa49317c33ddbd25a0b80ad74e5f9b4c3", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "b4d84669-bde2-45ff-9867-3fd174a84d14": {"doc_hash": "e5509005458b95f56bb45d05e441849627eff7816134599cd3b414318ec1d878", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "6b4150c2-c8a7-402a-a2c7-a8e9957c668e": {"doc_hash": "e60b82fd4a298ca6a502f3af93049950fcfd7048a2ea3d5086ba50bba13150fb", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "0b0f522d-72d1-4a39-9d3d-9440cf5aa039": {"doc_hash": "264da25bbc40aef82e29ade14cef6d5ed7a8198006e47fae8e98723290a38b06", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "0a7ad520-810c-4d3b-8402-6e2ac6363ea9": {"doc_hash": "ed1bd4da91a76fd84b26465b3e0f6dae5dd5796d952696999b13ccd44b582e29", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "7ae0b6fe-524f-48a1-82bc-dc3bfe0bf78e": {"doc_hash": "b34eb9eb5e6730faf6eb8d1dbab8ed37ad155924c47476a4645a699e6eb49b6d", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "f9bd7c0a-1d77-4f66-9b3c-a6ae4d37a1a7": {"doc_hash": "45a72627da8ebee98504a23a8ecc2b0cc3bf0dba2b0566d9e551382728915ddf", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "b7fde8df-6e12-4ab4-a76e-d8c7542e9e55": {"doc_hash": "b84ceab524a71ecc85c5b6b45c1bd4cf77841d5dde6298d81f498609c3b48d22", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "a65a646e-257f-4ca4-b433-2edb6a9c0e25": {"doc_hash": "b01ed333aa52729d327fbd7d5e7ab7dd0717c1b9f88e0cea4be4044afbdc6602", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "8474ecaf-d19d-418f-bbb1-6c70fdf3e2ab": {"doc_hash": "14d832e9298c7b2e68646301cc73ff4c185185e6214279ef479533e1cd7bb2d4", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "5cae8a8e-2461-41ed-9f4d-84ecb323a469": {"doc_hash": "b172f252fa92771b9ae6e01ed31b36733575dd260abf7e4a8089864b4167f7d9", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "126abbbc-585d-4278-a6f3-61a1593773fb": {"doc_hash": "601a1fb0e430d3c06123bc94f0eb4232aeca787e73b16eabef23442ab6913aae", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "9d22509a-fcdd-4cab-b86d-a30672628857": {"doc_hash": "23f5924cf4508b7478cd99a442e99f74cff352dd022131a4f5709fffb5a569f8", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "a18b872d-4a23-4fa1-8218-7953940e6511": {"doc_hash": "9f856258187186526efc583f83b8360c516fdc17a3865b3264bcd8f4af310422", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "e691f74a-8eff-4238-91f5-2bffa27b16b4": {"doc_hash": "f002ceece771f446eb8bc1c515d08c59f4b17e7a6b44fcd5d03657025cdbb122", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "ecc61760-3573-46d9-a2c9-34ddd81a791b": {"doc_hash": "60e1bb369a02472f784b0278083f1b557142a08a6af60c9fc9a7dd3130d00bd5", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "ea07a436-ffb3-4aad-be18-c36ad5aa6358": {"doc_hash": "13be5150e310e9ca1fd770fe68189805e0bfb41e4f4c2b60c07fe65f37429032", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "d26df7cd-905e-4ad8-8cf5-44c2b04b7aa3": {"doc_hash": "4dd7356a4dcfbb7674d7454240604eed31cd4c402ad7ce246a3d3e1d4ead5664", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "47cd5ba4-2c63-4b7f-8ae2-4ba862ec3bba": {"doc_hash": "f6e060f4277960d7eb2df012f5030282805448ff71a73afc3ec090e4fed99988", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "52724a04-e86c-42bf-89f2-6a0328353ef8": {"doc_hash": "257ea0d621efecd9070b68effb6b901b74f835a20e42ca39826fc5c8f26ccc08", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "b6f406bf-c6b3-474a-b5c6-4771d39de426": {"doc_hash": "bc5657bda93f21f9c92b9720741fcfeb77940b3701edb727d1e239c9882554bf", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "ecde0cd6-14a7-45d4-ae68-2f425caae914": {"doc_hash": "067e5cf10147e2979597b66cc8f084b61674daaab39c9cb517f4836321369255", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "07a57000-bdad-4b8f-a3bb-6ad10a2ed640": {"doc_hash": "5467e28f073fbce223e7d434a021d5b8116a7601b56d17d27a3fb59319478ec6", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "6de95ad1-f70c-4504-a3cb-daab98c3d4fb": {"doc_hash": "f8bda542ea7fdef7dfea7afbf53d009cf6d579770c4a11149523e98c4237be90", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "414fc246-cf1f-4387-b9e9-50a59269df94": {"doc_hash": "d45becda003c4bf9f61c8a10f1033425ad106b00930eee65f1f5b7d45cc32c3f", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "332f11eb-a03f-49f0-b0c2-57e31cb8579f": {"doc_hash": "2ee26b86ecd09414c2fc348d66b400a43afc6870490635d791c677a08f0e9b39", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "52270854-9553-4699-a114-1c433b7bb5da": {"doc_hash": "3c0fd5a5ec858160309104854651709378add0410f88ac3e992368bb06f07a6b", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "1dc3c49a-c099-48bc-b29e-7c863951889d": {"doc_hash": "d93361b4ae90646b498560bf5cedc8aad191cce763eab2cfeb2d92955f2aec05", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "fc4d2b0b-f3fe-4d60-be8f-d310998632cb": {"doc_hash": "9bb5c22d52fba151ca2940492787cb7be7ee1aa3a21332e13fff5c55f66606bb", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "9da16b3f-2b5c-4828-b597-86854a258b89": {"doc_hash": "6ff039217c9dce92eaa249f14dcda18c581eaec4251fd2448cf913edd677b00c", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "22cdf90a-2a95-47c0-b3c8-f12f7396ab15": {"doc_hash": "181585f26a9054358b31b9ed914ae5ac4d38dcf17c03cac7bfb45501dc04875a", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "b278d73c-a50e-446c-8179-937d59970111": {"doc_hash": "926ce17b02ce2b0c378d0e5125fe8567dd6dc15bf6e1ee4909ee4564e3ac3bf3", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "de74b1b6-6916-48f3-8b77-0b5c23d44a26": {"doc_hash": "48d7c477ef6400a48f7fa098ba00a157b67de53e6d13a4c6542ac77a1cc17e0e", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "e1433fcb-417c-42b0-9d33-480713fb5bdc": {"doc_hash": "2a7eb2e96d099dd4299e0e652f007c5b2c6d2eeaabc384b745a40a94ecb71cdf", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "ba0b9c0b-76ae-4e1b-b98b-2c5956f89abe": {"doc_hash": "6a5632919de6f62ac0a26863dfa3de070e2d59dcbbf1085ff47627e418e44238", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "d976fc42-f398-47cd-a442-46abf33022a6": {"doc_hash": "4c5a47f432639fa01f9fbbefa1fa3ef97dc725eac0324434455e9206576e6542", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "753d27f6-feb2-4496-9ae7-0b1726d4670c": {"doc_hash": "66d54288316b92341d124daee4964c42e6e22caffbb1ded4a001e6650a4c2d2f", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "a763ca33-232b-4b6c-9631-3c007080beb4": {"doc_hash": "1926b16ad949e8776cf5344cb2e2b81a53c8189295df0a6b0d52004f4b849f7d", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "550dc4b8-d885-43d0-bef9-696742bf0089": {"doc_hash": "9ad7be411c5292c8a7b141ae5a922450c25963a7c752e05ede4ba7ae6c8bd335", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "08a73a94-cbec-4e2f-892b-6e676479116d": {"doc_hash": "f570a93a84f4942ca09e0d840c1753f1ea882c7630ee982fa16295bc2a4c718d", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "2df5ffbc-5c79-4aa6-bd66-f7cc6a2e5967": {"doc_hash": "8e185f018d03938ce111ac8cf7eb1093b2c430c88c999593fc1efbc2b57c7271", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "b6479543-c9e2-4ed0-94ec-050bffee8255": {"doc_hash": "a689446c291c6db65de9b63ebdb19ae58166226431636a14cde150561fd0013b", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "afca20e9-5413-48ad-b52c-684eb8684299": {"doc_hash": "9a923a59e71a7747869bba9ce6ecd03e77156ac2a107a8f9ed39e8dbf2adb796", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "e2a94554-90db-4b7e-9cd4-504d5e21f9e5": {"doc_hash": "39772be9c1c53a265645d3b9286edb1ec634db39c9502a31a158ffda722fbd05", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "e49cfda0-7847-472d-9224-298a38f7a643": {"doc_hash": "accc73ce607f619f0a464a65fff6df0f01d12b77efc4b93e946fe6438e5c1534", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "8354677e-2e80-4a7c-a595-25d11f5012ef": {"doc_hash": "106892f1dd833b64cd4ce5822bf702199c56571f44c8045422e317a582d986f0", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "7a24a74b-898f-459c-9918-0d3c9355dc17": {"doc_hash": "93ab980c696e8c9deea7daa7b8c85a034751e855c3f0f646d8f763a360344b2a", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "6c57e169-5f39-4e06-ac8e-ff56b0d1c1fd": {"doc_hash": "0c9f26cc866f51249946f1cbb17b80c768c5e04b2ef3608b3f351b2d6e1bcd93", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "62655d5a-6d9b-4984-acab-29528a6aec7f": {"doc_hash": "83fb1863d90ede39d7c24043ac89cb17a450ddd9647ee8699af0e4723a141e27", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "38160117-0ac5-400c-a8f4-7ddd30499712": {"doc_hash": "8d715b4db11803e4c4135952c39390dfd54723a00cce2a97f002da6bc852746b", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "5a0e182a-f626-4801-87d7-5ad200c2439b": {"doc_hash": "46600d5eb3b37be49d10cebc2642406648d1d5be318f9581e871f7144b5427ae", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "4e019c0b-8b74-4e5e-b92c-05959c245376": {"doc_hash": "803fd8e707007b9294b13fff4795f147aef2bf7022d260d7c620a8941f40797c", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "d5384145-7a6a-47c4-a092-31ab4d25dcea": {"doc_hash": "12e7df57a630672c130efac1258763042f1a8f28eef73bc768bb442b4c699ea0", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "a4b071c0-100f-442a-a76f-8e521b021b15": {"doc_hash": "d57c2cea6358fe89245ab14de35478b06f29fef7d60559c6d4097ba868b8463d", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "7e2a0177-3005-4abd-b7a8-f56792e2dfa5": {"doc_hash": "d259069692866531ee8d3b89092bb5f6c7f71e66485521d49d62347bb6afcfb4", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "71c42d16-407f-4a27-b78c-6d9300942be4": {"doc_hash": "71464ff2cd456ffe61427e64a841bd76c0ae7b090cb0d83bc72d109732fdfb7f", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "f1a7e625-d391-4b87-9d1a-f1e1efb00005": {"doc_hash": "5280721825cff12bb276326b3715a305b32b24f7bd4dfb738e1f0c6aa983a5d8", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "fd8660e1-811b-4090-bf77-608d17849b66": {"doc_hash": "e0e63889c99e1ebd8bd569b04cea8c11310ec296e6c1fd232aae2df0111414c3", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "46f079f5-0da9-4c9d-9f7a-b37d9b3762f8": {"doc_hash": "5a2866896927d16c1a4d7861bceb0b28754a5bf65a7ea31819c19e8a7294d58d", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "b04af3bf-0277-41a5-823b-d60bd723bc33": {"doc_hash": "8a670da308df4764199f660dd9c2b6829fc921d88d56b9936427d720b2870cd1", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "ec77d0b1-8f3c-4f00-9230-e6b127533479": {"doc_hash": "9ba38e67066737ce18cafda259f7dd41bc47436232562ed87ae2f165faeee455", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "81313276-cf58-464d-9b71-d5eba1a3fba1": {"doc_hash": "69824f22155cf0470fe36367f9f33238932c9a921bca594409de4287cafa213f", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "b9029a78-c1dc-4828-bfee-7900a3378184": {"doc_hash": "f353b99e7732ac39389aa5f76fdcc66c8c581b83d1d2f26f6141ffa801124554", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "88fc90ea-8ffc-466f-90c0-fc10e285d819": {"doc_hash": "90645505b9fa15dc7d9eccaa335b698b9a44aa7acbf77c68a89c8fb41bcbd49c", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "98f79b34-99f6-42d4-a905-f611eb0ec699": {"doc_hash": "f7c3d89d24b344f6b3b1fc844ea60ec52530916517b1c47d52eee197d4871bcc", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "6210a687-7ffb-478e-9cd9-5c68fc6c3f2a": {"doc_hash": "0b2703b363783045bba68f7cb8380328b97f32db5e776e235914d6220c642aa2", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "2807e937-7ce2-4d9a-a8db-66b5af9d3cbd": {"doc_hash": "59b633c35d627c87118ebe2b4342719fba952e0c4f20178ef670062f1e5da296", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "f200de91-2ef5-48bc-8ba8-e374b3f44f01": {"doc_hash": "39b73572bea0ca6bfe5235b1c151813918a13926c2d3857552b309e8b76c03b1", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "93aa9ea2-be05-4aeb-9c07-be8880329809": {"doc_hash": "73cf59a42abce127d2ee6e32229cb320bc7a8e8b98b0b4e59495946c025e314b", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "19344792-e779-4473-89f4-e6ac9074a9df": {"doc_hash": "78a77022917ddd853b8e8fd6e55552ae96806b8cf4edbfb41b2b48da0ee32c2b", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "2a02f0b2-75af-49a8-bf12-a4e8b039e072": {"doc_hash": "cc8cb93401e458d207149cfada843f39596434f4efd649456e6fe65115d92dab", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "ec48e832-eb08-4475-a762-65bd6f32d68b": {"doc_hash": "35e39efb48c163d116f16ea539fd5db19a530ec73efbec566da4812c4ee0d59f", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "5801ae0e-e6f1-4b82-a4e5-7ade293997bb": {"doc_hash": "430caa4dfe2c3345977167395aee2bd65915b9001cef2e52de6f7ac6d7892628", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "46407f5c-13b7-48b6-b3d1-0916776cb806": {"doc_hash": "ea1686c03bbec8e6e51ca7847f54ec097971118c2fc5b0bdd6106e28dbbf3e72", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "45a74346-fb00-4e7a-98ed-4d61bd6ba7f0": {"doc_hash": "4a6f83e727e755f45900ad34f270a90c5b7a6bce1518336a300d45db8590ffba", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "ed15d0cb-37d6-4221-aa62-c438fb0baf9b": {"doc_hash": "da54d3c566dc549c83deb219f3a0be3f068d7412be6fc7cc5a73b738ad4e439a", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "6cc2bfa2-a695-4bd9-aadf-9bfea6e71547": {"doc_hash": "26e3a6f93fd27d5c3120868b76b4a3ff3c1d52c61ffeeaaadd4b71b19de4f354", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "e6f043b5-6131-4d29-8d23-5c00d56e2ae8": {"doc_hash": "e0438b7d72b9fec83654d566c69dc53db28374a6c60303a514ea85b176d0368f", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "4e79feec-40be-4184-837d-355b8ea6d3ef": {"doc_hash": "3ff429ba561f5fdb5dbc8d5def6bfe97e73efb522fa1a28a8c00caead9905de3", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "a2f59b53-2c62-45bc-a8a8-deb071cbefb4": {"doc_hash": "aeb1b54b2adf6419509f895581a6028a9bc5456f3bb5a20cb9ee194ccec25e39", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "e0bab5e1-8d68-4a58-9bd8-3a514e56ffb0": {"doc_hash": "c58a14ddbbf045ca25a514a5888f4378f3976a86ab325f6d07299a0724a93479", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "cd5c76d0-e34e-4835-9d4d-0ffdb141d1e4": {"doc_hash": "2c3d385de98d46883c40d14f86f986f055bb4ce79fda00828b083ee75cadb4a9", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "8e971391-34b4-417f-b177-0719365b2a88": {"doc_hash": "df1cfddfd0dac16a56985e308cc03e3b14bb993042026f5988e5107f998f4a63", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "20b2460b-100f-4f1d-9af9-05304a0cc4e0": {"doc_hash": "344f37728a6aa9990e7967a4fa461b96fbcc50e5e7374a47f43bea340543c889", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "89f70779-3fed-4981-affc-2c3464eaa30c": {"doc_hash": "9be373c6d6a17b803e4b0351b4eee95045bab8e63b4f8bb2ffc476cd1166579a", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "5eeb2ea6-00e1-4972-9b01-03bbcb378058": {"doc_hash": "1efe6a400b873c63ba10ebffe668b4d34144c870d8dca6ae34a4706917b2db1e", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "b37e3fcc-fd8a-446c-b8ec-1a44486c78f6": {"doc_hash": "cc4dad7fb52e3d52c17754b3fbc28b8a2e4c3a171502d05d58800938a5c30753", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "d4cf1b79-17a9-4d8d-889b-d16e071b8af3": {"doc_hash": "a62d890e1b471edec5cb73d0c73982485eae78af819c29d013bd406a70af5a30", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "62056447-7355-4d55-b8ce-1554f19bdc92": {"doc_hash": "3dbabbdf27b679f1a3b6fbb4b1ca3ab2b37457d80448f908973a55eed0271b60", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "a97a763b-9acb-4697-9a32-d91c65fdeb2b": {"doc_hash": "4f7d959c569405533e8815a3a5a23bdc006c59891fddcb3d264c0f5aade0fc00", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "4e79d1a1-6aca-4766-9adc-0ca5b90eb143": {"doc_hash": "0634426708117cf6630be6a115257856f023fad76a3c9791bf8975a41402e5d2", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "bee044a8-00b9-49fc-a02e-553193091c9b": {"doc_hash": "0832e2d4dfe18b9b684a9b8951d055b6cb255e05dbe539e54435632212404f87", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "c7710e3c-927f-468f-a50b-bfa09014a3eb": {"doc_hash": "c53a6bf0d0ece3a901872f72265427cda550ff93ec33ee7d612579fb3e1e6b79", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "a3e2c50f-7e64-455a-83e3-9a4d49b57115": {"doc_hash": "6c700351b4b8bb9455e48c6c2b5a33ad93f8b59258aa14fc5740553d746dfde4", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "bbe4f542-7fd0-42fb-9be4-cb5001c79bba": {"doc_hash": "6a12eefcac94b05293ef5b49c650671088a763be7cd8b6ba8ead0a1088dfd83f", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "a5c32338-016e-4986-9a9e-17a9c04475ea": {"doc_hash": "7f7145f0105ae91c03ac6f6e1b4d74f57fa770026305aa0cc8ca26721fe2357b", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "dd6b3eaf-6a94-4ccc-8a83-6d4c32083257": {"doc_hash": "18a4d7f587e6157e81ce5d0142d37a58eae12c412859ed61c13777466a4b797a", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "5bcdf5e3-0748-48d9-85bb-1b75a76ab3fa": {"doc_hash": "69451738f07e642db82effa7098dc6c64278f837a8681f9c81b5753d8b0ca1da", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "d692038f-f6ca-42c0-bac5-79bad48a0640": {"doc_hash": "bbe6784874019d652b5f759a246918f5277c6c66d12b2a16adc57148b9f4cf36", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "eaafd9d7-add8-438d-8f95-8311dd98b3b6": {"doc_hash": "d8d7fa570bc01012a920476b1a6337d599db000d1015a166f684a8cbdaa491d4", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "459179b4-bab5-454d-8a21-c47c762503ec": {"doc_hash": "c717054e4eae91169be4fe519388d009076f92390d493a9b6cb0e17bd26330fb", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "68197fa3-990a-4c8c-82a2-3c2d3628c1dd": {"doc_hash": "8693a567921841eb973c98293c78ed78224eb88ad047d9851acbd17f493aa88a", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "85ba0f77-435d-4b24-b3de-c2b44e76e957": {"doc_hash": "f6387cbafca83b97cfcfaea80d11a26cf5da7895ab6a01108683ff13460a44f8", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "b9f9093f-ee62-4bb0-8eec-5d493daf9c56": {"doc_hash": "99e4240f04fbbdc7376d7d396fe1d099ef6c973419e92eaabde9f8e94476cf8c", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "3f789b1f-d64c-40ae-ab06-de445014722d": {"doc_hash": "c2b78565431e0e209ad71bc5e2cd661030c8d5cafa18f8e8afea61e52933249e", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "1c2648ba-8874-49a5-a8c9-bb2f3b979370": {"doc_hash": "3eff48f637ca9c57049169c81f1e1f9ad1f53bcdfdf88f40ceff8c8cc0c91fa5", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "7fbf2474-06e2-45ca-b405-919ddf7f52d6": {"doc_hash": "498aac97c427ad70e5890a6c1756c63d877fd84429ab45bb946a21abd093101f", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "7ee13276-17e9-4acd-94eb-1b9fb5dce0d2": {"doc_hash": "14b36f982bf7a83eb0d4ed136cc9ca54ad36cfd81c290cdbe709dfc2a3ba1e59", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "1e9236bb-38f7-4bc8-b0f6-f1aabb261407": {"doc_hash": "5615c9a556038dd485c0721a5c2f4f2495bf0c31e1722cb23eda508d77190a33", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "3545c355-d643-4a8e-a499-c147943664d1": {"doc_hash": "8ba929ebca8ad79ed6323b6f07b17e7b41bfb3e8d0e4312cb455296709a35419", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "835c2031-d9f2-4215-8457-261c016a8877": {"doc_hash": "2ceb9e5684e8d25fb6fa855b668126014a3aed0f27898b8c2647fd8e6e6e11fb", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "51211326-f076-4aa1-bd03-8613f055845b": {"doc_hash": "094e499917ce69c9c3c222a4d76c2d9b07986a3bf0bb291ed05b354203b40311", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "f626da33-7ec5-4054-b01b-6ca8d9d382e4": {"doc_hash": "7b93813797c6b1081109a396d996dd875fa500722db284158dfd3306576dfe39", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "6b5ed22e-7e91-4de6-b81c-7920d97b76ce": {"doc_hash": "fd70d7aef5cf3e016ecbd055bb0674f97d98a083e07654144a70fe78f1760a8c", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "4576772d-15c7-4487-931c-e2febd229cb2": {"doc_hash": "25fb2f6b1f662af302a6c14e0290951ab5f7a168745b505843fd0f01c7ef69e7", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "d5ddcd3f-5221-4f7c-8ff9-a33d5231e6a2": {"doc_hash": "9b0c7c50c3bf877e4660f99ad00bbe6ce3bfb57cbd5757e2073f5e49e19b00d8", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}, "854f739d-2def-421e-a9eb-10b30f5fcf28": {"doc_hash": "115fc771cbabfc4616eafa7c88959f4a807ded29f2b4e566210d68c25b7135b5", "ref_doc_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774"}}, "docstore/data": {"ce0b9e05-f4ba-4c97-a8e3-1ecf42aac77f": {"__data__": {"id_": "ce0b9e05-f4ba-4c97-a8e3-1ecf42aac77f", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75d4fb0b-08fb-40eb-b50d-b1260a6592f1", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "a93d268fc4eb2296ec5d6cdd30c56ffdbbe24f6f9949caca5c4857be24d738cd", "class_name": "RelatedNodeInfo"}}, "hash": "13e757fcb30c376d6cb07471c55b33a4b47f48caa62d044ca3f1f97f34078fb9", "text": "# Contents\n\n  1. [Cover](html/Cover.xhtml)\n  2. [Front Matter](html/533412_1_En_BookFrontmatter_OnlinePDF.xhtml)\n  3. [Part I. Concepts and Technology](html/Part_1.xhtml)\n    1. [1\\. Natural Language Processing](html/533412_1_En_1_Chapter.xhtml)\n    2. [2\\. N-Gram Language Model](html/533412_1_En_2_Chapter.xhtml)\n    3. [3\\. Part-of-Speech (POS) Tagging](html/533412_1_En_3_Chapter.xhtml)\n    4. [4\\. Syntax and Parsing](html/533412_1_En_4_Chapter.xhtml)\n    5. [5\\. Meaning Representation](html/533412_1_En_5_Chapter.xhtml)\n    6. [6\\. Semantic Analysis](html/533412_1_En_6_Chapter.xhtml)\n    7. [7\\. Pragmatic Analysis and Discourse](html/533412_1_En_7_Chapter.xhtml)\n    8. [8\\. Transfer Learning and Transformer Technology](html/533412_1_En_8_Chapter.xhtml)\n    9. [9\\. Major NLP Applications](html/533412_1_En_9_Chapter.xhtml)\n  4. [Part II. Natural Language Processing Workshops with Python Implementation in 14 Hours](html/Part_2.xhtml)\n    1. [10\\. Workshop#1 Basics of Natural Language Toolkit (Hour 1\u20132)](html/533412_1_En_10_Chapter.xhtml)\n    2. [11\\. Workshop#2 N-grams in NLTK and Tokenization in SpaCy (Hour 3\u20134)](html/533412_1_En_11_Chapter.xhtml)\n    3. [12\\. Workshop#3 POS Tagging Using NLTK (Hour 5\u20136)](html/533412_1_En_12_Chapter.xhtml)\n    4. [13\\. Workshop#4 Semantic Analysis and Word Vectors Using spaCy (Hour 7\u20138)](html/533412_1_En_13_Chapter.xhtml)\n    5. [14\\. Workshop#5 Sentiment Analysis and Text Classification with LSTM Using spaCy (Hour 9\u201310)](html/533412_1_En_14_Chapter.xhtml)\n    6. [15\\. Workshop#6 Transformers with spaCy and TensorFlow (Hour 11\u201312)](html/533412_1_En_15_Chapter.xhtml)\n    7. [16\\. Workshop#7 Building Chatbot with TensorFlow and Transformer Technology (Hour 13\u201314)](html/533412_1_En_16_Chapter.xhtml)\n  5. [Back Matter](html/533412_1_En_BookBackmatter_OnlinePDF.xhtml)\n\n# Landmarks\n\n  1. [Cover](html/Cover.xhtml)\n  2. [Table of Contents](html/533412_1_En_BookFrontmatter_OnlinePDF.xhtml#Toc)\n  3. [Body Matter](html/533412_1_En_1_Chapter.xhtml)\n\n# Pages\n\n  1. [vii](html/533412_1_En_BookFrontmatter_OnlinePDF.xhtml#PBvii)\n  2. [viii](html/533412_1_En_BookFrontmatter_OnlinePDF.xhtml#PBviii)\n  3. [ix](html/533412_1_En_BookFrontmatter_OnlinePDF.xhtml#PBix)\n  4. [x](html/533412_1_En_BookFrontmatter_OnlinePDF.xhtml#PBx)\n  5. [xi](html/533412_1_En_BookFrontmatter_OnlinePDF.xhtml#PBxi)\n  6. [xii](html/533412_1_En_BookFrontmatter_OnlinePDF.xhtml#PBxii)\n  7. [xiii](html/533412_1_En_BookFrontmatter_OnlinePDF.xhtml#PBxiii)\n  8. [xv](html/533412_1_En_BookFrontmatter_OnlinePDF.xhtml#PBxv)\n  9.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75d4fb0b-08fb-40eb-b50d-b1260a6592f1": {"__data__": {"id_": "75d4fb0b-08fb-40eb-b50d-b1260a6592f1", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce0b9e05-f4ba-4c97-a8e3-1ecf42aac77f", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "13e757fcb30c376d6cb07471c55b33a4b47f48caa62d044ca3f1f97f34078fb9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ed7accd-39b3-47c9-9f7a-dc8ceb9e62bc", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "20f1815c8b2189c42846072ede13f9ce614569a3f013d395057d4fc36c872ea3", "class_name": "RelatedNodeInfo"}}, "hash": "a93d268fc4eb2296ec5d6cdd30c56ffdbbe24f6f9949caca5c4857be24d738cd", "text": "[viii](html/533412_1_En_BookFrontmatter_OnlinePDF.xhtml#PBviii)\n  3. [ix](html/533412_1_En_BookFrontmatter_OnlinePDF.xhtml#PBix)\n  4. [x](html/533412_1_En_BookFrontmatter_OnlinePDF.xhtml#PBx)\n  5. [xi](html/533412_1_En_BookFrontmatter_OnlinePDF.xhtml#PBxi)\n  6. [xii](html/533412_1_En_BookFrontmatter_OnlinePDF.xhtml#PBxii)\n  7. [xiii](html/533412_1_En_BookFrontmatter_OnlinePDF.xhtml#PBxiii)\n  8. [xv](html/533412_1_En_BookFrontmatter_OnlinePDF.xhtml#PBxv)\n  9. [xxix](html/533412_1_En_BookFrontmatter_OnlinePDF.xhtml#PBxxix)\n  10. [3](html/533412_1_En_1_Chapter.xhtml#PB3)\n  11. [4](html/533412_1_En_1_Chapter.xhtml#PB4)\n  12. [5](html/533412_1_En_1_Chapter.xhtml#PB5)\n  13. [6](html/533412_1_En_1_Chapter.xhtml#PB6)\n  14. [8](html/533412_1_En_1_Chapter.xhtml#PB8)\n  15. [9](html/533412_1_En_1_Chapter.xhtml#PB9)\n  16. [10](html/533412_1_En_1_Chapter.xhtml#PB10)\n  17. [11](html/533412_1_En_1_Chapter.xhtml#PB11)\n  18. [12](html/533412_1_En_1_Chapter.xhtml#PB12)\n  19. [13](html/533412_1_En_1_Chapter.xhtml#PB13)\n  20. [14](html/533412_1_En_1_Chapter.xhtml#PB14)\n  21. [15](html/533412_1_En_1_Chapter.xhtml#PB15)\n  22. [19](html/533412_1_En_2_Chapter.xhtml#PB19)\n  23. [20](html/533412_1_En_2_Chapter.xhtml#PB20)\n  24. [21](html/533412_1_En_2_Chapter.xhtml#PB21)\n  25. [22](html/533412_1_En_2_Chapter.xhtml#PB22)\n  26. [23](html/533412_1_En_2_Chapter.xhtml#PB23)\n  27. [24](html/533412_1_En_2_Chapter.xhtml#PB24)\n  28. [25](html/533412_1_En_2_Chapter.xhtml#PB25)\n  29. [26](html/533412_1_En_2_Chapter.xhtml#PB26)\n  30. [27](html/533412_1_En_2_Chapter.xhtml#PB27)\n  31. [28](html/533412_1_En_2_Chapter.xhtml#PB28)\n  32. [29](html/533412_1_En_2_Chapter.xhtml#PB29)\n  33. [30](html/533412_1_En_2_Chapter.xhtml#PB30)\n  34. [31](html/533412_1_En_2_Chapter.xhtml#PB31)\n  35. [32](html/533412_1_En_2_Chapter.xhtml#PB32)\n  36. [33](html/533412_1_En_2_Chapter.xhtml#PB33)\n  37. [34](html/533412_1_En_2_Chapter.xhtml#PB34)\n  38. [35](html/533412_1_En_2_Chapter.xhtml#PB35)\n  39. [36](html/533412_1_En_2_Chapter.xhtml#PB36)\n  40. [37](html/533412_1_En_2_Chapter.xhtml#PB37)\n  41.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ed7accd-39b3-47c9-9f7a-dc8ceb9e62bc": {"__data__": {"id_": "6ed7accd-39b3-47c9-9f7a-dc8ceb9e62bc", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75d4fb0b-08fb-40eb-b50d-b1260a6592f1", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "a93d268fc4eb2296ec5d6cdd30c56ffdbbe24f6f9949caca5c4857be24d738cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0724a624-7669-4eb7-bf18-7591e9632c85", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8de2d930e51f2d45073b9a57a9d09afaef9811d7d9b38b7632b4f4a11a274b2b", "class_name": "RelatedNodeInfo"}}, "hash": "20f1815c8b2189c42846072ede13f9ce614569a3f013d395057d4fc36c872ea3", "text": "[30](html/533412_1_En_2_Chapter.xhtml#PB30)\n  34. [31](html/533412_1_En_2_Chapter.xhtml#PB31)\n  35. [32](html/533412_1_En_2_Chapter.xhtml#PB32)\n  36. [33](html/533412_1_En_2_Chapter.xhtml#PB33)\n  37. [34](html/533412_1_En_2_Chapter.xhtml#PB34)\n  38. [35](html/533412_1_En_2_Chapter.xhtml#PB35)\n  39. [36](html/533412_1_En_2_Chapter.xhtml#PB36)\n  40. [37](html/533412_1_En_2_Chapter.xhtml#PB37)\n  41. [38](html/533412_1_En_2_Chapter.xhtml#PB38)\n  42. [39](html/533412_1_En_2_Chapter.xhtml#PB39)\n  43. [40](html/533412_1_En_2_Chapter.xhtml#PB40)\n  44. [41](html/533412_1_En_2_Chapter.xhtml#PB41)\n  45. [43](html/533412_1_En_3_Chapter.xhtml#PB43)\n  46. [44](html/533412_1_En_3_Chapter.xhtml#PB44)\n  47. [45](html/533412_1_En_3_Chapter.xhtml#PB45)\n  48. [46](html/533412_1_En_3_Chapter.xhtml#PB46)\n  49. [47](html/533412_1_En_3_Chapter.xhtml#PB47)\n  50. [48](html/533412_1_En_3_Chapter.xhtml#PB48)\n  51. [49](html/533412_1_En_3_Chapter.xhtml#PB49)\n  52. [50](html/533412_1_En_3_Chapter.xhtml#PB50)\n  53. [51](html/533412_1_En_3_Chapter.xhtml#PB51)\n  54. [52](html/533412_1_En_3_Chapter.xhtml#PB52)\n  55. [53](html/533412_1_En_3_Chapter.xhtml#PB53)\n  56. [54](html/533412_1_En_3_Chapter.xhtml#PB54)\n  57. [55](html/533412_1_En_3_Chapter.xhtml#PB55)\n  58. [56](html/533412_1_En_3_Chapter.xhtml#PB56)\n  59. [57](html/533412_1_En_3_Chapter.xhtml#PB57)\n  60. [58](html/533412_1_En_3_Chapter.xhtml#PB58)\n  61. [59](html/533412_1_En_3_Chapter.xhtml#PB59)\n  62. [60](html/533412_1_En_3_Chapter.xhtml#PB60)\n  63. [61](html/533412_1_En_3_Chapter.xhtml#PB61)\n  64. [62](html/533412_1_En_3_Chapter.xhtml#PB62)\n  65. [63](html/533412_1_En_3_Chapter.xhtml#PB63)\n  66. [64](html/533412_1_En_3_Chapter.xhtml#PB64)\n  67. [65](html/533412_1_En_3_Chapter.xhtml#PB65)\n  68. [67](html/533412_1_En_4_Chapter.xhtml#PB67)\n  69. [68](html/533412_1_En_4_Chapter.xhtml#PB68)\n  70. [69](html/533412_1_En_4_Chapter.xhtml#PB69)\n  71. [70](html/533412_1_En_4_Chapter.xhtml#PB70)\n  72. [71](html/533412_1_En_4_Chapter.xhtml#PB71)\n  73.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0724a624-7669-4eb7-bf18-7591e9632c85": {"__data__": {"id_": "0724a624-7669-4eb7-bf18-7591e9632c85", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ed7accd-39b3-47c9-9f7a-dc8ceb9e62bc", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "20f1815c8b2189c42846072ede13f9ce614569a3f013d395057d4fc36c872ea3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7f6502a-55e0-40ad-937f-c05a86f9eacd", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "38fbd9661dbc7cc76d105c65c785c744cf4ca84eefebea2159346d008f85df96", "class_name": "RelatedNodeInfo"}}, "hash": "8de2d930e51f2d45073b9a57a9d09afaef9811d7d9b38b7632b4f4a11a274b2b", "text": "[63](html/533412_1_En_3_Chapter.xhtml#PB63)\n  66. [64](html/533412_1_En_3_Chapter.xhtml#PB64)\n  67. [65](html/533412_1_En_3_Chapter.xhtml#PB65)\n  68. [67](html/533412_1_En_4_Chapter.xhtml#PB67)\n  69. [68](html/533412_1_En_4_Chapter.xhtml#PB68)\n  70. [69](html/533412_1_En_4_Chapter.xhtml#PB69)\n  71. [70](html/533412_1_En_4_Chapter.xhtml#PB70)\n  72. [71](html/533412_1_En_4_Chapter.xhtml#PB71)\n  73. [72](html/533412_1_En_4_Chapter.xhtml#PB72)\n  74. [73](html/533412_1_En_4_Chapter.xhtml#PB73)\n  75. [74](html/533412_1_En_4_Chapter.xhtml#PB74)\n  76. [75](html/533412_1_En_4_Chapter.xhtml#PB75)\n  77. [76](html/533412_1_En_4_Chapter.xhtml#PB76)\n  78. [77](html/533412_1_En_4_Chapter.xhtml#PB77)\n  79. [78](html/533412_1_En_4_Chapter.xhtml#PB78)\n  80. [79](html/533412_1_En_4_Chapter.xhtml#PB79)\n  81. [80](html/533412_1_En_4_Chapter.xhtml#PB80)\n  82. [81](html/533412_1_En_4_Chapter.xhtml#PB81)\n  83. [82](html/533412_1_En_4_Chapter.xhtml#PB82)\n  84. [83](html/533412_1_En_4_Chapter.xhtml#PB83)\n  85. [84](html/533412_1_En_4_Chapter.xhtml#PB84)\n  86. [85](html/533412_1_En_4_Chapter.xhtml#PB85)\n  87. [86](html/533412_1_En_4_Chapter.xhtml#PB86)\n  88. [87](html/533412_1_En_4_Chapter.xhtml#PB87)\n  89. [88](html/533412_1_En_4_Chapter.xhtml#PB88)\n  90. [89](html/533412_1_En_4_Chapter.xhtml#PB89)\n  91. [90](html/533412_1_En_4_Chapter.xhtml#PB90)\n  92. [91](html/533412_1_En_4_Chapter.xhtml#PB91)\n  93. [92](html/533412_1_En_4_Chapter.xhtml#PB92)\n  94. [93](html/533412_1_En_4_Chapter.xhtml#PB93)\n  95. [94](html/533412_1_En_4_Chapter.xhtml#PB94)\n  96. [95](html/533412_1_En_5_Chapter.xhtml#PB95)\n  97. [96](html/533412_1_En_5_Chapter.xhtml#PB96)\n  98. [97](html/533412_1_En_5_Chapter.xhtml#PB97)\n  99. [98](html/533412_1_En_5_Chapter.xhtml#PB98)\n  100. [99](html/533412_1_En_5_Chapter.xhtml#PB99)\n  101. [100](html/533412_1_En_5_Chapter.xhtml#PB100)\n  102. [101](html/533412_1_En_5_Chapter.xhtml#PB101)\n  103. [102](html/533412_1_En_5_Chapter.xhtml#PB102)\n  104. [103](html/533412_1_En_5_Chapter.xhtml#PB103)\n  105.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7f6502a-55e0-40ad-937f-c05a86f9eacd": {"__data__": {"id_": "d7f6502a-55e0-40ad-937f-c05a86f9eacd", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0724a624-7669-4eb7-bf18-7591e9632c85", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8de2d930e51f2d45073b9a57a9d09afaef9811d7d9b38b7632b4f4a11a274b2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b466d2f2-0f85-47f9-89d7-27a9d64af5f5", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3d05cb152b83790be6a0b18c745b993461a6305d6f38ead07c86063fcf86c703", "class_name": "RelatedNodeInfo"}}, "hash": "38fbd9661dbc7cc76d105c65c785c744cf4ca84eefebea2159346d008f85df96", "text": "[96](html/533412_1_En_5_Chapter.xhtml#PB96)\n  98. [97](html/533412_1_En_5_Chapter.xhtml#PB97)\n  99. [98](html/533412_1_En_5_Chapter.xhtml#PB98)\n  100. [99](html/533412_1_En_5_Chapter.xhtml#PB99)\n  101. [100](html/533412_1_En_5_Chapter.xhtml#PB100)\n  102. [101](html/533412_1_En_5_Chapter.xhtml#PB101)\n  103. [102](html/533412_1_En_5_Chapter.xhtml#PB102)\n  104. [103](html/533412_1_En_5_Chapter.xhtml#PB103)\n  105. [104](html/533412_1_En_5_Chapter.xhtml#PB104)\n  106. [105](html/533412_1_En_5_Chapter.xhtml#PB105)\n  107. [106](html/533412_1_En_5_Chapter.xhtml#PB106)\n  108. [107](html/533412_1_En_5_Chapter.xhtml#PB107)\n  109. [108](html/533412_1_En_5_Chapter.xhtml#PB108)\n  110. [109](html/533412_1_En_5_Chapter.xhtml#PB109)\n  111. [110](html/533412_1_En_5_Chapter.xhtml#PB110)\n  112. [111](html/533412_1_En_5_Chapter.xhtml#PB111)\n  113. [112](html/533412_1_En_5_Chapter.xhtml#PB112)\n  114. [113](html/533412_1_En_5_Chapter.xhtml#PB113)\n  115. [115](html/533412_1_En_6_Chapter.xhtml#PB115)\n  116. [116](html/533412_1_En_6_Chapter.xhtml#PB116)\n  117. [117](html/533412_1_En_6_Chapter.xhtml#PB117)\n  118. [118](html/533412_1_En_6_Chapter.xhtml#PB118)\n  119. [119](html/533412_1_En_6_Chapter.xhtml#PB119)\n  120. [120](html/533412_1_En_6_Chapter.xhtml#PB120)\n  121. [121](html/533412_1_En_6_Chapter.xhtml#PB121)\n  122. [122](html/533412_1_En_6_Chapter.xhtml#PB122)\n  123. [123](html/533412_1_En_6_Chapter.xhtml#PB123)\n  124. [124](html/533412_1_En_6_Chapter.xhtml#PB124)\n  125. [125](html/533412_1_En_6_Chapter.xhtml#PB125)\n  126. [126](html/533412_1_En_6_Chapter.xhtml#PB126)\n  127. [127](html/533412_1_En_6_Chapter.xhtml#PB127)\n  128. [129](html/533412_1_En_6_Chapter.xhtml#PB129)\n  129. [128](html/533412_1_En_6_Chapter.xhtml#PB128)\n  130. [130](html/533412_1_En_6_Chapter.xhtml#PB130)\n  131. [131](html/533412_1_En_6_Chapter.xhtml#PB131)\n  132. [132](html/533412_1_En_6_Chapter.xhtml#PB132)\n  133. [133](html/533412_1_En_6_Chapter.xhtml#PB133)\n  134. [134](html/533412_1_En_6_Chapter.xhtml#PB134)\n  135. [135](html/533412_1_En_6_Chapter.xhtml#PB135)\n  136. [136](html/533412_1_En_6_Chapter.xhtml#PB136)\n  137.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b466d2f2-0f85-47f9-89d7-27a9d64af5f5": {"__data__": {"id_": "b466d2f2-0f85-47f9-89d7-27a9d64af5f5", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d7f6502a-55e0-40ad-937f-c05a86f9eacd", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "38fbd9661dbc7cc76d105c65c785c744cf4ca84eefebea2159346d008f85df96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d41a7326-c21f-46f0-8d47-9dd42dfaef09", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "59e01320292b73f9f9339700b7121b40da7d86df7e5d37bb8b40e665644c9163", "class_name": "RelatedNodeInfo"}}, "hash": "3d05cb152b83790be6a0b18c745b993461a6305d6f38ead07c86063fcf86c703", "text": "[128](html/533412_1_En_6_Chapter.xhtml#PB128)\n  130. [130](html/533412_1_En_6_Chapter.xhtml#PB130)\n  131. [131](html/533412_1_En_6_Chapter.xhtml#PB131)\n  132. [132](html/533412_1_En_6_Chapter.xhtml#PB132)\n  133. [133](html/533412_1_En_6_Chapter.xhtml#PB133)\n  134. [134](html/533412_1_En_6_Chapter.xhtml#PB134)\n  135. [135](html/533412_1_En_6_Chapter.xhtml#PB135)\n  136. [136](html/533412_1_En_6_Chapter.xhtml#PB136)\n  137. [137](html/533412_1_En_6_Chapter.xhtml#PB137)\n  138. [138](html/533412_1_En_6_Chapter.xhtml#PB138)\n  139. [139](html/533412_1_En_6_Chapter.xhtml#PB139)\n  140. [140](html/533412_1_En_6_Chapter.xhtml#PB140)\n  141. [141](html/533412_1_En_6_Chapter.xhtml#PB141)\n  142. [142](html/533412_1_En_6_Chapter.xhtml#PB142)\n  143. [143](html/533412_1_En_6_Chapter.xhtml#PB143)\n  144. [144](html/533412_1_En_6_Chapter.xhtml#PB144)\n  145. [145](html/533412_1_En_6_Chapter.xhtml#PB145)\n  146. [146](html/533412_1_En_6_Chapter.xhtml#PB146)\n  147. [149](html/533412_1_En_7_Chapter.xhtml#PB149)\n  148. [150](html/533412_1_En_7_Chapter.xhtml#PB150)\n  149. [151](html/533412_1_En_7_Chapter.xhtml#PB151)\n  150. [152](html/533412_1_En_7_Chapter.xhtml#PB152)\n  151. [153](html/533412_1_En_7_Chapter.xhtml#PB153)\n  152. [154](html/533412_1_En_7_Chapter.xhtml#PB154)\n  153. [155](html/533412_1_En_7_Chapter.xhtml#PB155)\n  154. [157](html/533412_1_En_7_Chapter.xhtml#PB157)\n  155. [156](html/533412_1_En_7_Chapter.xhtml#PB156)\n  156. [158](html/533412_1_En_7_Chapter.xhtml#PB158)\n  157. [159](html/533412_1_En_7_Chapter.xhtml#PB159)\n  158. [160](html/533412_1_En_7_Chapter.xhtml#PB160)\n  159. [161](html/533412_1_En_7_Chapter.xhtml#PB161)\n  160. [162](html/533412_1_En_7_Chapter.xhtml#PB162)\n  161. [163](html/533412_1_En_7_Chapter.xhtml#PB163)\n  162. [164](html/533412_1_En_7_Chapter.xhtml#PB164)\n  163. [165](html/533412_1_En_7_Chapter.xhtml#PB165)\n  164. [166](html/533412_1_En_7_Chapter.xhtml#PB166)\n  165. [167](html/533412_1_En_7_Chapter.xhtml#PB167)\n  166. [168](html/533412_1_En_7_Chapter.xhtml#PB168)\n  167. [169](html/533412_1_En_7_Chapter.xhtml#PB169)\n  168. [170](html/533412_1_En_7_Chapter.xhtml#PB170)\n  169.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d41a7326-c21f-46f0-8d47-9dd42dfaef09": {"__data__": {"id_": "d41a7326-c21f-46f0-8d47-9dd42dfaef09", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b466d2f2-0f85-47f9-89d7-27a9d64af5f5", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3d05cb152b83790be6a0b18c745b993461a6305d6f38ead07c86063fcf86c703", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99b6da0f-83f7-4222-a681-9d783aeff388", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9a65c08b56f0eeb6ed4a242433c8b1f017fc7a83b645443e48d2ae5403e1e383", "class_name": "RelatedNodeInfo"}}, "hash": "59e01320292b73f9f9339700b7121b40da7d86df7e5d37bb8b40e665644c9163", "text": "[163](html/533412_1_En_7_Chapter.xhtml#PB163)\n  162. [164](html/533412_1_En_7_Chapter.xhtml#PB164)\n  163. [165](html/533412_1_En_7_Chapter.xhtml#PB165)\n  164. [166](html/533412_1_En_7_Chapter.xhtml#PB166)\n  165. [167](html/533412_1_En_7_Chapter.xhtml#PB167)\n  166. [168](html/533412_1_En_7_Chapter.xhtml#PB168)\n  167. [169](html/533412_1_En_7_Chapter.xhtml#PB169)\n  168. [170](html/533412_1_En_7_Chapter.xhtml#PB170)\n  169. [171](html/533412_1_En_7_Chapter.xhtml#PB171)\n  170. [172](html/533412_1_En_7_Chapter.xhtml#PB172)\n  171. [175](html/533412_1_En_8_Chapter.xhtml#PB175)\n  172. [176](html/533412_1_En_8_Chapter.xhtml#PB176)\n  173. [177](html/533412_1_En_8_Chapter.xhtml#PB177)\n  174. [178](html/533412_1_En_8_Chapter.xhtml#PB178)\n  175. [179](html/533412_1_En_8_Chapter.xhtml#PB179)\n  176. [180](html/533412_1_En_8_Chapter.xhtml#PB180)\n  177. [181](html/533412_1_En_8_Chapter.xhtml#PB181)\n  178. [183](html/533412_1_En_8_Chapter.xhtml#PB183)\n  179. [182](html/533412_1_En_8_Chapter.xhtml#PB182)\n  180. [184](html/533412_1_En_8_Chapter.xhtml#PB184)\n  181. [185](html/533412_1_En_8_Chapter.xhtml#PB185)\n  182. [186](html/533412_1_En_8_Chapter.xhtml#PB186)\n  183. [187](html/533412_1_En_8_Chapter.xhtml#PB187)\n  184. [188](html/533412_1_En_8_Chapter.xhtml#PB188)\n  185. [190](html/533412_1_En_8_Chapter.xhtml#PB190)\n  186. [191](html/533412_1_En_8_Chapter.xhtml#PB191)\n  187. [192](html/533412_1_En_8_Chapter.xhtml#PB192)\n  188. [193](html/533412_1_En_8_Chapter.xhtml#PB193)\n  189. [194](html/533412_1_En_8_Chapter.xhtml#PB194)\n  190. [195](html/533412_1_En_8_Chapter.xhtml#PB195)\n  191. [196](html/533412_1_En_8_Chapter.xhtml#PB196)\n  192. [199](html/533412_1_En_9_Chapter.xhtml#PB199)\n  193. [200](html/533412_1_En_9_Chapter.xhtml#PB200)\n  194. [201](html/533412_1_En_9_Chapter.xhtml#PB201)\n  195. [202](html/533412_1_En_9_Chapter.xhtml#PB202)\n  196. [203](html/533412_1_En_9_Chapter.xhtml#PB203)\n  197. [204](html/533412_1_En_9_Chapter.xhtml#PB204)\n  198. [205](html/533412_1_En_9_Chapter.xhtml#PB205)\n  199. [206](html/533412_1_En_9_Chapter.xhtml#PB206)\n  200. [207](html/533412_1_En_9_Chapter.xhtml#PB207)\n  201.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99b6da0f-83f7-4222-a681-9d783aeff388": {"__data__": {"id_": "99b6da0f-83f7-4222-a681-9d783aeff388", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d41a7326-c21f-46f0-8d47-9dd42dfaef09", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "59e01320292b73f9f9339700b7121b40da7d86df7e5d37bb8b40e665644c9163", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab4675ba-d576-4f01-a59d-e12f9a7439ec", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9c14e31351c590f000272c4d614dfffdfc69a3704e0527631b59117d09c03eff", "class_name": "RelatedNodeInfo"}}, "hash": "9a65c08b56f0eeb6ed4a242433c8b1f017fc7a83b645443e48d2ae5403e1e383", "text": "[200](html/533412_1_En_9_Chapter.xhtml#PB200)\n  194. [201](html/533412_1_En_9_Chapter.xhtml#PB201)\n  195. [202](html/533412_1_En_9_Chapter.xhtml#PB202)\n  196. [203](html/533412_1_En_9_Chapter.xhtml#PB203)\n  197. [204](html/533412_1_En_9_Chapter.xhtml#PB204)\n  198. [205](html/533412_1_En_9_Chapter.xhtml#PB205)\n  199. [206](html/533412_1_En_9_Chapter.xhtml#PB206)\n  200. [207](html/533412_1_En_9_Chapter.xhtml#PB207)\n  201. [208](html/533412_1_En_9_Chapter.xhtml#PB208)\n  202. [209](html/533412_1_En_9_Chapter.xhtml#PB209)\n  203. [210](html/533412_1_En_9_Chapter.xhtml#PB210)\n  204. [211](html/533412_1_En_9_Chapter.xhtml#PB211)\n  205. [212](html/533412_1_En_9_Chapter.xhtml#PB212)\n  206. [213](html/533412_1_En_9_Chapter.xhtml#PB213)\n  207. [214](html/533412_1_En_9_Chapter.xhtml#PB214)\n  208. [215](html/533412_1_En_9_Chapter.xhtml#PB215)\n  209. [216](html/533412_1_En_9_Chapter.xhtml#PB216)\n  210. [217](html/533412_1_En_9_Chapter.xhtml#PB217)\n  211. [218](html/533412_1_En_9_Chapter.xhtml#PB218)\n  212. [219](html/533412_1_En_9_Chapter.xhtml#PB219)\n  213. [220](html/533412_1_En_9_Chapter.xhtml#PB220)\n  214. [221](html/533412_1_En_9_Chapter.xhtml#PB221)\n  215. [222](html/533412_1_En_9_Chapter.xhtml#PB222)\n  216. [223](html/533412_1_En_9_Chapter.xhtml#PB223)\n  217. [224](html/533412_1_En_9_Chapter.xhtml#PB224)\n  218. [225](html/533412_1_En_9_Chapter.xhtml#PB225)\n  219. [226](html/533412_1_En_9_Chapter.xhtml#PB226)\n  220. [227](html/533412_1_En_9_Chapter.xhtml#PB227)\n  221. [229](html/533412_1_En_9_Chapter.xhtml#PB229)\n  222. [230](html/533412_1_En_9_Chapter.xhtml#PB230)\n  223. [231](html/533412_1_En_9_Chapter.xhtml#PB231)\n  224. [232](html/533412_1_En_9_Chapter.xhtml#PB232)\n  225. [233](html/533412_1_En_9_Chapter.xhtml#PB233)\n  226. [234](html/533412_1_En_9_Chapter.xhtml#PB234)\n  227. [235](html/533412_1_En_9_Chapter.xhtml#PB235)\n  228. [243](html/533412_1_En_10_Chapter.xhtml#PB243)\n  229. [244](html/533412_1_En_10_Chapter.xhtml#PB244)\n  230. [245](html/533412_1_En_10_Chapter.xhtml#PB245)\n  231. [246](html/533412_1_En_10_Chapter.xhtml#PB246)\n  232. [247](html/533412_1_En_10_Chapter.xhtml#PB247)\n  233.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab4675ba-d576-4f01-a59d-e12f9a7439ec": {"__data__": {"id_": "ab4675ba-d576-4f01-a59d-e12f9a7439ec", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99b6da0f-83f7-4222-a681-9d783aeff388", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9a65c08b56f0eeb6ed4a242433c8b1f017fc7a83b645443e48d2ae5403e1e383", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a133bbfb-864e-42cf-93c4-692cc1a8b49a", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "bec39f22d50643886a39edcad632c1b5e20b3dd83718c70a70a5e5534ca9ce84", "class_name": "RelatedNodeInfo"}}, "hash": "9c14e31351c590f000272c4d614dfffdfc69a3704e0527631b59117d09c03eff", "text": "[233](html/533412_1_En_9_Chapter.xhtml#PB233)\n  226. [234](html/533412_1_En_9_Chapter.xhtml#PB234)\n  227. [235](html/533412_1_En_9_Chapter.xhtml#PB235)\n  228. [243](html/533412_1_En_10_Chapter.xhtml#PB243)\n  229. [244](html/533412_1_En_10_Chapter.xhtml#PB244)\n  230. [245](html/533412_1_En_10_Chapter.xhtml#PB245)\n  231. [246](html/533412_1_En_10_Chapter.xhtml#PB246)\n  232. [247](html/533412_1_En_10_Chapter.xhtml#PB247)\n  233. [267](html/533412_1_En_11_Chapter.xhtml#PB267)\n  234. [268](html/533412_1_En_11_Chapter.xhtml#PB268)\n  235. [285](html/533412_1_En_12_Chapter.xhtml#PB285)\n  236. [313](html/533412_1_En_13_Chapter.xhtml#PB313)\n  237. [314](html/533412_1_En_13_Chapter.xhtml#PB314)\n  238. [315](html/533412_1_En_13_Chapter.xhtml#PB315)\n  239. [316](html/533412_1_En_13_Chapter.xhtml#PB316)\n  240. [318](html/533412_1_En_13_Chapter.xhtml#PB318)\n  241. [319](html/533412_1_En_13_Chapter.xhtml#PB319)\n  242. [320](html/533412_1_En_13_Chapter.xhtml#PB320)\n  243. [335](html/533412_1_En_14_Chapter.xhtml#PB335)\n  244. [336](html/533412_1_En_14_Chapter.xhtml#PB336)\n  245. [337](html/533412_1_En_14_Chapter.xhtml#PB337)\n  246. [338](html/533412_1_En_14_Chapter.xhtml#PB338)\n  247. [373](html/533412_1_En_15_Chapter.xhtml#PB373)\n  248. [374](html/533412_1_En_15_Chapter.xhtml#PB374)\n  249. [375](html/533412_1_En_15_Chapter.xhtml#PB375)\n  250. [376](html/533412_1_En_15_Chapter.xhtml#PB376)\n  251. [377](html/533412_1_En_15_Chapter.xhtml#PB377)\n  252. [378](html/533412_1_En_15_Chapter.xhtml#PB378)\n  253. [379](html/533412_1_En_15_Chapter.xhtml#PB379)\n  254. [380](html/533412_1_En_15_Chapter.xhtml#PB380)\n  255. [381](html/533412_1_En_15_Chapter.xhtml#PB381)\n  256. [382](html/533412_1_En_15_Chapter.xhtml#PB382)\n  257. [383](html/533412_1_En_15_Chapter.xhtml#PB383)\n  258. [384](html/533412_1_En_15_Chapter.xhtml#PB384)\n  259. [385](html/533412_1_En_15_Chapter.xhtml#PB385)\n  260. [386](html/533412_1_En_15_Chapter.xhtml#PB386)\n  261. [387](html/533412_1_En_15_Chapter.xhtml#PB387)\n  262. [388](html/533412_1_En_15_Chapter.xhtml#PB388)\n  263. [389](html/533412_1_En_15_Chapter.xhtml#PB389)\n  264. [390](html/533412_1_En_15_Chapter.xhtml#PB390)\n  265.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a133bbfb-864e-42cf-93c4-692cc1a8b49a": {"__data__": {"id_": "a133bbfb-864e-42cf-93c4-692cc1a8b49a", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab4675ba-d576-4f01-a59d-e12f9a7439ec", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9c14e31351c590f000272c4d614dfffdfc69a3704e0527631b59117d09c03eff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f82c496f-4376-4a62-b2de-0b496e690569", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9ebc6588d20cc0d1dc6eb2ecd4d95e1ba162bb2784afc22f103a9e31d4576f0f", "class_name": "RelatedNodeInfo"}}, "hash": "bec39f22d50643886a39edcad632c1b5e20b3dd83718c70a70a5e5534ca9ce84", "text": "[383](html/533412_1_En_15_Chapter.xhtml#PB383)\n  258. [384](html/533412_1_En_15_Chapter.xhtml#PB384)\n  259. [385](html/533412_1_En_15_Chapter.xhtml#PB385)\n  260. [386](html/533412_1_En_15_Chapter.xhtml#PB386)\n  261. [387](html/533412_1_En_15_Chapter.xhtml#PB387)\n  262. [388](html/533412_1_En_15_Chapter.xhtml#PB388)\n  263. [389](html/533412_1_En_15_Chapter.xhtml#PB389)\n  264. [390](html/533412_1_En_15_Chapter.xhtml#PB390)\n  265. [391](html/533412_1_En_15_Chapter.xhtml#PB391)\n  266. [392](html/533412_1_En_15_Chapter.xhtml#PB392)\n  267. [393](html/533412_1_En_15_Chapter.xhtml#PB393)\n  268. [394](html/533412_1_En_15_Chapter.xhtml#PB394)\n  269. [395](html/533412_1_En_15_Chapter.xhtml#PB395)\n  270. [401](html/533412_1_En_16_Chapter.xhtml#PB401)\n  271. [402](html/533412_1_En_16_Chapter.xhtml#PB402)\n  272. [403](html/533412_1_En_16_Chapter.xhtml#PB403)\n  273. [404](html/533412_1_En_16_Chapter.xhtml#PB404)\n  274. [405](html/533412_1_En_16_Chapter.xhtml#PB405)\n\n\n![Cover image](../images/978-981-99-1999-4_CoverFigure.jpg)\n\nBook cover of Natural Language Processing\n\n\nRaymond S. T. Lee\n\n# Natural Language Processing\n\n## A Textbook with Python Implementation\n\n![](../images/533412_1_En_BookFrontmatter_Figa_HTML.png)\n\nLogo of the publisher\n\nRaymond S. T. Lee\n\nUnited International College, Beijing Normal University-Hong Kong Baptist\nUniversity, Zhuhai, China\n\nISBN 978-981-99-1998-7 e-ISBN 978-981-99-1999-4\n\n<https://doi.org/10.1007/978-981-99-1999-4>\n\n\u00a9 The Editor(s) (if applicable) and The Author(s), under exclusive license to\nSpringer Nature Singapore Pte Ltd. 2024\n\nThis work is subject to copyright. All rights are solely and exclusively\nlicensed by the Publisher, whether the whole or part of the material is\nconcerned, specifically the rights of translation, reprinting, reuse of\nillustrations, recitation, broadcasting, reproduction on microfilms or in any\nother physical way, and transmission or information storage and retrieval,\nelectronic adaptation, computer software, or by similar or dissimilar\nmethodology now known or hereafter developed.\n\nThe use of general descriptive names, registered names, trademarks, service\nmarks, etc. in this publication does not imply, even in the absence of a\nspecific statement, that such names are exempt from the relevant protective\nlaws and regulations and therefore free for general use.\n\nThe publisher, the authors, and the editors are safe to assume that the advice\nand information in this book are believed to be true and accurate at the date\nof publication. Neither the publisher nor the authors or the editors give a\nwarranty, expressed or implied, with respect to the material contained herein\nor for any errors or omissions that may have been made. The publisher remains\nneutral with regard to jurisdictional claims in published maps and\ninstitutional affiliations.\n\nThis Springer imprint is published by the registered company Springer Nature\nSingapore Pte Ltd.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f82c496f-4376-4a62-b2de-0b496e690569": {"__data__": {"id_": "f82c496f-4376-4a62-b2de-0b496e690569", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a133bbfb-864e-42cf-93c4-692cc1a8b49a", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "bec39f22d50643886a39edcad632c1b5e20b3dd83718c70a70a5e5534ca9ce84", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "423d1d2f-8018-4f56-888f-bc984448017c", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "766af295130e6e03d37acf755e4859e42c8633372ef689f8ac27602c7838e668", "class_name": "RelatedNodeInfo"}}, "hash": "9ebc6588d20cc0d1dc6eb2ecd4d95e1ba162bb2784afc22f103a9e31d4576f0f", "text": "The use of general descriptive names, registered names, trademarks, service\nmarks, etc. in this publication does not imply, even in the absence of a\nspecific statement, that such names are exempt from the relevant protective\nlaws and regulations and therefore free for general use.\n\nThe publisher, the authors, and the editors are safe to assume that the advice\nand information in this book are believed to be true and accurate at the date\nof publication. Neither the publisher nor the authors or the editors give a\nwarranty, expressed or implied, with respect to the material contained herein\nor for any errors or omissions that may have been made. The publisher remains\nneutral with regard to jurisdictional claims in published maps and\ninstitutional affiliations.\n\nThis Springer imprint is published by the registered company Springer Nature\nSingapore Pte Ltd.\n\nThe registered company address is: 152 Beach Road, #21-01/04 Gateway East,\nSingapore 189721, Singapore\n\n_This book is dedicated to all readers and students taking my undergraduate\nand postgraduate courses in Natural Language Processing, your enthusiasm in\nseeking knowledge incited me to write this book._\n\nPreface\n\n## Motivation of This Book\n\nNatural Language Processing (NLP) and its related applications become part of\ndaily life with exponential growth of Artificial Intelligence (AI) in past\ndecades. NLP applications including Information Retrieval (IR) systems, Text\nSummarization System, and Question-and-Answering (Chatbot) System became one\nof the prevalent topics in both industry and academia that had evolved\nroutines and benefited immensely to a wide array of day-to-day services.\n\nThe objective of this book is to provide NLP concepts and knowledge to readers\nwith a 14-h 7 step-by-step workshops to practice various core Python-based NLP\ntools: NLTK, spaCy, TensorFlow Keras, Transformer, and BERT Technology to\nconstruct NLP applications.\n\n## Organization and Structure of This Book\n\nThis book consists of two parts:\n\nPart I Concepts and Technology (Chaps.\n[1](533412_1_En_1_Chapter.xhtml)\u2013[9](533412_1_En_9_Chapter.xhtml))\n\nDiscuss concepts and technology related to NLP including: Introduction, N-gram\nLanguage Model, Part-of-Speech Tagging, Syntax and Parsing, Meaning\nRepresentation, Semantic Analysis, Pragmatic Analysis, Transfer Learning and\nTransformer Technology, Major NLP Applications.\n\nPart II Natural Language Processing Workshops with Python Implementation\n(Chaps. [10](533412_1_En_10_Chapter.xhtml)\u2013[16](533412_1_En_16_Chapter.xhtml))\n\n7 Python workshops to provide step-by-step Python implementation tools\nincluding: NLTK, spaCy, TensorFlow Keras, Transformer, and BERT Technology.\n\nThis book is organized and structured as follows:\n\nPart I: Concepts and Technology\n\n  * Chapter [1](533412_1_En_1_Chapter.xhtml): Introduction to Natural Language Processing\n\nThis introductory chapter begins with human language and intelligence\nconstituting six levels of linguistics followed by a brief history of NLP with\nmajor components and applications. It serves as the cornerstone to the NLP\nconcepts and technology discussed in the following chapters. This chapter also\nserves as the conceptual basis for Workshop#1: Basics of Natural Language\nToolkit (NLTK) in Chap. [10](533412_1_En_10_Chapter.xhtml).\n\n  * Chapter [2](533412_1_En_2_Chapter.xhtml): N-gram Language Model\n\nLanguage model is the foundation of NLP. This chapter introduces N-gram\nlanguage model and Markov Chains using classical literature _The Adventures of\nSherlock Holmes_ by Sir Conan Doyle (1859\u20131930) to illustrate how N-gram model\nworks that form NLP basics in text analysis followed by Shannon\u2019s model and\ntext generation with evaluation schemes. This chapter also serves as the\nconceptual basis for Workshop#2 on N-gram modelling with NLTK in Chap.\n[11](533412_1_En_11_Chapter.xhtml).\n\n  * Chapter [3](533412_1_En_3_Chapter.xhtml): Part-of-Speech Tagging\n\nPart-of-Speech (POS) Tagging is the foundation of text processing in NLP. This\nchapter describes how it relates to NLP and Natural Language Understanding\n(NLU). There are types and algorithms for POS Tagging including Rule-based POS\nTagging, Stochastic POS Tagging, and Hybrid POS Tagging with Brill Tagger and\nevaluation schemes. This chapter also serves as the conceptual basis for\nWorkshop#3: Part-of-Speech using Natural Language Toolkit in Chap.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "423d1d2f-8018-4f56-888f-bc984448017c": {"__data__": {"id_": "423d1d2f-8018-4f56-888f-bc984448017c", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f82c496f-4376-4a62-b2de-0b496e690569", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9ebc6588d20cc0d1dc6eb2ecd4d95e1ba162bb2784afc22f103a9e31d4576f0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c218b3c-c3ac-443d-8ff5-38b7f8aad1b8", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "ad0a8931fec7f3ac1353bf43ce8ac2b6627ddb4e8829545757854bb1f46bee9f", "class_name": "RelatedNodeInfo"}}, "hash": "766af295130e6e03d37acf755e4859e42c8633372ef689f8ac27602c7838e668", "text": "This chapter also serves as the\nconceptual basis for Workshop#2 on N-gram modelling with NLTK in Chap.\n[11](533412_1_En_11_Chapter.xhtml).\n\n  * Chapter [3](533412_1_En_3_Chapter.xhtml): Part-of-Speech Tagging\n\nPart-of-Speech (POS) Tagging is the foundation of text processing in NLP. This\nchapter describes how it relates to NLP and Natural Language Understanding\n(NLU). There are types and algorithms for POS Tagging including Rule-based POS\nTagging, Stochastic POS Tagging, and Hybrid POS Tagging with Brill Tagger and\nevaluation schemes. This chapter also serves as the conceptual basis for\nWorkshop#3: Part-of-Speech using Natural Language Toolkit in Chap.\n[12](533412_1_En_12_Chapter.xhtml).\n\n  * Chapter [4](533412_1_En_4_Chapter.xhtml)\u2014Syntax and Parsing\n\nAs another major component of Natural Language Understanding (NLU), this\nchapter explores syntax analysis and introduces different types of\nconstituents in English language followed by the main concept of context-free\ngrammar (CFG) and CFG parsing. It also studies different major parsing\ntechniques, including lexical and probabilistic parsing with live examples for\nillustration.\n\n  * Chapter [5](533412_1_En_5_Chapter.xhtml): Meaning Representation\n\nBefore the study of Semantic Analysis, this chapter explores meaning\nrepresentation, a vital component in NLP. It studies four major meaning\nrepresentation techniques which include: first-order predicate calculus\n(FOPC), semantic net, conceptual dependency diagram (CDD), and frame-based\nrepresentation. After that it explores canonical form and introduces\nFillmore\u2019s theory of universal cases followed by predicate logic and inference\nwork using FOPC with live examples.\n\n  * Chapter [6](533412_1_En_6_Chapter.xhtml): Semantic Analysis\n\nThis chapter studies Semantic Analysis, one of the core concepts for learning\nNLP. First, it studies the two basic schemes of semantic analysis: lexical and\ncompositional semantic analysis. After that it explores word senses and six\ncommonly used lexical semantics followed by word sense disambiguation (WSD)\nand various WSD schemes. Further, it also studies WordNet and online thesauri\nfor word similarity and various distributed similarity measurement including\nPoint-wise Mutual Information (PMI) and Positive Point-wise Mutual information\n(PPMI) models with live examples for illustration. Chapters\n[4](533412_1_En_4_Chapter.xhtml) and [5](533412_1_En_5_Chapter.xhtml) also\nserve as the conceptual basis for Workshop#4: Semantic Analysis and Word\nVectors using spaCy in Chap. [13](533412_1_En_13_Chapter.xhtml).\n\n  * Chapter [7](533412_1_En_7_Chapter.xhtml): Pragmatic Analysis\n\nAfter the discussion of semantic meaning and analysis, this chapter explores\npragmatic analysis in linguistics and discourse phenomena. It also studies\ncoherence and coreference as the key components of pragmatics and discourse\ncritical to NLP, followed by discourse segmentation with different algorithms\non Co-reference Resolution including Hobbs Algorithm, Centering Algorithm,\nLog-Linear Model, the latest machine learning methods, and evaluation schemes.\nThis chapter also serves as the conceptual basis for Workshop#5: Sentiment\nAnalysis and Text Classification in Chap. [14](533412_1_En_14_Chapter.xhtml).\n\n  * Chapter [8](533412_1_En_8_Chapter.xhtml): Transfer Learning and Transformer Technology\n\nTransfer learning is a commonly used deep learning model to minimize\ncomputational resources. This chapter explores: (1) Transfer Learning (TL)\nagainst traditional Machine Learning (ML); (2) Recurrent Neural Networks\n(RNN), a significant component of transfer learning with core technologies\nsuch as Long Short-Term Memory (LSTM) Network and Bidirectional Recurrent\nNeural Networks (BRNNs) in NLP applications, and (3) Transformer technology\narchitecture, Bidirectional Encoder Representation from Transformers (BERT)\nModel, and related technologies including Transformer-XL and ALBERT\ntechnologies. This chapter also serves as the conceptual basis for Workshop#6:\nTransformers with spaCy and Tensorflow in Chap.\n[15](533412_1_En_15_Chapter.xhtml).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c218b3c-c3ac-443d-8ff5-38b7f8aad1b8": {"__data__": {"id_": "5c218b3c-c3ac-443d-8ff5-38b7f8aad1b8", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "423d1d2f-8018-4f56-888f-bc984448017c", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "766af295130e6e03d37acf755e4859e42c8633372ef689f8ac27602c7838e668", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b40620f-c940-49b5-8d5d-7267ee87459c", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9f758013df32e0757ffe85bb7d366d71f90763ece4b2e004d118dd4e22fce3f3", "class_name": "RelatedNodeInfo"}}, "hash": "ad0a8931fec7f3ac1353bf43ce8ac2b6627ddb4e8829545757854bb1f46bee9f", "text": "* Chapter [8](533412_1_En_8_Chapter.xhtml): Transfer Learning and Transformer Technology\n\nTransfer learning is a commonly used deep learning model to minimize\ncomputational resources. This chapter explores: (1) Transfer Learning (TL)\nagainst traditional Machine Learning (ML); (2) Recurrent Neural Networks\n(RNN), a significant component of transfer learning with core technologies\nsuch as Long Short-Term Memory (LSTM) Network and Bidirectional Recurrent\nNeural Networks (BRNNs) in NLP applications, and (3) Transformer technology\narchitecture, Bidirectional Encoder Representation from Transformers (BERT)\nModel, and related technologies including Transformer-XL and ALBERT\ntechnologies. This chapter also serves as the conceptual basis for Workshop#6:\nTransformers with spaCy and Tensorflow in Chap.\n[15](533412_1_En_15_Chapter.xhtml).\n\n  * Chapter [9](533412_1_En_9_Chapter.xhtml): Major Natural Language Processing Applications\n\nThis is a summary of Part I with three core NLP applications: Information\nRetrieval (IR) systems, Text Summarization (TS) systems, and Question-and-\nAnswering (Q&A) chatbot systems, how they work and related R&D in building NLP\napplications. This chapter also serves as the conceptual basis for Workshop#7:\nBuilding Chatbot with TensorFlow and Transformer Technology in Chap.\n[16](533412_1_En_16_Chapter.xhtml).\n\nPart II: Natural Language Processing Workshops with Python Implementation in\n14 h\n\n  * Chapter [10](533412_1_En_10_Chapter.xhtml): Workshop#1 Basics of Natural Language Toolkit (Hour 1\u20132)\n\nWith the basic NLP concept being learnt in Chap.\n[1](533412_1_En_1_Chapter.xhtml), this introductory workshop gives a NLTK\noverview and system installation procedures are the foundations of Python NLP\ndevelopment tool used for text processing which include simple text analysis,\ntext analysis with lexical dispersion plot, text tokenization, and basic\nstatistical tools in NLP.\n\n  * Chapter [11](533412_1_En_11_Chapter.xhtml): Workshop#2 N-grams Modelling with Natural Language Toolkit (Hour 3\u20134)\n\nThis is a coherent workshop of Chap. [2](533412_1_En_2_Chapter.xhtml) using\nNTLK technology for N-gram generation and statistics. This workshop consists\nof two parts. Part I introduces N-gram language model using NLTK in Python and\nN-grams class to generate N-gram statistics on any sentence, text objects,\nwhole document, literature to provide a foundation technique for text\nanalysis, parsing and semantic analysis in subsequent workshops. Part II\nintroduces spaCy, the second important NLP Python implementation tools not\nonly for teaching and learning (like NLTK) but also widely used for NLP\napplications including text summarization, information extraction, and Q&A\nchatbot. It is a critical mass to integrate with Transformer Technology in\nsubsequent workshops.\n\n  * Chapter [12](533412_1_En_12_Chapter.xhtml): Workshop#3 Part-of-Speech Tagging with Natural Language Toolkit (Hour 5\u20136)\n\nIn Chap. [3](533412_1_En_3_Chapter.xhtml), we studied basic concepts and\ntheories related to Part-of-Speech (POS) and various POS tagging techniques.\nThis workshop explores how to implement POS tagging by using NLTK starting\nfrom a simple recap on tokenization techniques and two fundamental processes\nin word-level progressing: stemming and stop-word removal, which will\nintroduce two types of stemming techniques: Porter Stemmer and Snowball\nStemmer that can be integrated with WordCloud commonly used in data\nvisualization followed by the main theme of this workshop with the\nintroduction of PENN Treebank Tagset and to create your own POS tagger.\n\n  * Chapter [13](533412_1_En_13_Chapter.xhtml): Workshop#4 Semantic Analysis and Word Vectors using spaCy (Hour 7\u20138)\n\nIn Chaps. [5](533412_1_En_5_Chapter.xhtml) and\n[6](533412_1_En_6_Chapter.xhtml), we studied the basic concepts and theories\nrelated to meaning representation and semantic analysis. This workshop\nexplores how to use spaCy technology to perform semantic analysis starting\nfrom a revisit on word vectors concept, implement and pre-train them followed\nby the study of similarity method and other advanced semantic analysis.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b40620f-c940-49b5-8d5d-7267ee87459c": {"__data__": {"id_": "3b40620f-c940-49b5-8d5d-7267ee87459c", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c218b3c-c3ac-443d-8ff5-38b7f8aad1b8", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "ad0a8931fec7f3ac1353bf43ce8ac2b6627ddb4e8829545757854bb1f46bee9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48c29261-138b-4f7e-ae82-e0d4871ad9e7", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "551f60b84e07e1af078c23c35e6ea0f46e201ea037fb091112566ee8078e6182", "class_name": "RelatedNodeInfo"}}, "hash": "9f758013df32e0757ffe85bb7d366d71f90763ece4b2e004d118dd4e22fce3f3", "text": "* Chapter [13](533412_1_En_13_Chapter.xhtml): Workshop#4 Semantic Analysis and Word Vectors using spaCy (Hour 7\u20138)\n\nIn Chaps. [5](533412_1_En_5_Chapter.xhtml) and\n[6](533412_1_En_6_Chapter.xhtml), we studied the basic concepts and theories\nrelated to meaning representation and semantic analysis. This workshop\nexplores how to use spaCy technology to perform semantic analysis starting\nfrom a revisit on word vectors concept, implement and pre-train them followed\nby the study of similarity method and other advanced semantic analysis.\n\n  * Chapter [14](533412_1_En_14_Chapter.xhtml): Workshop#5 Sentiment Analysis and Text Classification (Hour 9\u201310)\n\nThis is a coherent workshop of Chap. [7](533412_1_En_7_Chapter.xhtml), this\nworkshop explores how to position NLP implementation techniques into two\nimportant NLP applications: text classification and sentiment analysis.\nTensorFlow and Kera are two vital components to implement Long Short-Term\nMemory networks (LSTM networks), a commonly used Recurrent Neural Networks\n(RNN) on machine learning especially in NLP applications.\n\n  * Chapter [15](533412_1_En_15_Chapter.xhtml): Workshop#6 Transformers with spaCy and TensorFlow (Hour 11\u201312)\n\nIn Chap. [8](533412_1_En_8_Chapter.xhtml), the basic concept about Transfer\nLearning, its motivation and related background knowledge such as Recurrent\nNeural Networks (RNN) with Transformer Technology and BERT model are\nintroduced. This workshop explores how to put these concepts and theories into\npractice. More importantly, is to implement Transformers, BERT Technology with\nthe integration of spaCy\u2019s Transformer Pipeline Technology and TensorFlow.\nFirst, it gives an overview and summation on Transformer and BERT Technology.\nSecond, it explores Transformer implementation with TensorFlow by revisiting\nText Classification using BERT model as example. Third, it introduces spaCy\u2019s\nTransformer Pipeline Technology and how to implement Sentiment Analysis and\nText Classification system using Transformer Technology.\n\n  * Chapter [16](533412_1_En_16_Chapter.xhtml): Workshop#7 Building Chatbot with TensorFlow and Transformer Technology (Hour 13\u201314)\n\nIn previous six NLP workshops, we studied NLP implementation tools and\ntechniques ranging from tokenization, N-gram generation to semantic and\nsentiment analysis with various key NLP Python enabling technologies: NLTK,\nspaCy, TensorFlow and contemporary Transformer Technology. This final workshop\nexplores how to integrate them for the design and implementation of a live\ndomain-based chatbot system on a movie domain. First, it explores the basis of\nchatbot system and introduce a knowledge domain\u2014the Cornell Large Movie\nConversation Dataset. Second, it conducts a step-by-step implementation of\nmovie chatbot system which involves dialog preprocessing, model construction,\nattention learning implementation, system integration, and performance\nevaluation followed by live tests. Finally, it introduces a mini project for\nthis workshop and present related chatbot datasets with resources in summary.\n\n## Readers of This Book\n\nThis book is both an NLP textbook and NLP Python implementation book tailored\nfor:\n\n  * Undergraduates and postgraduates of various disciplines including AI, Computer Science, IT, Data Science, etc.\n\n  * Lecturers and tutors teaching NLP or related AI courses.\n\n  * NLP, AI scientists and developers who would like to learn NLP basic concepts, practice and implement via Python workshops.\n\n  * Readers who would like to learn NLP concepts, practice Python-based NLP workshops using various NLP implementation tools such as NLTK, spaCy, TensorFlow Keras, BERT, and Transformer technology.\n\n## How to Use This book?\n\nThis book can be served as a textbook for undergraduates and postgraduate\ncourses on Natural Language Processing, and a reference book for general\nreaders who would like to learn key technologies and implement NLP\napplications with contemporary implementation tools such as NLTK, spaCy,\nTensorFlow, BERT, and Transformer technology.\n\nPart I (Chaps.\n[1](533412_1_En_1_Chapter.xhtml)\u2013[9](533412_1_En_9_Chapter.xhtml)) covers the\nmain course materials of basic concepts and key technologies which include\nN-gram Language Model, Part-of-Speech Tagging, Syntax and Parsing, Meaning\nRepresentation, Semantic Analysis, Pragmatic Analysis, Transfer Learning and\nTransformer Technology, and Major NLP Applications. Part II (Chaps.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48c29261-138b-4f7e-ae82-e0d4871ad9e7": {"__data__": {"id_": "48c29261-138b-4f7e-ae82-e0d4871ad9e7", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b40620f-c940-49b5-8d5d-7267ee87459c", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9f758013df32e0757ffe85bb7d366d71f90763ece4b2e004d118dd4e22fce3f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9815be4-bcae-4703-8b18-69079213728e", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "b9dfe8c6dd73bef74b69cc9e33bd6ad527784d5339be45131802ddf4983ea32a", "class_name": "RelatedNodeInfo"}}, "hash": "551f60b84e07e1af078c23c35e6ea0f46e201ea037fb091112566ee8078e6182", "text": "## How to Use This book?\n\nThis book can be served as a textbook for undergraduates and postgraduate\ncourses on Natural Language Processing, and a reference book for general\nreaders who would like to learn key technologies and implement NLP\napplications with contemporary implementation tools such as NLTK, spaCy,\nTensorFlow, BERT, and Transformer technology.\n\nPart I (Chaps.\n[1](533412_1_En_1_Chapter.xhtml)\u2013[9](533412_1_En_9_Chapter.xhtml)) covers the\nmain course materials of basic concepts and key technologies which include\nN-gram Language Model, Part-of-Speech Tagging, Syntax and Parsing, Meaning\nRepresentation, Semantic Analysis, Pragmatic Analysis, Transfer Learning and\nTransformer Technology, and Major NLP Applications. Part II (Chaps.\n[10](533412_1_En_10_Chapter.xhtml)\u2013[16](533412_1_En_16_Chapter.xhtml))\nprovides materials for a 14-h, step-by-step Python-based NLP implementation in\n7 workshops.\n\nFor readers and AI scientists, this book can be served as both reference in\nlearning NLP and Python implementation toolbook on NLP applications by using\nthe latest Python-based NLP development tools, platforms, and libraries.\n\nFor seven NLP Workshops in Part II (Chaps.\n[10](533412_1_En_10_Chapter.xhtml)\u2013[16](533412_1_En_16_Chapter.xhtml)),\nreaders can download all JupyterNB files and data files from my NLP GitHub\ndirectory:\n[https://\u200bgithub.\u200bcom/\u200braymondshtlee/\u200bnlp/\u200b](https://github.com/raymondshtlee/nlp/).\nFor any query, please feel free to contact me via email:\nraymondshtlee@uic.\u200bedu.\u200bcn.\n\nRaymond S. T. Lee\n\nZhuhai, China\n\nAcknowledgements\n\nI would like to express my gratitude:\n\nTo my wife Iris for her patience, encouragement, and understanding, especially\nduring my time spent on research and writing in the past 30 years.\n\nTo Ms. Celine Cheng, executive editor of Springer NATURE and her professional\neditorial and book production team for their support, valuable comments, and\nadvice.\n\nTo Prof. Tang Tao, President of UIC, for the provision of excellent\nenvironment for research, teaching, and writing this book.\n\nTo Prof. Weijia Jia, Vice President (Research and Development) of UIC for\ntheir supports for R&D of NLP and related AI projects.\n\nTo Prof. Jianxin Pan, Dean of Faculty of Science and Technology of UIC, and\nProf. Weifeng Su, Head of Department of Computer Science of UIC for their\ncontinuous supports for AI and NLP courses.\n\nTo research assistant Mr. Zihao Huang for the help of NLP workshops\npreparation. To research student Ms. Clarissa Shi and student helpers Ms. Siqi\nLiu, Mr. Mingjie Wang, and Ms. Jie Lie to help with literature review on major\nNLP applications and Transformer technology, and Mr. Zhuohui Chen to help for\nbugs fixing and version update for the workshop programs.\n\nTo UIC for the prominent support in part by the Guangdong Provincial Key\nLaboratory IRADS (2022B1212010006, R0400001-22), Key Laboratory for Artificial\nIntelligence and Multi-Model Data Processing of Department of Education of\nGuangdong Province and Guangdong Province F1 project grant on Curriculum\nDevelopment and Teaching Enhancement on course development UICR0400050-21 CTL\nfor the provision of an excellent environment and computer facilities for the\npreparation of this book.\n\nDr. Raymond Lee\n\nDecember 2022\n\nBeijing Normal University-Hong Kong Baptist University United International\nCollege\n\nZhuhai\n\nChina\n\nAbout the Book\n\nThis textbook presents an up-to-date and comprehensive overview of Natural\nLanguage Processing (NLP) from basic concepts to core algorithms and key\napplications. It contains 7 step-by-step workshops (total 14 h) to practice\nessential Python tools like NLTK, spaCy, TensorFlow Kera, Transformer, and\nBERT.\n\nThe objective of this book is to provide readers with fundamental knowledge,\ncore technologies, and enable to build their own applications (e.g. Chatbot\nsystems) using Python-based NLP tools.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f9815be4-bcae-4703-8b18-69079213728e": {"__data__": {"id_": "f9815be4-bcae-4703-8b18-69079213728e", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48c29261-138b-4f7e-ae82-e0d4871ad9e7", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "551f60b84e07e1af078c23c35e6ea0f46e201ea037fb091112566ee8078e6182", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d70c0638-6e54-4009-a63d-5d033d34e276", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "e606b1d721ee4f9595c9dba350bbb97501272e0bc5b9b92f2092e52095cbe50e", "class_name": "RelatedNodeInfo"}}, "hash": "b9dfe8c6dd73bef74b69cc9e33bd6ad527784d5339be45131802ddf4983ea32a", "text": "Dr. Raymond Lee\n\nDecember 2022\n\nBeijing Normal University-Hong Kong Baptist University United International\nCollege\n\nZhuhai\n\nChina\n\nAbout the Book\n\nThis textbook presents an up-to-date and comprehensive overview of Natural\nLanguage Processing (NLP) from basic concepts to core algorithms and key\napplications. It contains 7 step-by-step workshops (total 14 h) to practice\nessential Python tools like NLTK, spaCy, TensorFlow Kera, Transformer, and\nBERT.\n\nThe objective of this book is to provide readers with fundamental knowledge,\ncore technologies, and enable to build their own applications (e.g. Chatbot\nsystems) using Python-based NLP tools. It is both a textbook and toolbook\nintended for undergraduate students from various disciplines who want to\nlearn, lecturers and tutors who want to teach courses or tutorials for\nundergraduate/graduate students on the subject and related AI topics, and\nreaders with various backgrounds who want to learn and build practicable\napplications after completing 14 h Python-based workshops.\n\nAbbreviations\n\nAI\n\n    \n\nArtificial intelligence\n\nASR\n\n    \n\nAutomatic speech recognition\n\nBERT\n\n    \n\nBidirectional encoder representations from transformers\n\nBRNN\n\n    \n\nBidirectional recurrent neural networks\n\nCDD\n\n    \n\nConceptual dependency diagram\n\nCFG\n\n    \n\nContext-free grammar\n\nCFL\n\n    \n\nContext-free language\n\nCNN\n\n    \n\nConvolutional neural networks\n\nCR\n\n    \n\nCoreference resolution\n\nDNN\n\n    \n\nDeep neural networks\n\nDT\n\n    \n\nDeterminer\n\nFOPC\n\n    \n\nFirst-order predicate calculus\n\nGRU\n\n    \n\nGate recurrent unit\n\nHMM\n\n    \n\nHidden Markov model\n\nIE\n\n    \n\nInformation extraction\n\nIR\n\n    \n\nInformation retrieval\n\nKAI\n\n    \n\nKnowledge acquisition and inferencing\n\nLSTM\n\n    \n\nLong short-term memory\n\nMEMM\n\n    \n\nMaximum entropy Markov model\n\nMeSH\n\n    \n\nMedical subject thesaurus\n\nML\n\n    \n\nMachine learning\n\nNER\n\n    \n\nNamed entity recognition\n\nNLP\n\n    \n\nNatural language processing\n\nNLTK\n\n    \n\nNatural language toolkit\n\nNLU\n\n    \n\nNatural language understanding\n\nNN\n\n    \n\nNoun\n\nNNP\n\n    \n\nProper noun\n\nNom\n\n    \n\nNominal\n\nNP\n\n    \n\nNoun phrase\n\nPCFG\n\n    \n\nProbabilistic context-free grammar\n\nPMI\n\n    \n\nPointwise mutual information\n\nPOS\n\n    \n\nPart-of-speech\n\nPOST\n\n    \n\nPart-of-speech tagging\n\nPPMI\n\n    \n\nPositive pointwise mutual information\n\nQ&A\n\n    \n\nQuestion-and-answering\n\nRNN\n\n    \n\nRecurrent neural networks\n\nTBL\n\n    \n\nTransformation-based learning\n\nVB\n\n    \n\nVerb\n\nVP\n\n    \n\nVerb phrase\n\nWSD\n\n    \n\nWord sense disambiguation\n\nContents\n\nPart I Concepts and Technology\n\n[ 1 Natural Language Processing ](533412_1_En_1_Chapter.xhtml) 3\n\n[ 1.\u200b1 Introduction ](533412_1_En_1_Chapter.xhtml#Sec1) 3\n\n[ 1.\u200b2 Human Language and Intelligence ](533412_1_En_1_Chapter.xhtml#Sec2) 4\n\n[ 1.\u200b3 Linguistic Levels of Human Language ](533412_1_En_1_Chapter.xhtml#Sec3)\n6\n\n[ 1.\u200b4 Human Language Ambiguity ](533412_1_En_1_Chapter.xhtml#Sec4) 7\n\n[ 1.\u200b5 A Brief History of NLP ](533412_1_En_1_Chapter.xhtml#Sec5) 8\n\n[ 1.\u200b5.\u200b1 First Stage:\u200b Machine Translation (Before 1960s)\n](533412_1_En_1_Chapter.xhtml#Sec6) 8\n\n[ 1.\u200b5.\u200b2 Second Stage:\u200b Early AI on NLP from 1960s to 1970s\n](533412_1_En_1_Chapter.xhtml#Sec7) 8\n\n[ 1.\u200b5.\u200b3 Third Stage:\u200b Grammatical Logic on NLP (1970s\u20131980s)\n](533412_1_En_1_Chapter.xhtml#Sec8) 9\n\n[ 1.\u200b5.\u200b4 Fourth Stage:\u200b AI and Machine Learning (1980s\u20132000s)\n](533412_1_En_1_Chapter.xhtml#Sec9) 9\n\n[ 1.\u200b5.\u200b5 Fifth Stage:\u200b AI, Big Data, and Deep Networks (2010s\u2013Present)\n](533412_1_En_1_Chapter.xhtml#Sec10) 10\n\n[ 1.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d70c0638-6e54-4009-a63d-5d033d34e276": {"__data__": {"id_": "d70c0638-6e54-4009-a63d-5d033d34e276", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9815be4-bcae-4703-8b18-69079213728e", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "b9dfe8c6dd73bef74b69cc9e33bd6ad527784d5339be45131802ddf4983ea32a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a191cbfb-4c98-4e78-975f-89cfec9078fd", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "14517740e41a20df982ae312fc11f62e5d7b925a95d798ebedb99be076c23668", "class_name": "RelatedNodeInfo"}}, "hash": "e606b1d721ee4f9595c9dba350bbb97501272e0bc5b9b92f2092e52095cbe50e", "text": "\u200b5.\u200b2 Second Stage:\u200b Early AI on NLP from 1960s to 1970s\n](533412_1_En_1_Chapter.xhtml#Sec7) 8\n\n[ 1.\u200b5.\u200b3 Third Stage:\u200b Grammatical Logic on NLP (1970s\u20131980s)\n](533412_1_En_1_Chapter.xhtml#Sec8) 9\n\n[ 1.\u200b5.\u200b4 Fourth Stage:\u200b AI and Machine Learning (1980s\u20132000s)\n](533412_1_En_1_Chapter.xhtml#Sec9) 9\n\n[ 1.\u200b5.\u200b5 Fifth Stage:\u200b AI, Big Data, and Deep Networks (2010s\u2013Present)\n](533412_1_En_1_Chapter.xhtml#Sec10) 10\n\n[ 1.\u200b6 NLP and AI ](533412_1_En_1_Chapter.xhtml#Sec11) 10\n\n[ 1.\u200b7 Main Components of NLP ](533412_1_En_1_Chapter.xhtml#Sec12) 11\n\n[ 1.\u200b8 Natural Language Understanding (NLU)\n](533412_1_En_1_Chapter.xhtml#Sec13) 12\n\n[ 1.\u200b8.\u200b1 Speech Recognition ](533412_1_En_1_Chapter.xhtml#Sec14) 13\n\n[ 1.\u200b8.\u200b2 Syntax Analysis ](533412_1_En_1_Chapter.xhtml#Sec15) 13\n\n[ 1.\u200b8.\u200b3 Semantic Analysis ](533412_1_En_1_Chapter.xhtml#Sec16) 13\n\n[ 1.\u200b8.\u200b4 Pragmatic Analysis ](533412_1_En_1_Chapter.xhtml#Sec17) 13\n\n[ 1.\u200b9 Potential Applications of NLP ](533412_1_En_1_Chapter.xhtml#Sec18) 14\n\n[ 1.\u200b9.\u200b1 Machine Translation (MT) ](533412_1_En_1_Chapter.xhtml#Sec19) 14\n\n[ 1.\u200b9.\u200b2 Information Extraction (IE) ](533412_1_En_1_Chapter.xhtml#Sec20) 15\n\n[ 1.\u200b9.\u200b3 Information Retrieval (IR) ](533412_1_En_1_Chapter.xhtml#Sec21) 15\n\n[ 1.\u200b9.\u200b4 Sentiment Analysis ](533412_1_En_1_Chapter.xhtml#Sec22) 15\n\n[ 1.\u200b9.\u200b5 Question-Answering (Q&\u200bA) Chatbots\n](533412_1_En_1_Chapter.xhtml#Sec23) 16\n\n[ References ](533412_1_En_1_Chapter.xhtml#Bib1) 16\n\n[ 2 N-Gram Language Model ](533412_1_En_2_Chapter.xhtml) 19\n\n[ 2.\u200b1 Introduction ](533412_1_En_2_Chapter.xhtml#Sec1) 19\n\n[ 2.\u200b2 N-Gram Language Model ](533412_1_En_2_Chapter.xhtml#Sec2) 21\n\n[ 2.\u200b2.\u200b1 Basic NLP Terminology ](533412_1_En_2_Chapter.xhtml#Sec3) 22\n\n[ 2.\u200b2.\u200b2 Language Modeling and Chain Rule ](533412_1_En_2_Chapter.xhtml#Sec4)\n24\n\n[ 2.\u200b3 Markov Chain in N-Gram Model ](533412_1_En_2_Chapter.xhtml#Sec5) 26\n\n[ 2.\u200b4 Live Example:\u200b The Adventures of Sherlock Holmes\n](533412_1_En_2_Chapter.xhtml#Sec6) 27\n\n[ 2.\u200b5 Shannon\u2019s Method in N-Gram Model ](533412_1_En_2_Chapter.xhtml#Sec7) 31\n\n[ 2.\u200b6 Language Model Evaluation and Smoothing Techniques\n](533412_1_En_2_Chapter.xhtml#Sec8) 34\n\n[ 2.\u200b6.\u200b1 Perplexity ](533412_1_En_2_Chapter.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a191cbfb-4c98-4e78-975f-89cfec9078fd": {"__data__": {"id_": "a191cbfb-4c98-4e78-975f-89cfec9078fd", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d70c0638-6e54-4009-a63d-5d033d34e276", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "e606b1d721ee4f9595c9dba350bbb97501272e0bc5b9b92f2092e52095cbe50e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4eb8364-7b9d-4ccf-a75e-5de32553ee2d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9e12bff790db2e0bf55915c5ff7b7695ec44e4aa6a810f778e27fb1a646ab4d6", "class_name": "RelatedNodeInfo"}}, "hash": "14517740e41a20df982ae312fc11f62e5d7b925a95d798ebedb99be076c23668", "text": "\u200b2.\u200b2 Language Modeling and Chain Rule ](533412_1_En_2_Chapter.xhtml#Sec4)\n24\n\n[ 2.\u200b3 Markov Chain in N-Gram Model ](533412_1_En_2_Chapter.xhtml#Sec5) 26\n\n[ 2.\u200b4 Live Example:\u200b The Adventures of Sherlock Holmes\n](533412_1_En_2_Chapter.xhtml#Sec6) 27\n\n[ 2.\u200b5 Shannon\u2019s Method in N-Gram Model ](533412_1_En_2_Chapter.xhtml#Sec7) 31\n\n[ 2.\u200b6 Language Model Evaluation and Smoothing Techniques\n](533412_1_En_2_Chapter.xhtml#Sec8) 34\n\n[ 2.\u200b6.\u200b1 Perplexity ](533412_1_En_2_Chapter.xhtml#Sec9) 34\n\n[ 2.\u200b6.\u200b2 Extrinsic Evaluation Scheme ](533412_1_En_2_Chapter.xhtml#Sec10) 35\n\n[ 2.\u200b6.\u200b3 Zero Counts Problems ](533412_1_En_2_Chapter.xhtml#Sec11) 35\n\n[ 2.\u200b6.\u200b4 Smoothing Techniques ](533412_1_En_2_Chapter.xhtml#Sec12) 36\n\n[ 2.\u200b6.\u200b5 Laplace (Add-One) Smoothing ](533412_1_En_2_Chapter.xhtml#Sec13) 36\n\n[ 2.\u200b6.\u200b6 Add-k Smoothing ](533412_1_En_2_Chapter.xhtml#Sec14) 38\n\n[ 2.\u200b6.\u200b7 Backoff and Interpolation Smoothing\n](533412_1_En_2_Chapter.xhtml#Sec15) 39\n\n[ 2.\u200b6.\u200b8 Good Turing Smoothing ](533412_1_En_2_Chapter.xhtml#Sec16) 40\n\n[ References ](533412_1_En_2_Chapter.xhtml#Bib1) 41\n\n[ 3 Part-of-Speech (POS) Tagging ](533412_1_En_3_Chapter.xhtml) 43\n\n[ 3.\u200b1 What Is Part-of-Speech (POS)?\u200b ](533412_1_En_3_Chapter.xhtml#Sec1) 43\n\n[ 3.\u200b1.\u200b1 Nine Major POS in English Language\n](533412_1_En_3_Chapter.xhtml#Sec2) 43\n\n[ 3.\u200b2 POS Tagging ](533412_1_En_3_Chapter.xhtml#Sec3) 44\n\n[ 3.\u200b2.\u200b1 What Is POS Tagging in Linguistics?\u200b\n](533412_1_En_3_Chapter.xhtml#Sec4) 44\n\n[ 3.\u200b2.\u200b2 What Is POS Tagging in NLP?\u200b ](533412_1_En_3_Chapter.xhtml#Sec5) 45\n\n[ 3.\u200b2.\u200b3 POS Tags Used in the PENN Treebank Project\n](533412_1_En_3_Chapter.xhtml#Sec6) 45\n\n[ 3.\u200b2.\u200b4 Why Do We Care About POS in NLP?\u200b\n](533412_1_En_3_Chapter.xhtml#Sec7) 46\n\n[ 3.\u200b3 Major Components in NLU ](533412_1_En_3_Chapter.xhtml#Sec8) 48\n\n[ 3.\u200b3.\u200b1 Computational Linguistics and POS\n](533412_1_En_3_Chapter.xhtml#Sec9) 48\n\n[ 3.\u200b3.\u200b2 POS and Semantic Meaning ](533412_1_En_3_Chapter.xhtml#Sec10) 49\n\n[ 3.\u200b3.\u200b3 Morphological and Syntactic Definition of POS\n](533412_1_En_3_Chapter.xhtml#Sec11) 49\n\n[ 3.\u200b4 9 Key POS in English ](533412_1_En_3_Chapter.xhtml#Sec12) 50\n\n[ 3.\u200b4.\u200b1 English Word Classes ](533412_1_En_3_Chapter.xhtml#Sec13) 51\n\n[ 3.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4eb8364-7b9d-4ccf-a75e-5de32553ee2d": {"__data__": {"id_": "c4eb8364-7b9d-4ccf-a75e-5de32553ee2d", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a191cbfb-4c98-4e78-975f-89cfec9078fd", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "14517740e41a20df982ae312fc11f62e5d7b925a95d798ebedb99be076c23668", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6620f44-b87c-4e9e-b972-a6ce35447511", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3e44ffd25de986a73dcd94320b8db8cf04c61e62b8b61d3ae4fbcd888fa86f5d", "class_name": "RelatedNodeInfo"}}, "hash": "9e12bff790db2e0bf55915c5ff7b7695ec44e4aa6a810f778e27fb1a646ab4d6", "text": "xhtml#Sec8) 48\n\n[ 3.\u200b3.\u200b1 Computational Linguistics and POS\n](533412_1_En_3_Chapter.xhtml#Sec9) 48\n\n[ 3.\u200b3.\u200b2 POS and Semantic Meaning ](533412_1_En_3_Chapter.xhtml#Sec10) 49\n\n[ 3.\u200b3.\u200b3 Morphological and Syntactic Definition of POS\n](533412_1_En_3_Chapter.xhtml#Sec11) 49\n\n[ 3.\u200b4 9 Key POS in English ](533412_1_En_3_Chapter.xhtml#Sec12) 50\n\n[ 3.\u200b4.\u200b1 English Word Classes ](533412_1_En_3_Chapter.xhtml#Sec13) 51\n\n[ 3.\u200b4.\u200b2 What Is a Preposition?\u200b ](533412_1_En_3_Chapter.xhtml#Sec14) 51\n\n[ 3.\u200b4.\u200b3 What Is a Conjunction?\u200b ](533412_1_En_3_Chapter.xhtml#Sec15) 52\n\n[ 3.\u200b4.\u200b4 What Is a Pronoun?\u200b ](533412_1_En_3_Chapter.xhtml#Sec16) 53\n\n[ 3.\u200b4.\u200b5 What Is a Verb?\u200b ](533412_1_En_3_Chapter.xhtml#Sec17) 53\n\n[ 3.\u200b5 Different Types of POS Tagset ](533412_1_En_3_Chapter.xhtml#Sec18) 56\n\n[ 3.\u200b5.\u200b1 What Is Tagset?\u200b ](533412_1_En_3_Chapter.xhtml#Sec19) 56\n\n[ 3.\u200b5.\u200b2 Ambiguous in POS Tags ](533412_1_En_3_Chapter.xhtml#Sec20) 57\n\n[ 3.\u200b5.\u200b3 POS Tagging Using Knowledge ](533412_1_En_3_Chapter.xhtml#Sec21) 58\n\n[ 3.\u200b6 Approaches for POS Tagging ](533412_1_En_3_Chapter.xhtml#Sec22) 58\n\n[ 3.\u200b6.\u200b1 Rule-Based Approach POS Tagging ](533412_1_En_3_Chapter.xhtml#Sec23)\n58\n\n[ 3.\u200b6.\u200b2 Example of Rule-Based POS Tagging\n](533412_1_En_3_Chapter.xhtml#Sec24) 59\n\n[ 3.\u200b6.\u200b3 Example of Stochastic-Based POS Tagging\n](533412_1_En_3_Chapter.xhtml#Sec25) 60\n\n[ 3.\u200b6.\u200b4 Hybrid Approach for POS Tagging Using Brill Taggers\n](533412_1_En_3_Chapter.xhtml#Sec26) 61\n\n[ 3.\u200b7 Taggers Evaluations ](533412_1_En_3_Chapter.xhtml#Sec30) 63\n\n[ 3.\u200b7.\u200b1 How Good Is an POS Tagging Algorithm?\u200b\n](533412_1_En_3_Chapter.xhtml#Sec31) 64\n\n[ References ](533412_1_En_3_Chapter.xhtml#Bib1) 65\n\n[ 4 Syntax and Parsing ](533412_1_En_4_Chapter.xhtml) 67\n\n[ 4.\u200b1 Introduction and Motivation ](533412_1_En_4_Chapter.xhtml#Sec1) 67\n\n[ 4.\u200b2 Syntax Analysis ](533412_1_En_4_Chapter.xhtml#Sec2) 68\n\n[ 4.\u200b2.\u200b1 What Is Syntax ](533412_1_En_4_Chapter.xhtml#Sec3) 68\n\n[ 4.\u200b2.\u200b2 Syntactic Rules ](533412_1_En_4_Chapter.xhtml#Sec4) 68\n\n[ 4.\u200b2.\u200b3 Common Syntactic Patterns ](533412_1_En_4_Chapter.xhtml#Sec5) 69\n\n[ 4.\u200b2.\u200b4 Importance of Syntax and Parsing in NLP\n](533412_1_En_4_Chapter.xhtml#Sec6) 70\n\n[ 4.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c6620f44-b87c-4e9e-b972-a6ce35447511": {"__data__": {"id_": "c6620f44-b87c-4e9e-b972-a6ce35447511", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4eb8364-7b9d-4ccf-a75e-5de32553ee2d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9e12bff790db2e0bf55915c5ff7b7695ec44e4aa6a810f778e27fb1a646ab4d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c545670a-68aa-4696-801a-f6fe38eb4075", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "b1a2f4ba32c66ff0055eb315dc79704302a54f2b22bad9f268cd6e1285e2da16", "class_name": "RelatedNodeInfo"}}, "hash": "3e44ffd25de986a73dcd94320b8db8cf04c61e62b8b61d3ae4fbcd888fa86f5d", "text": "xhtml#Sec1) 67\n\n[ 4.\u200b2 Syntax Analysis ](533412_1_En_4_Chapter.xhtml#Sec2) 68\n\n[ 4.\u200b2.\u200b1 What Is Syntax ](533412_1_En_4_Chapter.xhtml#Sec3) 68\n\n[ 4.\u200b2.\u200b2 Syntactic Rules ](533412_1_En_4_Chapter.xhtml#Sec4) 68\n\n[ 4.\u200b2.\u200b3 Common Syntactic Patterns ](533412_1_En_4_Chapter.xhtml#Sec5) 69\n\n[ 4.\u200b2.\u200b4 Importance of Syntax and Parsing in NLP\n](533412_1_En_4_Chapter.xhtml#Sec6) 70\n\n[ 4.\u200b3 Types of Constituents in Sentences ](533412_1_En_4_Chapter.xhtml#Sec7)\n70\n\n[ 4.\u200b3.\u200b1 What Is Constituent?\u200b ](533412_1_En_4_Chapter.xhtml#Sec8) 70\n\n[ 4.\u200b3.\u200b2 Kinds of Constituents ](533412_1_En_4_Chapter.xhtml#Sec9) 72\n\n[ 4.\u200b3.\u200b3 Noun-Phrase (NP) ](533412_1_En_4_Chapter.xhtml#Sec10) 72\n\n[ 4.\u200b3.\u200b4 Verb-Phrase (VP) ](533412_1_En_4_Chapter.xhtml#Sec11) 72\n\n[ 4.\u200b3.\u200b5 Complexity on Simple Constituents\n](533412_1_En_4_Chapter.xhtml#Sec12) 73\n\n[ 4.\u200b3.\u200b6 Verb Phrase Subcategorizatio\u200bn ](533412_1_En_4_Chapter.xhtml#Sec13)\n74\n\n[ 4.\u200b3.\u200b7 The Role of Lexicon in Parsing ](533412_1_En_4_Chapter.xhtml#Sec14)\n75\n\n[ 4.\u200b3.\u200b8 Recursion in Grammar Rules ](533412_1_En_4_Chapter.xhtml#Sec15) 76\n\n[ 4.\u200b4 Context-Free Grammar (CFG) ](533412_1_En_4_Chapter.xhtml#Sec16) 76\n\n[ 4.\u200b4.\u200b1 What Is Context-Free Language (CFL)?\u200b\n](533412_1_En_4_Chapter.xhtml#Sec17) 76\n\n[ 4.\u200b4.\u200b2 What Is Context-Free Grammar (CFG)?\u200b\n](533412_1_En_4_Chapter.xhtml#Sec18) 77\n\n[ 4.\u200b4.\u200b3 Major Components of CFG ](533412_1_En_4_Chapter.xhtml#Sec19) 77\n\n[ 4.\u200b4.\u200b4 Derivations Using CFG ](533412_1_En_4_Chapter.xhtml#Sec20) 78\n\n[ 4.\u200b5 CFG Parsing ](533412_1_En_4_Chapter.xhtml#Sec21) 79\n\n[ 4.\u200b5.\u200b1 Morphological Parsing ](533412_1_En_4_Chapter.xhtml#Sec22) 79\n\n[ 4.\u200b5.\u200b2 Phonological Parsing ](533412_1_En_4_Chapter.xhtml#Sec23) 79\n\n[ 4.\u200b5.\u200b3 Syntactic Parsing ](533412_1_En_4_Chapter.xhtml#Sec24) 79\n\n[ 4.\u200b5.\u200b4 Parsing as a Kind of Tree Searching\n](533412_1_En_4_Chapter.xhtml#Sec25) 80\n\n[ 4.\u200b5.\u200b5 CFG for Fragment of English ](533412_1_En_4_Chapter.xhtml#Sec26) 80\n\n[ 4.\u200b5.\u200b6 Parse Tree for \u201cPlay the Piano\u201d for Prior CFG\n](533412_1_En_4_Chapter.xhtml#Sec27) 80\n\n[ 4.\u200b5.\u200b7 Top-Down Parser ](533412_1_En_4_Chapter.xhtml#Sec28) 81\n\n[ 4.\u200b5.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c545670a-68aa-4696-801a-f6fe38eb4075": {"__data__": {"id_": "c545670a-68aa-4696-801a-f6fe38eb4075", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6620f44-b87c-4e9e-b972-a6ce35447511", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3e44ffd25de986a73dcd94320b8db8cf04c61e62b8b61d3ae4fbcd888fa86f5d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60f7b2bf-4ef4-480e-83a1-3c5a44c04f43", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "2b2a1375a78473e279e98afe35785439b9962559e41b7890ee3693c1d2741bd4", "class_name": "RelatedNodeInfo"}}, "hash": "b1a2f4ba32c66ff0055eb315dc79704302a54f2b22bad9f268cd6e1285e2da16", "text": "\u200b5.\u200b3 Syntactic Parsing ](533412_1_En_4_Chapter.xhtml#Sec24) 79\n\n[ 4.\u200b5.\u200b4 Parsing as a Kind of Tree Searching\n](533412_1_En_4_Chapter.xhtml#Sec25) 80\n\n[ 4.\u200b5.\u200b5 CFG for Fragment of English ](533412_1_En_4_Chapter.xhtml#Sec26) 80\n\n[ 4.\u200b5.\u200b6 Parse Tree for \u201cPlay the Piano\u201d for Prior CFG\n](533412_1_En_4_Chapter.xhtml#Sec27) 80\n\n[ 4.\u200b5.\u200b7 Top-Down Parser ](533412_1_En_4_Chapter.xhtml#Sec28) 81\n\n[ 4.\u200b5.\u200b8 Bottom-Up Parser ](533412_1_En_4_Chapter.xhtml#Sec29) 82\n\n[ 4.\u200b5.\u200b9 Control of Parsing ](533412_1_En_4_Chapter.xhtml#Sec30) 84\n\n[ 4.\u200b5.\u200b10 Pros and Cons of Top-Down vs.\u200b Bottom-Up Parsing\n](533412_1_En_4_Chapter.xhtml#Sec31) 84\n\n[ 4.\u200b6 Lexical and Probabilistic Parsing ](533412_1_En_4_Chapter.xhtml#Sec38)\n85\n\n[ 4.\u200b6.\u200b1 Why Using Probabilities in Parsing?\u200b\n](533412_1_En_4_Chapter.xhtml#Sec39) 85\n\n[ 4.\u200b6.\u200b2 Semantics with Parsing ](533412_1_En_4_Chapter.xhtml#Sec40) 86\n\n[ 4.\u200b6.\u200b3 What Is PCFG?\u200b ](533412_1_En_4_Chapter.xhtml#Sec41) 87\n\n[ 4.\u200b6.\u200b4 A Simple Example of PCFG ](533412_1_En_4_Chapter.xhtml#Sec42) 87\n\n[ 4.\u200b6.\u200b5 Using Probabilities for Language Modeling\n](533412_1_En_4_Chapter.xhtml#Sec43) 90\n\n[ 4.\u200b6.\u200b6 Limitations for PCFG ](533412_1_En_4_Chapter.xhtml#Sec44) 90\n\n[ 4.\u200b6.\u200b7 The Fix:\u200b Lexicalized Parsing ](533412_1_En_4_Chapter.xhtml#Sec45)\n91\n\n[ References ](533412_1_En_4_Chapter.xhtml#Bib1) 94\n\n[ 5 Meaning Representation ](533412_1_En_5_Chapter.xhtml) 95\n\n[ 5.\u200b1 Introduction ](533412_1_En_5_Chapter.xhtml#Sec1) 95\n\n[ 5.\u200b2 What Is Meaning?\u200b ](533412_1_En_5_Chapter.xhtml#Sec2) 95\n\n[ 5.\u200b3 Meaning Representations ](533412_1_En_5_Chapter.xhtml#Sec3) 96\n\n[ 5.\u200b4 Semantic Processing ](533412_1_En_5_Chapter.xhtml#Sec4) 97\n\n[ 5.\u200b5 Common Meaning Representation ](533412_1_En_5_Chapter.xhtml#Sec5) 98\n\n[ 5.\u200b5.\u200b1 First-Order Predicate Calculus (FOPC)\n](533412_1_En_5_Chapter.xhtml#Sec6) 98\n\n[ 5.\u200b5.\u200b2 Semantic Networks ](533412_1_En_5_Chapter.xhtml#Sec7) 98\n\n[ 5.\u200b5.\u200b3 Conceptual Dependency Diagram (CDD)\n](533412_1_En_5_Chapter.xhtml#Sec8) 99\n\n[ 5.\u200b5.\u200b4 Frame-Based Representation ](533412_1_En_5_Chapter.xhtml#Sec9) 99\n\n[ 5.\u200b6 Requirements for Meaning Representation\n](533412_1_En_5_Chapter.xhtml#Sec10) 100\n\n[ 5.\u200b6.\u200b1 Verifiability ](533412_1_En_5_Chapter.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60f7b2bf-4ef4-480e-83a1-3c5a44c04f43": {"__data__": {"id_": "60f7b2bf-4ef4-480e-83a1-3c5a44c04f43", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c545670a-68aa-4696-801a-f6fe38eb4075", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "b1a2f4ba32c66ff0055eb315dc79704302a54f2b22bad9f268cd6e1285e2da16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "59473d92-bda3-4a69-bbcf-5e5ff1f154c4", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "ce6a96d3935d838d3186bf7ae4381d3c8746b7ceee5dba298f2403fcc0744219", "class_name": "RelatedNodeInfo"}}, "hash": "2b2a1375a78473e279e98afe35785439b9962559e41b7890ee3693c1d2741bd4", "text": "\u200b5.\u200b1 First-Order Predicate Calculus (FOPC)\n](533412_1_En_5_Chapter.xhtml#Sec6) 98\n\n[ 5.\u200b5.\u200b2 Semantic Networks ](533412_1_En_5_Chapter.xhtml#Sec7) 98\n\n[ 5.\u200b5.\u200b3 Conceptual Dependency Diagram (CDD)\n](533412_1_En_5_Chapter.xhtml#Sec8) 99\n\n[ 5.\u200b5.\u200b4 Frame-Based Representation ](533412_1_En_5_Chapter.xhtml#Sec9) 99\n\n[ 5.\u200b6 Requirements for Meaning Representation\n](533412_1_En_5_Chapter.xhtml#Sec10) 100\n\n[ 5.\u200b6.\u200b1 Verifiability ](533412_1_En_5_Chapter.xhtml#Sec11) 100\n\n[ 5.\u200b6.\u200b2 Ambiguity ](533412_1_En_5_Chapter.xhtml#Sec12) 100\n\n[ 5.\u200b6.\u200b3 Vagueness ](533412_1_En_5_Chapter.xhtml#Sec13) 101\n\n[ 5.\u200b6.\u200b4 Canonical Forms ](533412_1_En_5_Chapter.xhtml#Sec14) 101\n\n[ 5.\u200b7 Inference ](533412_1_En_5_Chapter.xhtml#Sec20) 102\n\n[ 5.\u200b7.\u200b1 What Is Inference?\u200b ](533412_1_En_5_Chapter.xhtml#Sec21) 102\n\n[ 5.\u200b7.\u200b2 Example of Inferencing with FOPC\n](533412_1_En_5_Chapter.xhtml#Sec22) 103\n\n[ 5.\u200b8 Fillmore\u2019s Theory of Universal Cases\n](533412_1_En_5_Chapter.xhtml#Sec23) 103\n\n[ 5.\u200b8.\u200b1 What Is Fillmore\u2019s Theory of Universal Cases?\u200b\n](533412_1_En_5_Chapter.xhtml#Sec24) 104\n\n[ 5.\u200b8.\u200b2 Major Case Roles in Fillmore\u2019s Theory\n](533412_1_En_5_Chapter.xhtml#Sec25) 105\n\n[ 5.\u200b8.\u200b3 Complications in Case Roles ](533412_1_En_5_Chapter.xhtml#Sec26) 106\n\n[ 5.\u200b9 First-Order Predicate Calculus ](533412_1_En_5_Chapter.xhtml#Sec28) 107\n\n[ 5.\u200b9.\u200b1 FOPC Representation Scheme ](533412_1_En_5_Chapter.xhtml#Sec29) 107\n\n[ 5.\u200b9.\u200b2 Major Elements of FOPC ](533412_1_En_5_Chapter.xhtml#Sec30) 107\n\n[ 5.\u200b9.\u200b3 Predicate-Argument Structure of FOPC\n](533412_1_En_5_Chapter.xhtml#Sec31) 108\n\n[ 5.\u200b9.\u200b4 Meaning Representation Problems in FOPC\n](533412_1_En_5_Chapter.xhtml#Sec32) 110\n\n[ 5.\u200b9.\u200b5 Inferencing Using FOPC ](533412_1_En_5_Chapter.xhtml#Sec33) 111\n\n[ References ](533412_1_En_5_Chapter.xhtml#Bib1) 113\n\n[ 6 Semantic Analysis ](533412_1_En_6_Chapter.xhtml) 115\n\n[ 6.\u200b1 Introduction ](533412_1_En_6_Chapter.xhtml#Sec1) 115\n\n[ 6.\u200b1.\u200b1 What Is Semantic Analysis?\u200b ](533412_1_En_6_Chapter.xhtml#Sec2) 115\n\n[ 6.\u200b1.\u200b2 The Importance of Semantic Analysis in NLP\n](533412_1_En_6_Chapter.xhtml#Sec3) 116\n\n[ 6.\u200b1.\u200b3 How Human Is Good in Semantic Analysis?\u200b\n](533412_1_En_6_Chapter.xhtml#Sec4) 116\n\n[ 6.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59473d92-bda3-4a69-bbcf-5e5ff1f154c4": {"__data__": {"id_": "59473d92-bda3-4a69-bbcf-5e5ff1f154c4", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60f7b2bf-4ef4-480e-83a1-3c5a44c04f43", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "2b2a1375a78473e279e98afe35785439b9962559e41b7890ee3693c1d2741bd4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5642e195-1c2e-4a22-8de9-389dbfea0e1f", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "111dc09cc3548c578d307e6e449e8fce8e72f050f355d69a9bb339ff8f55f4b5", "class_name": "RelatedNodeInfo"}}, "hash": "ce6a96d3935d838d3186bf7ae4381d3c8746b7ceee5dba298f2403fcc0744219", "text": "xhtml#Bib1) 113\n\n[ 6 Semantic Analysis ](533412_1_En_6_Chapter.xhtml) 115\n\n[ 6.\u200b1 Introduction ](533412_1_En_6_Chapter.xhtml#Sec1) 115\n\n[ 6.\u200b1.\u200b1 What Is Semantic Analysis?\u200b ](533412_1_En_6_Chapter.xhtml#Sec2) 115\n\n[ 6.\u200b1.\u200b2 The Importance of Semantic Analysis in NLP\n](533412_1_En_6_Chapter.xhtml#Sec3) 116\n\n[ 6.\u200b1.\u200b3 How Human Is Good in Semantic Analysis?\u200b\n](533412_1_En_6_Chapter.xhtml#Sec4) 116\n\n[ 6.\u200b2 Lexical Vs Compositional Semantic Analysis\n](533412_1_En_6_Chapter.xhtml#Sec5) 117\n\n[ 6.\u200b2.\u200b1 What Is Lexical Semantic Analysis?\u200b\n](533412_1_En_6_Chapter.xhtml#Sec6) 117\n\n[ 6.\u200b2.\u200b2 What Is Compositional Semantic Analysis?\u200b\n](533412_1_En_6_Chapter.xhtml#Sec7) 117\n\n[ 6.\u200b3 Word Senses and Relations ](533412_1_En_6_Chapter.xhtml#Sec8) 118\n\n[ 6.\u200b3.\u200b1 What Is Word Sense?\u200b ](533412_1_En_6_Chapter.xhtml#Sec9) 118\n\n[ 6.\u200b3.\u200b2 Types of Lexical Semantics ](533412_1_En_6_Chapter.xhtml#Sec10) 119\n\n[ 6.\u200b4 Word Sense Disambiguation ](533412_1_En_6_Chapter.xhtml#Sec19) 123\n\n[ 6.\u200b4.\u200b1 What Is Word Sense Disambiguation (WSD)?\u200b\n](533412_1_En_6_Chapter.xhtml#Sec20) 123\n\n[ 6.\u200b4.\u200b2 Difficulties in Word Sense Disambiguation\n](533412_1_En_6_Chapter.xhtml#Sec21) 123\n\n[ 6.\u200b4.\u200b3 Method for Word Sense Disambiguation\n](533412_1_En_6_Chapter.xhtml#Sec22) 124\n\n[ 6.\u200b5 WordNet and Online Thesauri ](533412_1_En_6_Chapter.xhtml#Sec23) 126\n\n[ 6.\u200b5.\u200b1 What Is WordNet?\u200b ](533412_1_En_6_Chapter.xhtml#Sec24) 126\n\n[ 6.\u200b5.\u200b2 What Is Synsets?\u200b ](533412_1_En_6_Chapter.xhtml#Sec25) 126\n\n[ 6.\u200b5.\u200b3 Knowledge Structure of WordNet ](533412_1_En_6_Chapter.xhtml#Sec26)\n127\n\n[ 6.\u200b5.\u200b4 What Are Major Lexical Relations Captured in WordNet?\u200b\n](533412_1_En_6_Chapter.xhtml#Sec27) 129\n\n[ 6.\u200b5.\u200b5 Applications of WordNet and Thesauri?\u200b\n](533412_1_En_6_Chapter.xhtml#Sec28) 129\n\n[ 6.\u200b6 Other Online Thesauri:\u200b MeSH ](533412_1_En_6_Chapter.xhtml#Sec29) 130\n\n[ 6.\u200b6.\u200b1 What Is MeSH?\u200b ](533412_1_En_6_Chapter.xhtml#Sec30) 130\n\n[ 6.\u200b6.\u200b2 Uses of the MeSH Ontology ](533412_1_En_6_Chapter.xhtml#Sec31) 131\n\n[ 6.\u200b7 Word Similarity and Thesaurus Methods\n](533412_1_En_6_Chapter.xhtml#Sec32) 131\n\n[ 6.\u200b8 Introduction ](533412_1_En_6_Chapter.xhtml#Sec33) 131\n\n[ 6.\u200b8.\u200b1 Path-based Similarity ](533412_1_En_6_Chapter.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5642e195-1c2e-4a22-8de9-389dbfea0e1f": {"__data__": {"id_": "5642e195-1c2e-4a22-8de9-389dbfea0e1f", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59473d92-bda3-4a69-bbcf-5e5ff1f154c4", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "ce6a96d3935d838d3186bf7ae4381d3c8746b7ceee5dba298f2403fcc0744219", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8afa1e4c-4311-43c7-903c-49de079b1671", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9dce77e68266c8f78f8a2c4ce45bcbc7a1e3914d3b5d08dfdd1b5fa68851fc72", "class_name": "RelatedNodeInfo"}}, "hash": "111dc09cc3548c578d307e6e449e8fce8e72f050f355d69a9bb339ff8f55f4b5", "text": "\u200b6 Other Online Thesauri:\u200b MeSH ](533412_1_En_6_Chapter.xhtml#Sec29) 130\n\n[ 6.\u200b6.\u200b1 What Is MeSH?\u200b ](533412_1_En_6_Chapter.xhtml#Sec30) 130\n\n[ 6.\u200b6.\u200b2 Uses of the MeSH Ontology ](533412_1_En_6_Chapter.xhtml#Sec31) 131\n\n[ 6.\u200b7 Word Similarity and Thesaurus Methods\n](533412_1_En_6_Chapter.xhtml#Sec32) 131\n\n[ 6.\u200b8 Introduction ](533412_1_En_6_Chapter.xhtml#Sec33) 131\n\n[ 6.\u200b8.\u200b1 Path-based Similarity ](533412_1_En_6_Chapter.xhtml#Sec34) 132\n\n[ 6.\u200b8.\u200b2 Problems with Path-based Similarity\n](533412_1_En_6_Chapter.xhtml#Sec35) 133\n\n[ 6.\u200b8.\u200b3 Information Content Similarity ](533412_1_En_6_Chapter.xhtml#Sec36)\n134\n\n[ 6.\u200b8.\u200b4 The Resnik Method ](533412_1_En_6_Chapter.xhtml#Sec37) 135\n\n[ 6.\u200b8.\u200b5 The Dekang Lin Method ](533412_1_En_6_Chapter.xhtml#Sec38) 135\n\n[ 6.\u200b8.\u200b6 The (Extended) Lesk Algorithm ](533412_1_En_6_Chapter.xhtml#Sec39)\n136\n\n[ 6.\u200b9 Distributed Similarity ](533412_1_En_6_Chapter.xhtml#Sec40) 137\n\n[ 6.\u200b9.\u200b1 Distributional Models of Meaning\n](533412_1_En_6_Chapter.xhtml#Sec41) 137\n\n[ 6.\u200b9.\u200b2 Word Vectors ](533412_1_En_6_Chapter.xhtml#Sec42) 137\n\n[ 6.\u200b9.\u200b3 Term-Document Matrix ](533412_1_En_6_Chapter.xhtml#Sec43) 137\n\n[ 6.\u200b9.\u200b4 Point-wise Mutual Information (PMI)\n](533412_1_En_6_Chapter.xhtml#Sec44) 139\n\n[ 6.\u200b9.\u200b5 Example of Computing PPMI on a Term-Context Matrix\n](533412_1_En_6_Chapter.xhtml#Sec45) 140\n\n[ 6.\u200b9.\u200b6 Weighing PMI Techniques ](533412_1_En_6_Chapter.xhtml#Sec46) 141\n\n[ 6.\u200b9.\u200b7 K-Smoothing in PMI Computation ](533412_1_En_6_Chapter.xhtml#Sec47)\n142\n\n[ 6.\u200b9.\u200b8 Context and Word Similarity Measurement\n](533412_1_En_6_Chapter.xhtml#Sec48) 144\n\n[ 6.\u200b9.\u200b9 Evaluating Similarity ](533412_1_En_6_Chapter.xhtml#Sec49) 145\n\n[ References ](533412_1_En_6_Chapter.xhtml#Bib1) 146\n\n[ 7 Pragmatic Analysis and Discourse ](533412_1_En_7_Chapter.xhtml) 149\n\n[ 7.\u200b1 Introduction ](533412_1_En_7_Chapter.xhtml#Sec1) 149\n\n[ 7.\u200b2 Discourse Phenomena ](533412_1_En_7_Chapter.xhtml#Sec2) 149\n\n[ 7.\u200b2.\u200b1 Coreference Resolution ](533412_1_En_7_Chapter.xhtml#Sec3) 150\n\n[ 7.\u200b2.\u200b2 Why Is it Important?\u200b ](533412_1_En_7_Chapter.xhtml#Sec4) 150\n\n[ 7.\u200b2.\u200b3 Coherence and Coreference ](533412_1_En_7_Chapter.xhtml#Sec5) 151\n\n[ 7.\u200b2.\u200b4 Importance of Coreference Relations\n](533412_1_En_7_Chapter.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8afa1e4c-4311-43c7-903c-49de079b1671": {"__data__": {"id_": "8afa1e4c-4311-43c7-903c-49de079b1671", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5642e195-1c2e-4a22-8de9-389dbfea0e1f", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "111dc09cc3548c578d307e6e449e8fce8e72f050f355d69a9bb339ff8f55f4b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d031bc19-0a0b-4a01-a903-5a706e12bfce", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "86fcc583f4fa1c317bf6fff0adf5d0024a89d4cb93a74205f02633d28cf6caa4", "class_name": "RelatedNodeInfo"}}, "hash": "9dce77e68266c8f78f8a2c4ce45bcbc7a1e3914d3b5d08dfdd1b5fa68851fc72", "text": "\u200b1 Introduction ](533412_1_En_7_Chapter.xhtml#Sec1) 149\n\n[ 7.\u200b2 Discourse Phenomena ](533412_1_En_7_Chapter.xhtml#Sec2) 149\n\n[ 7.\u200b2.\u200b1 Coreference Resolution ](533412_1_En_7_Chapter.xhtml#Sec3) 150\n\n[ 7.\u200b2.\u200b2 Why Is it Important?\u200b ](533412_1_En_7_Chapter.xhtml#Sec4) 150\n\n[ 7.\u200b2.\u200b3 Coherence and Coreference ](533412_1_En_7_Chapter.xhtml#Sec5) 151\n\n[ 7.\u200b2.\u200b4 Importance of Coreference Relations\n](533412_1_En_7_Chapter.xhtml#Sec8) 152\n\n[ 7.\u200b2.\u200b5 Entity-Based Coherence ](533412_1_En_7_Chapter.xhtml#Sec9) 153\n\n[ 7.\u200b3 Discourse Segmentation ](533412_1_En_7_Chapter.xhtml#Sec10) 154\n\n[ 7.\u200b3.\u200b1 What Is Discourse Segmentation?\u200b\n](533412_1_En_7_Chapter.xhtml#Sec11) 154\n\n[ 7.\u200b3.\u200b2 Unsupervised Discourse Segmentation\n](533412_1_En_7_Chapter.xhtml#Sec12) 154\n\n[ 7.\u200b3.\u200b3 Hearst\u2019s TextTiling Method ](533412_1_En_7_Chapter.xhtml#Sec13) 155\n\n[ 7.\u200b3.\u200b4 TextTiling Algorithm ](533412_1_En_7_Chapter.xhtml#Sec14) 157\n\n[ 7.\u200b3.\u200b5 Supervised Discourse Segmentation\n](533412_1_En_7_Chapter.xhtml#Sec15) 158\n\n[ 7.\u200b4 Discourse Coherence ](533412_1_En_7_Chapter.xhtml#Sec16) 158\n\n[ 7.\u200b4.\u200b1 What Makes a Text Coherent?\u200b ](533412_1_En_7_Chapter.xhtml#Sec17)\n158\n\n[ 7.\u200b4.\u200b2 What Is Coherence Relation?\u200b ](533412_1_En_7_Chapter.xhtml#Sec18)\n159\n\n[ 7.\u200b4.\u200b3 Types of Coherence Relations ](533412_1_En_7_Chapter.xhtml#Sec19)\n159\n\n[ 7.\u200b4.\u200b4 Hierarchical Structure of Discourse Coherence\n](533412_1_En_7_Chapter.xhtml#Sec20) 160\n\n[ 7.\u200b4.\u200b5 Types of Referring Expressions ](533412_1_En_7_Chapter.xhtml#Sec21)\n161\n\n[ 7.\u200b4.\u200b6 Features for Filtering Potential Referents\n](533412_1_En_7_Chapter.xhtml#Sec22) 162\n\n[ 7.\u200b4.\u200b7 Preferences in Pronoun Interpretation\n](533412_1_En_7_Chapter.xhtml#Sec23) 162\n\n[ 7.\u200b5 Algorithms for Coreference Resolution\n](533412_1_En_7_Chapter.xhtml#Sec24) 163\n\n[ 7.\u200b5.\u200b1 Introduction ](533412_1_En_7_Chapter.xhtml#Sec25) 163\n\n[ 7.\u200b5.\u200b2 Hobbs Algorithm ](533412_1_En_7_Chapter.xhtml#Sec26) 163\n\n[ 7.\u200b5.\u200b3 Centering Algorithm ](533412_1_En_7_Chapter.xhtml#Sec31) 166\n\n[ 7.\u200b5.\u200b4 Machine Learning Method ](533412_1_En_7_Chapter.xhtml#Sec38) 169\n\n[ 7.\u200b6 Evaluation ](533412_1_En_7_Chapter.xhtml#Sec42) 171\n\n[ References ](533412_1_En_7_Chapter.xhtml#Bib1) 172\n\n[ 8 Transfer Learning and Transformer Technology\n](533412_1_En_8_Chapter.xhtml) 175\n\n[ 8.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d031bc19-0a0b-4a01-a903-5a706e12bfce": {"__data__": {"id_": "d031bc19-0a0b-4a01-a903-5a706e12bfce", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8afa1e4c-4311-43c7-903c-49de079b1671", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9dce77e68266c8f78f8a2c4ce45bcbc7a1e3914d3b5d08dfdd1b5fa68851fc72", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f33a452-5b20-49d4-a598-ae18401fc514", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "dd59e3584f94183f5c47e32bfbbb2238ae88e963ec97cbfef50a11a388957776", "class_name": "RelatedNodeInfo"}}, "hash": "86fcc583f4fa1c317bf6fff0adf5d0024a89d4cb93a74205f02633d28cf6caa4", "text": "xhtml#Sec25) 163\n\n[ 7.\u200b5.\u200b2 Hobbs Algorithm ](533412_1_En_7_Chapter.xhtml#Sec26) 163\n\n[ 7.\u200b5.\u200b3 Centering Algorithm ](533412_1_En_7_Chapter.xhtml#Sec31) 166\n\n[ 7.\u200b5.\u200b4 Machine Learning Method ](533412_1_En_7_Chapter.xhtml#Sec38) 169\n\n[ 7.\u200b6 Evaluation ](533412_1_En_7_Chapter.xhtml#Sec42) 171\n\n[ References ](533412_1_En_7_Chapter.xhtml#Bib1) 172\n\n[ 8 Transfer Learning and Transformer Technology\n](533412_1_En_8_Chapter.xhtml) 175\n\n[ 8.\u200b1 What Is Transfer Learning?\u200b ](533412_1_En_8_Chapter.xhtml#Sec1) 175\n\n[ 8.\u200b2 Motivation of Transfer Learning ](533412_1_En_8_Chapter.xhtml#Sec2) 176\n\n[ 8.\u200b2.\u200b1 Categories of Transfer Learning ](533412_1_En_8_Chapter.xhtml#Sec3)\n176\n\n[ 8.\u200b3 Solutions of Transfer Learning ](533412_1_En_8_Chapter.xhtml#Sec4) 178\n\n[ 8.\u200b4 Recurrent Neural Network (RNN) ](533412_1_En_8_Chapter.xhtml#Sec5) 180\n\n[ 8.\u200b4.\u200b1 What Is RNN?\u200b ](533412_1_En_8_Chapter.xhtml#Sec6) 180\n\n[ 8.\u200b4.\u200b2 Motivation of RNN ](533412_1_En_8_Chapter.xhtml#Sec7) 180\n\n[ 8.\u200b4.\u200b3 RNN Architecture ](533412_1_En_8_Chapter.xhtml#Sec8) 181\n\n[ 8.\u200b4.\u200b4 Long Short-Term Memory (LSTM) Network\n](533412_1_En_8_Chapter.xhtml#Sec9) 183\n\n[ 8.\u200b4.\u200b5 Gate Recurrent Unit (GRU) ](533412_1_En_8_Chapter.xhtml#Sec12) 185\n\n[ 8.\u200b4.\u200b6 Bidirectional Recurrent Neural Networks (BRNNs)\n](533412_1_En_8_Chapter.xhtml#Sec15) 186\n\n[ 8.\u200b5 Transformer Technology ](533412_1_En_8_Chapter.xhtml#Sec17) 188\n\n[ 8.\u200b5.\u200b1 What Is Transformer?\u200b ](533412_1_En_8_Chapter.xhtml#Sec18) 188\n\n[ 8.\u200b5.\u200b2 Transformer Architecture ](533412_1_En_8_Chapter.xhtml#Sec19) 188\n\n[ 8.\u200b5.\u200b3 Deep Into Encoder ](533412_1_En_8_Chapter.xhtml#Sec22) 189\n\n[ 8.\u200b6 BERT ](533412_1_En_8_Chapter.xhtml#Sec28) 192\n\n[ 8.\u200b6.\u200b1 What Is BERT?\u200b ](533412_1_En_8_Chapter.xhtml#Sec29) 192\n\n[ 8.\u200b6.\u200b2 Architecture of BERT ](533412_1_En_8_Chapter.xhtml#Sec30) 192\n\n[ 8.\u200b6.\u200b3 Training of BERT ](533412_1_En_8_Chapter.xhtml#Sec31) 192\n\n[ 8.\u200b7 Other Related Transformer Technology\n](533412_1_En_8_Chapter.xhtml#Sec35) 194\n\n[ 8.\u200b7.\u200b1 Transformer-XL ](533412_1_En_8_Chapter.xhtml#Sec36) 194\n\n[ 8.\u200b7.\u200b2 ALBERT ](533412_1_En_8_Chapter.xhtml#Sec39) 195\n\n[ References ](533412_1_En_8_Chapter.xhtml#Bib1) 196\n\n[ 9 Major NLP Applications ](533412_1_En_9_Chapter.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f33a452-5b20-49d4-a598-ae18401fc514": {"__data__": {"id_": "0f33a452-5b20-49d4-a598-ae18401fc514", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d031bc19-0a0b-4a01-a903-5a706e12bfce", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "86fcc583f4fa1c317bf6fff0adf5d0024a89d4cb93a74205f02633d28cf6caa4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5daa7103-08f4-468b-bcc3-59fb4d83307f", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9f5f37a6ea1e9088c1f3241764dad25cd52ccbcf8ca54dfd0cd9adeca55f5dab", "class_name": "RelatedNodeInfo"}}, "hash": "dd59e3584f94183f5c47e32bfbbb2238ae88e963ec97cbfef50a11a388957776", "text": "xhtml#Sec30) 192\n\n[ 8.\u200b6.\u200b3 Training of BERT ](533412_1_En_8_Chapter.xhtml#Sec31) 192\n\n[ 8.\u200b7 Other Related Transformer Technology\n](533412_1_En_8_Chapter.xhtml#Sec35) 194\n\n[ 8.\u200b7.\u200b1 Transformer-XL ](533412_1_En_8_Chapter.xhtml#Sec36) 194\n\n[ 8.\u200b7.\u200b2 ALBERT ](533412_1_En_8_Chapter.xhtml#Sec39) 195\n\n[ References ](533412_1_En_8_Chapter.xhtml#Bib1) 196\n\n[ 9 Major NLP Applications ](533412_1_En_9_Chapter.xhtml) 199\n\n[ 9.\u200b1 Introduction ](533412_1_En_9_Chapter.xhtml#Sec1) 199\n\n[ 9.\u200b2 Information Retrieval Systems ](533412_1_En_9_Chapter.xhtml#Sec2) 199\n\n[ 9.\u200b2.\u200b1 Introduction to IR Systems ](533412_1_En_9_Chapter.xhtml#Sec3) 199\n\n[ 9.\u200b2.\u200b2 Vector Space Model in IR ](533412_1_En_9_Chapter.xhtml#Sec4) 200\n\n[ 9.\u200b2.\u200b3 Term Distribution Models in IR ](533412_1_En_9_Chapter.xhtml#Sec5)\n202\n\n[ 9.\u200b2.\u200b4 Latent Semantic Indexing in IR ](533412_1_En_9_Chapter.xhtml#Sec6)\n207\n\n[ 9.\u200b2.\u200b5 Discourse Segmentation in IR ](533412_1_En_9_Chapter.xhtml#Sec9) 208\n\n[ 9.\u200b3 Text Summarization Systems ](533412_1_En_9_Chapter.xhtml#Sec10) 212\n\n[ 9.\u200b3.\u200b1 Introduction to Text Summarization Systems\n](533412_1_En_9_Chapter.xhtml#Sec11) 212\n\n[ 9.\u200b3.\u200b2 Text Summarization Datasets ](533412_1_En_9_Chapter.xhtml#Sec17) 214\n\n[ 9.\u200b3.\u200b3 Types of Summarization Systems ](533412_1_En_9_Chapter.xhtml#Sec18)\n214\n\n[ 9.\u200b3.\u200b4 Query-Focused Vs Generic Summarization Systems\n](533412_1_En_9_Chapter.xhtml#Sec19) 215\n\n[ 9.\u200b3.\u200b5 Single and Multiple Document Summarization\n](533412_1_En_9_Chapter.xhtml#Sec22) 217\n\n[ 9.\u200b3.\u200b6 Contemporary Text Summarization Systems\n](533412_1_En_9_Chapter.xhtml#Sec25) 218\n\n[ 9.\u200b4 Question-and-Answering Systems ](533412_1_En_9_Chapter.xhtml#Sec34) 224\n\n[ 9.\u200b4.\u200b1 QA System and AI ](533412_1_En_9_Chapter.xhtml#Sec35) 224\n\n[ 9.\u200b4.\u200b2 Overview of Industrial QA Systems\n](533412_1_En_9_Chapter.xhtml#Sec39) 228\n\n[ References ](533412_1_En_9_Chapter.xhtml#Bib1) 236\n\nPart II Natural Language Processing Workshops with Python Implementation in 14\nHours\n\n[ 10 Workshop#1 Basics of Natural Language Toolkit (Hour 1\u20132)\n](533412_1_En_10_Chapter.xhtml) 243\n\n[ 10.\u200b1 Introduction ](533412_1_En_10_Chapter.xhtml#Sec1) 243\n\n[ 10.\u200b2 What Is Natural Language Toolkit (NLTK)?\u200b\n](533412_1_En_10_Chapter.xhtml#Sec2) 243\n\n[ 10.\u200b3 A Simple Text Tokenization Example Using NLTK\n](533412_1_En_10_Chapter.xhtml#Sec3) 244\n\n[ 10.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5daa7103-08f4-468b-bcc3-59fb4d83307f": {"__data__": {"id_": "5daa7103-08f4-468b-bcc3-59fb4d83307f", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f33a452-5b20-49d4-a598-ae18401fc514", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "dd59e3584f94183f5c47e32bfbbb2238ae88e963ec97cbfef50a11a388957776", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "700aebbd-f2ee-48f1-bd3c-2a6f4dba2e6f", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "ec5cf84423c118a3a71b161cb9f6c61989d68daaad93b33469601a36eb1923c4", "class_name": "RelatedNodeInfo"}}, "hash": "9f5f37a6ea1e9088c1f3241764dad25cd52ccbcf8ca54dfd0cd9adeca55f5dab", "text": "xhtml#Sec39) 228\n\n[ References ](533412_1_En_9_Chapter.xhtml#Bib1) 236\n\nPart II Natural Language Processing Workshops with Python Implementation in 14\nHours\n\n[ 10 Workshop#1 Basics of Natural Language Toolkit (Hour 1\u20132)\n](533412_1_En_10_Chapter.xhtml) 243\n\n[ 10.\u200b1 Introduction ](533412_1_En_10_Chapter.xhtml#Sec1) 243\n\n[ 10.\u200b2 What Is Natural Language Toolkit (NLTK)?\u200b\n](533412_1_En_10_Chapter.xhtml#Sec2) 243\n\n[ 10.\u200b3 A Simple Text Tokenization Example Using NLTK\n](533412_1_En_10_Chapter.xhtml#Sec3) 244\n\n[ 10.\u200b4 How to Install NLTK?\u200b ](533412_1_En_10_Chapter.xhtml#Sec4) 245\n\n[ 10.\u200b5 Why Using Python for NLP?\u200b ](533412_1_En_10_Chapter.xhtml#Sec5) 246\n\n[ 10.\u200b6 NLTK with Basic Text Processing in NLP\n](533412_1_En_10_Chapter.xhtml#Sec6) 248\n\n[ 10.\u200b7 Simple Text Analysis with NLTK ](533412_1_En_10_Chapter.xhtml#Sec7)\n249\n\n[ 10.\u200b8 Text Analysis Using Lexical Dispersion Plot\n](533412_1_En_10_Chapter.xhtml#Sec8) 253\n\n[ 10.\u200b8.\u200b1 What Is a Lexical Dispersion Plot?\u200b\n](533412_1_En_10_Chapter.xhtml#Sec9) 253\n\n[ 10.\u200b8.\u200b2 Lexical Dispersion Plot Over Context Using Sense and Sensibility\n](533412_1_En_10_Chapter.xhtml#Sec10) 253\n\n[ 10.\u200b8.\u200b3 Lexical Dispersion Plot Over Time Using Inaugural Address Corpus\n](533412_1_En_10_Chapter.xhtml#Sec11) 254\n\n[ 10.\u200b9 Tokenization in NLP with NLTK ](533412_1_En_10_Chapter.xhtml#Sec12)\n255\n\n[ 10.\u200b9.\u200b1 What Is Tokenization in NLP?\u200b ](533412_1_En_10_Chapter.xhtml#Sec13)\n255\n\n[ 10.\u200b9.\u200b2 Different Between Tokenize() vs Split()\n](533412_1_En_10_Chapter.xhtml#Sec14) 256\n\n[ 10.\u200b9.\u200b3 Count Distinct Tokens ](533412_1_En_10_Chapter.xhtml#Sec15) 257\n\n[ 10.\u200b9.\u200b4 Lexical Diversity ](533412_1_En_10_Chapter.xhtml#Sec16) 258\n\n[ 10.\u200b10 Basic Statistical Tools in NLTK ](533412_1_En_10_Chapter.xhtml#Sec19)\n260\n\n[ 10.\u200b10.\u200b1 Frequency Distribution:\u200b FreqDist()\n](533412_1_En_10_Chapter.xhtml#Sec20) 260\n\n[ 10.\u200b10.\u200b2 Rare Words:\u200b Hapax ](533412_1_En_10_Chapter.xhtml#Sec24) 262\n\n[ 10.\u200b10.\u200b3 Collocations ](533412_1_En_10_Chapter.xhtml#Sec25) 263\n\n[ References ](533412_1_En_10_Chapter.xhtml#Bib1) 265\n\n[ 11 Workshop#2 N-grams in NLTK and Tokenization in SpaCy (Hour 3\u20134)\n](533412_1_En_11_Chapter.xhtml) 267\n\n[ 11.\u200b1 Introduction ](533412_1_En_11_Chapter.xhtml#Sec1) 267\n\n[ 11.\u200b2 What Is N-Gram?\u200b ](533412_1_En_11_Chapter.xhtml#Sec2) 267\n\n[ 11.\u200b3 Applications of N-Grams in NLP ](533412_1_En_11_Chapter.xhtml#Sec3)\n268\n\n[ 11.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "700aebbd-f2ee-48f1-bd3c-2a6f4dba2e6f": {"__data__": {"id_": "700aebbd-f2ee-48f1-bd3c-2a6f4dba2e6f", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5daa7103-08f4-468b-bcc3-59fb4d83307f", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9f5f37a6ea1e9088c1f3241764dad25cd52ccbcf8ca54dfd0cd9adeca55f5dab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0463471e-72fe-4443-bb7a-edcc7115419b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8d9fc97a15158dd66876c0b25e85517da684bbfd3ab3a2e674d4874bfc4f2088", "class_name": "RelatedNodeInfo"}}, "hash": "ec5cf84423c118a3a71b161cb9f6c61989d68daaad93b33469601a36eb1923c4", "text": "\u200b10.\u200b3 Collocations ](533412_1_En_10_Chapter.xhtml#Sec25) 263\n\n[ References ](533412_1_En_10_Chapter.xhtml#Bib1) 265\n\n[ 11 Workshop#2 N-grams in NLTK and Tokenization in SpaCy (Hour 3\u20134)\n](533412_1_En_11_Chapter.xhtml) 267\n\n[ 11.\u200b1 Introduction ](533412_1_En_11_Chapter.xhtml#Sec1) 267\n\n[ 11.\u200b2 What Is N-Gram?\u200b ](533412_1_En_11_Chapter.xhtml#Sec2) 267\n\n[ 11.\u200b3 Applications of N-Grams in NLP ](533412_1_En_11_Chapter.xhtml#Sec3)\n268\n\n[ 11.\u200b4 Generation of N-Grams in NLTK ](533412_1_En_11_Chapter.xhtml#Sec4) 268\n\n[ 11.\u200b5 Generation of N-Grams Statistics ](533412_1_En_11_Chapter.xhtml#Sec5)\n270\n\n[ 11.\u200b6 spaCy in NLP ](533412_1_En_11_Chapter.xhtml#Sec6) 276\n\n[ 11.\u200b6.\u200b1 What Is spaCy?\u200b ](533412_1_En_11_Chapter.xhtml#Sec7) 276\n\n[ 11.\u200b7 How to Install spaCy?\u200b ](533412_1_En_11_Chapter.xhtml#Sec8) 277\n\n[ 11.\u200b8 Tokenization using spaCy ](533412_1_En_11_Chapter.xhtml#Sec9) 278\n\n[ 11.\u200b8.\u200b1 Step 1:\u200b Import spaCy Module ](533412_1_En_11_Chapter.xhtml#Sec10)\n278\n\n[ 11.\u200b8.\u200b2 Step 2:\u200b Load spaCy Module \"en_\u200bcore_\u200bweb_\u200bsm\".\u200b\n](533412_1_En_11_Chapter.xhtml#Sec11) 278\n\n[ 11.\u200b8.\u200b3 Step 3:\u200b Open and Read Text File \"Adventures_\u200bHolmes.\u200btxt\" Into\nfile_\u200bhandler \"fholmes\" ](533412_1_En_11_Chapter.xhtml#Sec12) 278\n\n[ 11.\u200b8.\u200b4 Step 4:\u200b Read Adventures of Sherlock Holmes\n](533412_1_En_11_Chapter.xhtml#Sec13) 278\n\n[ 11.\u200b8.\u200b5 Step 5:\u200b Replace All Newline Symbols\n](533412_1_En_11_Chapter.xhtml#Sec14) 279\n\n[ 11.\u200b8.\u200b6 Step 6:\u200b Simple Counting ](533412_1_En_11_Chapter.xhtml#Sec15) 279\n\n[ 11.\u200b8.\u200b7 Step 7:\u200b Invoke nlp() Method in spaCy\n](533412_1_En_11_Chapter.xhtml#Sec16) 280\n\n[ 11.\u200b8.\u200b8 Step 8:\u200b Convert Text Document Into Sentence Object\n](533412_1_En_11_Chapter.xhtml#Sec17) 280\n\n[ 11.\u200b8.\u200b9 Step 9:\u200b Directly Tokenize Text Document\n](533412_1_En_11_Chapter.xhtml#Sec18) 282\n\n[ References ](533412_1_En_11_Chapter.xhtml#Bib1) 284\n\n[ 12 Workshop#3 POS Tagging Using NLTK (Hour 5\u20136)\n](533412_1_En_12_Chapter.xhtml) 285\n\n[ 12.\u200b1 Introduction ](533412_1_En_12_Chapter.xhtml#Sec1) 285\n\n[ 12.\u200b2 A Revisit on Tokenization with NLTK\n](533412_1_En_12_Chapter.xhtml#Sec2) 285\n\n[ 12.\u200b3 Stemming Using NLTK ](533412_1_En_12_Chapter.xhtml#Sec3) 288\n\n[ 12.\u200b3.\u200b1 What Is Stemming?\u200b ](533412_1_En_12_Chapter.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0463471e-72fe-4443-bb7a-edcc7115419b": {"__data__": {"id_": "0463471e-72fe-4443-bb7a-edcc7115419b", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "700aebbd-f2ee-48f1-bd3c-2a6f4dba2e6f", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "ec5cf84423c118a3a71b161cb9f6c61989d68daaad93b33469601a36eb1923c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4fa797d7-de8f-47a7-98c9-88ecb074fbc1", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "bed0f039248ef336d861d2b39fc75bb09743230618a6ff7424e04ab6f888e4c5", "class_name": "RelatedNodeInfo"}}, "hash": "8d9fc97a15158dd66876c0b25e85517da684bbfd3ab3a2e674d4874bfc4f2088", "text": "xhtml#Sec18) 282\n\n[ References ](533412_1_En_11_Chapter.xhtml#Bib1) 284\n\n[ 12 Workshop#3 POS Tagging Using NLTK (Hour 5\u20136)\n](533412_1_En_12_Chapter.xhtml) 285\n\n[ 12.\u200b1 Introduction ](533412_1_En_12_Chapter.xhtml#Sec1) 285\n\n[ 12.\u200b2 A Revisit on Tokenization with NLTK\n](533412_1_En_12_Chapter.xhtml#Sec2) 285\n\n[ 12.\u200b3 Stemming Using NLTK ](533412_1_En_12_Chapter.xhtml#Sec3) 288\n\n[ 12.\u200b3.\u200b1 What Is Stemming?\u200b ](533412_1_En_12_Chapter.xhtml#Sec4) 288\n\n[ 12.\u200b3.\u200b2 Why Stemming?\u200b ](533412_1_En_12_Chapter.xhtml#Sec5) 289\n\n[ 12.\u200b3.\u200b3 How to Perform Stemming?\u200b ](533412_1_En_12_Chapter.xhtml#Sec6) 289\n\n[ 12.\u200b3.\u200b4 Porter Stemmer ](533412_1_En_12_Chapter.xhtml#Sec7) 289\n\n[ 12.\u200b3.\u200b5 Snowball Stemmer ](533412_1_En_12_Chapter.xhtml#Sec8) 291\n\n[ 12.\u200b4 Stop-Words Removal with NLTK ](533412_1_En_12_Chapter.xhtml#Sec9) 292\n\n[ 12.\u200b4.\u200b1 What Are Stop-Words?\u200b ](533412_1_En_12_Chapter.xhtml#Sec10) 292\n\n[ 12.\u200b4.\u200b2 NLTK Stop-Words List ](533412_1_En_12_Chapter.xhtml#Sec11) 292\n\n[ 12.\u200b4.\u200b3 Try Some Texts ](533412_1_En_12_Chapter.xhtml#Sec12) 294\n\n[ 12.\u200b4.\u200b4 Create Your Own Stop-Words ](533412_1_En_12_Chapter.xhtml#Sec13)\n295\n\n[ 12.\u200b5 Text Analysis with NLTK ](533412_1_En_12_Chapter.xhtml#Sec18) 296\n\n[ 12.\u200b6 Integration with WordCloud ](533412_1_En_12_Chapter.xhtml#Sec19) 299\n\n[ 12.\u200b6.\u200b1 What Is WordCloud?\u200b ](533412_1_En_12_Chapter.xhtml#Sec20) 299\n\n[ 12.\u200b7 POS Tagging with NLTK ](533412_1_En_12_Chapter.xhtml#Sec21) 301\n\n[ 12.\u200b7.\u200b1 What Is POS Tagging?\u200b ](533412_1_En_12_Chapter.xhtml#Sec22) 301\n\n[ 12.\u200b7.\u200b2 Universal POS Tagset ](533412_1_En_12_Chapter.xhtml#Sec23) 301\n\n[ 12.\u200b7.\u200b3 PENN Treebank Tagset (English and Chinese)\n](533412_1_En_12_Chapter.xhtml#Sec24) 302\n\n[ 12.\u200b7.\u200b4 Applications of POS Tagging ](533412_1_En_12_Chapter.xhtml#Sec25)\n303\n\n[ 12.\u200b8 Create Own POS Tagger with NLTK ](533412_1_En_12_Chapter.xhtml#Sec26)\n306\n\n[ References ](533412_1_En_12_Chapter.xhtml#Bib1) 312\n\n[ 13 Workshop#4 Semantic Analysis and Word Vectors Using spaCy (Hour 7\u20138)\n](533412_1_En_13_Chapter.xhtml) 313\n\n[ 13.\u200b1 Introduction ](533412_1_En_13_Chapter.xhtml#Sec1) 313\n\n[ 13.\u200b2 What Are Word Vectors?\u200b ](533412_1_En_13_Chapter.xhtml#Sec2) 313\n\n[ 13.\u200b3 Understanding Word Vectors ](533412_1_En_13_Chapter.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4fa797d7-de8f-47a7-98c9-88ecb074fbc1": {"__data__": {"id_": "4fa797d7-de8f-47a7-98c9-88ecb074fbc1", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0463471e-72fe-4443-bb7a-edcc7115419b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8d9fc97a15158dd66876c0b25e85517da684bbfd3ab3a2e674d4874bfc4f2088", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f5fcfa4-ca5b-46a6-85a9-8e527600a09e", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "359465c3d057023e8aea62fab6cd7944746801f637f2b32ee989c908cb92f59d", "class_name": "RelatedNodeInfo"}}, "hash": "bed0f039248ef336d861d2b39fc75bb09743230618a6ff7424e04ab6f888e4c5", "text": "xhtml#Sec25)\n303\n\n[ 12.\u200b8 Create Own POS Tagger with NLTK ](533412_1_En_12_Chapter.xhtml#Sec26)\n306\n\n[ References ](533412_1_En_12_Chapter.xhtml#Bib1) 312\n\n[ 13 Workshop#4 Semantic Analysis and Word Vectors Using spaCy (Hour 7\u20138)\n](533412_1_En_13_Chapter.xhtml) 313\n\n[ 13.\u200b1 Introduction ](533412_1_En_13_Chapter.xhtml#Sec1) 313\n\n[ 13.\u200b2 What Are Word Vectors?\u200b ](533412_1_En_13_Chapter.xhtml#Sec2) 313\n\n[ 13.\u200b3 Understanding Word Vectors ](533412_1_En_13_Chapter.xhtml#Sec3) 314\n\n[ 13.\u200b3.\u200b1 Example:\u200b A Simple Word Vector ](533412_1_En_13_Chapter.xhtml#Sec4)\n314\n\n[ 13.\u200b4 A Taste of Word Vectors ](533412_1_En_13_Chapter.xhtml#Sec5) 316\n\n[ 13.\u200b5 Analogies and Vector Operations ](533412_1_En_13_Chapter.xhtml#Sec6)\n319\n\n[ 13.\u200b6 How to Create Word Vectors?\u200b ](533412_1_En_13_Chapter.xhtml#Sec7) 320\n\n[ 13.\u200b7 spaCy Pre-trained Word Vectors ](533412_1_En_13_Chapter.xhtml#Sec8)\n320\n\n[ 13.\u200b8 Similarity Method in Semantic Analysis\n](533412_1_En_13_Chapter.xhtml#Sec9) 323\n\n[ 13.\u200b9 Advanced Semantic Similarity Methods with spaCy\n](533412_1_En_13_Chapter.xhtml#Sec10) 326\n\n[ 13.\u200b9.\u200b1 Understanding Semantic Similarity\n](533412_1_En_13_Chapter.xhtml#Sec11) 326\n\n[ 13.\u200b9.\u200b2 Euclidian Distance ](533412_1_En_13_Chapter.xhtml#Sec12) 326\n\n[ 13.\u200b9.\u200b3 Cosine Distance and Cosine Similarity\n](533412_1_En_13_Chapter.xhtml#Sec13) 327\n\n[ 13.\u200b9.\u200b4 Categorizing Text with Semantic Similarity\n](533412_1_En_13_Chapter.xhtml#Sec14) 329\n\n[ 13.\u200b9.\u200b5 Extracting Key Phrases ](533412_1_En_13_Chapter.xhtml#Sec15) 330\n\n[ 13.\u200b9.\u200b6 Extracting and Comparing Named Entities\n](533412_1_En_13_Chapter.xhtml#Sec16) 331\n\n[ References ](533412_1_En_13_Chapter.xhtml#Bib1) 333\n\n[ 14 Workshop#5 Sentiment Analysis and Text Classification with LSTM Using\nspaCy (Hour 9\u201310) ](533412_1_En_14_Chapter.xhtml) 335\n\n[ 14.\u200b1 Introduction ](533412_1_En_14_Chapter.xhtml#Sec1) 335\n\n[ 14.\u200b2 Text Classification with spaCy and LSTM Technology\n](533412_1_En_14_Chapter.xhtml#Sec2) 335\n\n[ 14.\u200b3 Technical Requirements ](533412_1_En_14_Chapter.xhtml#Sec3) 336\n\n[ 14.\u200b4 Text Classification in a Nutshell ](533412_1_En_14_Chapter.xhtml#Sec4)\n336\n\n[ 14.\u200b4.\u200b1 What Is Text Classification?\u200b ](533412_1_En_14_Chapter.xhtml#Sec5)\n336\n\n[ 14.\u200b4.\u200b2 Text Classification as AI Applications\n](533412_1_En_14_Chapter.xhtml#Sec6) 337\n\n[ 14.\u200b5 Text Classifier with spaCy NLP Pipeline\n](533412_1_En_14_Chapter.xhtml#Sec7) 338\n\n[ 14.\u200b5.\u200b1 TextCategorizer Class ](533412_1_En_14_Chapter.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f5fcfa4-ca5b-46a6-85a9-8e527600a09e": {"__data__": {"id_": "4f5fcfa4-ca5b-46a6-85a9-8e527600a09e", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4fa797d7-de8f-47a7-98c9-88ecb074fbc1", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "bed0f039248ef336d861d2b39fc75bb09743230618a6ff7424e04ab6f888e4c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4efa5c2-b683-46cd-b9dd-b0167b13e735", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "56c3c809dfbaea3cebc84938e7305a32d35ed9bcaca152525a4dd343cba980a2", "class_name": "RelatedNodeInfo"}}, "hash": "359465c3d057023e8aea62fab6cd7944746801f637f2b32ee989c908cb92f59d", "text": "\u200b3 Technical Requirements ](533412_1_En_14_Chapter.xhtml#Sec3) 336\n\n[ 14.\u200b4 Text Classification in a Nutshell ](533412_1_En_14_Chapter.xhtml#Sec4)\n336\n\n[ 14.\u200b4.\u200b1 What Is Text Classification?\u200b ](533412_1_En_14_Chapter.xhtml#Sec5)\n336\n\n[ 14.\u200b4.\u200b2 Text Classification as AI Applications\n](533412_1_En_14_Chapter.xhtml#Sec6) 337\n\n[ 14.\u200b5 Text Classifier with spaCy NLP Pipeline\n](533412_1_En_14_Chapter.xhtml#Sec7) 338\n\n[ 14.\u200b5.\u200b1 TextCategorizer Class ](533412_1_En_14_Chapter.xhtml#Sec8) 339\n\n[ 14.\u200b5.\u200b2 Formatting Training Data for the TextCategorizer\n](533412_1_En_14_Chapter.xhtml#Sec9) 340\n\n[ 14.\u200b5.\u200b3 System Training ](533412_1_En_14_Chapter.xhtml#Sec10) 344\n\n[ 14.\u200b5.\u200b4 System Testing ](533412_1_En_14_Chapter.xhtml#Sec11) 346\n\n[ 14.\u200b5.\u200b5 Training TextCategorizer for Multi-Label Classification\n](533412_1_En_14_Chapter.xhtml#Sec12) 347\n\n[ 14.\u200b6 Sentiment Analysis with spaCy ](533412_1_En_14_Chapter.xhtml#Sec13)\n351\n\n[ 14.\u200b6.\u200b1 IMDB Large Movie Review Dataset\n](533412_1_En_14_Chapter.xhtml#Sec14) 351\n\n[ 14.\u200b6.\u200b2 Explore the Dataset ](533412_1_En_14_Chapter.xhtml#Sec15) 351\n\n[ 14.\u200b6.\u200b3 Training the TextClassfier ](533412_1_En_14_Chapter.xhtml#Sec16)\n355\n\n[ 14.\u200b7 Artificial Neural Network in a Nutshell\n](533412_1_En_14_Chapter.xhtml#Sec17) 357\n\n[ 14.\u200b8 An Overview of TensorFlow and Keras\n](533412_1_En_14_Chapter.xhtml#Sec18) 358\n\n[ 14.\u200b9 Sequential Modeling with LSTM Technology\n](533412_1_En_14_Chapter.xhtml#Sec19) 358\n\n[ 14.\u200b10 Keras Tokenizer in NLP ](533412_1_En_14_Chapter.xhtml#Sec20) 359\n\n[ 14.\u200b10.\u200b1 Embedding Words ](533412_1_En_14_Chapter.xhtml#Sec21) 363\n\n[ 14.\u200b11 Movie Sentiment Analysis with LTSM Using Keras and spaCy\n](533412_1_En_14_Chapter.xhtml#Sec22) 364\n\n[ 14.\u200b11.\u200b1 Step 1:\u200b Dataset ](533412_1_En_14_Chapter.xhtml#Sec23) 365\n\n[ 14.\u200b11.\u200b2 Step 2:\u200b Data and Vocabulary Preparation\n](533412_1_En_14_Chapter.xhtml#Sec24) 366\n\n[ 14.\u200b11.\u200b3 Step 3:\u200b Implement the Input Layer\n](533412_1_En_14_Chapter.xhtml#Sec25) 368\n\n[ 14.\u200b11.\u200b4 Step 4:\u200b Implement the Embedding Layer\n](533412_1_En_14_Chapter.xhtml#Sec26) 368\n\n[ 14.\u200b11.\u200b5 Step 5:\u200b Implement the LSTM Layer\n](533412_1_En_14_Chapter.xhtml#Sec27) 368\n\n[ 14.\u200b11.\u200b6 Step 6:\u200b Implement the Output Layer\n](533412_1_En_14_Chapter.xhtml#Sec28) 369\n\n[ 14.\u200b11.\u200b7 Step 7:\u200b System Compilation ](533412_1_En_14_Chapter.xhtml#Sec29)\n369\n\n[ 14.\u200b11.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4efa5c2-b683-46cd-b9dd-b0167b13e735": {"__data__": {"id_": "a4efa5c2-b683-46cd-b9dd-b0167b13e735", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f5fcfa4-ca5b-46a6-85a9-8e527600a09e", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "359465c3d057023e8aea62fab6cd7944746801f637f2b32ee989c908cb92f59d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c9dbe0c-27f1-4b65-9822-836a0c505fae", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "954d626dc44207de20760a1fcf5fbc706eb55c41e20ed4c56d53cca468bbd02e", "class_name": "RelatedNodeInfo"}}, "hash": "56c3c809dfbaea3cebc84938e7305a32d35ed9bcaca152525a4dd343cba980a2", "text": "\u200b3 Step 3:\u200b Implement the Input Layer\n](533412_1_En_14_Chapter.xhtml#Sec25) 368\n\n[ 14.\u200b11.\u200b4 Step 4:\u200b Implement the Embedding Layer\n](533412_1_En_14_Chapter.xhtml#Sec26) 368\n\n[ 14.\u200b11.\u200b5 Step 5:\u200b Implement the LSTM Layer\n](533412_1_En_14_Chapter.xhtml#Sec27) 368\n\n[ 14.\u200b11.\u200b6 Step 6:\u200b Implement the Output Layer\n](533412_1_En_14_Chapter.xhtml#Sec28) 369\n\n[ 14.\u200b11.\u200b7 Step 7:\u200b System Compilation ](533412_1_En_14_Chapter.xhtml#Sec29)\n369\n\n[ 14.\u200b11.\u200b8 Step 8:\u200b Model Fitting and Experiment Evaluation\n](533412_1_En_14_Chapter.xhtml#Sec30) 370\n\n[ References ](533412_1_En_14_Chapter.xhtml#Bib1) 371\n\n[ 15 Workshop#6 Transformers with spaCy and TensorFlow (Hour 11\u201312)\n](533412_1_En_15_Chapter.xhtml) 373\n\n[ 15.\u200b1 Introduction ](533412_1_En_15_Chapter.xhtml#Sec1) 373\n\n[ 15.\u200b2 Technical Requirements ](533412_1_En_15_Chapter.xhtml#Sec2) 373\n\n[ 15.\u200b3 Transformers and Transfer Learning in a Nutshell\n](533412_1_En_15_Chapter.xhtml#Sec3) 374\n\n[ 15.\u200b4 Why Transformers?\u200b ](533412_1_En_15_Chapter.xhtml#Sec4) 375\n\n[ 15.\u200b5 An Overview of BERT Technology ](533412_1_En_15_Chapter.xhtml#Sec5)\n377\n\n[ 15.\u200b5.\u200b1 What Is BERT?\u200b ](533412_1_En_15_Chapter.xhtml#Sec6) 377\n\n[ 15.\u200b5.\u200b2 BERT Architecture ](533412_1_En_15_Chapter.xhtml#Sec7) 378\n\n[ 15.\u200b5.\u200b3 BERT Input Format ](533412_1_En_15_Chapter.xhtml#Sec8) 378\n\n[ 15.\u200b5.\u200b4 How to Train BERT?\u200b ](533412_1_En_15_Chapter.xhtml#Sec9) 380\n\n[ 15.\u200b6 Transformers with TensorFlow ](533412_1_En_15_Chapter.xhtml#Sec10) 382\n\n[ 15.\u200b6.\u200b1 HuggingFace Transformers ](533412_1_En_15_Chapter.xhtml#Sec11) 382\n\n[ 15.\u200b6.\u200b2 Using the BERT Tokenizer ](533412_1_En_15_Chapter.xhtml#Sec12) 383\n\n[ 15.\u200b6.\u200b3 Word Vectors in BERT ](533412_1_En_15_Chapter.xhtml#Sec13) 386\n\n[ 15.\u200b7 Revisit Text Classification Using BERT\n](533412_1_En_15_Chapter.xhtml#Sec14) 388\n\n[ 15.\u200b7.\u200b1 Data Preparation ](533412_1_En_15_Chapter.xhtml#Sec15) 388\n\n[ 15.\u200b7.\u200b2 Start the BERT Model Construction\n](533412_1_En_15_Chapter.xhtml#Sec19) 389\n\n[ 15.\u200b8 Transformer Pipeline Technology ](533412_1_En_15_Chapter.xhtml#Sec27)\n392\n\n[ 15.\u200b8.\u200b1 Transformer Pipeline for Sentiment Analysis\n](533412_1_En_15_Chapter.xhtml#Sec28) 393\n\n[ 15.\u200b8.\u200b2 Transformer Pipeline for QA System\n](533412_1_En_15_Chapter.xhtml#Sec29) 393\n\n[ 15.\u200b9 Transformer and spaCy ](533412_1_En_15_Chapter.xhtml#Sec30) 394\n\n[ References ](533412_1_En_15_Chapter.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c9dbe0c-27f1-4b65-9822-836a0c505fae": {"__data__": {"id_": "0c9dbe0c-27f1-4b65-9822-836a0c505fae", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4efa5c2-b683-46cd-b9dd-b0167b13e735", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "56c3c809dfbaea3cebc84938e7305a32d35ed9bcaca152525a4dd343cba980a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02735595-f141-4f9b-86dc-363c92f33548", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "aa09e398ccf3cbf623f312188a8dae04f1bf03c11032149d2115c198dff7401f", "class_name": "RelatedNodeInfo"}}, "hash": "954d626dc44207de20760a1fcf5fbc706eb55c41e20ed4c56d53cca468bbd02e", "text": "xhtml#Sec15) 388\n\n[ 15.\u200b7.\u200b2 Start the BERT Model Construction\n](533412_1_En_15_Chapter.xhtml#Sec19) 389\n\n[ 15.\u200b8 Transformer Pipeline Technology ](533412_1_En_15_Chapter.xhtml#Sec27)\n392\n\n[ 15.\u200b8.\u200b1 Transformer Pipeline for Sentiment Analysis\n](533412_1_En_15_Chapter.xhtml#Sec28) 393\n\n[ 15.\u200b8.\u200b2 Transformer Pipeline for QA System\n](533412_1_En_15_Chapter.xhtml#Sec29) 393\n\n[ 15.\u200b9 Transformer and spaCy ](533412_1_En_15_Chapter.xhtml#Sec30) 394\n\n[ References ](533412_1_En_15_Chapter.xhtml#Bib1) 398\n\n[ 16 Workshop#7 Building Chatbot with TensorFlow and Transformer Technology\n(Hour 13\u201314) ](533412_1_En_16_Chapter.xhtml) 401\n\n[ 16.\u200b1 Introduction ](533412_1_En_16_Chapter.xhtml#Sec1) 401\n\n[ 16.\u200b2 Technical Requirements ](533412_1_En_16_Chapter.xhtml#Sec2) 401\n\n[ 16.\u200b3 AI Chatbot in a Nutshell ](533412_1_En_16_Chapter.xhtml#Sec3) 402\n\n[ 16.\u200b3.\u200b1 What Is a Chatbot?\u200b ](533412_1_En_16_Chapter.xhtml#Sec4) 402\n\n[ 16.\u200b3.\u200b2 What Is a Wake Word in Chatbot?\u200b\n](533412_1_En_16_Chapter.xhtml#Sec5) 403\n\n[ 16.\u200b3.\u200b3 NLP Components in a Chatbot ](533412_1_En_16_Chapter.xhtml#Sec8)\n404\n\n[ 16.\u200b4 Building Movie Chatbot by Using TensorFlow and Transformer Technology\n](533412_1_En_16_Chapter.xhtml#Sec9) 404\n\n[ 16.\u200b4.\u200b1 The Chatbot Dataset ](533412_1_En_16_Chapter.xhtml#Sec10) 405\n\n[ 16.\u200b4.\u200b2 Movie Dialog Preprocessing ](533412_1_En_16_Chapter.xhtml#Sec11)\n405\n\n[ 16.\u200b4.\u200b3 Tokenization of Movie Conversation\n](533412_1_En_16_Chapter.xhtml#Sec12) 407\n\n[ 16.\u200b4.\u200b4 Filtering and Padding Process ](533412_1_En_16_Chapter.xhtml#Sec13)\n408\n\n[ 16.\u200b4.\u200b5 Creation of TensorFlow Movie Dataset Object (mDS)\n](533412_1_En_16_Chapter.xhtml#Sec14) 409\n\n[ 16.\u200b4.\u200b6 Calculate Attention Learning Weights\n](533412_1_En_16_Chapter.xhtml#Sec15) 410\n\n[ 16.\u200b4.\u200b7 Multi-Head-Attention (MHAttention)\n](533412_1_En_16_Chapter.xhtml#Sec16) 411\n\n[ 16.\u200b4.\u200b8 System Implementation ](533412_1_En_16_Chapter.xhtml#Sec17) 412\n\n[ 16.\u200b5 Related Works ](533412_1_En_16_Chapter.xhtml#Sec31) 430\n\n[ References ](533412_1_En_16_Chapter.xhtml#Bib1) 431\n\n[ Index ](533412_1_En_BookBackmatter_OnlinePDF.xhtml#Ind1) 433\n\nAbout the Author\n\nRaymond Lee\n\nis the founder of the Quantum Finance Forecast System (QFFC)\n([https://\u200bqffc.\u200buic.\u200bedu.\u200bcn](https://qffc.uic.edu.cn)) and currently an\nAssociate Professor at United International College (UIC) with 25+ years\u2019\nexperience in AI research and consultancy, Chaotic Neural Networks, NLP,\nIntelligent Fintech Systems, Quantum Finance, and Intelligent E-Commerce\nSystems.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02735595-f141-4f9b-86dc-363c92f33548": {"__data__": {"id_": "02735595-f141-4f9b-86dc-363c92f33548", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c9dbe0c-27f1-4b65-9822-836a0c505fae", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "954d626dc44207de20760a1fcf5fbc706eb55c41e20ed4c56d53cca468bbd02e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "20d76e62-0b68-4554-8dd6-67cd64ee446e", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "03d767be09628ea560549cdfc32513f05a25f030a454d67c968de47cb3ee05a7", "class_name": "RelatedNodeInfo"}}, "hash": "aa09e398ccf3cbf623f312188a8dae04f1bf03c11032149d2115c198dff7401f", "text": "xhtml#Sec17) 412\n\n[ 16.\u200b5 Related Works ](533412_1_En_16_Chapter.xhtml#Sec31) 430\n\n[ References ](533412_1_En_16_Chapter.xhtml#Bib1) 431\n\n[ Index ](533412_1_En_BookBackmatter_OnlinePDF.xhtml#Ind1) 433\n\nAbout the Author\n\nRaymond Lee\n\nis the founder of the Quantum Finance Forecast System (QFFC)\n([https://\u200bqffc.\u200buic.\u200bedu.\u200bcn](https://qffc.uic.edu.cn)) and currently an\nAssociate Professor at United International College (UIC) with 25+ years\u2019\nexperience in AI research and consultancy, Chaotic Neural Networks, NLP,\nIntelligent Fintech Systems, Quantum Finance, and Intelligent E-Commerce\nSystems. He has published over 100 publications and authored 8 textbooks in\nthe fields of AI, chaotic neural networks, AI-based fintech systems,\nintelligent agent technology, chaotic cryptosystems, ontological agents,\nneural oscillators, biometrics, and weather simulation and forecasting\nsystems. Upon completion of the QFFC project, in 2018 he joined United\nInternational College (UIC), China, to pursue further R&D work on AI-Fintech\nand to share his expertise in AI-Fintech, chaotic neural networks, and related\nintelligent systems with fellow students and the community. His three latest\ntextbooks, Quantum Finance: Intelligent Forecast and Trading Systems (2019),\nArtificial Intelligence in Daily Life (2020), and this NLP book have been\nadopted as the main textbooks for various AI courses in UIC.\n\n\n#  Part I Concepts and Technology\n\n\n\u00a9 The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.\n2024\n\nR. S. T. Lee Natural Language Processing\n<https://doi.org/10.1007/978-981-99-1999-4_1>\n\n# 1\\. Natural Language Processing\n\nRaymond S. T. Lee 1\n\n(1)\n\nUnited International College, Beijing Normal University-Hong Kong Baptist\nUniversity, Zhuhai, China\n\n_Consider this scenario: Late in the evening, Jack starts a mobile app and\ntalks with_ _AI Tutor_ _Max._\n\n## 1.1 Introduction\n\nThere are many chatbots that allow humans to communicate with a device in\nnatural language nowadays. Figure 1.1 illustrates dialogue between a student\nwho had returned to dormitory after a full day classes and initiated\ncommunication with a mobile application called _AI Tutor_ _2.0_ (Cui et al.\n2020) from our latest research on AI tutor chatbot. The objective is to enable\nthe user (Jack) not only can learn from book reading but also can communicate\ncandidly with _AI Tutor 2.0_ (Max) to provide knowledge responses in natural\nlanguage. It is different from chatbots that respond with basic commands but\nis human\u2013computer interaction to demonstrate how a user wishes to communicate\nin a way like a student convers with a tutor about subject knowledge in the\nphysical world. It is a dynamic process consisting of (1) world knowledge on\nsimple handshaking dialogue such as greetings and general discussions. This is\nnot an easy task as it involves knowledge and common sense to construct a\nfunctional chatbot with daily dialogues, and (2) technical knowledge of a\nparticular knowledge domain, or domain expert as it required to learn from\nauthor\u2019s book _AI in Daily Life_ (Lee 2020) first which covers all basic\nknowledge on the subject to form a knowledge tree or ontology graph that can\nbe served as a new type of publication and interactive device between human\nand computer to learn new knowledge.\n\n![](../images/533412_1_En_1_Chapter/533412_1_En_1_Fig1_HTML.jpg)\n\nFig. 1.1\n\nA snapshot of AI Tutor chatbot\n\nNatural language processing (NLP) is related to several disciplines including\nhuman linguistic, computation linguistic, statistical engineering, AI in\nmachine learning, data mining, human voice processing recognition and\nsynthesis, etc. There are many genius chatbots initiated by NLP and AI\nscientists which become commercial products in past decades.\n\nThis chapter will introduce this prime technology and components followed by\npertinent technologies in subsequent chapters.\n\n## 1.2 Human Language and Intelligence\n\nThere is an old saying: _The way you behave says more about who you are_.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "20d76e62-0b68-4554-8dd6-67cd64ee446e": {"__data__": {"id_": "20d76e62-0b68-4554-8dd6-67cd64ee446e", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02735595-f141-4f9b-86dc-363c92f33548", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "aa09e398ccf3cbf623f312188a8dae04f1bf03c11032149d2115c198dff7401f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7752d95b-e048-4ea9-abae-9126d0c3e226", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f4f75af869695f1b7ef88178ce7b6ff75d7fe7a190547bd26d08955b02b88801", "class_name": "RelatedNodeInfo"}}, "hash": "03d767be09628ea560549cdfc32513f05a25f030a454d67c968de47cb3ee05a7", "text": "![](../images/533412_1_En_1_Chapter/533412_1_En_1_Fig1_HTML.jpg)\n\nFig. 1.1\n\nA snapshot of AI Tutor chatbot\n\nNatural language processing (NLP) is related to several disciplines including\nhuman linguistic, computation linguistic, statistical engineering, AI in\nmachine learning, data mining, human voice processing recognition and\nsynthesis, etc. There are many genius chatbots initiated by NLP and AI\nscientists which become commercial products in past decades.\n\nThis chapter will introduce this prime technology and components followed by\npertinent technologies in subsequent chapters.\n\n## 1.2 Human Language and Intelligence\n\nThere is an old saying: _The way you behave says more about who you are_. It\nis because we never know what people think, the only method is to evaluate and\njudge their behaviors.\n\nNLP core technologies and methodologies arose from famous _Turing Test_\n(Eisenstein 2019; Bender 2013; Turing 1936, 1950) proposed by Sir Alan Turing\n(1912\u20131954) in 1950s, the father of AI. Figure 1.2 shows a human judge convers\nwith two individuals in two rooms. One is a human, the other is either a\nrobot, a chatbot, or an NLP application. During a 20 min conversation, the\njudge can ask human/machine technical/non-technical questions and require\nresponse on every question so that the judge can decide whether the respondent\nis a human or a machine. NLP in Turing Test is to recognize, understand\nquestions, and respond in human language. It remains a popular topic in AI\ntoday because we cannot see and judge people\u2019s thinking to define\nintelligence. It is the ultimate challenge in AI.\n\n![](../images/533412_1_En_1_Chapter/533412_1_En_1_Fig2_HTML.png)\n\nFig. 1.2\n\nTuring test (Tuchong 2020a)\n\nHuman language is a significant component in human behavior and civilization.\nIt can be categorized into (1) written and (2) oral aspects generally. Written\nlanguage undertakes to process, store, and pass human/natural language\nknowledge to next generations. Oral or spoken language acts as a communication\nmedia among other individuals.\n\nNLP has examined the basic effects on philosophy such as meaning and\nknowledge, psychology in words meanings, linguistics in phrases and sentences\nformation, computational linguists in language models. Hence, NLP is cross-\ndisciplinary integration of disciplines such as philosophy in human language\nontology models, psychology behavior between natural and human language,\nlinguistics in mathematical and language models, computational linguistics in\nagents and ontology trees technology as shown in Fig. 1.3.\n\n![](../images/533412_1_En_1_Chapter/533412_1_En_1_Fig3_HTML.png)\n\nFig. 1.3\n\nVarious discipline related to NLP\n\n## 1.3 Linguistic Levels of Human Language\n\nLinguistic levels (Hausser 2014) are regarded as functional analysis of human-\nwritten and spoken languages. There are six levels in linguistics analysis (1)\nphonetics, (2) phonology, (3) morphology, (4) syntax, (5) semantics, and (6)\npragmatics (discourse) classified in basic sound linguistic. The six-levels of\nlinguistic are shown in Fig. 1.4.\n\n![](../images/533412_1_En_1_Chapter/533412_1_En_1_Fig4_HTML.png)\n\nFig. 1.4\n\nLinguistic levels of human languages\n\nThe basic linguistic structure of spoken language includes phonetics and\nphonology. Phonetics refers to the physical aspects of sound, the study of\nproduction and perception of sounds called _phones_. Phonetics governs the\nproduction of human speech often without preceding knowledge of spoken\nlanguage, organizes sounds, and studies the phonemes of languages that can\nprovide various meanings between words and phrases.\n\nDirect language structure is related to morphological and syntactic levels.\n_Morphology_ is the _form_ and _word level_ determined by grammar and syntax\ngenerally. It refers to the smallest form in linguistic analysis, consisting\nof sounds, to combine words with grammatical or lexical function.\n\n_Lexicology_ is the study of vocabulary from a word form to a derived-form.\nSyntax represents the primary level of clauses and sentences to organize\nmeaning of different words order, i.e. addition and subtraction of spoken\nlanguage, and deals with related sentence patterns and ambiguous analysis.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7752d95b-e048-4ea9-abae-9126d0c3e226": {"__data__": {"id_": "7752d95b-e048-4ea9-abae-9126d0c3e226", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20d76e62-0b68-4554-8dd6-67cd64ee446e", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "03d767be09628ea560549cdfc32513f05a25f030a454d67c968de47cb3ee05a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5faf170a-4714-4769-a44b-73a42b2af98d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "885d28347d812badd69bd50c85deb98bd60dbeeb4131a4b30e1ef98fc94e93bd", "class_name": "RelatedNodeInfo"}}, "hash": "f4f75af869695f1b7ef88178ce7b6ff75d7fe7a190547bd26d08955b02b88801", "text": "Phonetics refers to the physical aspects of sound, the study of\nproduction and perception of sounds called _phones_. Phonetics governs the\nproduction of human speech often without preceding knowledge of spoken\nlanguage, organizes sounds, and studies the phonemes of languages that can\nprovide various meanings between words and phrases.\n\nDirect language structure is related to morphological and syntactic levels.\n_Morphology_ is the _form_ and _word level_ determined by grammar and syntax\ngenerally. It refers to the smallest form in linguistic analysis, consisting\nof sounds, to combine words with grammatical or lexical function.\n\n_Lexicology_ is the study of vocabulary from a word form to a derived-form.\nSyntax represents the primary level of clauses and sentences to organize\nmeaning of different words order, i.e. addition and subtraction of spoken\nlanguage, and deals with related sentence patterns and ambiguous analysis.\n\nThe advanced structure deals with actual language meaning at semantic and\npragmatic levels. Semantic level is the domain of meaning that consists of\nmorphology and syntax but is seen as a level that requires one\u2019s own learning\nto assign correct meaning promptly with vocabulary, terminology form, grammar,\nsentence, and discourse perspective. Pragmatics is the use of language in\ndefinitive settings. The meaning of discourse does not have to be the same as\nabstract form in actual use. It is largely based on concept of speech acts and\nthe contents of statement with intent and effect analysis of language\nperformance.\n\n## 1.4 Human Language Ambiguity\n\nIn many language models, cultural differences often produce identical\nutterance with more than single meaning in conversation. Ambiguity are the\ncapabilities to understand sentence structures in many ways. There are (1)\nlexical, (2) syntactic, (3) semantic, and (4) pragmatics ambiguities in NLP.\n\n_Lexical ambiguity_ arises from words where a word meaning depends on\ncontextual utterance. For instance, the word _green_ is normally a noun for\ncolor. But it can be an adjective or even a verb in different situations.\n\n_Syntactic ambiguity_ arises from sentences that are parsed differently, e.g.\n_Jack watched Helen with a telescope_. It can describe either Jack watched\nHelen by using a telescope or Jack watched Helen holding a telescope.\n\n_Semantic ambiguity_ arises from word meaning that can be misinterpreted, or a\nsentence has ambiguous words or phrases, e.g. _The van hits the boar while it\nis moving_. It can describe either _the van hits the boar while the van is\nmoving_ , or _the van hits the boar while the boar is moving_. It has more\nthan a simple syntactic meaning and required to work out the correct\ninterpretation.\n\n_Pragmatic ambiguity_ arises from a statement that is not clearly defined when\nthe context of a sentence provides multiple interpretations such as _I like\nthat too_. It can describe _I like that too_ , _other likes that too_ but the\ndescription of _that_ is uncertain.\n\nNLP analyzes sentences ambiguity incessantly. If they can be identified\nearlier, it will be easier to define proper meanings.\n\n## 1.5 A Brief History of NLP\n\nThere are several major NLP transformation stages in NLP history (Santilal\n2020).\n\n### 1.5.1 First Stage: Machine Translation (Before 1960s)\n\nThe concept of NLP was introduced in seventeenth century by philosopher and\nmathematician Gottfried Wilhelm Leibniz (1646\u20131716) and polymath Ren\u00e9\nDescartes (1596\u20131650). Their studies of the relationships between words and\nlanguages formed the basis for language translation engine development\n(Santilal 2020).\n\nThe first patent for an invention related to machine translation was filed by\ninventor and engineer Georges Artsrouni in 1933, but formal study and research\nwas rendered by Sir Alan Turing from his remarkable article _Computing\nMachinery and Intelligence_ published in 1950 (Turing 1936, 1950) and his\nfamous _Turing test_ officially used as an evaluation criterion for machine\nintelligence since NLP research and development were mainly focused on\nlanguage translation at that time.\n\nThe first and second International Conference on Machine Translation held in\n1952 and 1956 used basic rule-based and stochastic techniques. The 1954\nGeorgetown-IBM experiment engaged wholly automatic machine translation of more\nthan 60 Russian sentences into English and was over optimistic that the whole\nmachine translation problem can be solved within a few years. However,\nbreakthrough on NLP was achieved by Emeritus Prof.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5faf170a-4714-4769-a44b-73a42b2af98d": {"__data__": {"id_": "5faf170a-4714-4769-a44b-73a42b2af98d", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7752d95b-e048-4ea9-abae-9126d0c3e226", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f4f75af869695f1b7ef88178ce7b6ff75d7fe7a190547bd26d08955b02b88801", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8df08d6c-1ec8-464b-8902-fede8a6e9662", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "c249b5e761e55048e28ca32502f435a4b142486671d5e2245cc0ad448cec06de", "class_name": "RelatedNodeInfo"}}, "hash": "885d28347d812badd69bd50c85deb98bd60dbeeb4131a4b30e1ef98fc94e93bd", "text": "The first patent for an invention related to machine translation was filed by\ninventor and engineer Georges Artsrouni in 1933, but formal study and research\nwas rendered by Sir Alan Turing from his remarkable article _Computing\nMachinery and Intelligence_ published in 1950 (Turing 1936, 1950) and his\nfamous _Turing test_ officially used as an evaluation criterion for machine\nintelligence since NLP research and development were mainly focused on\nlanguage translation at that time.\n\nThe first and second International Conference on Machine Translation held in\n1952 and 1956 used basic rule-based and stochastic techniques. The 1954\nGeorgetown-IBM experiment engaged wholly automatic machine translation of more\nthan 60 Russian sentences into English and was over optimistic that the whole\nmachine translation problem can be solved within a few years. However,\nbreakthrough on NLP was achieved by Emeritus Prof. Noam Chomsky on universal\ngrammar for linguistics in 1957, but since the ALPAC report published in 1966\nrevealed deficient progress for AI and machine translation in the past 10\nyears signified the first winter of AI.\n\n### 1.5.2 Second Stage: Early AI on NLP from 1960s to 1970s\n\nNLP major development was focused on how it can be used in different areas\nsuch as knowledge engineering called agent ontology to shape meaning\nrepresentations following AI grew popular over time. BASEBALL system (Green et\nal. 1961) was a typical example of Q&A-based domain expert system of human and\ncomputer interaction developed in 1960s, but inputs were restrictive and\nlanguage processing techniques remained in basic language processing.\n\nIn 1968, Prof. Marvin Minsky (1927\u20132016) developed a more powerful NLP system.\nThis advanced system used an AI-based question-answering inference engine\nbetween humans and computers to provide knowledge-based interpretations of\nquestions and answers. Further, Prof. William A. Woods proposed an augmented\ntranslation network (ATN) to represent natural language input in 1970. During\nthis period, many programmers started to transcribe codes in different AI\nlanguages to conceptualize natural language ontology knowledge of real-world\nstructural information into human understanding mode status. Yet these expert\nsystems were unable to meet expectation signified the second winter of AI.\n\n### 1.5.3 Third Stage: Grammatical Logic on NLP (1970s\u20131980s)\n\nResearch turned to knowledge representation, programming logic, and reasoning\nin AI. This period was regarded as the grammatical logic phase of NLP in which\npowerful sentence processing techniques such as SRI\u2019s core language engine and\ndiscourse representation theory, a new pragmatic representation and discourse\ninterpretation with practical resources and tools such as parsers and Q&A\nchatbots. Although R&D was hampered by computational power but lexicon in\n1980s aimed to expand NLP.\n\n### 1.5.4 Fourth Stage: AI and Machine Learning (1980s\u20132000s)\n\nThe revolutionary success of Hopfield Network in the field of machine learning\nproposed by Prof. Emeritus John Hopfield activated a new era of NLP research\nusing machine learning techniques as an alternative to complex rule-based and\nstochastic methods in the past decades.\n\nComputational technology upgrades in computational power and memory\ncomplemented Chomsky\u2019s theory of linguistics had enhanced language processing\nfrom machine learning methods of corpus linguistics. This development stage\nwas also known as NLP lexical, and corpus referred to grammar emergence in\nlexicalization method in late 1980s, which signified the IBM DeepQA project\nled by Dr. David Ferrucci for their remarkable question-answering system\ndeveloped in 2006.\n\n### 1.5.5 Fifth Stage: AI, Big Data, and Deep Networks (2010s\u2013Present)\n\nNLP statistical technique and rule-based system R&D had evolved into cloud\ncomputing technology on mobile computing and big data in deep network\nanalysis, e.g. recurrent neural networks using LSTM and related networks.\nGoogle, Amazon, Facebook contributed to agent technologies and deep neural\nnetworks development in 2010 to devise products such as auto-driving, Q&A\nchatbots, and storage development are under way.\n\n## 1.6 NLP and AI\n\nNLP can be regarded as automatic or semi-automatic processing of human\nlanguage (Eisenstein 2019). It requires extensive knowledge of linguistics and\nlogical theory in theoretical mathematics, also known as _computational\nlinguistics_. It is a multidisciplinary study of epistemology, philosophy,\npsychology, cognitive science, and agent ontology.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8df08d6c-1ec8-464b-8902-fede8a6e9662": {"__data__": {"id_": "8df08d6c-1ec8-464b-8902-fede8a6e9662", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5faf170a-4714-4769-a44b-73a42b2af98d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "885d28347d812badd69bd50c85deb98bd60dbeeb4131a4b30e1ef98fc94e93bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa84c709-a13e-4950-bfa0-a6fc091396eb", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "5b3dfa1d810ea41642b565387242c7f91810f6d147148b44f5e140e8266f914f", "class_name": "RelatedNodeInfo"}}, "hash": "c249b5e761e55048e28ca32502f435a4b142486671d5e2245cc0ad448cec06de", "text": "### 1.5.5 Fifth Stage: AI, Big Data, and Deep Networks (2010s\u2013Present)\n\nNLP statistical technique and rule-based system R&D had evolved into cloud\ncomputing technology on mobile computing and big data in deep network\nanalysis, e.g. recurrent neural networks using LSTM and related networks.\nGoogle, Amazon, Facebook contributed to agent technologies and deep neural\nnetworks development in 2010 to devise products such as auto-driving, Q&A\nchatbots, and storage development are under way.\n\n## 1.6 NLP and AI\n\nNLP can be regarded as automatic or semi-automatic processing of human\nlanguage (Eisenstein 2019). It requires extensive knowledge of linguistics and\nlogical theory in theoretical mathematics, also known as _computational\nlinguistics_. It is a multidisciplinary study of epistemology, philosophy,\npsychology, cognitive science, and agent ontology.\n\nNLP is an area of AI which computer machines can analyze and interpret human\nspeech for human\u2013computer interaction (HCI) to generate structural knowledge\nfor information retrieval operations, text and automatic text summarization,\nsentiment and speech recognition analysis, data mining, deep learning, and\nmachine translation agent ontologies at different levels of Q&A chatbots (Fig.\n1.5).\n\n![](../images/533412_1_En_1_Chapter/533412_1_En_1_Fig5_HTML.png)\n\nFig. 1.5\n\nNLP and AI (Tuchong 2020b)\n\n## 1.7 Main Components of NLP\n\nNLP consists of (1) Natural Language Understanding (NLU), (2) Knowledge\nAcquisition and Inferencing (KAI), (3) Natural Language Generation (NLG)\ncomponents as shown in Fig. 1.6.\n\n![](../images/533412_1_En_1_Chapter/533412_1_En_1_Fig6_HTML.png)\n\nFig. 1.6\n\nNLP main components\n\nNLU is a technique and method devised to understand the meanings of human\nspoken languages by syntax, semantic, and pragmatic analyses.\n\nKAI is a system to generate proper responses after spoken languages are fully\nrecognized by NLU. It is an unresolved knowledge acquisition and inferencing\nproblem in machine learning and AI by conventional rule-based system due to\nthe intricacies of natural language and conversation, i.e. an _if-then-else_\ntypes of query-response used in expert systems, most KAI systems strive to\nregulate knowledge domain at a specific industry for resolution, i.e. customer\nservice knowledge for insurance, medical, etc. Further, agent ontology has\nachieved favorable outcome.\n\nNLG includes answer, response, and feedback generation in human\u2013machine\ndialogue. It is a multi-facet machine translation process that converts\nresponses into text and sentences to perform text-to-speech synthesis from\ntarget language and produce near human speech responses.\n\n## 1.8 Natural Language Understanding (NLU)\n\nNatural Language Understanding (NLU) is a process of recognizing and\nunderstanding spoken language in four stages (Allen 1994): (1) speech\nrecognition, (2) syntactic (syntax) analysis, (3) semantic analysis, and (4)\npragmatic analysis as shown in Fig. 1.7.\n\n![](../images/533412_1_En_1_Chapter/533412_1_En_1_Fig7_HTML.png)\n\nFig. 1.7\n\nNLU systematic diagram\n\n### 1.8.1 Speech Recognition\n\n_Speech recognition_ (Li et al. 2015) is the first stage in NLU that performs\nphonetic, phonological, and morphological processing to analyze spoken\nlanguage. The task involves breaking down the stems of spoken words called\nutterances, into distinct tokens representing paragraphs, sentences, and words\nin different parts. Current speech recognition models apply spectrogram\nanalysis to extract distinct frequencies, e.g. the word _uncanny_ can be split\ninto two-word tokens _un_ and _canny_. Different languages have different\nspectrogram analysis.\n\n### 1.8.2 Syntax Analysis\n\n_Syntax analysis_ (Sportier et al. 2013) is the second stage of NLU direct\nresponse speech recognition, analyzing the structural meaning of spoken\nsentences. This task has two purposes: (1) check syntax correctness of the\nsentence/utterance, (2) break down spoken sentences into syntactic structures\nto reflect syntactic relationship between words. For instance, the utterance\n_oranges to the boys_ will be rejected by syntax parser because of syntactic\nerrors.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa84c709-a13e-4950-bfa0-a6fc091396eb": {"__data__": {"id_": "aa84c709-a13e-4950-bfa0-a6fc091396eb", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8df08d6c-1ec8-464b-8902-fede8a6e9662", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "c249b5e761e55048e28ca32502f435a4b142486671d5e2245cc0ad448cec06de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ac1f787-e014-40cf-b7fc-221d16d412d9", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "57b80c53ab26e082bc5300b57afdbd407af171bcc86c3f13964c2d1a91533ad6", "class_name": "RelatedNodeInfo"}}, "hash": "5b3dfa1d810ea41642b565387242c7f91810f6d147148b44f5e140e8266f914f", "text": "The task involves breaking down the stems of spoken words called\nutterances, into distinct tokens representing paragraphs, sentences, and words\nin different parts. Current speech recognition models apply spectrogram\nanalysis to extract distinct frequencies, e.g. the word _uncanny_ can be split\ninto two-word tokens _un_ and _canny_. Different languages have different\nspectrogram analysis.\n\n### 1.8.2 Syntax Analysis\n\n_Syntax analysis_ (Sportier et al. 2013) is the second stage of NLU direct\nresponse speech recognition, analyzing the structural meaning of spoken\nsentences. This task has two purposes: (1) check syntax correctness of the\nsentence/utterance, (2) break down spoken sentences into syntactic structures\nto reflect syntactic relationship between words. For instance, the utterance\n_oranges to the boys_ will be rejected by syntax parser because of syntactic\nerrors.\n\n### 1.8.3 Semantic Analysis\n\n_Semantic analysis_ (Goddard 1998) is the third stage in NLU which corresponds\nto syntax analysis. This task is to extract the precise meaning of a\nsentence/utterance, or dictionary meanings defined by the text and reject\nmeaningless, e.g. semantic analyzer rejects word phrase like _hot snowflakes_\ndespite correct syntactic words meaning but incorrect semantic meaning.\n\n### 1.8.4 Pragmatic Analysis\n\n_Pragmatic analysis_ (Ibileye 2018) is the fourth stage in NLU and a\nchallenging part in spoken language analysis involving high level or expert\nknowledge with common sense, e.g. _will you crack open_ the _door? I\u2019m getting\nhot_. This sentence/utterance requires extra knowledge in the second clause to\nunderstand _crack_ is to break in semantic meaning, but it should be\ninterpreted as to open in pragmatic meaning.\n\n## 1.9 Potential Applications of NLP\n\nAfter years of research and development from machine translation and rule-\nbased systems to data mining and deep networks, NLP technology has a wide\nrange of applications in everyday activities such as machine translation,\ninformation retrieval, sentiment analysis, information extraction, and\nquestion-answering chatbots as in Fig. 1.8.\n\n![](../images/533412_1_En_1_Chapter/533412_1_En_1_Fig8_HTML.png)\n\nFig. 1.8\n\nPotential NLP applications\n\n### 1.9.1 Machine Translation (MT)\n\n_Machine translation_ (Scott 2018) is the earliest application in NLP since\n1950s. Although it is not difficult to translate one language to another yet\nthere are two major challenges (1) naturalness (or fluency) means different\nlanguages have different styles and usages and (2) adequacy (or accuracy)\nmeans different languages may present independent ideas in different\nlanguages. Experienced human translators address this trade-off in creative\nways such as statistical methods, or case-by-case rule-based systems in the\npast but since there have been many ambiguity scenarios in language\ntranslation, the goal of machine translation R&D nowadays strive several AI\ntechniques applications for recurrent networks, or deep networks backbox\nsystems to enhance machine learning capabilities.\n\n### 1.9.2 Information Extraction (IE)\n\n_Information extraction_ (Hemdev 2011) is an application task to extract key\nlanguage information from texts or utterances automatically. It can be\nstructural, semi-structural machine-readable documents or from users\u2019\nlanguages of NLP in most cases. Recent activities in complex formats such as\naudio, video and even interactive dialogue can be extracted from multiple\nmedias. Hence, many commercial IE programs become domain-specific like medical\nscience, law, or AI Tutor specified AI knowledge in our case. By doing so, it\nis easier to set up an ontology graph and ontology knowledgebase to contain\nall the retrieved information can be referenced to these domain knowledge\ngraphs to extract useful knowledge.\n\n### 1.9.3 Information Retrieval (IR)\n\n_Information retrieval_ (Peters et al. 2012) is an application for organizing,\nretrieving, storing, and evaluating information from documents, source\nrepositories, especially textual information, and multimedia such as video and\naudio knowledge bases. It helps users to locate relevant documents without\nanswering any questions explicitly. The user must make a request for IR system\nto retrieve the relevant output and respond in document form.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ac1f787-e014-40cf-b7fc-221d16d412d9": {"__data__": {"id_": "4ac1f787-e014-40cf-b7fc-221d16d412d9", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa84c709-a13e-4950-bfa0-a6fc091396eb", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "5b3dfa1d810ea41642b565387242c7f91810f6d147148b44f5e140e8266f914f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "219d9685-fedd-49fb-ae25-91b470521aa2", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "03ad9d24e77ba38f1ce84a87dcd8bf0f1f84c046407df5d8dc7aaf4906127cef", "class_name": "RelatedNodeInfo"}}, "hash": "57b80c53ab26e082bc5300b57afdbd407af171bcc86c3f13964c2d1a91533ad6", "text": "Recent activities in complex formats such as\naudio, video and even interactive dialogue can be extracted from multiple\nmedias. Hence, many commercial IE programs become domain-specific like medical\nscience, law, or AI Tutor specified AI knowledge in our case. By doing so, it\nis easier to set up an ontology graph and ontology knowledgebase to contain\nall the retrieved information can be referenced to these domain knowledge\ngraphs to extract useful knowledge.\n\n### 1.9.3 Information Retrieval (IR)\n\n_Information retrieval_ (Peters et al. 2012) is an application for organizing,\nretrieving, storing, and evaluating information from documents, source\nrepositories, especially textual information, and multimedia such as video and\naudio knowledge bases. It helps users to locate relevant documents without\nanswering any questions explicitly. The user must make a request for IR system\nto retrieve the relevant output and respond in document form.\n\n### 1.9.4 Sentiment Analysis\n\n_Sentiment analysis_ (Liu 2012) is a kind of data mining system in NLP to\nanalyze user sentiment towards products, people, ideas from social media,\nforums, and online platforms. It is an important application for extracting\ndata from messages, comments, and conversations published on these platforms;\nand assigning a labeled sentiment classification as in Fig. 1.9 to understand\nnatural language and utterances. Deep networks are ways to analyze large\namounts of data. In Part II: NLP Implementation Workshop will explore how to\nimplement sentiment analysis in detail using Python spaCy and Transformer\ntechnology.\n\n![](../images/533412_1_En_1_Chapter/533412_1_En_1_Fig9_HTML.png)\n\nFig. 1.9\n\nNLP on _sentiment analysis_\n\n### 1.9.5 Question-Answering (Q&A) Chatbots\n\n_Q &A systems_ is the objective in NLP (Raj 2018). A process flow is necessary\nto implement a Q&A chatbot. It includes voice recognition to convert into a\nlist of tokens in sentences/utterances, syntactic grammatical analysis,\nsemantic meaning analysis of whole sentences, and pragmatic analysis for\nembedded or complex meanings. When enquirer\u2019s utterance meaning is generated,\nit is necessary to search from knowledge base for the most appropriate answer\nor response through inferencing either by rule-based system, statistical\nsystem, or deep network, e.g. Google BERT system. Once a response is\navailable, reverse engineering is required to generate natural voice from\nverbal language called voice synthesis. Hence, Q&A system in NLP is an\nimportant technology that can apply to daily activities such as human\u2013computer\ninteraction in auto-driving, customer services support, and language skills\nimprovement.\n\nThe final workshop will discuss how to integrate various Python NLP\nimplementation tools including NLTK, spaCy, TensorFlow Keras, and Transformer\nTechnology to implement a Q&A movies chatbot system.\n\nReferences\n\n  1. Allen, J. (1994) Natural Language Understanding (2nd edition). Pearson\n\n  2. Bender, E. M. (2013) Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Morphology and Syntax (Synthesis Lectures on Human Language Technologies). Morgan & Claypool Publishers[Crossref](https://doi.org/10.1007/978-3-031-02150-3)\n\n  3. Cui, Y., Huang, C., Lee, Raymond (2020). AI Tutor: A Computer Science Domain Knowledge Graph-Based QA System on JADE platform. World Academy of Science, Engineering and Technology, Open Science Index 168, International Journal of Industrial and Manufacturing Engineering, 14(12), 543 - 553.\n\n  4. Eisenstein, J. (2019) Introduction to Natural Language Processing (Adaptive Computation and Machine Learning series). The MIT Press.\n\n  5. Goddard, C. (1998) Semantic Analysis: A Practical Introduction (Oxford Textbooks in Linguistics). Oxford University Press.\n\n  6. Green, B., Wolf, A., Chomsky, C. and Laughery, K. (1961). BASEBALL: an automatic question-answerer. In Papers presented at the May 9-11, 1961, western joint IRE-AIEE-ACM computer conference (IRE-AIEE-ACM \u201961 (Western)). Association for Computing Machinery, New York, NY, USA, 219\u2013224.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "219d9685-fedd-49fb-ae25-91b470521aa2": {"__data__": {"id_": "219d9685-fedd-49fb-ae25-91b470521aa2", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ac1f787-e014-40cf-b7fc-221d16d412d9", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "57b80c53ab26e082bc5300b57afdbd407af171bcc86c3f13964c2d1a91533ad6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5df39e9a-5fdd-4eee-92a9-bbf3fe9802b2", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "4c3a3febd4d029916432166a433011fb117a37d284a4083cef3fc74222bb17d4", "class_name": "RelatedNodeInfo"}}, "hash": "03ad9d24e77ba38f1ce84a87dcd8bf0f1f84c046407df5d8dc7aaf4906127cef", "text": "4. Eisenstein, J. (2019) Introduction to Natural Language Processing (Adaptive Computation and Machine Learning series). The MIT Press.\n\n  5. Goddard, C. (1998) Semantic Analysis: A Practical Introduction (Oxford Textbooks in Linguistics). Oxford University Press.\n\n  6. Green, B., Wolf, A., Chomsky, C. and Laughery, K. (1961). BASEBALL: an automatic question-answerer. In Papers presented at the May 9-11, 1961, western joint IRE-AIEE-ACM computer conference (IRE-AIEE-ACM \u201961 (Western)). Association for Computing Machinery, New York, NY, USA, 219\u2013224.[Crossref](https://doi.org/10.1145/1460690.1460714)\n\n  7. Hausser, R. (2014) Foundations of Computational Linguistics: Human-Computer Communication in Natural Language (3rd edition). Springer.[Crossref](https://doi.org/10.1007/978-3-642-41431-2)[zbMATH](http://www.emis.de/MATH-item?1281.68002)\n\n  8. Hemdev, P. (2011) Information Extraction: A Smart Calendar Application: Using NLP, Computational Linguistics, Machine Learning and Information Retrieval Techniques. VDM Verlag Dr. M\u00fcller.\n\n  9. Ibileye, G. (2018) Discourse Analysis and Pragmatics: Issues in Theory and Practice. Malthouse Press.\n\n  10. Lee, R. S. T. (2020). AI in Daily Life. Springer.\n\n  11. Li, J. et al. (2015) Robust Automatic Speech Recognition: A Bridge to Practical Applications. Academic Press.\n\n  12. Liu, B. (2012) Sentiment Analysis and Opinion Mining. Morgan & Claypool Publishers.[Crossref](https://doi.org/10.1007/978-3-031-02145-9)\n\n  13. Peters, C. et al. (2012) Multilingual Information Retrieval: From Research To Practice. Springer.[Crossref](https://doi.org/10.1007/978-3-642-23008-0)\n\n  14. Raj, S. (2018) Building Chatbots with Python: Using Natural Language Processing and Machine Learning. Apress.\n\n  15. Santilal, U. (2020) Natural Language Processing: NLP & its History (Kindle edition). [Amazon.\u200bcom](http://amazon.com).\n\n  16. Scott, B. (2018) Translation, Brains and the Computer: A Neurolinguistic Solution to Ambiguity and Complexity in Machine Translation (Machine Translation: Technologies and Applications Book 2). Springer.\n\n  17. Sportier, D. et al. (2013) An Introduction to Syntactic Analysis and Theory. Wiley-Blackwell.\n\n  18. Tuchong (2020a) The Turing Test. [https://\u200bstock.\u200btuchong.\u200bcom/\u200bimage/\u200bdetail?\u200bimageId=\u200b9212246577423319\u200b26](https://stock.tuchong.com/image/detail?imageId=921224657742331926). Accessed 14 May 2022.\n\n  19. Tuchong (2020b) NLP and AI. [https://\u200bstock.\u200btuchong.\u200bcom/\u200bimage/\u200bdetail?\u200bimageId=\u200b1069700818174345\u200b308](https://stock.tuchong.com/image/detail?imageId=1069700818174345308). Accessed 14 May 2022.\n\n  20. Turing, A. (1936) On computable numbers, with an application to the Entscheidungs problem. In: Proc. London Mathematical Society, Series 2, 42:230\u201326\n\n  21. Turing, A. (1950) Computing Machinery and Intelligence. Mind, LIX (236): 433\u2013460.[MathSciNet](http://www.ams.org/mathscinet-getitem?mr=37064)[Crossref](https://doi.org/10.1093/mind/LIX.236.433)\n\n\n\u00a9 The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5df39e9a-5fdd-4eee-92a9-bbf3fe9802b2": {"__data__": {"id_": "5df39e9a-5fdd-4eee-92a9-bbf3fe9802b2", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "219d9685-fedd-49fb-ae25-91b470521aa2", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "03ad9d24e77ba38f1ce84a87dcd8bf0f1f84c046407df5d8dc7aaf4906127cef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb24882b-de0f-47d5-a14a-89930bfb4952", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "2cbe895377dd7ea1097fb1800a42bede58ee04cc15e67183583b1a770c9c23b3", "class_name": "RelatedNodeInfo"}}, "hash": "4c3a3febd4d029916432166a433011fb117a37d284a4083cef3fc74222bb17d4", "text": "Accessed 14 May 2022.\n\n  20. Turing, A. (1936) On computable numbers, with an application to the Entscheidungs problem. In: Proc. London Mathematical Society, Series 2, 42:230\u201326\n\n  21. Turing, A. (1950) Computing Machinery and Intelligence. Mind, LIX (236): 433\u2013460.[MathSciNet](http://www.ams.org/mathscinet-getitem?mr=37064)[Crossref](https://doi.org/10.1093/mind/LIX.236.433)\n\n\n\u00a9 The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.\n2024\n\nR. S. T. Lee Natural Language Processing\n<https://doi.org/10.1007/978-981-99-1999-4_2>\n\n# 2\\. N-Gram Language Model\n\nRaymond S. T. Lee 1\n\n(1)\n\nUnited International College, Beijing Normal University-Hong Kong Baptist\nUniversity, Zhuhai, China\n\n## 2.1 Introduction\n\nNLP entities like word-to-word tokenization using NTLK, spaCy technologies in\nWorkshop 1 (Chap. [10](533412_1_En_10_Chapter.xhtml)) analyzed words in\ninsolation, but the relationship between words is important in NLP. This\nchapter will focus on word sequences, its modeling and analysis.\n\nIn many NLP applications, there are noises and disruptions effecting incorrect\nwords pronunciation regularly in applications like speech recognition, text\nclassification, text generation, machine translation, Q&A chatbots, Q&A\nconversation machines or agents being used in auto-driving.\n\nHumans experience mental confusion about spelling errors as in Fig. 2.1 often\ncaused by pronunciations, typing speeds, and keystroke\u2019s location. They can be\ncorrected by looking up in a dictionary, a spell checker, and grammars usage.\n\n![](../images/533412_1_En_2_Chapter/533412_1_En_2_Fig1_HTML.png)\n\nFig. 2.1\n\nCommon spelling errors\n\nApplying _word prediction_ in a word sequence can provide automatic spell-\ncheck corrections, its corresponding concept terminology can model words\nrelationships, estimate occurrence frequency to generate new texts with\nclassification, and apply in machine translation to correct errors.\n\nProbability or word counting method can work on a large databank called\n_corpus_ (Pustejovsky and Stubbs 2012) which can be the collection of text\ndocuments, literatures, public speeches, conversations, and other online\ncomments/opinions.\n\nA text highlights spelling and grammatic errors in yellow and blue colors is\nshown in Fig. 2.2. This method can calculate words probabilities occurrence\nfrequency to provide substitution of higher frequency probability but cannot\nalways present accurate options.\n\n![](../images/533412_1_En_2_Chapter/533412_1_En_2_Fig2_HTML.png)\n\nFig. 2.2\n\nSpelling and grammar checking tools\n\nFigure 2.3 illustrates a simple scenario of _next_ _word prediction_ in sample\nutterances _I like photography_ , _I like science,_ and _I love mathematics_.\nThe probability of _I like_ is _0.67 (2/3)_ compared with _I love_ is _0.33\n(1/3)_ , the probability of _like photography_ and _like science_ is similar\nat _0.5 (1/2)_. Assigning probability to scenarios, _I like photography_ and\n_I like science_ are both _0.67 \u00d7 0.5 = 0.335_ , and _I love mathematics_ is\n_0.33 \u00d7 1 = 0.33_.\n\n![](../images/533412_1_En_2_Chapter/533412_1_En_2_Fig3_HTML.png)\n\nFig. 2.3\n\nNext word prediction in simple utterances\n\nWhen applying probability on language models, it must always note (1) domain\nspecific verity of keywords togetherness and terminology knowledge varies\naccording to domains, e.g. medical science, AI, etc., (2) syntactic knowledge\nattributes to syntax, lexical knowledge, and (3) common sense or world\nknowledge attributes to the collection of habitual behaviors from past\nexperiences, and (4) languages usage significance in high-level NLP.\n\nWhen applying probability on words prediction in an utterance, there are words\noften proposed by _rank_ and _frequency_ to provide a sequential optimum\nestimation.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb24882b-de0f-47d5-a14a-89930bfb4952": {"__data__": {"id_": "fb24882b-de0f-47d5-a14a-89930bfb4952", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5df39e9a-5fdd-4eee-92a9-bbf3fe9802b2", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "4c3a3febd4d029916432166a433011fb117a37d284a4083cef3fc74222bb17d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c758939b-e72c-41b7-a48e-63e7efdee069", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "304ab9bac6f83cfd6e7eba7627d4d378acfe0158688e5cee0a585ca32969a3cf", "class_name": "RelatedNodeInfo"}}, "hash": "2cbe895377dd7ea1097fb1800a42bede58ee04cc15e67183583b1a770c9c23b3", "text": "![](../images/533412_1_En_2_Chapter/533412_1_En_2_Fig3_HTML.png)\n\nFig. 2.3\n\nNext word prediction in simple utterances\n\nWhen applying probability on language models, it must always note (1) domain\nspecific verity of keywords togetherness and terminology knowledge varies\naccording to domains, e.g. medical science, AI, etc., (2) syntactic knowledge\nattributes to syntax, lexical knowledge, and (3) common sense or world\nknowledge attributes to the collection of habitual behaviors from past\nexperiences, and (4) languages usage significance in high-level NLP.\n\nWhen applying probability on words prediction in an utterance, there are words\noften proposed by _rank_ and _frequency_ to provide a sequential optimum\nestimation.\n\nFor example:\n\n  * [2.1] I notice three children standing on the ??? (ground, bench \u2026)\n\n  * [2.2] I just bought some oranges from the ??? (supermarket, shop \u2026)\n\n  * [2.3] She stopped the car and then opened the ??? (door, window, \u2026)\n\nThe structure of [2.3] is perplexed because word counting method with a\nsizeable knowledge domain is adequate but _common sense_ , _world knowledge,_\nor specific _domain knowledge_ are among the sources. It involves scenario\nsyntactic knowledge that attributes to do something with superior level at\nscene such as descriptive knowledge to help the guesswork. Although it is\nplain and mundane to study preceding and words tracking but it is one the most\nuseful techniques on words prediction. Let us begin with some simple word\ncounting methods in NLP, the _N-gram_ _language model_.\n\n## 2.2 N-Gram Language Model\n\nIt was learnt that the motivations on words prediction can apply to voice\nrecognition, text generation, and Q&A chatbot. _N-gram_ _language model_ ,\nalso called _N-gram model_ or _N-gram_ (Sidorov 2019; Liu et al. 2020) is a\nfundamental method to formalize words prediction using probability\ncalculation. N-gram is statistical model that consists of word sequence in _N_\n-number, commonly used N-grams include:\n\n  * _Unigram_ refers to a single word, i.e. _N_ = 1. It is seldomly used in practice because it contains only one word in N-gram. However, it is important to serve as the base for higher order N-gram probability normalization.\n\n  * _Bigram_ refers to a collection of two words, i.e. _N_ = 2. For example: _I have, I do, he thinks, she knows, etc_. It is used in many applications because its occurrence frequency is high and easy to count.\n\n  * _Trigram_ refers to a collection of three words, i.e. _N_ = 3. For example: _I noticed that, noticed three children, children standing on, standing on the_. It is useful because it contains more meanings and not lengthy. Given a count knowledge of first three words can easily guess the next word in a sequence. However, its occurrence frequency is low in a moderate corpus.\n\n  * _Quadrigram_ refers to a collection of four words, i.e. _N_ = 4. For example: _I noticed that three, noticed that three children, three children standing on_. It is useful with literatures or large corpus like Brown Corpus because of their extensive words\u2019 combinations.\n\nA sizeable N-gram can present more central knowledge but can pose a problem.\nIf is too large, it means that probability and occurrence of word sequence is\ninfrequent and even 0 in terms of probability counts.\n\nCorpus volume and other factors also affect performance. N-gram model training\nis based on an extensive _knowledge base (KB)_ or _databank_ from specific\ndomains such as public speeches, literatures, topic articles like news,\nfinance, medical, science, or chat messages from social media platforms.\nHence, a moderate N-gram is the balance by _frequency_ and _proportions_.\n\nThe knowledge of counts acquired by a N-gram can assess to conditional\nprobability of candidate words as the next word in a sequence, e.g. _It is not\ndifficult.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c758939b-e72c-41b7-a48e-63e7efdee069": {"__data__": {"id_": "c758939b-e72c-41b7-a48e-63e7efdee069", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb24882b-de0f-47d5-a14a-89930bfb4952", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "2cbe895377dd7ea1097fb1800a42bede58ee04cc15e67183583b1a770c9c23b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6121ac62-6174-4244-825e-4f6c29e43a00", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "ac6e7af74363068e52d9594500ba90f14c07f5386c7c9bfa4f0c358a655145c3", "class_name": "RelatedNodeInfo"}}, "hash": "304ab9bac6f83cfd6e7eba7627d4d378acfe0158688e5cee0a585ca32969a3cf", "text": "It is useful with literatures or large corpus like Brown Corpus because of their extensive words\u2019 combinations.\n\nA sizeable N-gram can present more central knowledge but can pose a problem.\nIf is too large, it means that probability and occurrence of word sequence is\ninfrequent and even 0 in terms of probability counts.\n\nCorpus volume and other factors also affect performance. N-gram model training\nis based on an extensive _knowledge base (KB)_ or _databank_ from specific\ndomains such as public speeches, literatures, topic articles like news,\nfinance, medical, science, or chat messages from social media platforms.\nHence, a moderate N-gram is the balance by _frequency_ and _proportions_.\n\nThe knowledge of counts acquired by a N-gram can assess to conditional\nprobability of candidate words as the next word in a sequence, e.g. _It is not\ndifficult. It is_ a bigram which means to count the occurrence of _is_ given\nthat _it_ has already mentioned from a large corpus, or the conditional\nprobability of _it is_ given that _it_ has already mentioned or can apply it\none by one to calculate the conditional probability of an entire words\nsequence. It is like words and sentences formation of day-to-day conversations\nwhich is a psychological interpretation in logical thinking. N-gram progresses\nin this orderly fashion.\n\nIt serves to rank the likelihood of a sequence consisting of various\nalternative hypotheses in a sentence/utterance for application like automatic\nspeech recognition (ASR), e.g _. [2.4] The cinema staff told me that\npopcorn/amelcorn sales have doubled._ It is understood that it refers to\n_popcorn_ and not _amelcorn_ because the concept of _popcorn_ is always\nattributed to conversations about cinema. Since the occurrence of _popcorn_ in\na sentence/utterance has a higher rank than _amelcorn_ , it is natural to\nselect _popcorn_ as the best answer.\n\nAnother purpose is to assess the likelihood of a sentence/utterance for text\ngeneration or machine translation, e.g. _[2.5] The doctor recommended a cat\nscan to the patient_. It may be difficult to understand what a _cat scan_ is\nor how can a scan be related to a _cat_ without any domain knowledge. Since\nthe occurrence of doctor is attributed to medical domains, it is natural to\nsearch articles, literatures, websites about medical knowledge to learn that\n_CAT scan_ refers to a _computerized axial tomography scanner_ as in Fig. 2.4\ninstead of _a cat_. This type of words prediction is often domain specific\nassociated with the preceding word as guidance to select an appropriate\nexpression.\n\n![](../images/533412_1_En_2_Chapter/533412_1_En_2_Fig4_HTML.jpg)\n\nFig. 2.4\n\nComputerized axial tomography scanner (aka. _CAT scan_ ) (Tuchong 2022)\n\n### 2.2.1 Basic NLP Terminology\n\nHere is a list of common terminologies in NLP (Jurafsky et al. 1999;\nEisenstein 2019):\n\n  * _Sentence_ is a unit of written language. It is a basic entity in a conversation or utterance.\n\n  * _Utterance_ is a unit of spoken language. Different from the concept of _sentence_ , _utterance_ is usually domain and culture specific which means it varies according to countries and even within country.\n\n  * _Word Form_ is an inflected form occurs in a corpus. It is another basic entity in a corpus.\n\n  * _Types/Word Types_ are distinct words in a corpus.\n\n  * _Tokens_ are generic entities or objects of a passage. It is different from word form as tokens can be meaningful words or symbols, punctuations, simple and distinct character(s).\n\n  * _Stem_ is a root form of words. _Stemming_ is the process of reducing inflected, or derived words from their word stem.\n\n  * _Lemma_ is an abstract form shared by word forms in the same stem, part of speech, and word sense. _Lemmatization_ is the process of grouping together the inflected forms of a word so that they can be analyzed as a single item which can be identified by the word\u2019s lemma or dictionary form.\n\nAn example to demonstrate meaning representations between _lemma_ and _stem_\nis shown in Fig. 2.5. _Lemmatization_ is the abstract form to generate a\nconcept.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6121ac62-6174-4244-825e-4f6c29e43a00": {"__data__": {"id_": "6121ac62-6174-4244-825e-4f6c29e43a00", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c758939b-e72c-41b7-a48e-63e7efdee069", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "304ab9bac6f83cfd6e7eba7627d4d378acfe0158688e5cee0a585ca32969a3cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "732f037c-2f31-4f4b-911a-6aece2dd565d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "698be9c12f53e9b2a0626b03594b8e0edfdff927a6f844f942d3df573aa049c6", "class_name": "RelatedNodeInfo"}}, "hash": "ac6e7af74363068e52d9594500ba90f14c07f5386c7c9bfa4f0c358a655145c3", "text": "* _Types/Word Types_ are distinct words in a corpus.\n\n  * _Tokens_ are generic entities or objects of a passage. It is different from word form as tokens can be meaningful words or symbols, punctuations, simple and distinct character(s).\n\n  * _Stem_ is a root form of words. _Stemming_ is the process of reducing inflected, or derived words from their word stem.\n\n  * _Lemma_ is an abstract form shared by word forms in the same stem, part of speech, and word sense. _Lemmatization_ is the process of grouping together the inflected forms of a word so that they can be analyzed as a single item which can be identified by the word\u2019s lemma or dictionary form.\n\nAn example to demonstrate meaning representations between _lemma_ and _stem_\nis shown in Fig. 2.5. _Lemmatization_ is the abstract form to generate a\nconcept. It indicated that _stem_ or _root word_ can be a meaningful word, or\nmeaningless, or a symbol such as _inform_ or _comput_ to formulate meaningful\nwords such as _information_ , _informative_ , _computer,_ or _computers_.\n\n![](../images/533412_1_En_2_Chapter/533412_1_En_2_Fig5_HTML.png)\n\nFig. 2.5\n\nStemming vs. lemmatization\n\nThere are several corpora frequently used in NLP applications.\n\n_Google_ (2022) is the largest corpus as it contains words and texts from its\nsearch engine and the internet. It has over trillion English tokens with over\nmillion meaningful wordform types sufficient to generate sentences/utterances\nfor daily use.\n\n_Brown Corpus_ (2022) is an important and well-known corpus because it is the\nfirst well-organized corpus in human history founded by Brown University from\n1961 with continuous updates. At present, it has over 583 million tokens,\n293,181 wordform types and words in foreign languages. It is one of the most\ncomprehensive corpora for daily use, and a KB used in many N-grams, related\nNLP models and applications.\n\nFurther, there are many domain specific corpora such as _Wall Street Journal_\nis one of earliest corpora to discover knowledge from financial news,\n_Associated Press_ focus on news and world events, _Hansard_ is a prominent\n_Corpus of British Parliament speeches_ , _Boston University Radio News_\n_corpus_ , _NLTK Corpora Library_ and others etc. (Bird et al. 2009;\nEisenstein 2019; Pustejovsky and Stubbs 2012).\n\nLet us return to words prediction. A _language model_ is a traditional word\ncounting model to count and calculate conditional probability to predict the\nprobability based on a word sequence, e.g. when applying to utterance _it is\ndifficult to\u2026 that_ with a sizeable corpus like _Brown Corpus_ , the\ntraditional word counting method may suggest either _say/tell/guess_ based on\noccurrence frequency. This traditional language model terminology is applied\nto predictions and forecasts at advanced computer systems and research in\nspecialized deep networks and models in AI. Although there has been a\ntechnology shift, statistical model is always the fundamental model in many\ncases (Jurafsky et al. 1999; Eisenstein 2019).\n\n### 2.2.2 Language Modeling and Chain Rule\n\nConditional probability calculation is to study the definition of conditional\nprobabilities and look for counts, given by\n\n![$$ P\\\\left\\(A|B\\\\right\\)=\\\\frac{P\\\\left\\(A\\\\cap B\\\\right\\)}{P\\(B\\)}\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equ1.png)\n\n(2.1)\n\nFor example, to evaluate conditional probability: _The garden is so beautiful\nthat_ given by the word sequence \u201c _The garden is so beautiful\u201d_ will be\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "732f037c-2f31-4f4b-911a-6aece2dd565d": {"__data__": {"id_": "732f037c-2f31-4f4b-911a-6aece2dd565d", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6121ac62-6174-4244-825e-4f6c29e43a00", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "ac6e7af74363068e52d9594500ba90f14c07f5386c7c9bfa4f0c358a655145c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5e7e4a3-93e3-4d4b-a375-555a6edef2df", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "31fc357c6631440cc2d35ede1d5db5ad56c0bb19554f940fda88eede04b91c21", "class_name": "RelatedNodeInfo"}}, "hash": "698be9c12f53e9b2a0626b03594b8e0edfdff927a6f844f942d3df573aa049c6", "text": "Although there has been a\ntechnology shift, statistical model is always the fundamental model in many\ncases (Jurafsky et al. 1999; Eisenstein 2019).\n\n### 2.2.2 Language Modeling and Chain Rule\n\nConditional probability calculation is to study the definition of conditional\nprobabilities and look for counts, given by\n\n![$$ P\\\\left\\(A|B\\\\right\\)=\\\\frac{P\\\\left\\(A\\\\cap B\\\\right\\)}{P\\(B\\)}\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equ1.png)\n\n(2.1)\n\nFor example, to evaluate conditional probability: _The garden is so beautiful\nthat_ given by the word sequence \u201c _The garden is so beautiful\u201d_ will be\n\n![$$ {\\\\displaystyle \\\\begin{array}{ll}P\\\\left\\(\\\\mathrm{that}|\\\\mathrm{The}\\\\\n\\\\mathrm{garden}\\\\\n\\\\mathrm{is}\\\\;\\\\mathrm{so}\\\\;\\\\mathrm{beautiful}\\\\right\\)&amp;\n=\\\\frac{P\\\\left\\(\\\\mathrm{The}\\\\ \\\\mathrm{garden}\\\\\n\\\\mathrm{is}\\\\;\\\\mathrm{so}\\\\;\\\\mathrm{beautiful}\\\\\n\\\\mathrm{that}\\\\right\\)}{P\\\\left\\(\\\\mathrm{The}\\\\ \\\\mathrm{garden}\\\\\n\\\\mathrm{is}\\\\;\\\\mathrm{so}\\\\;\\\\mathrm{beautiful}\\\\right\\)}\\\\\\\\ {}&amp;\n=\\\\frac{\\\\mathrm{Count}\\\\left\\(\\\\mathrm{The}\\\\ \\\\mathrm{garden}\\\\\n\\\\mathrm{is}\\\\;\\\\mathrm{so}\\\\;\\\\mathrm{beautiful}\\\\\n\\\\mathrm{that}\\\\right\\)}{\\\\mathrm{Count}\\\\left\\(\\\\mathrm{The}\\\\\n\\\\mathrm{garden}\\\\\n\\\\mathrm{is}\\\\;\\\\mathrm{so}\\\\;\\\\mathrm{beautiful}\\\\right\\)}\\\\end{array}}\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equ2.png)\n\n(2.2)\n\nAlthough the calculation is straightforward but if the corpus or text\ncollection is moderate, this conditional probability (counts) will probably be\nzero.\n\nChain rules of probability is useful as an independent assumption to rectify\nthis problem.\n\nBy rewriting the conditional probability equation (2.1), it will be\n\n![$$ P\\\\left\\(A\\\\cap B\\\\right\\)=P\\\\left\\(A|B\\\\right\\)P\\(B\\)\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equ3.png)\n\n(2.3)\n\nFor a sequence of events, _A_ , _B_ , _C_ and _D_ , the Chain Rule formulation\nwill become\n\n![$$\nP\\\\left\\(A,B,C,D\\\\right\\)=P\\(A\\)P\\\\left\\(B|A\\\\right\\)P\\\\left\\(C|A,B\\\\right\\)P\\\\left\\(D|,A|,B|,C\\\\right\\)\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equ4.png)\n\n(2.4)\n\nIn general:\n\n![$$ P\\\\left\\({x}_1,{x}_2,{x}_3,\\\\dots\n{x}_n\\\\right\\)=P\\\\left\\({x}_1\\\\right\\)P\\\\left\\({x}_2|{x}_1\\\\right\\)P\\\\left\\({x}_3|{x}_1,{x}_2\\\\right\\)\\\\dots\nP\\\\left\\({x}_n|{x}_1\\\\dots {x}_{n-1}\\\\right\\)\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equ5.png)\n\n(2.5)\n\nIf word sequence from position 1 to _n_ as  ![$$ {w}_1^n\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_IEq1.png) is\ndefined, the Chain Rule applied to word sequence will become\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5e7e4a3-93e3-4d4b-a375-555a6edef2df": {"__data__": {"id_": "b5e7e4a3-93e3-4d4b-a375-555a6edef2df", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "732f037c-2f31-4f4b-911a-6aece2dd565d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "698be9c12f53e9b2a0626b03594b8e0edfdff927a6f844f942d3df573aa049c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60271cf2-42b6-4201-8fa2-5900ba62713b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "97d1248dd2cc56e79d0eb95263099b15f879c5e21c7652f5a09e11da80a1fa4d", "class_name": "RelatedNodeInfo"}}, "hash": "31fc357c6631440cc2d35ede1d5db5ad56c0bb19554f940fda88eede04b91c21", "text": "[$$ {w}_1^n\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_IEq1.png) is\ndefined, the Chain Rule applied to word sequence will become\n\n![$$ {\\\\displaystyle \\\\begin{array}{ll}P\\\\left\\({w}_1^n\\\\right\\)&amp;\n=P\\\\left\\({w}_1\\\\right\\)P\\\\left\\({w}_2|{w}_1\\\\right\\)P\\\\left\\({w}_3|{w}_1^2\\\\right\\)\\\\dots\nP\\\\left\\({w}_n|{w}_1^{n-1}\\\\right\\)\\\\\\\\ {}&amp; =\\\\prod\n\\\\limits_{k=1}^nP\\\\left\\({w}_k|{w}_1^{k-1}\\\\right\\)\\\\end{array}}\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equ6.png)\n\n(2.6)\n\nSo, the conditional probability for previous example will be\n\n![$$ {\\\\displaystyle \\\\begin{array}{l}P\\\\left\\(\\\\mathrm{the}\\\\\n\\\\mathrm{garden}\\\\ \\\\mathrm{is}\\\\;\\\\mathrm{so}\\\\;\\\\mathrm{beautiful}\\\\\n\\\\mathrm{that}\\\\right\\)=P\\\\left\\(\\\\mathrm{the}\\\\right\\)\\\\ast\nP{\\\\left\\(\\\\mathrm{garden}|\\\\mathrm{the}\\\\right\\)}^{\\\\ast}\\\\\\\\ {}\\\\kern1.6em\nP\\\\left\\(\\\\mathrm{is}|\\\\mathrm{the}\\\\ \\\\mathrm{garden}\\\\right\\)\\\\ast\nP\\\\left\\(\\\\mathrm{so}|\\\\mathrm{the}\\\\ \\\\mathrm{garden}\\\\\n\\\\mathrm{is}\\\\right\\)\\\\ast P{\\\\left\\(\\\\mathrm{beautiful}|\\\\mathrm{the}\\\\\n\\\\mathrm{garden}\\\\ \\\\mathrm{is}\\\\;\\\\mathrm{so}\\\\right\\)}^{\\\\ast}\\\\\\\\\n{}\\\\kern1.6em P\\\\left\\(\\\\mathrm{that}|\\\\mathrm{the}\\\\ \\\\mathrm{garden}\\\\\n\\\\mathrm{is}\\\\;\\\\mathrm{so}\\\\;\\\\mathrm{beautiful}\\\\right\\)\\\\end{array}}\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equ7.png)\n\n(2.7)\n\nNote: Normally, <s> and </s> are used to denote the start and end of\nsentence/utterance for better formulation.\n\nThis method seems fair and easy to understand but poses two major problems.\nFirst, it is unlikely to gather the right statistics for prefixes which means\nthat not knowing the starting point of the sentence. Second, the calculation\nfor word sequence probability is mundane. If it is a long sentence,\nconditional probability at the end of this equation is complex to calculate.\n\nLet us explore how genius _Markov Chain_ is applied to solve this problem.\n\n## 2.3 Markov Chain in N-Gram Model\n\nProf. Andrey Andrevevich Markov (1856\u20131922) is a renowned Russian\nmathematician and academician who made significant contribution to science\nstudying the theory of probability. His primary contribution called _Markov\nchains_ or _Markov process_ had applied to biology, chemistry, computer\nscience, and statistics (Ching et al. 2013). _Markov chains theory_ can be\napplied to speech recognitions, N-gram language model, internet ranking,\ninformation, and queueing theories (Eisenstein 2019). There is a single\ndimension domain Markov chain modeling called _Hidden_ _Markov Chain_ in\nhandwritten characters and human voice recognitions. This model has an\nimportant concept called _Markov Assumption_ which assumes the entire prefix\nhistory is not necessary, in other words, an event does not depend on its\nwhole history; it is only a fixed length nearby history is the essence of\n_Markov chain theory_.\n\n_Markov chain_ is a stochastic process to describe a sequence of possible\nevents which the probability of each event depends only on the state attained\nin previous event. There are many kinds of _Markov chain_ conventions. An\nimportant convention called _descriptive Markov chain_ is shown in Fig. 2.6.\nIt revealed that an event of Markov chain can be a list of relationship of\nevery single event. Another concept is that the previous state is important\nbut not all previous sequences.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60271cf2-42b6-4201-8fa2-5900ba62713b": {"__data__": {"id_": "60271cf2-42b6-4201-8fa2-5900ba62713b", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5e7e4a3-93e3-4d4b-a375-555a6edef2df", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "31fc357c6631440cc2d35ede1d5db5ad56c0bb19554f940fda88eede04b91c21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b02307a-749a-448b-9fee-a287c51518f9", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8cd7ae7af906f59c3a3ea032bb78663f4a39d8d374c4e4ebd00e7f3010d580d2", "class_name": "RelatedNodeInfo"}}, "hash": "97d1248dd2cc56e79d0eb95263099b15f879c5e21c7652f5a09e11da80a1fa4d", "text": "This model has an\nimportant concept called _Markov Assumption_ which assumes the entire prefix\nhistory is not necessary, in other words, an event does not depend on its\nwhole history; it is only a fixed length nearby history is the essence of\n_Markov chain theory_.\n\n_Markov chain_ is a stochastic process to describe a sequence of possible\nevents which the probability of each event depends only on the state attained\nin previous event. There are many kinds of _Markov chain_ conventions. An\nimportant convention called _descriptive Markov chain_ is shown in Fig. 2.6.\nIt revealed that an event of Markov chain can be a list of relationship of\nevery single event. Another concept is that the previous state is important\nbut not all previous sequences. Hence, this model can apply in thermodynamics,\nstatistical mechanics, physics, chemistry, economy, finance, information\ntheory, and NLP. A complete _Markov Chain_ _event_ is like a conversation in a\nsentence/utterance, each word is equivalent to an object in _Markov chain_.\nAlthough the whole chain of conditional probability can be calculated but the\nlast event is the most important one.\n\n![](../images/533412_1_En_2_Chapter/533412_1_En_2_Fig6_HTML.png)\n\nFig. 2.6\n\nMarkov chain model\n\nBy applying _Markov chain model_ , the conditional probability for N-gram\nprobability of a word sequence  ![$$ {w}_1^n\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_IEq2.png) will\nbe approximated by (assuming a prefix of _N_ words)\n\n![$$ P\\\\left\\({w}_n|{w}_1^{n-1}\\\\right\\)\\\\approx\nP\\\\left\\({w}_n|{w}_{n-N+1}^{n-1}\\\\right\\)\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equ8.png)\n\n(2.8)\n\nIn general:\n\n![$$ P\\\\left\\({w}_1^n\\\\right\\)\\\\approx \\\\prod\n\\\\limits_{k=1}^nP\\\\left\\({w}_k|{w}_{k-1}\\\\right\\)\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equ9.png)\n\n(2.9)\n\nIn other words, the original complex conditional probability of a word\nsequence stated in Eq. (2.6) can be easily evaluated by a sequence of bigram\nprobability calculations.\n\nLet us look at an N-gram example _The big white cat._ Unigram probability is\n_P_ (cat). Bigram probability is the _P_ (cat | white). Trigram probability is\n_P_ (cat | big white) _= P_ (white | big) _* P_ (cat _|_ white) and quadrigram\nprobability is _P_ (cat | the big white) _= P_ (big | the) _* P_ (white | big)\n_* P_ (cat | white). All can be easily evaluated by simple sequence of bigram\nprobability multiplications applying Eq. (2.9).\n\nHowever, it is cautious to note that the probability of a word formulation\ngiven fixed prefixes may not always appropriate in many cases. They may be\nverifiable events in real-time speeches as words uttered are often correlated\nto the previous but in cases with pragmatic or embedded meanings at both right\nand left contexts, there is no priori reason adhered to left contexts.\n\n## 2.4 Live Example: The Adventures of Sherlock Holmes\n\nN-gram probability calculation usually comes from a training _corpus_ or\n_knowledge base (KB)_ in two extremes. One is an overly narrow corpus and the\nother one is an overly general corpus. An overly narrow corpus is a\nrestricted, specific corpus, can be domain specific on a particular knowledge\nwith significant counts to be found during conditional probability counting.\nAn overly general corpus cannot reflect a specific domain but counting can\nalways be found. Hence, a balance between the two dimensions is required.\nAnother consideration is a separate text corpus applied to evaluate in\nstandard metrics called _held-out_ test set, or _development_ test set.\nFurther, cross validations and results tested for statistical significance are\nalso required.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b02307a-749a-448b-9fee-a287c51518f9": {"__data__": {"id_": "4b02307a-749a-448b-9fee-a287c51518f9", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60271cf2-42b6-4201-8fa2-5900ba62713b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "97d1248dd2cc56e79d0eb95263099b15f879c5e21c7652f5a09e11da80a1fa4d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24b2c9d8-9a7b-439c-9e3d-0881b3d444f5", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "a4dbc5a0770efb73d198aa36ac548b2809a5e1ea1ffb5a6a7e66ab8fb4aab78f", "class_name": "RelatedNodeInfo"}}, "hash": "8cd7ae7af906f59c3a3ea032bb78663f4a39d8d374c4e4ebd00e7f3010d580d2", "text": "## 2.4 Live Example: The Adventures of Sherlock Holmes\n\nN-gram probability calculation usually comes from a training _corpus_ or\n_knowledge base (KB)_ in two extremes. One is an overly narrow corpus and the\nother one is an overly general corpus. An overly narrow corpus is a\nrestricted, specific corpus, can be domain specific on a particular knowledge\nwith significant counts to be found during conditional probability counting.\nAn overly general corpus cannot reflect a specific domain but counting can\nalways be found. Hence, a balance between the two dimensions is required.\nAnother consideration is a separate text corpus applied to evaluate in\nstandard metrics called _held-out_ test set, or _development_ test set.\nFurther, cross validations and results tested for statistical significance are\nalso required.\n\nLet us begin with a corpus came from _Project Gutenberg_ website (Gutenberg\n2022) on _The Adventures of_ _Sherlock Holmes_ (Doyle 2019), a famous\nliterature by writer and physician Sir Arthur Canon Doyle (1859\u20131930).\nGutenberg is a website consisting of primarily copyrights clearance, free\naccess, and download western cultural tradition literatures available to\npublic. This literature has 12 outstanding detective stories of _Sherlock\nHolmes_ ranging from _A scandal in Bohemia_ to _The Adventure of the Copper\nBeeches_ with other statistics below. It is a domain specific corpus with\ncomprehensive detective knowledge to form a meaningful KB and perform N-gram\nmodeling in NLP.\n\n  * No. of pages:424\n\n  * No. of characters (exclude spaces):470,119\n\n  * No. of words:110,087\n\n  * No. of tokens:113,749\n\n  * No. of sentences:6830\n\n  * No. of word types ( _V_ ):9886\n\nN-gram modeling in this example is to analyze an influential quote of Sherlock\nHolmes: _I have no doubt that I \u2026_. This quote does not occur often in other\nliteratures but because it is a detective story, the character has unique\naptitude for deducing hypotheses and notions to solve cases. Applying _Markov\nchain model_ can avoid mundane conditional probability, the N-gram probability\nis given by\n\n![$$ {\\\\displaystyle\n\\\\begin{array}{l}P\\\\left\\(\\\\mathrm{I}\\\\;\\\\mathrm{have}\\\\;\\\\mathrm{no}\\\\;\\\\mathrm{doubt}\\\\\n\\\\mathrm{that}\\\\;\\\\mathrm{I}\\\\right\\)=P\\\\left\\(\\\\mathrm{I}|\\\\mathrm{s}\\\\right\\)\\\\ast\nP\\\\left\\(\\\\mathrm{have}|\\\\mathrm{I}\\\\right\\)\\\\ast\nP{\\\\left\\(\\\\mathrm{no}|\\\\mathrm{hav}e\\\\right\\)}^{\\\\ast}\\\\\\\\ {}\\\\kern2.5em\nP\\\\left\\(\\\\mathrm{doubt}|\\\\mathrm{no}\\\\right\\)\\\\ast\nP\\\\left\\(\\\\mathrm{that}|\\\\mathrm{doubt}\\\\right\\)\\\\ast\nP\\\\left\\(\\\\mathrm{I}|\\\\mathrm{that}\\\\right\\)\\\\end{array}}\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equ10.png)\n\n(2.10)\n\nUnigram checking on word counting for _I have no doubt_ is necessary as basis\nto calculate the conditional probability for all bigrams as shown in Fig. 2.7.\nSo, given the unigram count of _I_ is 2755, the bigram probability of _I have_\napplying Markov chain method will be 288/2755, which is 0.105 as shown in Fig.\n2.8. It is a list of all related bigram counts and probabilities for a given\nbigram such as _I have, I had, I am, I was, I knew, I hear, I don\u2019t_ up to _I\nshould_ which are common words found in many literatures. The probability also\nshowed _I have_ is the most frequent with 0.105 which means that _I have no\ndoubt that_ is quoted by the character regularly. The occurrence of _I think_\nis high and general phrases such as _I have_ , _I had_.\n\n![](../images/533412_1_En_2_Chapter/533412_1_En_2_Fig7_HTML.png)\n\nFig. 2.7\n\nUnigram counts for words \u201c _I have no doubt that\u201d_ from The Adventures of\nSherlock Holmes\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24b2c9d8-9a7b-439c-9e3d-0881b3d444f5": {"__data__": {"id_": "24b2c9d8-9a7b-439c-9e3d-0881b3d444f5", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b02307a-749a-448b-9fee-a287c51518f9", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8cd7ae7af906f59c3a3ea032bb78663f4a39d8d374c4e4ebd00e7f3010d580d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a09e5836-dc3d-4176-9c21-778a391a9e6f", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "62ec3535905c26fcf61f2ecdf50c4347732d16145c4ded4532a3c48d006de17d", "class_name": "RelatedNodeInfo"}}, "hash": "a4dbc5a0770efb73d198aa36ac548b2809a5e1ea1ffb5a6a7e66ab8fb4aab78f", "text": "2.8. It is a list of all related bigram counts and probabilities for a given\nbigram such as _I have, I had, I am, I was, I knew, I hear, I don\u2019t_ up to _I\nshould_ which are common words found in many literatures. The probability also\nshowed _I have_ is the most frequent with 0.105 which means that _I have no\ndoubt that_ is quoted by the character regularly. The occurrence of _I think_\nis high and general phrases such as _I have_ , _I had_.\n\n![](../images/533412_1_En_2_Chapter/533412_1_En_2_Fig7_HTML.png)\n\nFig. 2.7\n\nUnigram counts for words \u201c _I have no doubt that\u201d_ from The Adventures of\nSherlock Holmes\n\n![](../images/533412_1_En_2_Chapter/533412_1_En_2_Fig8_HTML.png)\n\nFig. 2.8\n\nBigram grammar fragment from The Adventures of Sherlock Holmes\n\nA bigram grammar fragments related to _I have no doubt that \u2026_ is shown in\nFig. 2.9 for the counting and probability occurrence frequency beginning with\n_< s>I , <s>I\u2019d, <s>The, <s>It, I have, I had, I can, have no, have to, have\nbeen_ to compare with several versions or combinations related to, _I have no\ndoubt that_ means to compare occurrence frequency of _I have_ with _I had_ or\n_I can,_ which is similar to compare occurrence of _no doubt, no sign,_ and\n_no harm_ or _that I, that he, that she, that it_. It is noted that the\noccurrence of _I have no doubt that_ is high and distinct in this literature.\n\n![](../images/533412_1_En_2_Chapter/533412_1_En_2_Fig9_HTML.png)\n\nFig. 2.9\n\nBigram grammar fragment related to utterance _\u201cI have no doubt that I\u201d_ from\nThe Adventures of Sherlock Holmes\n\nCounting all conditional bigram probabilities based on unigram count in Fig.\n2.7 showed _I have no doubt that_ for _I_ is at 0.138 which is very high, but\nit is interested to note that _no doubt_ is even higher at 0.167 but again\nsince it is a detective story with a restricted domain, _doubt that_ is very\nhigh at 0.202 because the character always involves guesswork and frequent\ngrammar usage. Further, the probability of bigram _that I_ is much higher than\nother combination like _that he, that she,_ and _that it_. The occurrence\nfrequency in other literatures is much lower but because the character is a\nself-assured and intelligent expert, so he said _that I_ is more often than\n_that he_ or _that she._ That is the significance of a domain specific\nKB/corpus to check for N-gram probability.\n\nSo, let us look at some N-gram probabilities calculation, e.g. the probability\nof _P_ (I have no doubt that I) given by Eq. (2.10) :\n\n![$$ {\\\\displaystyle\n\\\\begin{array}{ll}P\\\\left\\(\\\\mathrm{I}\\\\;\\\\mathrm{have}\\\\;\\\\mathrm{no}\\\\;\\\\mathrm{doubt}\\\\\n\\\\mathrm{that}\\\\;\\\\mathrm{I}\\\\right\\)&amp; =0.138\\\\times 0.105\\\\times\n0.040\\\\times 0.167\\\\times 0.202\\\\times 0.129\\\\\\\\ {}&amp;\n=0.000002526\\\\end{array}}\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equa.png)\n\nIt is compared with _P_ (I have no doubt that he):\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a09e5836-dc3d-4176-9c21-778a391a9e6f": {"__data__": {"id_": "a09e5836-dc3d-4176-9c21-778a391a9e6f", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24b2c9d8-9a7b-439c-9e3d-0881b3d444f5", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "a4dbc5a0770efb73d198aa36ac548b2809a5e1ea1ffb5a6a7e66ab8fb4aab78f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd6ea1ce-514a-451a-8d45-df160c663ea3", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "520cd6d6c2577ccaa0b274d410cb7fb7f97631b4b456e70498c87a367d69d74f", "class_name": "RelatedNodeInfo"}}, "hash": "62ec3535905c26fcf61f2ecdf50c4347732d16145c4ded4532a3c48d006de17d", "text": "the probability\nof _P_ (I have no doubt that I) given by Eq. (2.10) :\n\n![$$ {\\\\displaystyle\n\\\\begin{array}{ll}P\\\\left\\(\\\\mathrm{I}\\\\;\\\\mathrm{have}\\\\;\\\\mathrm{no}\\\\;\\\\mathrm{doubt}\\\\\n\\\\mathrm{that}\\\\;\\\\mathrm{I}\\\\right\\)&amp; =0.138\\\\times 0.105\\\\times\n0.040\\\\times 0.167\\\\times 0.202\\\\times 0.129\\\\\\\\ {}&amp;\n=0.000002526\\\\end{array}}\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equa.png)\n\nIt is compared with _P_ (I have no doubt that he):\n\n![$$ {\\\\displaystyle\n\\\\begin{array}{ll}P\\\\left\\(\\\\mathrm{I}\\\\;\\\\mathrm{have}\\\\;\\\\mathrm{no}\\\\;\\\\mathrm{doubt}\\\\\n\\\\mathrm{that}\\\\;\\\\mathrm{he}\\\\right\\)&amp; =0.138\\\\times 0.105\\\\times\n0.040\\\\times 0.167\\\\times 0.202\\\\times 0.079\\\\\\\\ {}&amp;\n=0.000001540\\\\end{array}}\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equb.png)\n\nThis example test results led to several observations. It is noted that all\nthese probabilities are limited in general. Conditional probability is limited\nin a long sentence and required for _Markov chain_. If applying traditional\nmethod on conditional probability with complex calculation, most of the time\nthe probability is diminished. Further, the probability seems to capture both\n_syntactic facts_ and _world knowledge_. Although _that I_ or _that he_ are\noften used in English grammar, the probability in this literature _that I_ is\nmore frequent. Hence, it is related to both _syntactic usage_ , _common sense_\n, and _specific domain knowledge_. It depends on KB domains leading to diverse\nprobability calculation results.\n\nIt is also noted that most of the conditional probabilities are limited\nbecause the multiplication of all probability calculation in a long sentence\nbecomes diminished, so it is important to apply Markov chain and convert\ncomplex conditional probabilities into bigram probabilities. Although the\noccurrence of bigram is infrequent but still exists. Nevertheless, if it is\nnot sizeable KB or corpus, most of the bigrams will be 0. Hence, the selection\nfor corpus KB or corpus is important. An effective N-gram is related to word\ncounting, conditional probabilities calculation, and normalization.\n\nAnother observation is that it showed all these conditional probabilities are\nlimited and underflows as mentioned. A method is to convert them into _natural\nlog_. By applying _natural log_ will become additions to calculate conditional\nprobability with _Markov chain_ operation.\n\n_Maximum Likelihood Estimates_ _(MLE)_ is another principal method to\ncalculate N-gram model. They are parameters of a model M from a training set\nT. It is the estimate that maximizes the likelihood of training set T given\nthe model M. Suppose the word _language,_ for example, occurred 380 times in a\ncorpus with a million words, e.g. Brown corpus, the probability of a random\nword from other text forms with the same distribution will be _language_ ,\nwhich it will be 380/1,000,000 = 0.00038. This may be a poor estimate for\nother corpora but this type of calculation is domain specific as mentioned\nmeaning that the calculation varies according to different corpora.\n\nLet us return to _The Adventures of_ _Sherlock Holmes_ _\u2019_ famous quote _I\nhave no doubt that_ example _._ This time the counting and probability\ncalculation of these words are tabulated as shown in Figs. 2.10 and 2.11,\nrespectively. It showed that _I have_ has the most occurrence frequency with\n288, _that I_ is the next with 228 occurrences, _no doubt_ with surprising\nhigh 46 occurrences, _doubt that_ is 17 followed by _that no_ and _so on_.\nAnother discovery is that most of the other combinations is 0. It is intuitive\nbecause they are not grammatically or syntactically possible, e.g.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd6ea1ce-514a-451a-8d45-df160c663ea3": {"__data__": {"id_": "dd6ea1ce-514a-451a-8d45-df160c663ea3", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a09e5836-dc3d-4176-9c21-778a391a9e6f", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "62ec3535905c26fcf61f2ecdf50c4347732d16145c4ded4532a3c48d006de17d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "528afe42-98b6-4296-ab83-b61d8db1d54e", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "7dc039d26f21f65ae5150e4c46eee7ffa7f0adca0727da374b2a44d086c0fbd7", "class_name": "RelatedNodeInfo"}}, "hash": "520cd6d6c2577ccaa0b274d410cb7fb7f97631b4b456e70498c87a367d69d74f", "text": "This may be a poor estimate for\nother corpora but this type of calculation is domain specific as mentioned\nmeaning that the calculation varies according to different corpora.\n\nLet us return to _The Adventures of_ _Sherlock Holmes_ _\u2019_ famous quote _I\nhave no doubt that_ example _._ This time the counting and probability\ncalculation of these words are tabulated as shown in Figs. 2.10 and 2.11,\nrespectively. It showed that _I have_ has the most occurrence frequency with\n288, _that I_ is the next with 228 occurrences, _no doubt_ with surprising\nhigh 46 occurrences, _doubt that_ is 17 followed by _that no_ and _so on_.\nAnother discovery is that most of the other combinations is 0. It is intuitive\nbecause they are not grammatically or syntactically possible, e.g. _no I_ or\n_I, I_ and many are infrequent in English usage.\n\n![](../images/533412_1_En_2_Chapter/533412_1_En_2_Fig10_HTML.png)\n\nFig. 2.10\n\nBigram counts for \u201c _I have no doubt that I\u201d_ in The Adventures of Sherlock\nHolmes\n\n![](../images/533412_1_En_2_Chapter/533412_1_En_2_Fig11_HTML.png)\n\nFig. 2.11\n\nBigram probability (normalized) for \u201c _I have no doubt that I_ \u201d in _The\nAdventures of_ _Sherlock Holmes_\n\nBigram normalization is achieved by the division of each bigram counts by\nappropriate unigram counts for _w_ _n_ \u2212 1. Here is the bigram normalization\nresult for _I have no doubt that,_ e.g. computing bigram probability of _no\ndoubt_ is the counting of _no doubt_ which is _46_ as shown in Fig. 2.10\nagainst the counting of _no_ which is _276_ as in Fig. 2.7 which becomes\n_46/276 = 0.167_. In fact, such bigram probability _P_ (no doubt) is much\nhigher than _P_ (I have) _= 0.105_ , which is infrequent in other corpora\nbecause not many corpora have frequency of _no doubt_ as compared with _I\nhave_ as _I have_ is common in English usage. Since it is a detective\nliterature and the character is an expert at his field, it is unsurprised to\nidentify the occurrence frequency of _no doubt_ is very high.\n\nThe overall bigram probability (normalized) findings are _I have_ is _0.105_ ,\n_no doubt_ is _0.167_ the highest, _that I_ is _0.129_ as shown in Fig. 2.11.\nThis is special because the occurrence frequency of _I_ is not high as\ncompared with _I have. doubt that_ is _0.202_ which is very high, and others\nare mostly _0_. These findings showed that, first, all conditional\nprobabilities are limited because of N-gram calculation characteristics comes\nfrom an extensive corpus. But it does not mean that there is no comparison. It\ncan be compared if they are not _0_. Second, 0s are meaningful as most of\nthese words\u2019 combinations are neither syntactically nor grammatically\nincorrect. Third, these conditional probabilities and MLE are domain specific,\nwhich may not be the same in other situations.\n\n## 2.5 Shannon\u2019s Method in N-Gram Model\n\n_Shannon\u2019s method_ (Jurafsky et al. 1999) is another important topic in N-gram\nmodel. Prof. Claude Shannon (1916\u20132001) is a renowned American mathematician,\nelectrical engineer, cryptographer, also known as the father of information\ntheory and a major founder of contemporary cryptography. He wrote his famous\nthesis at age 21, a master\u2019s degree student at MIT demonstrating Boolean\nalgebra electrical applications to construct any logical numerical\nrelationship with meaning. One of his most influential papers, _A mathematical\ntheory of communications_ (Shannon 1948) published in 1948 had defined a\nmathematical notion by which information could be quantified and delivered\nreliably over imperfect communication channels like phone lines or wireless\nconnections nowadays. His groundbreaking innovation had provided the tools for\nnetwork communications and internet technologies. This method showed that\nassigning probabilities to sentences are well but less informative for\nlanguage generation in NLP.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "528afe42-98b6-4296-ab83-b61d8db1d54e": {"__data__": {"id_": "528afe42-98b6-4296-ab83-b61d8db1d54e", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd6ea1ce-514a-451a-8d45-df160c663ea3", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "520cd6d6c2577ccaa0b274d410cb7fb7f97631b4b456e70498c87a367d69d74f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1122825-051d-422c-b717-4b077a1677fb", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "557d7a694af1c8371bc03a728c48f07ee93e3dc7dc6141b9ef990c367642e2f6", "class_name": "RelatedNodeInfo"}}, "hash": "7dc039d26f21f65ae5150e4c46eee7ffa7f0adca0727da374b2a44d086c0fbd7", "text": "1999) is another important topic in N-gram\nmodel. Prof. Claude Shannon (1916\u20132001) is a renowned American mathematician,\nelectrical engineer, cryptographer, also known as the father of information\ntheory and a major founder of contemporary cryptography. He wrote his famous\nthesis at age 21, a master\u2019s degree student at MIT demonstrating Boolean\nalgebra electrical applications to construct any logical numerical\nrelationship with meaning. One of his most influential papers, _A mathematical\ntheory of communications_ (Shannon 1948) published in 1948 had defined a\nmathematical notion by which information could be quantified and delivered\nreliably over imperfect communication channels like phone lines or wireless\nconnections nowadays. His groundbreaking innovation had provided the tools for\nnetwork communications and internet technologies. This method showed that\nassigning probabilities to sentences are well but less informative for\nlanguage generation in NLP. However, it has a more interesting task to turn it\naround by applying N-gram and its probabilities to generate random sentences\nlike human sentences by which the model is derived.\n\nThere are four steps of Shannon\u2019s Method for _language generation_ as shown in\nFig. 2.12:\n\n![](../images/533412_1_En_2_Chapter/533412_1_En_2_Fig12_HTML.png)\n\nFig. 2.12\n\nAlgorithm of Shannon\u2019s method for language generation\n\nAn example of four N-gram texts generation methods from _The Complete Works of\nShakespeare_ by William Shakespeare (1564\u20131616) (Shakespeare 2021) applying\nShannon\u2019s Method is shown in Fig. 2.13.\n\n![](../images/533412_1_En_2_Chapter/533412_1_En_2_Fig13_HTML.png)\n\nFig. 2.13\n\nSentence generation using Shannon method from \u201cThe Complete Works of\nShakespeare\u201d\n\nIn summary:\n\n  * _Unigram_ results showed that the four random sentences are almost meaningless because it used a single word to calculate probability that is mostly without relations.\n\n  * _Bigram_ results showed that the four random sentences have little meanings because it used two words to calculate. It reflected its high occurrence probability frequency but not grammatically correct.\n\n  * _Trigram_ results showed that words relations are coherent because it used three words to calculate. It reflected the conditional probability ranking had improved grammar and meanings like human language.\n\n  * _Quadrigram_ results showed that the language of sentences is almost perfect per original sentences since it used four words co-relation to calculate, but its high occurrence conditional probability frequency are the words encountered with low-ranking options due to copious information to search. It may not be beneficial to text generation.\n\nAlthough quadrigrams can provide realistic language, sentence/utterance but\nlack freedoms to generate new sentences. Hence, trigram is often a suitable\noption for language generation. Again, if corpus is not sizeable enough to\naccommodate tokens and words volume like this literature, trigram will be\nunable to provide the frequent words for N-gram may need to switch using\nbigram in this case. Hence, quadrigram is unsuitable for text generation\nbecause it will be too close to the original words or sentences.\n\nCorpus used in this example is also domain specific from _The Complete Works\nof Shakespeare_. It consists of 884,647 tokens and 29,066 distinct words that\nare approximately 10 times more as compared with _The Adventures of_ _Sherlock\nHolmes_ _._ It has approximately 300,000 bigram types out of all these tokens\nand the number of bigram combinations will be 844 million possible bigrams. In\nother words, less than 1% is used and other 99.96% of possible bigrams are\nnever used. It makes sense because most of these random bigram generations are\ngrammatic, syntactic, or even pragmatic meaningless, but pose a problem in\nN-gram calculations for text generation.\n\nFor illustration purposes on how domain knowledge affects N-gram generation,\nFig. 2.14 shows some sample sentences generated by Wall Street Journal (WSJ)\narticles as the corpus (Jurafsky et al. 1999). It showed that trigram has the\nbest performance in terms of sentence structure and meaningfulness on text\ngeneration.\n\n![](../images/533412_1_En_2_Chapter/533412_1_En_2_Fig14_HTML.png)\n\nFig.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1122825-051d-422c-b717-4b077a1677fb": {"__data__": {"id_": "a1122825-051d-422c-b717-4b077a1677fb", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "528afe42-98b6-4296-ab83-b61d8db1d54e", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "7dc039d26f21f65ae5150e4c46eee7ffa7f0adca0727da374b2a44d086c0fbd7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "535d0953-a27d-4542-a114-6c06ac41d09e", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "d22a3ec3768a0e29a2fa5a3ebf45a1692ca29c920bc11bd65a383f52d2946772", "class_name": "RelatedNodeInfo"}}, "hash": "557d7a694af1c8371bc03a728c48f07ee93e3dc7dc6141b9ef990c367642e2f6", "text": "In\nother words, less than 1% is used and other 99.96% of possible bigrams are\nnever used. It makes sense because most of these random bigram generations are\ngrammatic, syntactic, or even pragmatic meaningless, but pose a problem in\nN-gram calculations for text generation.\n\nFor illustration purposes on how domain knowledge affects N-gram generation,\nFig. 2.14 shows some sample sentences generated by Wall Street Journal (WSJ)\narticles as the corpus (Jurafsky et al. 1999). It showed that trigram has the\nbest performance in terms of sentence structure and meaningfulness on text\ngeneration.\n\n![](../images/533412_1_En_2_Chapter/533412_1_En_2_Fig14_HTML.png)\n\nFig. 2.14\n\nSample sentence generation using Shannon method with Wall Street Journal\narticles\n\n## 2.6 Language Model Evaluation and Smoothing Techniques\n\n_Language Model Evaluation_ _(LME)_ (Jurafsky et al. 1999) is a standard\nmethod to train parameters on a training set and to review model performance\nwith new data constantly. That is often occurred in real world to learn how\nthe models perform called _training data (training set)_ on language model and\nsee whether it works with unseen information called _test data (test set)_. A\n_test set_ is a dataset completely different than the training set model but\nis drawn from the same source which is a specific domain. Then apply an\nevaluation metric, e.g. _perplexity_ to determine language model\neffectiveness.\n\n_Unknown words_ are words unseen prior looking at test data regardless of how\nmuch training data is available. It can be managed by an open vocabulary task\nwith steps below:\n\n  1. 1.\n\nCreate an unknown word token <UNK>\n\n  2. 2.\n\nTrain <UNK> probabilities\n\n    1. (a)\n\nCreate a fix lexicon L, of size V from a dictionary or a subset of terms from\nthe training set\n\n    2. (b)\n\nA subnet of terms from the training set\n\n    3. (c)\n\nAt text normalization phase, any training word not in L changed o <UNK>\n\n    4. (d)\n\nNow can count that like a normal word\n\n  3. 3.\n\nTest\n\n    1. (a)\n\nUse <UNK> counts for any word not in training.\n\n### 2.6.1 Perplexity\n\n_Perplexity_ _(PP)_ is the probability of test set assigned by the language\nmodel, normalized by the number of words as given by\n\n![$$ \\\\mathrm{PP}\\(W\\)=\\\\sqrt\\[N\\]{\\\\frac{1}{P\\\\left\\({w}_1{w}_2\\\\dots\n{w}_N\\\\right\\)}}\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equ11.png)\n\n(2.11)\n\nBy applying Chain rule, it will become\n\n![$$ \\\\mathrm{PP}\\(W\\)=\\\\sqrt\\[N\\]{\\\\prod\n\\\\limits_{k=1}^N\\\\frac{1}{P\\\\left\\({w}_k|{w}_1{w}_2\\\\dots {w}_{k-1}\\\\right\\)}}\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equ12.png)\n\n(2.12)\n\nFor bigrams, it will be given by\n\n![$$ \\\\mathrm{PP}\\(W\\)=\\\\sqrt\\[N\\]{\\\\prod\n\\\\limits_{k=1}^N\\\\frac{1}{P\\\\left\\({w}_k|{w}_{k-1}\\\\right\\)}}\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equ13.png)\n\n(2.13)\n\nIn general, minimizing perplexity is the same as maximizing probability for\nmodel performance, which means the best language model is the one that can\nbest predict an unseen test set with minimized perplexity rate.\n\nAn example of perplexity values for WSJ is shown in Fig. 2.15 indicating that\ntrigram with minimized perplexity has performed better than bigram and unigram\nsupported this principle for text generation (Jurafsky et al. 1999).\n\n![](../images/533412_1_En_2_Chapter/533412_1_En_2_Fig15_HTML.png)\n\nFig.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "535d0953-a27d-4542-a114-6c06ac41d09e": {"__data__": {"id_": "535d0953-a27d-4542-a114-6c06ac41d09e", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1122825-051d-422c-b717-4b077a1677fb", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "557d7a694af1c8371bc03a728c48f07ee93e3dc7dc6141b9ef990c367642e2f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6791e156-fcca-482f-bbe3-fbd41a9e2804", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f357f4799ef17d102bce4707c12417be5b8da558122caabb0db2a461964b4b3b", "class_name": "RelatedNodeInfo"}}, "hash": "d22a3ec3768a0e29a2fa5a3ebf45a1692ca29c920bc11bd65a383f52d2946772", "text": "An example of perplexity values for WSJ is shown in Fig. 2.15 indicating that\ntrigram with minimized perplexity has performed better than bigram and unigram\nsupported this principle for text generation (Jurafsky et al. 1999).\n\n![](../images/533412_1_En_2_Chapter/533412_1_En_2_Fig15_HTML.png)\n\nFig. 2.15\n\nPerplexity values for WSJ from unigram to trigram\n\n### 2.6.2 Extrinsic Evaluation Scheme\n\nAn _extrinsic evaluation_ is a popular method for N-gram evaluation, its\ntheory is straightforward as follows:\n\n  1. 1.\n\nPut model A into an application, e.g. a speech recognizer or even a QA\nchatbot;\n\n  2. 2.\n\nEvaluate application performance with model A;\n\n  3. 3.\n\nPut model B into the application and evaluate;\n\n  4. 4.\n\nCompare two models\u2019 application performance.\n\nThe good thing about extrinsic evaluation is that it can perform exact testing\nat two models which is fair and objective, but it is time-consuming for system\ntesting and implementations, i.e. take days to perform experiments if is a\nsophisticated system. So, a temporary solution is to use _intrinsic\nevaluation_ with an approximation called perplexity to evaluate N-gram. It is\neasier to implement if the same system is used but perplexity is a poor\napproximation unless the test data looks identical to the training data.\nHence, it is generally useful in pilot experiments.\n\n### 2.6.3 Zero Counts Problems\n\nNext step is to manage _zero counts problems_. Let us return to _The\nAdventures of_ _Sherlock Holmes_ example, this literature had produced 109,139\nbigram types over 100 million of possible bigrams as recalled, so there is\napproximately 99.89% of possible bigrams never seen that have zero entries in\nthe bigram table. In other words, most of these 0s conditional probability are\nbigrams that required to manage especially in different NLP applications like\ntext generation and speech recognition.\n\nThere is a brief synopsis in such zero-count dilemma. Some of these 0s are\ntruly zeros which means that cannot or should not occur because it will not\nmake grammatical or syntactic sense, however, some are only rare events which\nmeans they occurred infrequently, i.e. with an extensive training corpus.\n\nFurther, _Zipf\u2019s law_ (Saichev et al. 2010) stated that, a long tail\nphenomenon is rare events occurred in a very high frequency, and large events\nnumbers occurred in a low frequency constantly. These are two extremes which\nmeans some popular words always occurred in a high frequency, and most are\nbigrams in low frequency. Hence, it is clear to collect statistics on high\nfrequency events and may have to wait for a long time until a rare event\noccurs, e.g. a bigram to take a count on this low occurrence frequency event.\nIn other words, high occurrence frequency events always dominate the whole\ncorpus. This phenomenon is essential because it always occurs in website\nstatistics or website counting. These high frequency websites and N-grams are\nusually the top 100 and others with limited visit counts and occurrence, so\nthe estimate results are sparse as there are neither counts nor rare events\nthat required to estimate the likelihood of unseen or 0 count N-grams.\n\n### 2.6.4 Smoothing Techniques\n\nEvery N-gram training matrix is sparse even with large corpora because of\n_Zipf\u2019s law_ phenomenon. The solution is to use likelihood estimation for\nfigures on unseen N-grams or _0 count_ N-grams to judge the rest of corpus\naccommodated with these phantom/shadow N-grams. It will affect the rest of\ncorpus.\n\nLet us assume that an N-gram is used, all the words are known and seen\nbeforehand. When assigning a probability to a sequence where one of these\ncomponents is 0, the initial process is to search for a low N-gram order and\n_backoff_ from a bigram to unigram and replace 0 with something else, or a\nvalue with several methods to resolve zero count problems based on this\nconcept; these collective methods are called _smoothing techniques_.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6791e156-fcca-482f-bbe3-fbd41a9e2804": {"__data__": {"id_": "6791e156-fcca-482f-bbe3-fbd41a9e2804", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "535d0953-a27d-4542-a114-6c06ac41d09e", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "d22a3ec3768a0e29a2fa5a3ebf45a1692ca29c920bc11bd65a383f52d2946772", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53073738-ab94-4899-b213-31eb2e3023bc", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "923e2d2e4801838d406ef6ee7855dc562ac863041d43ec97b209e6b508ff1ef2", "class_name": "RelatedNodeInfo"}}, "hash": "f357f4799ef17d102bce4707c12417be5b8da558122caabb0db2a461964b4b3b", "text": "### 2.6.4 Smoothing Techniques\n\nEvery N-gram training matrix is sparse even with large corpora because of\n_Zipf\u2019s law_ phenomenon. The solution is to use likelihood estimation for\nfigures on unseen N-grams or _0 count_ N-grams to judge the rest of corpus\naccommodated with these phantom/shadow N-grams. It will affect the rest of\ncorpus.\n\nLet us assume that an N-gram is used, all the words are known and seen\nbeforehand. When assigning a probability to a sequence where one of these\ncomponents is 0, the initial process is to search for a low N-gram order and\n_backoff_ from a bigram to unigram and replace 0 with something else, or a\nvalue with several methods to resolve zero count problems based on this\nconcept; these collective methods are called _smoothing techniques_.\n\nThis section explores four commonly used _smoothing techniques_ : (1) _Laplace\n(Add-one) Smoothing_ , (2) _Add-k Smoothing_ , (3) _Backoff and Interpolation\nSmoothing,_ and (4) _Good Turing Smoothing_ (Chen and Goodman 1999; Eisenstein\n2019; Jurafsky et al. 1999).\n\n### 2.6.5 Laplace (Add-One) Smoothing\n\n_Laplace (Add-one) Smoothing_ (Chen and Goodman 1999; Jurafsky et al. 1999)\nlogic is to consider all zero counts are rare events and add 1 into them.\nThese rare events are neither occurred nor sampled during corpus training.\n\nFor unigram:\n\n  1. 1.\n\nAdd 1 to every single word (type) count.\n\n  2. 2.\n\nNormalize _N token/(N (tokens) + V (types))._\n\n  3. 3.\n\nSmooth count  ![$$ {c}_i^{\\\\ast }\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_IEq3.png)\n(adjusted for additions to _N_ ) given by\n\n![$$ {c}_i^{\\\\ast }=\\\\left\\({c}_i+1\\\\right\\)\\\\frac{N}{N+V}\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equ14.png)\n\n(2.14)\n\n  1. 4.\n\nNormalize _N_ to obtain a new unigram probability _p_ \u2217given by\n\n![$$ {p}^{\\\\ast }=\\\\frac{ci+1}{N+V}\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equ15.png)\n\n(2.15)\n\nFor bigram:\n\n  1. 1.\n\nAdd 1 into every bigram _c_ ( _w_ _n_ \u2212 1 _w_ _n_ ) + 1.\n\n  2. 2.\n\nIncrease unigram count by vocabulary size _c_ ( _w_ _n_ \u2212 1) + _V_.\n\nFigure 2.16 showed a bigram count with and without Laplace Method for previous\nexample _I have no doubt that I_ from _The Adventures of_ _Sherlock Holmes_.\nIt indicated that all 0s become 1 so that _no I_ becomes 1, others like _I\nhave_ will come from 288 to 289, the calculation is simple but effective.\n\n![](../images/533412_1_En_2_Chapter/533412_1_En_2_Fig16_HTML.png)\n\nFig. 2.16\n\nBigram counts with and without Laplace method\n\nFor bigram probability calculation is given by\n\n![$$\nP\\\\left\\({w}_n|{w}_{n-1}\\\\right\\)=\\\\frac{C\\\\left\\({w}_{n-1}{w}_n\\\\right\\)}{C\\\\Big\\({w}_{n-1\\\\Big\\)}}\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equ16.png)\n\n(2.16)\n\nSo, the bigram probability with _Laplace method_ will be given by\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "53073738-ab94-4899-b213-31eb2e3023bc": {"__data__": {"id_": "53073738-ab94-4899-b213-31eb2e3023bc", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6791e156-fcca-482f-bbe3-fbd41a9e2804", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f357f4799ef17d102bce4707c12417be5b8da558122caabb0db2a461964b4b3b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "925f9454-708c-499d-86b4-96d54525be28", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "a49e650084dbe2c932b0891ca7b74f35605092f203d5ceeb292a5e1f125f59b8", "class_name": "RelatedNodeInfo"}}, "hash": "923e2d2e4801838d406ef6ee7855dc562ac863041d43ec97b209e6b508ff1ef2", "text": "![](../images/533412_1_En_2_Chapter/533412_1_En_2_Fig16_HTML.png)\n\nFig. 2.16\n\nBigram counts with and without Laplace method\n\nFor bigram probability calculation is given by\n\n![$$\nP\\\\left\\({w}_n|{w}_{n-1}\\\\right\\)=\\\\frac{C\\\\left\\({w}_{n-1}{w}_n\\\\right\\)}{C\\\\Big\\({w}_{n-1\\\\Big\\)}}\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equ16.png)\n\n(2.16)\n\nSo, the bigram probability with _Laplace method_ will be given by\n\n![$$\n{P}_{\\\\mathrm{Lap}}\\\\left\\({w}_n|{w}_{n-1}\\\\right\\)=\\\\frac{C\\\\left\\({w}_{n-1}{w}_n\\\\right\\)+1}{\\\\sum_w\\\\left\\(C\\\\left\\({w}_{n-1}w\\\\right\\)+1\\\\right\\)}=\\\\frac{C\\\\left\\({w}_{n-1}{w}_n\\\\right\\)+1}{C\\\\left\\({w}_{n-1}\\\\right\\)+V}\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equ17.png)\n\n(2.17)\n\nFigure 2.17 shows the bigram probabilities with and without _Laplace Method_\nfor previous example _I have no doubt that I_ from _The Adventures of_\n_Sherlock Holmes_.\n\n![](../images/533412_1_En_2_Chapter/533412_1_En_2_Fig17_HTML.png)\n\nFig. 2.17\n\nBigram probabilities with and without laplace method\n\nNote: The bigram probability is calculated by the division of unigram\noriginally but now it will be the division by _the count of unigram + total\nnumber of word type (V)_ which is equal to 9886 e.g. _P(have | I) = 288/2755 =\n0.105._ Applying Laplace method, it becomes _289/(2755 + 9886) = 0.023_. It\nshowed that all zero cases will become 1 which is simple for text generation,\nbut the problem is, some probabilities have changed notably such as _I have_\nfrom _0.105_ to _0.023_ , and _no doubt_ has the highest change from _0.1667_\nto only _0.00463._\n\nAlthough it is adequate to assign a number to all zero events but the one with\nhigh frequency becomes insignificant because of copious word types in corpus\nbase, indicating that the performance of _Laplace Add-one smoothing_ may not\nbe effective in many cases and required to look for alternatives.\n\n### 2.6.6 Add-k Smoothing\n\n_Add-k Smoothing_ (Chen and Goodman 1999; Jurafsky et al. 1999) logic is to\nassume that each N-gram is seen in _k_ times, but the occurrence is too rare\nto be observed. These zeros are rare events that are less than 1 and\nunnoticeable meaning that there is a line between 0 and 1, it can be 0.1,\n0.01, 0.2 or even smaller; so a non-integer count is added instead of 1 to\neach count, e.g. 0.05, 0.1, 0.2, typically, 0 < _k_ < 1 provided that _k_ must\nbe a small number less than 1 in practical applications. It is because if _k_\nis too large, it will cause similar problem occurred in _Laplace method_.\n\nBy using the same logical as _Add-1 method_ , _Add-k Smoothing_ is given by\n\n![$$\n{P}_{\\\\mathrm{Add}-k}^{\\\\ast}\\\\left\\({w}_n|{w}_{n-1}\\\\right\\)=\\\\frac{C\\\\left\\({w}_{n-1}{w}_n\\\\right\\)+k}{C\\\\left\\({w}_n\\\\right\\)+\nkV} $$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equ18.png)\n\n(2.18)\n\nwhere 0 < _k_ < 1.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "925f9454-708c-499d-86b4-96d54525be28": {"__data__": {"id_": "925f9454-708c-499d-86b4-96d54525be28", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "53073738-ab94-4899-b213-31eb2e3023bc", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "923e2d2e4801838d406ef6ee7855dc562ac863041d43ec97b209e6b508ff1ef2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e667b938-5533-4206-999c-036a1610e504", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3236a1dee3578c89f03598466c285a9b0fdbd7f8e65d6463c22abb40d367c7b2", "class_name": "RelatedNodeInfo"}}, "hash": "a49e650084dbe2c932b0891ca7b74f35605092f203d5ceeb292a5e1f125f59b8", "text": "It is because if _k_\nis too large, it will cause similar problem occurred in _Laplace method_.\n\nBy using the same logical as _Add-1 method_ , _Add-k Smoothing_ is given by\n\n![$$\n{P}_{\\\\mathrm{Add}-k}^{\\\\ast}\\\\left\\({w}_n|{w}_{n-1}\\\\right\\)=\\\\frac{C\\\\left\\({w}_{n-1}{w}_n\\\\right\\)+k}{C\\\\left\\({w}_n\\\\right\\)+\nkV} $$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equ18.png)\n\n(2.18)\n\nwhere 0 < _k_ < 1.\n\nIt is adequate to compare with _Laplace method_ is that the whole _V_ is not\nused if _V_ is very large such as 9886 in _The Adventure of_ _Sherlock\nHolmes_. When the event is, say 0.05, means that it will be even smaller, but\nthe new number will not be too small. Although _add-k_ is useful for many\ntasks including text classification and generation, but not for all language\nmodeling, generating counts with poor variance and often inappropriate\ndiscounts (Gale and Church 1994). Another add- _k_ model consideration is to\nselect an appropriate _k_ number through trial and error but that will lead to\nproblems in practical applications. Nevertheless, _Add-k smoothing_ usually\nprovides a better and viable solution as compared with Add-1 method.\n\n### 2.6.7 Backoff and Interpolation Smoothing\n\n_Backoff and Interpolation (B &I) Smoothing_ (Chen and Goodman 1999; Suyanto\n2020) logic is to look for a lower dimension N-gram if there is no example of\na particular N-gram. If _N_ \u2212 1 gram has insufficient number count (or does\nnot exist), then will switch to _N_ \u2212 2 gram and so on. Although it is not the\nperfect option but at least it can produce some viable counting for words\nprediction. That is to estimate a probability with a bigram instead of trigram\nif there is none to be found. Furthermore, it can look up to unigram if no\nbigram either. This is a kind of _backoff method_ and by _interpolation_ , can\nalways weight and combine with quadrigram, trigram, bigram, and unigram\nprobabilities counts, e.g. when calculating trigram probability with unigram,\nbigram, and trigram, each weighted by some _\u03bb_ values. Note the sum of all _\u03bb_\ns must be 1 given by these equations:\n\n![$$ {\\\\displaystyle\n\\\\begin{array}{ll}{P}_{B\\\\&amp;I}\\\\left\\({w}_n|{w}_{n-2}{w}_{n-1}\\\\right\\)&amp;\n={\\\\lambda}_1P\\\\left\\({w}_n\\\\right\\)\\\\\\\\ {}&amp;\n+{\\\\lambda}_2P\\\\left\\({w}_n|{w}_{n-1}\\\\right\\)\\\\\\\\ {}&amp;\n+{\\\\lambda}_3P\\\\left\\({w}_n|{w}_{n-2}{w}_{n-1}\\\\right\\)\\\\end{array}}\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equ19.png)\n\n(2.19)\n\nFor a sophisticated version of linear interpolation, each _\u03bb_ value can be\ncalculated by conditioning on the context which means it can be done by using\nconditional probabilities as well. In this way, if a particular bigram has\naccurate numbers, it can assume that the trigrams numbers are based on this\nbigram, which will be a robust method to implement given by the following\nequation:\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e667b938-5533-4206-999c-036a1610e504": {"__data__": {"id_": "e667b938-5533-4206-999c-036a1610e504", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "925f9454-708c-499d-86b4-96d54525be28", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "a49e650084dbe2c932b0891ca7b74f35605092f203d5ceeb292a5e1f125f59b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d72652b4-ad63-4c20-8710-4ee8a46e01b6", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "5c6563fd794067001a49db47b6227683af53aebdd1ebf978e552e77e684a17a7", "class_name": "RelatedNodeInfo"}}, "hash": "3236a1dee3578c89f03598466c285a9b0fdbd7f8e65d6463c22abb40d367c7b2", "text": "In this way, if a particular bigram has\naccurate numbers, it can assume that the trigrams numbers are based on this\nbigram, which will be a robust method to implement given by the following\nequation:\n\n![$$ {\\\\displaystyle\n\\\\begin{array}{ll}{P}_{B\\\\&amp;I}\\\\left\\({w}_n|{w}_{n-2}{w}_{n-1}\\\\right\\)&amp;\n={\\\\lambda}_1\\\\left\\({w}_{n-2:n-1}\\\\right\\)P\\\\left\\({w}_n\\\\right\\)\\\\\\\\ {}&amp;\n+{\\\\lambda}_2\\\\left\\({w}_{n-2:n-1}\\\\right\\)P\\\\left\\({w}_n|{w}_{n-1}\\\\right\\)\\\\\\\\\n{}&amp;\n+{\\\\lambda}_3\\\\left\\({w}_{n-2:n-1}\\\\right\\)P\\\\left\\({w}_n|{w}_{n-2}{w}_{n-1}\\\\right\\)\\\\end{array}}\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equ20.png)\n\n(2.20)\n\nIt is noted that by comparing with previous Eq. (2.19), this equation also\nconsiders conditional probability in all N-gram levels. Hence, both simple\ninterpolation and conditional interpolation methods are learnt from a _held-\nout_ _corpus_. A _held-out corpus_ is an additional training corpus to set\nhyperparameters like _\u03bb_ values by choosing _\u03bb_ values that can maximize the\nlikelihood of held-out corpus. By adjusting N-gram probabilities and search\nfor _\u03bb_ value is to provide the highest probability of _held-out set_. In\nfact, there are numerous approaches to find this optimal set of _\u03bb_ , a simple\nway is applying EM algorithm which is an interactive learning algorithm to\nconverge locally optimal _\u03bb_.\n\n### 2.6.8 Good Turing Smoothing\n\n_Good Turing (GT) Smoothing_ (Chen and Goodman 1999; Gale and Sampson 1995)\nlogic is to use the total frequency of events that occurred only once to\nestimate how much mass shift to unseen events, e.g. using a bag of green color\nbeans to estimate the probability of an unseen red color bean.\n\nThis technique uses the frequency of N-grams occurrence to reallocate\nprobability distribution in two criteria, e.g. N-gram statistics of _The\nAdventures of_ _Sherlock Holmes_ in Fig. 2.17. It showed that the probability\nof _have doubt_ = 0 without smoothing, so by using bigrams frequency that\noccurred once, i.e. probability of _I doubt_ to represent the total number of\nbigrams for unknown bigrams given by\n\n![$$\n{P}_{\\\\mathrm{unknown}}\\\\left\\({w}_i|{w}_{i-1}\\\\right\\)=\\\\frac{\\\\mathrm{Count}\\\\\n\\\\mathrm{of}\\\\ \\\\mathrm{bigrams}\\\\ \\\\mathrm{that}\\\\ \\\\mathrm{appeared}\\\\\n\\\\mathrm{once}}{\\\\mathrm{Count}\\\\ \\\\mathrm{of}\\\\ \\\\mathrm{total}\\\\\n\\\\mathrm{bigrams}}\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equ21.png)\n\n(2.21)\n\nIt is an intuitive method because it only considers conditional probability of\nbigrams that occurred once to represent unknown probabilities instead of\nadding 1 to them. In other words, the conditional probability of unknown\nbigram of word will be the count for bigram that occurred once over the count\nof total bigrams.\n\nFor known bigrams such as _no doubt_ , the frequency of bigrams that occurred\nmore than one of the current bigram frequency _N_ _c_ +1, frequency of bigrams\nthat occurred the same as the current bigram frequency _N_ _c_ , and the total\nnumber of bigram _N_ are given by\n\n![$$ {P}_{\\\\mathrm{known}}\\\\left\\({w}_i|{w}_{i-1}\\\\right\\)=\\\\frac{c^{\\\\ast\n}}{N} $$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equc.png)\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d72652b4-ad63-4c20-8710-4ee8a46e01b6": {"__data__": {"id_": "d72652b4-ad63-4c20-8710-4ee8a46e01b6", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e667b938-5533-4206-999c-036a1610e504", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3236a1dee3578c89f03598466c285a9b0fdbd7f8e65d6463c22abb40d367c7b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e88ea21d-5360-467f-892d-14ef3b4ff6db", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8b1ec89c6ae5d4cc80c6b62bbac0f7feef9cbccfe523de972f21af73f24ea52f", "class_name": "RelatedNodeInfo"}}, "hash": "5c6563fd794067001a49db47b6227683af53aebdd1ebf978e552e77e684a17a7", "text": "In other words, the conditional probability of unknown\nbigram of word will be the count for bigram that occurred once over the count\nof total bigrams.\n\nFor known bigrams such as _no doubt_ , the frequency of bigrams that occurred\nmore than one of the current bigram frequency _N_ _c_ +1, frequency of bigrams\nthat occurred the same as the current bigram frequency _N_ _c_ , and the total\nnumber of bigram _N_ are given by\n\n![$$ {P}_{\\\\mathrm{known}}\\\\left\\({w}_i|{w}_{i-1}\\\\right\\)=\\\\frac{c^{\\\\ast\n}}{N} $$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equc.png)\n\n![$$ \\\\mathrm{where}\\\\;{c}^{\\\\ast }=\\\\left\\(c+1\\\\right\\)\\\\ast\n\\\\frac{N_{c+1}}{N_c}\\\\;\\\\mathrm{and}\\\\;c=\\\\mathrm{count}\\\\ \\\\mathrm{of}\\\\\n\\\\mathrm{input}\\\\ \\\\mathrm{bigram}.\n$$](../images/533412_1_En_2_Chapter/533412_1_En_2_Chapter_TeX_Equ22.png)\n\n(2.22)\n\nExercise: Try to calculate these probabilities from data provided by Fig.\n2.17.\n\nExercises\n\n  1. 2.1\n\nWhat is _Language Model (LM)_? Discuss the roles and importance of language\nmodel in NLP.\n\n  2. 2.2\n\nWhat is N-gram? Discuss and explain the importance of N-gram in NLP and text\nanalysis.\n\n  3. 2.3\n\nState the _Chain Rule_ and explain how it works for the formulation of N-gram\nprobabilities. Use trigram as example to illustrate.\n\n  4. 2.4\n\nWhat is a _Markov Chain_? State and explain how it works for the formulation\nof N-gram probabilities.\n\n  5. 2.5\n\nUse _The Adventures of_ _Sherlock Holmes_ as corpus, calculate N-gram\nprobability for sentence \u201c _I don\u2019t believe in that_ \u201d with _Markov Chain_ and\nevaluate all related bigram probabilities.\n\n  6. 2.6\n\nRepeat Exercise 2.5 by using another famous literature _Little Women_ by\nLouisa May Alcott (1832\u20131888) _(_ Alcott 2017 _)_ to calculate N-gram\nprobability of sentence \u201c _I don\u2019t believe in that_ \u201d and compare with results\nin 2.5. What is (are) the finding(s)?\n\n  7. 2.7\n\nUse _Shannon\u2019s text generation_ scheme on _The Adventures of_ _Sherlock\nHolmes_ as corpus, generate sample sentences like Fig. 2.14 using unigram,\nbigram, trigram, and quadrigram text generation methods.\n\n  8. 2.8\n\nRepeat Exercise 2.7 using literature _Little Women (_ Alcott 2017 _)_ to\ngenerate corresponding sample sentences and compare with results in 2.7. What\nis (are) the finding(s)?\n\n  9. 2.9\n\nWhat is _Perplexity_ _(PP)_ in N-gram model evaluation? Use _The Adventures\nof_ _Sherlock Holmes_ as corpus with sample test set, evaluate PP values from\nunigram to trigram and compare with Fig. 2.15. What is (are) the finding(s)?\n\n  10. 2.10\n\nUse _Little Women (_ Alcott 2017 _)_ as corpus and some sample test set.\nCompare the performance of _Add-1 smoothing_ against Add- _k_ ( _k_ = 0.5).\nWhich one is better? Why?\n\n  11. 2.11\n\nWhat is _Backoff and Interpolation (B &I) method_ in N-gram smoothing? Repeat\n2.10 using _B &I smoothing method_ with _\u03bb_ 1 = 0.4, _\u03bb_ 2 = 0.3 _and \u03bb_ 3 =\n0.3. Compare the performance with results obtained in 2.10.\n\n  12. 2.12\n\nWhat is _Good Turing (GT) Smoothing_ in N-gram smoothing? Repeat Exercise 2.10\nusing _GT Smoothing_ and compare performance results obtained in 2.10 and\n2.11. Which one is better? Why?\n\nReferences\n\n  1.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e88ea21d-5360-467f-892d-14ef3b4ff6db": {"__data__": {"id_": "e88ea21d-5360-467f-892d-14ef3b4ff6db", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d72652b4-ad63-4c20-8710-4ee8a46e01b6", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "5c6563fd794067001a49db47b6227683af53aebdd1ebf978e552e77e684a17a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca46d90a-6a0a-4054-b8bd-b909a3001165", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3a027c52c42ec086d3b462cbb4627425cd951a0f64c758ce314039bbfdc1266b", "class_name": "RelatedNodeInfo"}}, "hash": "8b1ec89c6ae5d4cc80c6b62bbac0f7feef9cbccfe523de972f21af73f24ea52f", "text": "Compare the performance of _Add-1 smoothing_ against Add- _k_ ( _k_ = 0.5).\nWhich one is better? Why?\n\n  11. 2.11\n\nWhat is _Backoff and Interpolation (B &I) method_ in N-gram smoothing? Repeat\n2.10 using _B &I smoothing method_ with _\u03bb_ 1 = 0.4, _\u03bb_ 2 = 0.3 _and \u03bb_ 3 =\n0.3. Compare the performance with results obtained in 2.10.\n\n  12. 2.12\n\nWhat is _Good Turing (GT) Smoothing_ in N-gram smoothing? Repeat Exercise 2.10\nusing _GT Smoothing_ and compare performance results obtained in 2.10 and\n2.11. Which one is better? Why?\n\nReferences\n\n  1. Alcott, L. M. (2017) Little Women (AmazonClassics Edition). AmazonClassics.\n\n  2. Bird, S., Klein, E., and Loper, E. (2009). Natural language processing with python. O\u2019Reilly.[zbMATH](http://www.emis.de/MATH-item?1187.68630)\n\n  3. Chen, S. F. and J. Goodman. 1999. An empirical study of smoothing techniques for language modeling. Computer Speech and Language, 13:359\u2013394.[Crossref](https://doi.org/10.1006/csla.1999.0128)\n\n  4. Ching, W. K., Huang, X., Ng, M. K. and Siu, T. K. (2013) Markov Chains: Models, Algorithms and Applications. Springer.[Crossref](https://doi.org/10.1007/978-1-4614-6312-2)[zbMATH](http://www.emis.de/MATH-item?1270.60001)\n\n  5. Doyle, A. C. (2019) The Adventures of Sherlock Holmes (AmazonClassics Edition). AmazonClassics.\n\n  6. Eisenstein, J. (2019) Introduction to Natural Language Processing (Adaptive Computation and Machine Learning series). The MIT Press.\n\n  7. Gale, W. A. and Church, K. W. (1994) What is wrong with adding one? In N. Oostdijk and P. de Haan (eds), Corpus-Based Research into Language, pp. 189\u2013198. Rodopi.\n\n  8. Gale, W. A. and Sampson, G. (1995). Good-Turing frequency estimation without tears. Journal of Quantitative Linguistics, 2(3), 217-237.[Crossref](https://doi.org/10.1080/09296179508590051)\n\n  9. Google (2022) Google official site. [http://\u200bgoogle.\u200bcom](http://google.com). Accessed 12 July 2022.\n\n  10. Gutenberg (2022) Project Gutenberg official site. [https://\u200bwww.\u200bgutenberg.\u200borg/\u200b](https://www.gutenberg.org/). Accessed 13 July 2022.\n\n  11. Jurafsky, D., Marin, J., Kehler, A., Linden, K., Ward, N. (1999). Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition. Prentice Hall.\n\n  12. Liu, Z., Lin, Y. and Sun, M. (2020) Representation Learning for Natural Language Processing. Springer.[Crossref](https://doi.org/10.1007/978-981-15-5573-2)\n\n  13. Pustejovsky, J. and Stubbs, A. (2012) Natural Language Annotation for Machine Learning: A Guide to Corpus-Building for Applications. O\u2019Reilly Media.\n\n  14. Saichev, A. I., Malevergne, Y. and Sornette, D. (2010) Theory of Zipf\u2019s Law and Beyond (Lecture Notes in Economics and Mathematical Systems, 632). Springer.[zbMATH](http://www.emis.de/MATH-item?1189.91009)\n\n  15. Shakespeare, W. (2021) The Complete Works of Shakespeare (AmazonClassics Edition). AmazonClassics.\n\n  16.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca46d90a-6a0a-4054-b8bd-b909a3001165": {"__data__": {"id_": "ca46d90a-6a0a-4054-b8bd-b909a3001165", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e88ea21d-5360-467f-892d-14ef3b4ff6db", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8b1ec89c6ae5d4cc80c6b62bbac0f7feef9cbccfe523de972f21af73f24ea52f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "15890fde-4b03-4090-9ed1-7191f72f00c3", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "4766563bd96357808d8158f3eb1a3906e387b07b4db19232713539c19b354c8a", "class_name": "RelatedNodeInfo"}}, "hash": "3a027c52c42ec086d3b462cbb4627425cd951a0f64c758ce314039bbfdc1266b", "text": "(2020) Representation Learning for Natural Language Processing. Springer.[Crossref](https://doi.org/10.1007/978-981-15-5573-2)\n\n  13. Pustejovsky, J. and Stubbs, A. (2012) Natural Language Annotation for Machine Learning: A Guide to Corpus-Building for Applications. O\u2019Reilly Media.\n\n  14. Saichev, A. I., Malevergne, Y. and Sornette, D. (2010) Theory of Zipf\u2019s Law and Beyond (Lecture Notes in Economics and Mathematical Systems, 632). Springer.[zbMATH](http://www.emis.de/MATH-item?1189.91009)\n\n  15. Shakespeare, W. (2021) The Complete Works of Shakespeare (AmazonClassics Edition). AmazonClassics.\n\n  16. Shannon, C. (1948). A Mathematical Theory of Communication. Bell System Technical Journal. 27 (3): 379\u2013423.[MathSciNet](http://www.ams.org/mathscinet-getitem?mr=26286)[Crossref](https://doi.org/10.1002/j.1538-7305.1948.tb01338.x)[zbMATH](http://www.emis.de/MATH-item?1154.94303)\n\n  17. Sidorov, G. (2019) Syntactic n-grams in Computational Linguistics. Springer.[Crossref](https://doi.org/10.1007/978-3-030-14771-6)\n\n  18. Suyanto, S. (2020). Phonological similarity-based backoff smoothing to boost a bigram syllable boundary detection. International Journal of Speech Technology, 23(1), 191-204.[Crossref](https://doi.org/10.1007/s10772-020-09677-z)\n\n  19. Tuchong (2022) Computerized Axial Tomography Scanner (\u201cCat scan\u201d). [https://\u200bstock.\u200btuchong.\u200bcom/\u200bimage/\u200bdetail?\u200bimageId=\u200b9020019131345797\u200b22](https://stock.tuchong.com/image/detail?imageId=902001913134579722). Accessed 12 July 2022.\n\n\n\u00a9 The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.\n2024\n\nR. S. T. Lee Natural Language Processing\n<https://doi.org/10.1007/978-981-99-1999-4_3>\n\n# 3\\. Part-of-Speech (POS) Tagging\n\nRaymond S. T. Lee 1\n\n(1)\n\nUnited International College, Beijing Normal University-Hong Kong Baptist\nUniversity, Zhuhai, China\n\n## 3.1 What Is Part-of-Speech (POS)?\n\n_Part-of-Speech_ ( _PoS_ or _POS_ ) is a category of words normally in lexical\nterms that have similar grammatic behaviors or properties (Bender 2013;\nJurafsky et al. 1999). These are words assigned to the same POS exhibited in\nsyntactic or functional behaviors and roles in grammatic structure sentence,\ne.g. English verbs and nouns. They have close morphological sometimes that can\nundergo inflection for similar properties and semantic behaviors. To explore\nhow POS works, it is necessary to understand the concept of _inflection_.\n\n_Inflection_ can be considered as the process of word formation in which items\nare added to the baseform of a word to convey grammatical meanings. The word\n_inflection_ comes from Latin word _inflectere_ which means _to bend,_ e.g.\n(1) inflection _-s_ of _cats_ signifies the noun is plural, (2) the same _-s_\ninflection of _gets_ signifies the subject is a third-person singular, e.g.\n[3.1] _He gets the book_ , and (3) inflection of _-ed_ often signifies past\ntense, e.g. _arrive \u2192 arrived_ , _close \u2192 closed_ etc. Thus, _inflections_ are\nto express grammatical types such as persons, quantities, and tenses. There\nare several types of POS to define inflection characteristics.\n\n### 3.1.1 Nine Major POS in English Language\n\nEvery word in English sentences fall into nine major POS types.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "15890fde-4b03-4090-9ed1-7191f72f00c3": {"__data__": {"id_": "15890fde-4b03-4090-9ed1-7191f72f00c3", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca46d90a-6a0a-4054-b8bd-b909a3001165", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3a027c52c42ec086d3b462cbb4627425cd951a0f64c758ce314039bbfdc1266b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b13915e-5495-4ff4-904a-49ef7b277270", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "6d946f85da8af850894681d0b280464d52df5cd99163d2e66792926a4cb2ed11", "class_name": "RelatedNodeInfo"}}, "hash": "4766563bd96357808d8158f3eb1a3906e387b07b4db19232713539c19b354c8a", "text": "The word\n_inflection_ comes from Latin word _inflectere_ which means _to bend,_ e.g.\n(1) inflection _-s_ of _cats_ signifies the noun is plural, (2) the same _-s_\ninflection of _gets_ signifies the subject is a third-person singular, e.g.\n[3.1] _He gets the book_ , and (3) inflection of _-ed_ often signifies past\ntense, e.g. _arrive \u2192 arrived_ , _close \u2192 closed_ etc. Thus, _inflections_ are\nto express grammatical types such as persons, quantities, and tenses. There\nare several types of POS to define inflection characteristics.\n\n### 3.1.1 Nine Major POS in English Language\n\nEvery word in English sentences fall into nine major POS types. They are (1)\nadjectives, (2) verbs, (3) pronouns, (4) conjunctions, (5) prepositions, (6)\narticles (determiners), (7) adverbs, (8) nouns, and (9) interjections as shown\nin Fig. 3.1. Some linguists include only first eight as major POS and leave\ninterjections as an individual category.\n\n![](../images/533412_1_En_3_Chapter/533412_1_En_3_Fig1_HTML.png)\n\nFig. 3.1\n\nMajor POS in English language\n\nPOS is important to study:\n\n  1. 1.\n\nWord classes categorization and usage in linguistics\n\n  2. 2.\n\nGrammars in English usage\n\n  3. 3.\n\nWord functions categorization in NLP and\n\n  4. 4.\n\nPOS tagging\n\n## 3.2 POS Tagging\n\n### 3.2.1 What Is POS Tagging in Linguistics?\n\n_Part-of-Speech_ _Tagging_ (Khanam 2022; Sree and Thottempudi 2011), also\ncalled _POS tagging_ , _POST,_ or _grammatical tagging_ is the operation of\nlabelling a word in a text, or corpus according to a particular POS based on\ndefinition and contexts in linguistics. A simplified format is usually learnt\nby students to identify word types such as adjectives, adverbs, nouns, verbs,\netc. Grammars vary in foreign languages leading to several POS tagging\ncategorization.\n\n### 3.2.2 What Is POS Tagging in NLP?\n\nTagging is a kind of classification process that may be defined as automatic\ndescription assignment to words or tokens in NLP (Eisenstein 2019). They are\ncalled _POS tags_ or _tags_ to represent one of the POS, semantic information\nin a sentence/utterance. Some words may have different meanings and roles in\nPOS, e.g. _book_ can be used as a noun or _booking a table_ as a verb.\n\nIn NLP, POS tagging is the operation of converting a sentence/utterance to\nforms, or list of words and list of tuples, where each tuple has a word or tag\nform to signify noun, verb, adjective, pronoun, conjunction, and their\nsubcategories. Figure 3.2 shows how tagging is applied to sample\nsentence/utterance: [3.2] _She sells seashells on the seashore._\n\n![](../images/533412_1_En_3_Chapter/533412_1_En_3_Fig2_HTML.png)\n\nFig. 3.2\n\nPOS example for utterance \u201c _She sells seashells on the seashore\u201d_\n\nMachine learning and rule-based models can produce POS tags in NLP. They\ngenerally fall into (1) Rule-based POS tagging, (2) Stochastic POS tagging,\nand (3) Hybrid POS tagging using advanced technology like Transformation-based\ntagging (Jurafsky et al. 1999; Khanam 2022; Pustejovsky and Stubbs 2012). We\nwill study how they work with NLTK and spaCy technologies at workshops in Part\nII. First, let us look at some realistic POS databanks.\n\n### 3.2.3 POS Tags Used in the PENN Treebank Project\n\n_PENN Treebank_ is a frequently used POS tag databank provided by the PENN\nTreebank corpus (Marcus et al. 1993). It is an English corpus marked by a\nTreeTagger tool developed by Prof. Helmut Schmid at University of Stuttgart in\nGermany.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b13915e-5495-4ff4-904a-49ef7b277270": {"__data__": {"id_": "7b13915e-5495-4ff4-904a-49ef7b277270", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "15890fde-4b03-4090-9ed1-7191f72f00c3", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "4766563bd96357808d8158f3eb1a3906e387b07b4db19232713539c19b354c8a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "781e5e69-16ec-459e-aaac-5752fa3a22f4", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f294f50c1b7deebd74784d3b0087817f4774ef41e7c4cea96ac50401b9c012c4", "class_name": "RelatedNodeInfo"}}, "hash": "6d946f85da8af850894681d0b280464d52df5cd99163d2e66792926a4cb2ed11", "text": "They\ngenerally fall into (1) Rule-based POS tagging, (2) Stochastic POS tagging,\nand (3) Hybrid POS tagging using advanced technology like Transformation-based\ntagging (Jurafsky et al. 1999; Khanam 2022; Pustejovsky and Stubbs 2012). We\nwill study how they work with NLTK and spaCy technologies at workshops in Part\nII. First, let us look at some realistic POS databanks.\n\n### 3.2.3 POS Tags Used in the PENN Treebank Project\n\n_PENN Treebank_ is a frequently used POS tag databank provided by the PENN\nTreebank corpus (Marcus et al. 1993). It is an English corpus marked by a\nTreeTagger tool developed by Prof. Helmut Schmid at University of Stuttgart in\nGermany. It classifies 9 major POS into subclasses that has a total of 45 POS\ntags with punctuation and examples as shown in Fig. 3.3, its English Penn\nTreebank (PTB) corpus has a comprehensive section of Wall Street Journal (WSJ)\narticles to be used on sequential labelling models\u2019 evaluation as well as\ncharacters and word levels language modelling.\n\n![](../images/533412_1_En_3_Chapter/533412_1_En_3_Fig3_HTML.png)\n\nFig. 3.3\n\nPenn Treebank POS Tags (with punctuation)\n\nA POS tagging table for sentence [3.3] _David has purchased a new laptop from\nApple store_ in Fig. 3.4 showed that _Apple_ is a proper noun because it can\nbe differentiated by capital letter A as a product brand name.\n\n![](../images/533412_1_En_3_Chapter/533412_1_En_3_Fig4_HTML.png)\n\nFig. 3.4\n\nPenn Treebank POS tags of sample sentence \u201c _David has purchased a new laptop\nfrom Apple store_ \u201d\n\n### 3.2.4 Why Do We Care About POS in NLP?\n\nPOS is a fundamental concept to understand proper use of language, e.g.\nEnglish. Without this, we cannot differentiate usages or roles of different\nwords in a sentence whether it is a noun, verb, adjective, and determiners.\nThe major concerns include:\n\n  1. 1.\n\nPronunciation often differs from the same word with different roles, e.g.\n[3.4] _Here are the students\u2019_ _records_ vs. [3.5] _The teacher_ _records_\n_his lecture_.\n\n  2. 2.\n\nPrediction of the following word, e.g. (a) _they_ should use _will_ instead of\nshall, (b) word after _to_ is not past tense. It is natural in grammar rules\nas compared with N-gram solely relied on counting words relationship.\n\n  3. 3.\n\nStemming is within a restricted tagset, e.g. _comput_ for _computer_.\n\n  4. 4.\n\nSyntactic parsing base and then meaning extraction, e.g. [3.6] _Better get\ngoing or you will be late._\n\n  5. 5.\n\nMachine translation for the same word with different POS classes most likely\nhas different translation in other languages, e.g. translation from English to\nFrench.\n\n(E) book + N \u2192 (F) acheter + N (Buy a book \u2192 Ach\u00e8te un livre)\n\n(E) book + VB \u2192 (F) r\u00e9server + VB (Book a room \u2192 R\u00e9server une chambre)\n\nA proper POS tagging can provide correct translation between foreign\nlanguages. Further, it is to stress different accents and avoid confusion of\nthe same word (word type) with different POS in a sentence/utterance. There\nare three types:\n\n  1. 1.\n\nNoun vs. Verb confusion, e.g. _ABstract (noun)_ vs. _abstRACT (verb)_\n\n  2. 2.\n\nAdjective vs. Verb confusion, e.g. _PERfect (adjective)_ vs. _perFECT (verb)_\n\n  3. 3.\n\nAdjective vs. Noun confusion, e.g. _miNUTE (adjective)_ vs. _MInute (noun)_\n\nFigure 3.5 shows some common examples of English words from CELEX online\ndictionary with different stress accents and meanings often occurred when\ndealing with noisy channels to differentiate every word\u2019s role in a\nsentence/utterance. They can be solved by applying statistical probabilistic\nN-gram method or stochastic techniques and corpora to facts analysis.\nNevertheless, POS tagging is the initial step for resolution.\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "781e5e69-16ec-459e-aaac-5752fa3a22f4": {"__data__": {"id_": "781e5e69-16ec-459e-aaac-5752fa3a22f4", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b13915e-5495-4ff4-904a-49ef7b277270", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "6d946f85da8af850894681d0b280464d52df5cd99163d2e66792926a4cb2ed11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84822d53-e37d-4927-ae72-dee3c757a1f8", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "caa89b4649f426e9c03f2b79e9105f4e588c4e91df1882fe4e2d37e13c40871c", "class_name": "RelatedNodeInfo"}}, "hash": "f294f50c1b7deebd74784d3b0087817f4774ef41e7c4cea96ac50401b9c012c4", "text": "There\nare three types:\n\n  1. 1.\n\nNoun vs. Verb confusion, e.g. _ABstract (noun)_ vs. _abstRACT (verb)_\n\n  2. 2.\n\nAdjective vs. Verb confusion, e.g. _PERfect (adjective)_ vs. _perFECT (verb)_\n\n  3. 3.\n\nAdjective vs. Noun confusion, e.g. _miNUTE (adjective)_ vs. _MInute (noun)_\n\nFigure 3.5 shows some common examples of English words from CELEX online\ndictionary with different stress accents and meanings often occurred when\ndealing with noisy channels to differentiate every word\u2019s role in a\nsentence/utterance. They can be solved by applying statistical probabilistic\nN-gram method or stochastic techniques and corpora to facts analysis.\nNevertheless, POS tagging is the initial step for resolution.\n\n![](../images/533412_1_En_3_Chapter/533412_1_En_3_Fig5_HTML.png)\n\nFig. 3.5\n\nCommon example of same English word with different stress accents\n\n## 3.3 Major Components in NLU\n\n_Natural Language Understanding_ _(NLU)_ (Allen 1994; Mitkov 2005) is a\ncritical component in various NLP applications including text summarization,\nsentiment analysis, information retrievals to Q&A Chatbot systems. It composes\nof five basic modules: (1) morphology, (2) POS tagging, (3) syntax, (4)\nsemantics, and (5) discourse integration as shown in Fig. 3.6.\n\n![](../images/533412_1_En_3_Chapter/533412_1_En_3_Fig6_HTML.png)\n\nFig. 3.6\n\nMajor components in NLU\n\n_Morphology_ is the understandings of shapes and patterns for every word of a\nsentence/utterance.\n\n_POS tagging_ is key process to provide functions and categories of words.\n\n_Syntax_ is syntactic analysis to understand the syntactic role and usage of\nevery word or word pattern.\n\n_Semantics_ is an analysis to understand semantic meaning of a\nsentence/utterance and its overall meaning.\n\n_Discourse_ _integration_ is to understand the relationship between different\nsentences and its contents.\n\n### 3.3.1 Computational Linguistics and POS\n\n_Computational linguistics_ _(CL)_ (Bender 2013; Clark et al. 2012; Mitkov\n2005) can be considered as the understanding of written or spoken language\nfrom computational and scientific perspective. It focuses on building\nartifacts to process and analyze language. Language is like a mirror of mind\nto reflect of what humans think. A computational interpretation of language\nprovides a new insight to how human thinks and intelligence works.\n\nAs human language is natural and the most polytropic means of communication\neither person-to-person or person-to-machine, linguistically enabled computer\nsystems provide a new era of NLP applications. There are two major issues to\naddress in computational linguistics: (1) linguistic itself refers to facts\nabout language and (2) algorithmic refers to effective computational\nprocedures dealing with these facts.\n\nThe major goals of computational linguists include:\n\n  1. 1.\n\nConstruction of grammatical and semantic frameworks/models for languages\ncharacterization,\n\n  2. 2.\n\nRealization of learning models for the exploration of both structural and\ndistributional properties of language, and\n\n  3. 3.\n\nExploration of neuroscience and cognitive oriented computational models of how\nlanguage processing and learning works in our brains.\n\nThus, POS and POS tagging can be considered as the fundamental process in\ncomputational linguistics to understand and model human languages.\n\n### 3.3.2 POS and Semantic Meaning\n\nThe elementary level of language _semantics_ (Goddard 1998) is to describe\nactual meaning of _word forms_. For example, a _noun_ may be a category of\nwords for people, locations, and things. _Adjective_ may be the category of\nwords for properties of nouns.\n\nConsider: [3.7] _green book_ in which _green_ is an _adjective_ while _book_\nis a _noun_.\n\nIn fact, the word _book_ can have two meanings: (1) description of word _book_\nfrom dictionary, (2) noun in a sentence which is an object. For the word\n_green_ , it has an in-depth interpretation of (1) an adjective to describe\nthe book in green color and (2) semantic meaning to describe the book in\ngreen.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84822d53-e37d-4927-ae72-dee3c757a1f8": {"__data__": {"id_": "84822d53-e37d-4927-ae72-dee3c757a1f8", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "781e5e69-16ec-459e-aaac-5752fa3a22f4", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f294f50c1b7deebd74784d3b0087817f4774ef41e7c4cea96ac50401b9c012c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf61401b-767e-46c8-a081-d3eef3139e70", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "6e7ee39a88f76223106984e0dc938707af212542d7afae6e8649aec797e832de", "class_name": "RelatedNodeInfo"}}, "hash": "caa89b4649f426e9c03f2b79e9105f4e588c4e91df1882fe4e2d37e13c40871c", "text": "### 3.3.2 POS and Semantic Meaning\n\nThe elementary level of language _semantics_ (Goddard 1998) is to describe\nactual meaning of _word forms_. For example, a _noun_ may be a category of\nwords for people, locations, and things. _Adjective_ may be the category of\nwords for properties of nouns.\n\nConsider: [3.7] _green book_ in which _green_ is an _adjective_ while _book_\nis a _noun_.\n\nIn fact, the word _book_ can have two meanings: (1) description of word _book_\nfrom dictionary, (2) noun in a sentence which is an object. For the word\n_green_ , it has an in-depth interpretation of (1) an adjective to describe\nthe book in green color and (2) semantic meaning to describe the book in\ngreen.\n\nNow consider: [3.8] _book worm_???\n\n[3.9] _This green is very smoothing???_\n\nHere the word _book_ has the same spelling and pronunciation as [3.8] but it\nbecomes an adjective instead of a noun because of the _semantic meaning_ of\n_book worm_. In [3.9], _green_ becomes a noun instead of an adjective because\nof semantic meaning consideration in the whole sentence/utterance. So, POS of\nevery word/word pattern can be varied when considering the role in overall\nsemantic meaning of sentence/utterance.\n\n### 3.3.3 Morphological and Syntactic Definition of POS\n\nBased on morphological well-defined grammatic rules\u2019 structure, when there is\nan adjective that can fill in the blank, e.g. [3.10] _It\u2019s so ______ , it can\nbe _difficult, expensive, small_ etc. This rules\u2019 structure gives shape to\nappropriate POS tag for description, e.g. when a noun is a word that can be\nlabelled as plural means it can be defined in either singular or plural form\nwith _s_ , or the other way round which is a two-way process. Thus, when a\ntagger tags a word with _s_ , it gives hints that the word may contain _s_ or\na noun in plural, e.g. cat or cats.\n\nConversely, when there is a noun that can fill in the blank, e.g. [3.11] _the\n_____ is so pretty_ , it can be _decoration_ , _house_ , _painting_ etc. and\nconscious of not using a proper noun, e.g. the _Tesla_.\n\nConsider the following situations, what is the POS for word _purple_ :\n\n  * [3.12] _It\u2019s so purple._\n\n  * [3.13] _Both purples should be okay for the room._\n\n  * [3.14] _The purple is a bit odd for the white carpet._\n\nIn [3.12] purple is an adjective. However, in [3.13] is a particular noun in\nplural forms. Same notion for purple in [3.14] is also an indifferent noun to\nclassify as a group against uncountable objects in purple.\n\n## 3.4 9 Key POS in English\n\nThere are nine key POS in English: (1) pronoun, (2) verb, (3) adjective, (4)\ninterjection, (5) noun, (6) adverb, (7) conjunction, (8) preposition, and (9)\narticle as shown in Fig. 3.7. Some linguists consider interjections as\nseparate POS category to express strong feeling or emotion in a single word or\na phrase, e.g. [3.15] _Hooray! It\u2019s the last day of school_. It is distinct\ncompared with other POS.\n\n![](../images/533412_1_En_3_Chapter/533412_1_En_3_Fig7_HTML.png)\n\nFig. 3.7\n\nNine major POS in English language with description\n\n### 3.4.1 English Word Classes\n\nThere are two types of English _word classes_ : (1) _closed-class_ and (2)\n_open-class_. Both classes are important to understand proper sentences in\ndifferent languages.\n\n_Closed-class_ words are also known as _functional/grammar words_. They are\n_closed_ since new words are seldom created in the class. For example,\n_conjunctions_ , _determiners_ , _pronouns_ _,_ and _prepositions_ are\n_closed-class_.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf61401b-767e-46c8-a081-d3eef3139e70": {"__data__": {"id_": "bf61401b-767e-46c8-a081-d3eef3139e70", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84822d53-e37d-4927-ae72-dee3c757a1f8", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "caa89b4649f426e9c03f2b79e9105f4e588c4e91df1882fe4e2d37e13c40871c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0076e169-797d-4bf7-820b-b71ddd3cc632", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "46adc2e427f3c97b051f6c0d35594f65834d8b6578bae30fc83dbcc3bd0df974", "class_name": "RelatedNodeInfo"}}, "hash": "6e7ee39a88f76223106984e0dc938707af212542d7afae6e8649aec797e832de", "text": "[3.15] _Hooray! It\u2019s the last day of school_. It is distinct\ncompared with other POS.\n\n![](../images/533412_1_En_3_Chapter/533412_1_En_3_Fig7_HTML.png)\n\nFig. 3.7\n\nNine major POS in English language with description\n\n### 3.4.1 English Word Classes\n\nThere are two types of English _word classes_ : (1) _closed-class_ and (2)\n_open-class_. Both classes are important to understand proper sentences in\ndifferent languages.\n\n_Closed-class_ words are also known as _functional/grammar words_. They are\n_closed_ since new words are seldom created in the class. For example,\n_conjunctions_ , _determiners_ , _pronouns_ _,_ and _prepositions_ are\n_closed-class_. Conversely, new items are added to _open classes_ regularly.\nAs _closed-class_ words are usually used with a particular grammatical\nstructure, it cannot be interpreted in isolation, e.g. [3.16] _the style of\nthis painting_ , both _the_ and _this_ have no special meaning as compared\nwith _painting_ that has a specific meaning in usual knowledge.\n\n_Open-class_ words are also known as _lexical/content words_. They are _open_\nsince the meaning of an _open-class_ word to be found in dictionary so the\nmeaning can be interpreted in isolation. For example, _noun_ , _verb_ ,\n_adjective_ , and _adverbs_ are _open-class_ that made up of the entire\nsubclass of words. These connective words are restrictive and used frequently\nto describe different _scenarios_ or _meanings_ about spatial positions of two\nobject nouns, e.g. [3.17] _The cat sits by/under/above the piano_. Further,\nthere are new types of _open-class_ objects created from scratch or\ncombination of existing word according to contemporary times, e.g. _fax,\ntelex, internet, iPhone, hub, bitcoin_ , _metaverse,_ etc.\n\n### 3.4.2 What Is a Preposition?\n\n_Preposition_ _(PP)_ is POS with a word (group of words) being used before a\nnoun, pronoun, or a noun phrase to indicate direction, location, spatial\nrelationships, time; or to describe an object; or information to the\nrecipient. There are approximately 80\u2013100 prepositions in English to generate\nfunctional sentences/utterances.\n\nThis information can include where something takes place, e.g. [3.18] _before\ndinner_ , or general descriptive information e.g. [3.19] _the girl with\nponytail_. The target of preposition is the noun that followed the\npreposition. It is also the ending point for each preposition phrase. For\ninstance, [3.20] _to the supermarket_. The word _to_ is a preposition and\n_supermarket_ is the target of preposition, and [3.21] _over the rainbow_ ,\nthe word _over_ is the preposition and _rainbow_ is the target of preposition.\nA list of top 40 preposition from CELEX online dictionary (CELEX 2022) of\nCOBUILD 16-million-word corpus is shown in Fig. 3.8. It showed _of, in, for,\nto_ and _with_ are the top five prepositions to correlate with ideas and\nadditional information of a sentence/utterance.\n\n![](../images/533412_1_En_3_Chapter/533412_1_En_3_Fig8_HTML.png)\n\nFig. 3.8\n\nTOP 40 commonly used prepositions extracted from CELEX online dictionary\n\n### 3.4.3 What Is a Conjunction?\n\n_Conjunction_ _(CONJ or CNJ)_ is POS to connect words, clauses, or phrases\nthat are known as _conjuncts_. This definition may sometime overlap with other\nPOS so that the constitute of a conjunction must be defined for each foreign\nlanguage. For instance, a word in English may have several senses and\nmeanings. It can be considered as either a conjunction or preposition highly\ndependable on syntax of the sentence/utterance, e.g. _after_ is a preposition\nin [3.22] _Jane left after the show_ but is a conjunction in [3.23] _Jane left\nafter she finished her homework_.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0076e169-797d-4bf7-820b-b71ddd3cc632": {"__data__": {"id_": "0076e169-797d-4bf7-820b-b71ddd3cc632", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf61401b-767e-46c8-a081-d3eef3139e70", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "6e7ee39a88f76223106984e0dc938707af212542d7afae6e8649aec797e832de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "344ec229-3569-4665-a445-eb81a1513958", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3be566ebe398cfbd905808e7e26649e474389c134442f79ffc230dff49c27675", "class_name": "RelatedNodeInfo"}}, "hash": "46adc2e427f3c97b051f6c0d35594f65834d8b6578bae30fc83dbcc3bd0df974", "text": "3.8\n\nTOP 40 commonly used prepositions extracted from CELEX online dictionary\n\n### 3.4.3 What Is a Conjunction?\n\n_Conjunction_ _(CONJ or CNJ)_ is POS to connect words, clauses, or phrases\nthat are known as _conjuncts_. This definition may sometime overlap with other\nPOS so that the constitute of a conjunction must be defined for each foreign\nlanguage. For instance, a word in English may have several senses and\nmeanings. It can be considered as either a conjunction or preposition highly\ndependable on syntax of the sentence/utterance, e.g. _after_ is a preposition\nin [3.22] _Jane left after the show_ but is a conjunction in [3.23] _Jane left\nafter she finished her homework_.\n\n_Co-ordinating conjunction_ allows joining words, clauses, or phrases of equal\ngrammatic rank in a sentence/utterance. Common coordinating conjunctions are\n_and, but, for, nor_ or _yet_ which include logical meaning at times.\n\n_Subordinating_ _conjunctions_ join independent and dependent clauses. A\nsubordinating conjunction can present a _causation_ relationship, or some\nkinds of relationship between different words, clauses, or phrases. Common\nsubordinating conjunctions are _as, although, because, since, though, while,_\nand _whereas_. In many situations, a conjunction is a non-inflected\ngrammatical item, it may or may not link up the items being conjoined, e.g.\n[3.24] _the book is so difficult that is hard for children to read_ , _that_\nis to describe about the book to connect two ideas and [3.25] _this painting\nis very beautiful but is expensive_. In this case _but_ is to explain an\ninitial idea to correlate with second idea. A list of top 50 commonly used\ncoordinating and subordinating conjunctions from CELEX online dictionary is\nshown in Fig. 3.9. It showed _and, that, or,_ and _as_ are used frequently to\nconvey more than one concept at the same time or further explanation.\n\n![](../images/533412_1_En_3_Chapter/533412_1_En_3_Fig9_HTML.png)\n\nFig. 3.9\n\nTOP 50 commonly used conjunctions extracted from CELEX online dictionary\n\n### 3.4.4 What Is a Pronoun?\n\n_Pronoun_ _(PRN or PN)_ is POS that can be considered as a word (phrase) to\nserve as substitution for a noun or noun phrase. It is also called the\npronoun\u2019s _antecedent_. Pronouns are usually appeared as short words to\nreplace a noun (noun phrase) for the construction of a sentence/utterance.\nCommonly used pronouns are _I_ , _he, she, you, me, we, us, this, them, that_.\n\nA pronoun can be served as a subject, direct (indirect) object, object of\npreposition and more to substitute any person, location, animal, or thing. It\ncan replace a person\u2019s name in a sentence/utterance, e.g. [3.26] _Jack is sick\ntoday, he cannot attend the evening seminar_. Pronoun is also a powerful tool\nto simplify the contents of a dialogue and conversation by replacing with\nsimple token. A list of top 50 commonly used pronouns extracted from CELEX\nonline dictionary is shown in Fig. 3.10. It showed _it, I, he, you,_ and _his_\nare used frequently.\n\n![](../images/533412_1_En_3_Chapter/533412_1_En_3_Fig10_HTML.png)\n\nFig. 3.10\n\nTOP 50 commonly used pronouns extracted from CELEX online dictionary\n\nThe truth is, without pronouns, nouns become repetitive and cumbersome in\nspeech and writing. However, pronoun may cause ambiguity, e.g. [3.27] _Jack\nblamed Ivan for losing the car key, he felt sorry for that_. _He_ normally\nrefers to the first person which is _Jack_ but make sense in pragmatic meaning\nfor Ivan to _feel sorry_ because _Jack_ blamed him for the loss.\n\n### 3.4.5 What Is a Verb?\n\n_Verb_ _(VB)_ can be considered as a word syntax to conduct an action,\nprocess, occurrence, or state-of-being.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "344ec229-3569-4665-a445-eb81a1513958": {"__data__": {"id_": "344ec229-3569-4665-a445-eb81a1513958", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0076e169-797d-4bf7-820b-b71ddd3cc632", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "46adc2e427f3c97b051f6c0d35594f65834d8b6578bae30fc83dbcc3bd0df974", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2cd35ce-3af4-468c-af3d-b58d6778b03a", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "57db6c8a9fd9993bc41d508a565ad86a368f8689f951ea7e4ab8ea557872bdad", "class_name": "RelatedNodeInfo"}}, "hash": "3be566ebe398cfbd905808e7e26649e474389c134442f79ffc230dff49c27675", "text": "![](../images/533412_1_En_3_Chapter/533412_1_En_3_Fig10_HTML.png)\n\nFig. 3.10\n\nTOP 50 commonly used pronouns extracted from CELEX online dictionary\n\nThe truth is, without pronouns, nouns become repetitive and cumbersome in\nspeech and writing. However, pronoun may cause ambiguity, e.g. [3.27] _Jack\nblamed Ivan for losing the car key, he felt sorry for that_. _He_ normally\nrefers to the first person which is _Jack_ but make sense in pragmatic meaning\nfor Ivan to _feel sorry_ because _Jack_ blamed him for the loss.\n\n### 3.4.5 What Is a Verb?\n\n_Verb_ _(VB)_ can be considered as a word syntax to conduct an action,\nprocess, occurrence, or state-of-being. In general, verbs are inflected to\nencode tense, aspect, mood, and voice in many languages, but are\ninterchangeable with nouns of a word in some foreign languages. In English, a\nverb may also conform with gender, person, or numbers of arguments such as its\nsubject or object.\n\nEnglish verbs have tenses consideration: (1) present tense to notify that an\naction is being carried out, (2) past tense to notify that an action has been\ncompleted, (3) future tense to notify that an action to be happened in future,\nand (4) future perfect tense to notify an action will be completed in future.\n\nA _modal verb_ is a category of verb that contextually indicates a modality\nsuch as ability, advice, capacity, likelihood, order, obligation, permission,\nrequest, or suggestion. It is usually accompanied by the base (infinitive\nform) of another word with semantic contents. Common modal verbs are _can,\ncould, may, might, shall, should, will, would,_ and _must_. A list of top 25\ncommonly used verbs from CELEX online dictionary is shown in Fig. 3.11. It\nshowed _can, will, may, would,_ and _should_ are used frequently. They also\nexpress significance in subsequent verb, e.g. verb following _can_ and _will_\nmust use present tense, not past tense.\n\n![](../images/533412_1_En_3_Chapter/533412_1_En_3_Fig11_HTML.png)\n\nFig. 3.11\n\nTOP 25 commonly used modal verbs extracted from CELEX online dictionary\n\n## 3.5 Different Types of POS Tagset\n\n### 3.5.1 What Is Tagset?\n\nThere are nine POS in English, pronoun, verb, adjective, interjection, noun,\nadverb, conjunction, preposition, and article learnt as students but there are\nclearly more subcategories that can be further divided. For example, in nouns,\nthe plural, possessive, and singular forms can be distinguished and further\nclassified.\n\nA _Tagset_ is a batch of POS tags ( _POS tags or POST_ ) to indicate the part\nof speech and sometimes other grammatical categories such as case, tense for\nthe classification of each word in a sentence/utterance.\n\n_Brown Corpus_ _Tagset_ (Brown 2022), _PENN Treebank Tagset_ (Treebank 2022),\nand _CLAWS_ (CLAWS7 2022) are commonly used. _Brown Corpus_ was the first\nwell-organized corpus of English for NLP analysis developed by Profs Emeritus\nHenry Ku\u010dera (1925\u20132010) and W. Nelson Francis (1910\u20132002) at Brown\nUniversity, USA in mid-1960s. It consists of over 1 million of English words\nwhich extracted from over 500 samples of randomly chosen publications. Each\nsample consists of over 2000 words with 87 tags defined (Brown 2022).\n\nThe English _PENN Treebank_ _Tagset_ originated by English corpora is\nannotated with TreeTagger tool. PENN Treebank Tagset is developed by Prof.\nHelmud Schmid in the University of Stuttgart, Germany. It consists of 45\ndistinct tags (Abeill\u00e9 2003; Treebank 2022).\n\nEnglish _CLAWS_ _part-of-speech_ _Tagset_ _version 7_ , also called _C7\nTagset_ is available in English corpora annotated with tools using CLAWS\n(Constituent Likelihood Automatic Word-tagging System). C7 Tagset is developed\nby the University Centre for Computer Corpus Research on Language at Lancaster\nUniversity.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2cd35ce-3af4-468c-af3d-b58d6778b03a": {"__data__": {"id_": "a2cd35ce-3af4-468c-af3d-b58d6778b03a", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "344ec229-3569-4665-a445-eb81a1513958", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3be566ebe398cfbd905808e7e26649e474389c134442f79ffc230dff49c27675", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8f116ff-73b1-43df-a4a2-230fa30df2b7", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "318190849db807d7e56d33df238c51e8d53209c1892a36b0d2bc01bd8a5cfa14", "class_name": "RelatedNodeInfo"}}, "hash": "57db6c8a9fd9993bc41d508a565ad86a368f8689f951ea7e4ab8ea557872bdad", "text": "It consists of over 1 million of English words\nwhich extracted from over 500 samples of randomly chosen publications. Each\nsample consists of over 2000 words with 87 tags defined (Brown 2022).\n\nThe English _PENN Treebank_ _Tagset_ originated by English corpora is\nannotated with TreeTagger tool. PENN Treebank Tagset is developed by Prof.\nHelmud Schmid in the University of Stuttgart, Germany. It consists of 45\ndistinct tags (Abeill\u00e9 2003; Treebank 2022).\n\nEnglish _CLAWS_ _part-of-speech_ _Tagset_ _version 7_ , also called _C7\nTagset_ is available in English corpora annotated with tools using CLAWS\n(Constituent Likelihood Automatic Word-tagging System). C7 Tagset is developed\nby the University Centre for Computer Corpus Research on Language at Lancaster\nUniversity. It bases on Hidden Markov model to determine the likelihood of\nsentences, sequences of words in anticipating each POS label. It consists of\n146 distinct tags (CLAWS7 2022).\n\n### 3.5.2 Ambiguous in POS Tags\n\nIt may wonder the necessity of tagset databank against dictionary to check out\nPOS. A reason is that there are ambiguities in POS tags for many words:\n\n  1. 1.\n\nNoun-verb ambiguity\n\nFor example: record: [3.28] _records the lecture_ vs. [3.29] _play CD\nrecords._\n\n  2. 2.\n\nAdjective-verb ambiguity\n\nFor example: perfect: [3.30] _a perfect plan_ vs. [3.31] _Jack perfects the\ninvention_.\n\n  3. 3.\n\nAdjective-noun ambiguity\n\nFor example: complex: [3.32] a complex case vs. [3.33] a shopping complex.\n\nFigure 3.12 shows an ambiguous analysis of words in Brown corpus (DeRose\n1988). One tag refers to a word tagged with single POS type, 2\u20137 tags refer to\na word tagged with several POS types. For example, a 3 POS ambiguous tag for\n_green_ : (a) [3.34] _colour green_ (noun), (b) [3.35] _a green apple_\n(adjective) and (c) [3.36] _the roof was greening with leaves_ (verb). A 7 POS\nambiguous tag for _still:_ (a) _[3.37] the still status_ (adjective), (b)\n[3.38] _the still of the night_ (noun), (c) [3.39] _it was still snowing_\n(adverb), and (d) [3.40] _Her quiet words stilled the animal_ (verb) (Note: As\nan exercise, find out the other three POS tag usages for _still_ ). Overall\nspeaking, there is a total of 10.4% ambiguous word types often used in\nlanguage in which over 40% ambiguous words are easy to disambiguate.\n\n![](../images/533412_1_En_3_Chapter/533412_1_En_3_Fig12_HTML.png)\n\nFig. 3.12\n\nAmbiguous analysis of words in Brown corpus\n\n### 3.5.3 POS Tagging Using Knowledge\n\nThere are four methods to acquire knowledge from POS tagging: (1) dictionary,\n(2) morphological rules, (3) N-gram frequencies, and (4) structural\nrelationships combination.\n\n_Dictionary_ is the basic method for tag usage, but it may not be fully\nreliable because there are ambiguous words meaning that the same word can have\nmore than single POS tagging in diverse scenarios.\n\n_Morphological rules_ are to identify well-known words shapes and patterns,\ne.g. the inflection _-ed_ for _past tense_ , verb + _-ing_ for _continuous\nform_ , _-tion_ for _noun description_ , _-ly_ for _adjective_ , and\ncapitalization such as _New York_ for _proper noun_.\n\n_N-gram_ _frequencies_ checking, also called _next word prediction,_ e.g.\ngrammatic pattern _to ____. When there is a _to_ , if the next word is a\n_verb_ , it must be in _present_ and not _past tense_. If it is a _determiner_\n, the next word must be a _noun_.\n\n_Structural relationships combination method_ means to combine several methods\nto acquire tag information. e.g.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8f116ff-73b1-43df-a4a2-230fa30df2b7": {"__data__": {"id_": "b8f116ff-73b1-43df-a4a2-230fa30df2b7", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2cd35ce-3af4-468c-af3d-b58d6778b03a", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "57db6c8a9fd9993bc41d508a565ad86a368f8689f951ea7e4ab8ea557872bdad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6eb8953-5820-4348-92fc-6b2374b0eba3", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "804f206815c1b9b7edfba1aeaa8ac401efe3274e66457b70d4b2124f758227a0", "class_name": "RelatedNodeInfo"}}, "hash": "318190849db807d7e56d33df238c51e8d53209c1892a36b0d2bc01bd8a5cfa14", "text": "_Morphological rules_ are to identify well-known words shapes and patterns,\ne.g. the inflection _-ed_ for _past tense_ , verb + _-ing_ for _continuous\nform_ , _-tion_ for _noun description_ , _-ly_ for _adjective_ , and\ncapitalization such as _New York_ for _proper noun_.\n\n_N-gram_ _frequencies_ checking, also called _next word prediction,_ e.g.\ngrammatic pattern _to ____. When there is a _to_ , if the next word is a\n_verb_ , it must be in _present_ and not _past tense_. If it is a _determiner_\n, the next word must be a _noun_.\n\n_Structural relationships combination method_ means to combine several methods\nto acquire tag information. e.g. [3.41] _She barely heard the foghorns_\n_knelling_ _her demise_ vs. [3.42] The hunter\u2019s horn sounded the final knell.\nIf there is no understanding on what knell means, there is an -ing pattern to\nindicate that is a verb in continuous tense, and final is an adjective\ndescription to indicate that knell is likely a noun.\n\n## 3.6 Approaches for POS Tagging\n\nThere are three basic approaches of POS Tagging: (1) Rule-based, (2)\nStochastic-based, and (3) Hybrid Tagging.\n\n### 3.6.1 Rule-Based Approach POS Tagging\n\n_Rule-based_ is classical approach in linguistic (Sree and Thottempudi 2011).\nThe grammars knowledge learnt in primary schools are in fact grammatic rules\nwhich means that rule-based approach is the transfer of linguistic rule base\nusage into POS tagging.\n\nIt is a two stages process: (1) dictionary consists of all possible POS tags\nfor basic concepts of words as abovementioned, (2) words with more than single\ntag ambiguity applied handwritten or grammatic rules to assign the correct\ntag(s) according to surrounding words. The obtained rule sets directly affect\ntagging results accuracy. The lexicon is used initially for basic segmentation\nand tagging of the corpus, listing all possible lexical properties of the\nobject, and combine rule-base with contextual information to disambiguate and\nretain the only suitable lexical properties.\n\nThe rule generation can be achieved by (1) hand creation and (2) training from\na corpus with machine learning. The advantages of hand creation are that it is\nmore sensible and explainable to humans, but manual construction of rules is\nusually labor intensive. Also, if rules are described with too many details,\nthe coverage of rules will be greatly reduced and difficult to adjust\naccording to actual situation. Conversely, if rules are not based on contexts\nbut rather on the lexical nature of rules, ambiguity may arise, i.e. If the\npreceding of a word is an article, then the word must be a noun.\n\nFor example, consider: [3.43] _a book_. _a_ is an article as per possible tags\nthat can assign directly, but _a book_ can either be a noun or a verb. If\nconsider _a book_ , _a_ is an article and follow rules above, _book_ should be\na _noun_ because _article_ is often followed by a _noun,_ so a tag of _noun_\nis assigned to _book_. Word structures are often complex leading to more\nambiguities and rules are required for differentiation.\n\n### 3.6.2 Example of Rule-Based POS Tagging\n\nStep 1: Assign each word with a list of possible tags based on a dictionary.\n\nStep 2: Work out unknown and ambiguous words with two approaches: (1) rules\nthat specify what (1) _to do_ ; and (2) _not to do_.\n\nFigure 3.13 shows a sample adverbial _that_ rule (Jurafsky et al. 1999):\n\n![](../images/533412_1_En_3_Chapter/533412_1_En_3_Fig13_HTML.png)\n\nFig. 3.13\n\nSample rule for adverbial \u201c _that_ \u201d rule\n\nIt showed that:\n\n  * The first two statements of this rule verify the word _that_ is directly precedes a sentence/utterance\u2019s final adjective, adverb, or quantifier.\n\n  * For all other cases, the adverb reading is eliminated.\n\n  * The last clause eliminates cases which are preceded by verbs like _consider_ or _believe_ which can take a noun and an adjective.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6eb8953-5820-4348-92fc-6b2374b0eba3": {"__data__": {"id_": "e6eb8953-5820-4348-92fc-6b2374b0eba3", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8f116ff-73b1-43df-a4a2-230fa30df2b7", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "318190849db807d7e56d33df238c51e8d53209c1892a36b0d2bc01bd8a5cfa14", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "13e765d8-be27-4c9e-976d-ba30c50de3a7", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "2759a61332c4833af55eba368c10120b9b98d60301f9853c95a3ba8a18b6802b", "class_name": "RelatedNodeInfo"}}, "hash": "804f206815c1b9b7edfba1aeaa8ac401efe3274e66457b70d4b2124f758227a0", "text": "Step 2: Work out unknown and ambiguous words with two approaches: (1) rules\nthat specify what (1) _to do_ ; and (2) _not to do_.\n\nFigure 3.13 shows a sample adverbial _that_ rule (Jurafsky et al. 1999):\n\n![](../images/533412_1_En_3_Chapter/533412_1_En_3_Fig13_HTML.png)\n\nFig. 3.13\n\nSample rule for adverbial \u201c _that_ \u201d rule\n\nIt showed that:\n\n  * The first two statements of this rule verify the word _that_ is directly precedes a sentence/utterance\u2019s final adjective, adverb, or quantifier.\n\n  * For all other cases, the adverb reading is eliminated.\n\n  * The last clause eliminates cases which are preceded by verbs like _consider_ or _believe_ which can take a noun and an adjective.\n\n  * The logic behind is to avoid tagging the following instance of _that_ as an adverb such as [3.44] _It isn\u2019t that odd_.\n\n  * The other rule is used to verify if the previous word is a verb which expects a complement (like _think_ or _hope_ ), and if _that_ is followed by the beginning of a noun phrase, and a finite verb such as [3.45] _I consider that a win_ or more complex structure such as [3.46] _I hope that she is confident._\n\n_Stochastic-based approach_ (Dermatas and Kokkinakis 1995) is different from\nrule-based approach in which it is a supervised model using frequencies or\nprobabilities of tags appeared in the training corpus to assign a tag to a new\nword. This tagging method depends on tag occurrence statistics, i.e.\nprobability of the tags. Stochastic taggers are further categorized into two\nparts: (1) _word frequency_ and (2) _tag sequence frequency_ to determine a\ntag.\n\n_Word frequency_ is to identify the tag that has a notable occurrence of the\nword, e.g. based on the counting from a corpus, the word _list_ occurs ten\ntimes in which six times as _noun_ and four times as _verb_ , and the word\n_cloud_ will always be assigned as _noun_ since it has a notable occurrence in\nthe training corpus. Hence, a word frequency approach is not very reliable in\ncertain scenario.\n\n_Tag sequence frequency_ , also called N-gram approach is assigned the best\ntag to a word evaluated by the probability of N previous words tags. Although\nit provides better outcomes than word frequency approach, it may be unable to\nprovide accurate outcomes for some rare words and phrases.\n\n_Stochastic POS tag model_ allows features to be non-independent and allows\nfor the addition of various granularities features. Hidden Markov Model (HMM)\nTagger is a common stochastic-based approach, its Maximum Entropy Markov Model\n(MEMM) (Huang and Zhang 2009) is a stochastic POS tagging model that\ndetermines an exponential algorithm for each state as the conditional\nprobability of the next state given the current state, which has the\nadvantages of a stochastic POS tagging model. However, it also suffers from\nlabel bias problems. Unlike MEMM model, the Conditional Random Field (CRF)\nmodel uses only one model as the joint probability of the entire label\nsequence given the observations sequence. Lafferty et al. (2001) verified that\nthis model can effectively solve the tagging bias problems.\n\n### 3.6.3 Example of Stochastic-Based POS Tagging\n\nLet us use HMM Tagger as example. The rationale of HMM tagger is applying\nN-gram frequencies to determine the best tag for a given word, like the same\nconcept to investigate N-gram with Markov Chain. Mathematically, all is needed\nto maximize the conditional probability. The conditional probability _w_ _i_\nis tag _t_ _i_ in the context given _w_ _i_ by\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13e765d8-be27-4c9e-976d-ba30c50de3a7": {"__data__": {"id_": "13e765d8-be27-4c9e-976d-ba30c50de3a7", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6eb8953-5820-4348-92fc-6b2374b0eba3", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "804f206815c1b9b7edfba1aeaa8ac401efe3274e66457b70d4b2124f758227a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2797a072-f1ee-4430-90d2-be5174db837a", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "b23f973442601604bb6ab0d7db26e1c834fdbb91d8e44e77db19d7dcfd00eff5", "class_name": "RelatedNodeInfo"}}, "hash": "2759a61332c4833af55eba368c10120b9b98d60301f9853c95a3ba8a18b6802b", "text": "However, it also suffers from\nlabel bias problems. Unlike MEMM model, the Conditional Random Field (CRF)\nmodel uses only one model as the joint probability of the entire label\nsequence given the observations sequence. Lafferty et al. (2001) verified that\nthis model can effectively solve the tagging bias problems.\n\n### 3.6.3 Example of Stochastic-Based POS Tagging\n\nLet us use HMM Tagger as example. The rationale of HMM tagger is applying\nN-gram frequencies to determine the best tag for a given word, like the same\nconcept to investigate N-gram with Markov Chain. Mathematically, all is needed\nto maximize the conditional probability. The conditional probability _w_ _i_\nis tag _t_ _i_ in the context given _w_ _i_ by\n\n![$$ P\\\\left\\({t}_i\\\\mathrm{in}\\\\\n\\\\mathrm{context}|{w}_i\\\\right\\)=\\\\frac{P\\\\left\\({w}_i|{t}_i\\\\mathrm{in}\\\\\n\\\\mathrm{context}\\\\right\\)P\\\\left\\({t}_i\\\\;\\\\mathrm{in}\\\\\n\\\\mathrm{context}\\\\right\\)}{P\\\\left\\({w}_i\\\\right\\)}\n$$](../images/533412_1_En_3_Chapter/533412_1_En_3_Chapter_TeX_Equ1.png)\n\n(3.1)\n\nIn other words, given a sentence/utterance or word sequence, HMM taggers\nselect tag sequence that maximizes the following formula given by\n\n![$$ P\\\\left\\(\\\\mathrm{word}|\\\\mathrm{tag}\\\\right\\)\\\\ast\nP\\\\left\\(\\\\mathrm{tag}|\\\\mathrm{previous}\\\\;n\\\\;\\\\mathrm{tags}\\\\right\\)\n$$](../images/533412_1_En_3_Chapter/533412_1_En_3_Chapter_TeX_Equ2.png)\n\n(3.2)\n\nFor bigram-HMM tagger, select tag _t_ _i_ for _w_ _i_ , that is most probable\ngiven the previous tag _t_ _i_ \u22121, and the current word _w_ _i_ in this\nequation by\n\n![$$ {t}_i={}_{j\\\\kern2.33em\n}{}^{\\\\mathrm{argmax}}P\\\\left\\({t}_j|,{t}_{i-1}|,{w}_i\\\\right\\)\n$$](../images/533412_1_En_3_Chapter/533412_1_En_3_Chapter_TeX_Equ3.png)\n\n(3.3)\n\nBy simplifying Markov assumptions, the previous equation is applied to give\nbasic HMM equation for a single tag as follows:\n\n![$$ {t}_i={}_{j\\\\kern2.33em\n}{}^{\\\\mathrm{argmax}}P\\\\left\\({t}_j|{t}_{i-1}\\\\right\\)P\\\\left\\({w}_i|{t}_j\\\\right\\)\n$$](../images/533412_1_En_3_Chapter/533412_1_En_3_Chapter_TeX_Equ4.png)\n\n(3.4)\n\n### 3.6.4 Hybrid Approach for POS Tagging Using Brill Taggers\n\nHybrid approach is the integration of rule-based and stochastic with high-\nlevel methods including neural networks such as LSTM and other machine\nlearning related methods often applied in NLP nowadays. Let us study an\nimportant hybrid approach for POS Tagging\u2014Transformation-based tagging, also\ncalled Brill Taggers invented by Dr. Eric Brill in 1995 (Brill 1995). It is a\ndirect Transformation-Based Learning (TBL) implementation based on the\nintegration of these two approaches.\n\n#### 3.6.4.1 What Is Transformation-Based Learning?\n\nThere are five steps in TBL by comparison to analogue on oil painting with\n_layering-and-refinement_ approach.\n\n  1. 1.\n\nStart with background _theme_ such as sky or household background.\n\n  2. 2.\n\nPaint background first, e.g. if sky is the background scheme, paint clouds\nover it.\n\n  3. 3.\n\nPaint the _main theme_ or _object_ over the background, e.g. landscape, birds.\n\n  4. 4.\n\nRefine the _main theme_ or _object_ over background to make it more precise,\ne.g. paint landscape, add trees and animals layer-by-layer.\n\n  5. 5.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2797a072-f1ee-4430-90d2-be5174db837a": {"__data__": {"id_": "2797a072-f1ee-4430-90d2-be5174db837a", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "13e765d8-be27-4c9e-976d-ba30c50de3a7", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "2759a61332c4833af55eba368c10120b9b98d60301f9853c95a3ba8a18b6802b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80576584-c57c-4e47-895e-33f0c5f9e477", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "00d27fe148c12e591cf3ac8e2bef7375d55a45605bcae43c1292a3d7678098a2", "class_name": "RelatedNodeInfo"}}, "hash": "b23f973442601604bb6ab0d7db26e1c834fdbb91d8e44e77db19d7dcfd00eff5", "text": "Eric Brill in 1995 (Brill 1995). It is a\ndirect Transformation-Based Learning (TBL) implementation based on the\nintegration of these two approaches.\n\n#### 3.6.4.1 What Is Transformation-Based Learning?\n\nThere are five steps in TBL by comparison to analogue on oil painting with\n_layering-and-refinement_ approach.\n\n  1. 1.\n\nStart with background _theme_ such as sky or household background.\n\n  2. 2.\n\nPaint background first, e.g. if sky is the background scheme, paint clouds\nover it.\n\n  3. 3.\n\nPaint the _main theme_ or _object_ over the background, e.g. landscape, birds.\n\n  4. 4.\n\nRefine the _main theme_ or _object_ over background to make it more precise,\ne.g. paint landscape, add trees and animals layer-by-layer.\n\n  5. 5.\n\nFurther refine _objects_ or _main theme_ until perfect, e.g. apply layering\nprocess or refinement for every single tree and animal (Fig. 3.14).\n\n![](../images/533412_1_En_3_Chapter/533412_1_En_3_Fig14_HTML.jpg)\n\nFig. 3.14\n\nOil painting analog to Brill Tagger transformation technique (Tuchong 2022)\n\n#### 3.6.4.2 Hybrid POS Tagging: Brill Tagger\n\n_Brill Tagger_ is a type of hybrid TBL. Hybrid refers to integrate _rule-\nbased_ and _stochastic-based_ methods in a Brill\u2019s algorithm.\n\nRule 1: Label each word of the tag that is mostly likely given on contextual\ninformation, e.g.\n\n![$$\n\\\\mathrm{Race}:P\\\\left\\(\\\\mathrm{NN}\\\\;|\\\\;\\\\mathrm{race}\\\\right\\)=0.98;\\\\kern1.6em\nP\\\\left\\(\\\\mathrm{VB}\\\\;|\\\\;\\\\mathrm{race}\\\\right\\)=0.02\n$$](../images/533412_1_En_3_Chapter/533412_1_En_3_Chapter_TeX_Equa.png)\n\nRule 2: Apply transformation rule based on the context established.\n\nExample:\n\n  * _Race: change NN to VB when the previous tag is TO_.\n\n  * [3.47] _Secretariat is expected to race tomorrow_. -- change tag _race_ from NN to VB.\n\n  * [3.48] _The race is already over_. -- no change, _race_ remains as NN.\n\nFor [3.47] _race_ has a higher probability of a _noun_ , it will be treated as\nsuch by applying rule 1 initially. However, when there is a _verb_ prior _to_\n, it should apply rule 2 to change into a _verb_ instead of a _noun_ according\nto grammatic rules.\n\nFor [3.48] _race_ again has a high probability of a noun but due to grammatic\nrule is invalid, it remains as a _noun_. Thus, TBL is often applied to\nidentify stochastic probabilities of tag frequencies for initial guesswork\nfollowed by grammatic rules for refinement.\n\n#### 3.6.4.3 Learning Brill Tagger Transformations\n\nThere are three stages to learn _Brill tagger_ transformations:\n\n  1. 1.\n\nLabel every word with its best tag with stochastic method,\n\n  2. 2.\n\nExamine every possible transformation to select one with the most improved\ntagging, and\n\n  3. 3.\n\nRetag data according to tagging rules.\n\nThese three stages are repetitive until a stopping criterion with no more\nrules to apply. TBL output is an ordered list of transformations which\nconstitute a POS tagging procedure to a new corpus. Sample rules of a Brill\u2019s\nTBL model are shown in Fig. 3.15 (Jurafsky et al. 1999).\n\n![](../images/533412_1_En_3_Chapter/533412_1_En_3_Fig15_HTML.png)\n\nFig. 3.15\n\nSample rules used in Brill\u2019s TBL scheme\n\nThere are many NLP applications applying Brill tagger\u2019s model because TBL is a\nwell integration of rule-based model to provide detailed refinement, and\nstochastic model to provide an efficient tagging solution. Further, it would\nbe effortless to implement Brill\u2019s models either in world or knowledge\ndomains, e.g. medical knowledge domain that may have specific rules or\nterminologies for a corpus.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80576584-c57c-4e47-895e-33f0c5f9e477": {"__data__": {"id_": "80576584-c57c-4e47-895e-33f0c5f9e477", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2797a072-f1ee-4430-90d2-be5174db837a", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "b23f973442601604bb6ab0d7db26e1c834fdbb91d8e44e77db19d7dcfd00eff5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ddc99327-8512-4467-8f63-1f1df4c7e861", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "311c0c73627476d2f65f3fa4043ebaf4b077b39aab69118e4df3dd6ec2ddb857", "class_name": "RelatedNodeInfo"}}, "hash": "00d27fe148c12e591cf3ac8e2bef7375d55a45605bcae43c1292a3d7678098a2", "text": "TBL output is an ordered list of transformations which\nconstitute a POS tagging procedure to a new corpus. Sample rules of a Brill\u2019s\nTBL model are shown in Fig. 3.15 (Jurafsky et al. 1999).\n\n![](../images/533412_1_En_3_Chapter/533412_1_En_3_Fig15_HTML.png)\n\nFig. 3.15\n\nSample rules used in Brill\u2019s TBL scheme\n\nThere are many NLP applications applying Brill tagger\u2019s model because TBL is a\nwell integration of rule-based model to provide detailed refinement, and\nstochastic model to provide an efficient tagging solution. Further, it would\nbe effortless to implement Brill\u2019s models either in world or knowledge\ndomains, e.g. medical knowledge domain that may have specific rules or\nterminologies for a corpus.\n\n## 3.7 Taggers Evaluations\n\nThere are several considerations when POS taggers have implemented (Padro and\nMarquez 1998):\n\n  1. 1.\n\nEvaluate algorithm adequacy\n\n  2. 2.\n\nIdentify errors origin\n\n  3. 3.\n\nRepair and solve\n\nA confusion matrix suggests that current taggers face with major problems:\n\n  1. 1.\n\nNoun-single or mass vs. proper noun-singular vs. adjective (NN vs. NNP vs.\nJJ). These are hard to distinguish as proper noun is crucial for information\nextraction, retrieval, and machine translation for different languages have\ndiverse tagging algorithms or classification schemes.\n\n  2. 2.\n\nAdverb vs. adverb vs. preposition-sub-conjunction (RP vs. RB vs. IN). All of\nthese can appear in satellite sequences following a verb immediately.\n\n  3. 3.\n\nVerb-base form vs. verb-past participle vs. adjective (VB vs. VBN vs. JJ).\nThey are crucial to distinguish for partial parsing, i.e. participles to\nidentify passives, and to label the edges of noun phrases correctly.\n\nThe confusion matrix from HMM error analysis of _The Adventures of Sherlock\nHolmes_ (Doyle 2019) is shown in Fig. 3.16. For example, mis-tagging of (1) NN\nby JJ is 7.56%, (2) NNP by NN is 5.23%, and (3) JJ by NN is 4.35%. Hence,\nmistaking NN by JJ is occurred more often than JJ by NN in English texts but\nit may vary in other foreign languages.\n\n![](../images/533412_1_En_3_Chapter/533412_1_En_3_Fig16_HTML.png)\n\nFig. 3.16\n\nConfusion matrix from HMM of The Adventures of Sherlock Holmes\n\n### 3.7.1 How Good Is an POS Tagging Algorithm?\n\nA satisfied POS tagging algorithm depends on the maximum performance it can\nachieve. It must be realistic, naturally the higher the better but there are\nlimitations. For instance, a POS Tagging system with over 90% accuracy should\nbe considered satisfactory. But how to define satisfactory? E.g. (1) a speech\ndialogue system 97% assigns a correct meaning to user\u2019s input is satisfactory\nbecause ambiguity often occurred in noisy backgrounds with incorrect\npronunciations; or (2) an OCR system determines 97% words correctly is\nsatisfactory. Hence, it depends on scenario, environment, complexity, domain\nproblems, and applications to achieve.\n\nExercises\n\n  1. 3.1\n\nWhat is Part-of-Speech (POS)? How it is critical for NLP systems/applications\nimplementation?\n\n  2. 3.2\n\nState and explain NINE basic types of POS in English Language. For each POS\ntype, give an example for illustration.\n\n  3. 3.3\n\nWhat is POS Tagging in NLP? How is it important to NLP systems/applications\nimplementation? Give two examples of NLP systems/applications for\nillustration.\n\n  4. 3.4\n\nState and explain THREE types of POS Tagging methods in NLP.\n\n  5. 3.5\n\nWhat is PENN Treebank tagset? Perform POS Tagging for the following\nsentences/utterance using PENN Treebank tagset.\n\n_[3.47]_ _POS tagging_ _is a very interesting topic_.\n\n_[3.48] It is not difficult to learn PENN Treebank tagset provided that we\nhave sufficient examples_.\n\n  6.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ddc99327-8512-4467-8f63-1f1df4c7e861": {"__data__": {"id_": "ddc99327-8512-4467-8f63-1f1df4c7e861", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80576584-c57c-4e47-895e-33f0c5f9e477", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "00d27fe148c12e591cf3ac8e2bef7375d55a45605bcae43c1292a3d7678098a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0f39855-071a-49bc-b7f8-48e96c142da8", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "bd5eef27e89c6016baed154d13e76d51c70dfb75dbd135b289784d4ca523d806", "class_name": "RelatedNodeInfo"}}, "hash": "311c0c73627476d2f65f3fa4043ebaf4b077b39aab69118e4df3dd6ec2ddb857", "text": "2. 3.2\n\nState and explain NINE basic types of POS in English Language. For each POS\ntype, give an example for illustration.\n\n  3. 3.3\n\nWhat is POS Tagging in NLP? How is it important to NLP systems/applications\nimplementation? Give two examples of NLP systems/applications for\nillustration.\n\n  4. 3.4\n\nState and explain THREE types of POS Tagging methods in NLP.\n\n  5. 3.5\n\nWhat is PENN Treebank tagset? Perform POS Tagging for the following\nsentences/utterance using PENN Treebank tagset.\n\n_[3.47]_ _POS tagging_ _is a very interesting topic_.\n\n_[3.48] It is not difficult to learn PENN Treebank tagset provided that we\nhave sufficient examples_.\n\n  6. 3.6\n\nWhat is _Natural Language Understanding_ _(NLU)_? State and explain FIVE major\ncomponents of NLU in NLP.\n\n  7. 3.7\n\nWhy _semantic meaning_ is an important factor in POS tagging? Give two\nexamples to support your answer.\n\n  8. 3.8\n\nWhat is _ambiguous_ in POS tags? Give two examples word with three and four\nambiguous of POS tags.\n\n  9. 3.9\n\nWhat is Rule-based approach in POS Tagging? Give an example of POS tagging\nrule to illustrate how it works.\n\n  10. 3.10\n\nWhat is Stochastic-based approach in POS Tagging? Give a live example to\nexplain how _word frequency_ and _tag sequences frequency_ are applied for POS\ntagging.\n\n  11. 3.11\n\nState and explain Transformation-based Learning (TBL). Give a live example to\nsupport your answer.\n\nReferences\n\n  1. Abeill\u00e9, A. (ed) (2003) Treebanks: Building and Using Parsed Corpora (Text, Speech and Language Technology Book 20). Springer.[zbMATH](http://www.emis.de/MATH-item?1030.68090)\n\n  2. Allen, J. (1994) Natural Language Understanding (2nd edition). Pearson\n\n  3. Bender, E. M. (2013) Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Morphology and Syntax (Synthesis Lectures on Human Language Technologies). Morgan & Claypool Publishers[Crossref](https://doi.org/10.1007/978-3-031-02150-3)\n\n  4. Brill, E. (1995). Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. Computational Linguistics, 21(4), 543\u2013566.[MathSciNet](http://www.ams.org/mathscinet-getitem?mr=1610030)\n\n  5. Brown (2022) Brown corpus tagset. [https://\u200bweb.\u200barchive.\u200borg/\u200bweb/\u200b20080706074336/\u200bhttp://\u200bwww.\u200bscs.\u200bleeds.\u200bac.\u200buk/\u200bccalas/\u200btagsets/\u200bbrown.\u200bhtml](https://web.archive.org/web/20080706074336/http://www.scs.leeds.ac.uk/ccalas/tagsets/brown.html). Accessed 15 July 2022.\n\n  6. CELEX (2022) CELEX corpus official site. [https://\u200bcatalog.\u200bldc.\u200bupenn.\u200bedu/\u200bLDC96L14](https://catalog.ldc.upenn.edu/LDC96L14). Accessed 15 July 2022.\n\n  7. Clark, A., Fox, C. and Lappin, S. (2012) The Handbook of Computational Linguistics and Natural Language Processing. Wiley-Blackwell.\n\n  8. CLAWS7 (2022) UCREL CLAWS7 Tagset. [https://\u200bucrel.\u200blancs.\u200bac.\u200buk/\u200bclaws7tags.\u200bhtml](https://ucrel.lancs.ac.uk/claws7tags.html). Accessed 15 July 2022.\n\n  9. DeRose, S. J. (1988). Grammatical category disambiguation by statistical optimization. Computational Linguistics, 14, 31\u201339.\n\n  10. Dermatas, E. and Kokkinakis, G. (1995). Automatic stochastic tagging of natural language texts.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0f39855-071a-49bc-b7f8-48e96c142da8": {"__data__": {"id_": "b0f39855-071a-49bc-b7f8-48e96c142da8", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ddc99327-8512-4467-8f63-1f1df4c7e861", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "311c0c73627476d2f65f3fa4043ebaf4b077b39aab69118e4df3dd6ec2ddb857", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "668ce8c6-232e-4c86-8eb6-45ce8ec0a1a4", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "86092da5308a61e8e7bb9e96a13b1316cd6e4ea1a41d0610d08f1ef45d909081", "class_name": "RelatedNodeInfo"}}, "hash": "bd5eef27e89c6016baed154d13e76d51c70dfb75dbd135b289784d4ca523d806", "text": "7. Clark, A., Fox, C. and Lappin, S. (2012) The Handbook of Computational Linguistics and Natural Language Processing. Wiley-Blackwell.\n\n  8. CLAWS7 (2022) UCREL CLAWS7 Tagset. [https://\u200bucrel.\u200blancs.\u200bac.\u200buk/\u200bclaws7tags.\u200bhtml](https://ucrel.lancs.ac.uk/claws7tags.html). Accessed 15 July 2022.\n\n  9. DeRose, S. J. (1988). Grammatical category disambiguation by statistical optimization. Computational Linguistics, 14, 31\u201339.\n\n  10. Dermatas, E. and Kokkinakis, G. (1995). Automatic stochastic tagging of natural language texts. Computational Linguistics, 21(2), 137\u2013164.\n\n  11. Doyle, A. C. (2019) The Adventures of Sherlock Holmes (AmazonClassics Edition). AmazonClassics.\n\n  12. Eisenstein, J. (2019) Introduction to Natural Language Processing (Adaptive Computation and Machine Learning series). The MIT Press.\n\n  13. Goddard, C. (1998) Semantic Analysis: A Practical Introduction (Oxford Textbooks in Linguistics). Oxford University Press.\n\n  14. Huang H, Zhang X. (2009) Part-of-speech tagger based on maximum entropy model. 2009 2nd IEEE International Conference on Computer Science and Information Technology. IEEE; pp 26-29. [https://\u200bdoi.\u200borg/\u200b10.\u200b1109/\u200bICCSIT.\u200b2009.\u200b5234787](https://doi.org/10.1109/ICCSIT.2009.5234787).\n\n  15. Jurafsky, D., Marin, J., Kehler, A., Linden, K., Ward, N. (1999). Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition. Prentice Hall.\n\n  16. Khanam, H. M. (2022) Natural Language Processing Applications: Part of Speech Tagging. Scholars\u2019 Press.\n\n  17. Lafferty, J., McCallum, A. and Pereira, F. (2001). Conditional random fields: Probabilistic models for segmenting and labeling sequence data. Proc. 18th International Conf. on Machine Learning. Morgan Kaufmann. pp. 282\u2013289.\n\n  18. Marcus, M., Santorini, B. and Marcinkiewicz, M. A. (1993). Building a large, annotated corpus of English: The Penn Treebank. In Computational Linguistics, volume 19, number 2, pp. 313\u2013330.\n\n  19. Mitkov, R. (2005) The Oxford Handbook of Computational Linguistics. Oxford University Press.[zbMATH](http://www.emis.de/MATH-item?1503.68001)\n\n  20. Padro, L. and Marquez, L. (1998). On the evaluation and comparison of taggers: The effect of noise in testing corpora. Cornell University Library, [arXiv.\u200borg](http://arxiv.org). [https://\u200barxiv.\u200borg/\u200babs/\u200bcs/\u200b9809112](https://arxiv.org/abs/cs/9809112).\n\n  21. Pustejovsky, J. and Stubbs, A. (2012) Natural Language Annotation for Machine Learning: A Guide to Corpus-Building for Applications. O\u2019Reilly Media.\n\n  22. Sree, R. and Thottempudi, S. G. (2011) Parts-of-Speech Tagging: A hybrid approach with rule based and machine learning techniques. LAP Lambert Academic Publishing.\n\n  23. Treebank (2022) Penn Treebank Tagset. [https://\u200bwww.\u200bling.\u200bupenn.\u200bedu/\u200bcourses/\u200bFall_\u200b2003/\u200bling001/\u200bpenn_\u200btreebank_\u200bpos.\u200bhtml](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html). Accessed 15 July 2022.\n\n  24. Tuchong (2022) Oil painting analog to Brill Tagger transformation technique.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "668ce8c6-232e-4c86-8eb6-45ce8ec0a1a4": {"__data__": {"id_": "668ce8c6-232e-4c86-8eb6-45ce8ec0a1a4", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0f39855-071a-49bc-b7f8-48e96c142da8", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "bd5eef27e89c6016baed154d13e76d51c70dfb75dbd135b289784d4ca523d806", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d319b514-73cb-4d7c-b438-4f376d791c26", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8819c0c1c01165060fe7600f8f77f20b723941a57ab227b833671049cabb95fd", "class_name": "RelatedNodeInfo"}}, "hash": "86092da5308a61e8e7bb9e96a13b1316cd6e4ea1a41d0610d08f1ef45d909081", "text": "(2012) Natural Language Annotation for Machine Learning: A Guide to Corpus-Building for Applications. O\u2019Reilly Media.\n\n  22. Sree, R. and Thottempudi, S. G. (2011) Parts-of-Speech Tagging: A hybrid approach with rule based and machine learning techniques. LAP Lambert Academic Publishing.\n\n  23. Treebank (2022) Penn Treebank Tagset. [https://\u200bwww.\u200bling.\u200bupenn.\u200bedu/\u200bcourses/\u200bFall_\u200b2003/\u200bling001/\u200bpenn_\u200btreebank_\u200bpos.\u200bhtml](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html). Accessed 15 July 2022.\n\n  24. Tuchong (2022) Oil painting analog to Brill Tagger transformation technique. [https://\u200bstock.\u200btuchong.\u200bcom/\u200bimage/\u200bdetail?\u200bimageId=\u200b9650340629973565\u200b55](https://stock.tuchong.com/image/detail?imageId=965034062997356555). Accessed 15 July 2022.\n\n\n\u00a9 The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.\n2024\n\nR. S. T. Lee Natural Language Processing\n<https://doi.org/10.1007/978-981-99-1999-4_4>\n\n# 4\\. Syntax and Parsing\n\nRaymond S. T. Lee 1\n\n(1)\n\nUnited International College, Beijing Normal University-Hong Kong Baptist\nUniversity, Zhuhai, China\n\n## 4.1 Introduction and Motivation\n\nThis chapter will explore syntax analysis and introduce different types of\nconstituents in English language followed by the main concept of context-free\ngrammar (CFG) and CFG parsing. We will also study different major parsing\ntechniques, including lexical and probabilistic parsing with live examples for\nillustration.\n\nLinguistic and grammatical aspects are addressed in NLP to identify patterns\nthat govern the creation of language sentences like English. They include the\ninvestigation of _Part-of-Speech_ _(POS)_ mentioned in Chap.\n[3](533412_1_En_3_Chapter.xhtml) and _grammatic rules_ to create\nsentences/utterances with _syntactic rules_. These syntactic rules relied on\neffective computational procedures such as rule-based, stochastic-based,\ntechniques and machine learning to deal with language syntax (Bender 2013;\nGorrell 2006).\n\nAnother motivation is to study syntax and parsing methods or algorithms so\nthat they can fall into an automatic system like forming a parser to\nunderstand syntactic structure during the construction process. Figure 4.1\nillustrates the relationship between grammar, syntax, and corresponding parse\ntree of a sentence/utterance with four tokens: _Tom pushed the car_. Syntax\nlevel analysis is to analyze structure and the relationship between tokens to\ncreate a parse tree accordingly.\n\n![](../images/533412_1_En_4_Chapter/533412_1_En_4_Fig1_HTML.png)\n\nFig. 4.1\n\nGrammar, syntax, and parse tree\n\n## 4.2 Syntax Analysis\n\n### 4.2.1 What Is Syntax\n\n_Syntax_ can be considered as the rules that manage group of words are\ncombined to create phrases, clauses, and sentences/utterances in linguistics\n(Bender 2013; Brown and Miller 2020). The term _syntax_ come from Greek word\n_\u03c3\u03cd\u03bd\u03c4\u03b1\u03be\u03b7_ meaning _arrange words togetherness_. _Syntax_ provides a proper and\norganized way to form meaningful phrase or sentence. It is a vital tool in\ntechnical writing and sentence construction.\n\nThe fact is all native speakers learn proper syntax of their mother languages\nby nature. The complexity sentences by a writer or speaker create formal or\ninformal level, or phrase and clauses presentation to audiences.\n\n_Syntax_ can be considered as the proper ordering of word tokens in\nwritten/spoken sentences/utterances so that computer systems can understand\nhow to process these tokens without knowing the exact meaning from NLP\nperspective.\n\n### 4.2.2 Syntactic Rules\n\nPOS in English often follows patterns orders in sentences and clauses (Khanam\n2022; Jurafsky et al. 1999). For instance, compound sentences are combined by\nconjunctions like _and, or, with_ or multiple adjectives transformation of the\nsame noun based on order(s) according to their classes, e.g.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d319b514-73cb-4d7c-b438-4f376d791c26": {"__data__": {"id_": "d319b514-73cb-4d7c-b438-4f376d791c26", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "668ce8c6-232e-4c86-8eb6-45ce8ec0a1a4", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "86092da5308a61e8e7bb9e96a13b1316cd6e4ea1a41d0610d08f1ef45d909081", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05178036-8e53-443a-b2d0-9d58ed583caa", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "0716203615f64f7acc910e18032424407290ca853adf046450770204d25c9737", "class_name": "RelatedNodeInfo"}}, "hash": "8819c0c1c01165060fe7600f8f77f20b723941a57ab227b833671049cabb95fd", "text": "It is a vital tool in\ntechnical writing and sentence construction.\n\nThe fact is all native speakers learn proper syntax of their mother languages\nby nature. The complexity sentences by a writer or speaker create formal or\ninformal level, or phrase and clauses presentation to audiences.\n\n_Syntax_ can be considered as the proper ordering of word tokens in\nwritten/spoken sentences/utterances so that computer systems can understand\nhow to process these tokens without knowing the exact meaning from NLP\nperspective.\n\n### 4.2.2 Syntactic Rules\n\nPOS in English often follows patterns orders in sentences and clauses (Khanam\n2022; Jurafsky et al. 1999). For instance, compound sentences are combined by\nconjunctions like _and, or, with_ or multiple adjectives transformation of the\nsame noun based on order(s) according to their classes, e.g. [4.1] _The big\nblack dog_.\n\nSyntactic rules also described to assist language parts make sense. For\nexample, sentences/utterances in English usually begin with a subject followed\nby a _predicate_ (i.e. a verb in the simplest form) and an object or a\ncomplement to show what\u2019s acted upon, e.g. [4.2] _Jack chased the dog_ is a\ntypical sentence with a _subject-verb-object_ pattern of syntactic rule in\nEnglish. However, [4.3] _Jack quickly chased the dog at lush green field_\ncontains adverbs and adjectives to take their places in front of the sentence\ntransformation _(quickly chased, lush green field)_ with informative\ndescription.\n\n### 4.2.3 Common Syntactic Patterns\n\nThere are seven common syntactic patterns:\n\n  1. 1.\n\nSubject \u2192 Verb\n\nFor example: [4.4] _The cat meowed._\n\nThis syntactic pattern is a standardized pattern containing only minimum\nsubject and verb requirements. The topic always comes first in usual\nsituation.\n\n  2. 2.\n\nSubject \u2192 Verb \u2192 Direct Object\n\nFor example: [4.5] _The cat plays the ball._\n\nWhen verb is transitive with a direct object, the direct object usually goes\nafter the verb in this syntactic pattern.\n\n  3. 3.\n\nSubject \u2192 Verb \u2192 Subject Complement\n\nFor example: [4.6] _The cat is playful._\n\nSubject complement usually goes after the verb in this syntactic pattern.\nLinking verbs such as _be, is, like_ or _seem_ are usually used with subject\ncomplement.\n\n  4. 4.\n\nSubject \u2192 Verb \u2192 Adverbial Complement\n\nFor example: [4.7] _The cat paced slowly._\n\nAdverbial complement usually goes after the verb like previous case (3).\n\n  5. 5.\n\nSubject \u2192 Verb \u2192 Indirect Object \u2192 Direct Object\n\nFor example: [4.8] _The cat gave me the ball._\n\nThis syntactic pattern contains direct and indirect objects. Direct object\nusually goes after indirect object and the indirect object usually goes right\nafter verb. For example, [4.8] can be rephrased as [4.9] _The cat gave the\nball to me_.\n\n  6. 6.\n\nSubject \u2192 Verb \u2192 Direct Object \u2192 Direct Complement\n\nFor example: [4.10] _The cat made the ball dirty._\n\nObject complement usually goes after direct object and the direct object is\nusually followed by verb in this syntactic pattern.\n\n  7. 7.\n\nSubject \u2192 Verb \u2192 Direct Object \u2192 Adverbial Complement\n\nFor example: [4.11] _The cat perked its ears up._\n\n_up_ is the adverbial complement to describe how the cat behaves. Direct\ncomplement is replaced by adverbial complement like previous case (6).\n\n### 4.2.4 Importance of Syntax and Parsing in NLP\n\nThere are five major components in NLU as recalled in Fig. 4.2, _syntax and\nparsing_ components have central roles to link up natural language with its\nsyntactic structure prior understanding its semantic or embedded (discourse\npragmatic) meanings in NLP (Allen 1994; Eisenstein 2019). It is the first tier\nto analyze whether sentences/utterances make sense or not. In other words, if\na sentence/utterance or dialog has syntactic error, _e.g. Jack buys_ (buys\nwhat?), it will not make sense, let alone to study semantic meaning.\n\n![](../images/533412_1_En_4_Chapter/533412_1_En_4_Fig2_HTML.png)\n\nFig.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05178036-8e53-443a-b2d0-9d58ed583caa": {"__data__": {"id_": "05178036-8e53-443a-b2d0-9d58ed583caa", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d319b514-73cb-4d7c-b438-4f376d791c26", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8819c0c1c01165060fe7600f8f77f20b723941a57ab227b833671049cabb95fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5501ae58-814e-4f3e-9e74-1be94f455d12", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "08e26acad63c152ee8adf97844cc6d4cfc26b64d08e63ab80ae831695dbfe095", "class_name": "RelatedNodeInfo"}}, "hash": "0716203615f64f7acc910e18032424407290ca853adf046450770204d25c9737", "text": "### 4.2.4 Importance of Syntax and Parsing in NLP\n\nThere are five major components in NLU as recalled in Fig. 4.2, _syntax and\nparsing_ components have central roles to link up natural language with its\nsyntactic structure prior understanding its semantic or embedded (discourse\npragmatic) meanings in NLP (Allen 1994; Eisenstein 2019). It is the first tier\nto analyze whether sentences/utterances make sense or not. In other words, if\na sentence/utterance or dialog has syntactic error, _e.g. Jack buys_ (buys\nwhat?), it will not make sense, let alone to study semantic meaning.\n\n![](../images/533412_1_En_4_Chapter/533412_1_En_4_Fig2_HTML.png)\n\nFig. 4.2\n\nMajor components in NLU\n\nSyntax and parsing are sole process beneficial to:\n\n  1. 1.\n\nCheck grammar by word-processing applications such as Microsoft Word.\n\n  2. 2.\n\nSpeech recognizer at human speech real-time syntactic level in noisy\nenvironment.\n\nIt has significance in high-level NLP applications such as machine translation\nand Q&A chatbot systems.\n\n## 4.3 Types of Constituents in Sentences\n\n### 4.3.1 What Is Constituent?\n\nA _constituent_ is considered as the linguistic component of a language\n(Bender 2013; Brown and Miller 2020). For example, words or phrases that\ncombine into a sentence/utterance are constituents. It can be a word,\nmorpheme, clause, or phrase. Parsing is a kind of sentence analysis to\nidentify the subject or predicate with different POS and parse\nsentences/utterances into corresponding constituents, e.g. There are several\nways to describe the cat in Fig. 4.3.\n\n  * [4.12] _The milky cat with long tail (as a_ _constituent_ _of a clause) is meowing_.\n\n![](../images/533412_1_En_4_Chapter/533412_1_En_4_Fig3_HTML.jpg)\n\nFig. 4.3\n\nThe milky cat with long tail is meowing (Tuchong 2022)\n\nA single pronoun _it_ to replace the identified constituent. This makes sense\nas it described the milky cat with long tail is meowing, _it is meowing_ , or\na name to the cat _Coco_ , like:\n\n  * [4.13] _Coco is meowing_ or\n\n  * [4.14] _Coco with long tail is meowing._\n\nwhich means a word or phrase form can be replaced by a simple token, or\ncomplex constituents with additional description:\n\n  * [4.15] _The milky cat with long tail is meowing in the late afternoon._\n\n  * [4.16] _The milky cat with long tail is meowing the late afternoon while Jack is asleep_.\n\nConstituents can also be a _time unit_ with usage variations instead of an\nobject unit in noun phrase (NP), they are syntactically acceptable, but some\nare not:\n\n  * [4.17] _Jane wants to go to Greece_ _late this winter_ _._\n\n  * [4.18] _Late this winter_ _Jane wants to go to Greece._\n\n  * [4.19] _Jane wants_ _late this winter_ _to go to Greece._\n\nIt makes sense wherever the location of _late this winter_ as it is a\nconstituent describing a particular time in syntax, but there are syntactic\nerrors as below:\n\n  * [4.20] _Late_ _Jane wants to go to Greece_ _this winter_ _._\n\n\u2013 Cannot separate time unit into two parts.\n\n  * [4.21] _Jane wants_ _late_ _to go to Greece_ _this winter_ _._\n\n\u2013 Senseless meaning\n\n  * [4.22] _The_ _late this winter_ _Jane wants to Greece._\n\n\u2013 Incorrect syntactic pattern\n\n### 4.3.2 Kinds of Constituents\n\nConstituents exist in every sentence, phrase, and clause. Every sentence, in\nother words, is constructed by the combination of all these components into\nmeaningful sentences/utterances (Bender 2013; Brown and Miller 2020). The\ncommonly used constituent types include: (1) _noun phrase_ , (2) _verb\nphrase,_ and 3) _preposition phrase_.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5501ae58-814e-4f3e-9e74-1be94f455d12": {"__data__": {"id_": "5501ae58-814e-4f3e-9e74-1be94f455d12", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05178036-8e53-443a-b2d0-9d58ed583caa", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "0716203615f64f7acc910e18032424407290ca853adf046450770204d25c9737", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77a515e5-0eb4-4703-ac62-7ce0f899bb9d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "e059842cfd463730871190963bbfd77c2185482386484d2b9b4ef93661e80f13", "class_name": "RelatedNodeInfo"}}, "hash": "08e26acad63c152ee8adf97844cc6d4cfc26b64d08e63ab80ae831695dbfe095", "text": "* [4.21] _Jane wants_ _late_ _to go to Greece_ _this winter_ _._\n\n\u2013 Senseless meaning\n\n  * [4.22] _The_ _late this winter_ _Jane wants to Greece._\n\n\u2013 Incorrect syntactic pattern\n\n### 4.3.2 Kinds of Constituents\n\nConstituents exist in every sentence, phrase, and clause. Every sentence, in\nother words, is constructed by the combination of all these components into\nmeaningful sentences/utterances (Bender 2013; Brown and Miller 2020). The\ncommonly used constituent types include: (1) _noun phrase_ , (2) _verb\nphrase,_ and 3) _preposition phrase_. For instance:\n\n  * [4.23] _My cat Coco scratches the UPS courier on the table._\n\n\u2013 These constituents are made up of noun phrase ( _my cat Coco_ ), predicate\nand verb phrase _(scratches the UPS courier on the table)_.\n\n### 4.3.3 Noun-Phrase (NP)\n\nAn NP consists of a noun and its modifiers. Modifiers that go before the noun\nsuch as adjectives, articles, participles, possessive nouns, or possessive\npronouns; or go after the noun such as adjective clauses, participle phrases,\nor prepositional phrases. For example, in [4.23] _My cat Coco_ is a NP\nconsists of determiner (DT) _My_ \\+ noun (NN) _cat_ \\+ proper noun (NNP)\n_Coco_.\n\nThere are other NPs appear as objects of prepositions or objects of verbs:\n\n  * [4.24] _The milky cat with long tail_ _is meowing._\n\n  * [4.25] _Very few cats_ _wore a collar._\n\n  * [4.26] _The long tail_ _is brought to room._\n\n  * [4.27] _Many places_ _hear meowing._\n\n  * [4.28] _A cat with a long tail and a collar_ _is meowing._\n\n  * [4.29] _Jane_ _saw so many cats in the room._\n\n### 4.3.4 Verb-Phrase (VP)\n\nA _VerbPhrase_ _(VP)_ consists of a main verb followed by other linking verbs\nor modifiers that act as a sentence\u2019s verb. Modifiers in VP are words that can\nchange, adapt, limit, expand, or help to define a certain word in a sentence.\nThey are usually auxiliary verbs such as _is, has, am, and are_ that work with\nthe main verb. A main verb in VP holds information about an event or activity\nbeing referred to, and auxiliary verbs add meaning by relating to time or\naspect of the phrase.\n\nThere are nine common VP types:\n\n  1. 1.\n\nSingular main verb\n\n[4.30] _Jack catches a deer._\n\n  2. 2.\n\nAuxiliary verb (to be) + main verb -ing form\n\nWhen the main verb is used in _-ing_ form, e.g. _walking, talking_ , it\nexpresses a continuous aspect to show whether is in the past, present, or\nfuture.\n\n[4.31] _Jack is singing._\n\n  3. 3.\n\nAuxiliary verb (have) + main verb (past participle form)\n\nWhen verb _to have_ (i.e. have, has, had) and the main verb in past participle\nform.\n\n[4.32] _Jack has broken the vase._\n\n  4. 4.\n\nModal verb + main verb\n\nWhen a modal verb is combinedly used with a main verb, it includes things such\nas possibility, probability, ability, permission, and obligation. Examples of\nmodal words include _must, shall, will, should, would, can, could, may, and\nmight_.\n\n[4.33] _Jack will leave._\n\n  5. 5.\n\nAuxiliary verb (have + been) + main verb (-ing form)\n\nWhen both continuous and perfect aspects are expressed, the continuous aspect\ncomes from _-ing_ verb and the perfect aspect comes from auxiliary verb _have\nbeen_.\n\n[4.34] _Jack has been washing the car._\n\n  6. 6.\n\nAuxiliary verb (to be) + main verb (past participle form)\n\nWhen a verb _to be_ is combined with main verb in past participle form to\nexpress a passive voice. The passive voice is used to indicate an action is\nhappening to the subject of sentence than the subject performing the action.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77a515e5-0eb4-4703-ac62-7ce0f899bb9d": {"__data__": {"id_": "77a515e5-0eb4-4703-ac62-7ce0f899bb9d", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5501ae58-814e-4f3e-9e74-1be94f455d12", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "08e26acad63c152ee8adf97844cc6d4cfc26b64d08e63ab80ae831695dbfe095", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32043cd8-4f1b-40aa-a915-b20b24cc58b7", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "2a35fd85d4a12361c1f1ec478ba55c8808c984228e610782f6f9f68af4781eda", "class_name": "RelatedNodeInfo"}}, "hash": "e059842cfd463730871190963bbfd77c2185482386484d2b9b4ef93661e80f13", "text": "Examples of\nmodal words include _must, shall, will, should, would, can, could, may, and\nmight_.\n\n[4.33] _Jack will leave._\n\n  5. 5.\n\nAuxiliary verb (have + been) + main verb (-ing form)\n\nWhen both continuous and perfect aspects are expressed, the continuous aspect\ncomes from _-ing_ verb and the perfect aspect comes from auxiliary verb _have\nbeen_.\n\n[4.34] _Jack has been washing the car._\n\n  6. 6.\n\nAuxiliary verb (to be) + main verb (past participle form)\n\nWhen a verb _to be_ is combined with main verb in past participle form to\nexpress a passive voice. The passive voice is used to indicate an action is\nhappening to the subject of sentence than the subject performing the action.\n\n[4.35] _The lunch was served._\n\n  7. 7.\n\nNegative and interrogative verb phrases\n\nVP gets separated when sentences have negative or interrogative nature.\n\n[4.36] _Jack is not answering the exam questions._\n\n  8. 8.\n\nEmphasize verb phrases\n\nUse auxiliary verbs, e.g. _do, does, did_ to emphasize a sentence.\n\n[4.37] _Jack did enjoy the vacation._\n\n  9. 9.\n\nComposite VP\n\nWhen it consists of other VP or NP\n\n[4.38] _My cat Coco scratches the UPS courier on the table._\n\n\u2013 _Scratch_ is the main verb in VP to describe action/event happens to object\n_UPS courier_ , _on the table_ is auxiliary information to further explain the\nevent. It still makes sense with/without it. It includes _scratch_ VP + _UPS\ncourier_ NP + _on the table_ PP.\n\n### 4.3.5 Complexity on Simple Constituents\n\nSingle word constituents are POS studied in Chap.\n[3](533412_1_En_3_Chapter.xhtml), types of single word constituent types\ndepend on tagset sizes. There are other complex design decisions:\n\n  * [4.39] _Jane bought the_ _big red_ _handbag_ \u2611 vs.\n\n  * [4.40] _Jane bought the_ _red big_ _handbag_ \u2612\n\nAlthough there are two POS correct syntactic meanings but incorrect syntax,\ne.g. _red_ before _big_. There are also incomplete simple constituent types.\n\n  * [4.41] _The cat with a long tail meowing a collar._ \u2612\n\n\u2013 Does not make sense although NP is correct, _collar_ is an incorrect\ndescription.\n\n  * [4.42] _Jane imagined a cat with a long tail._ \u2611\n\n  * [4.43] _Jane decided to go._ \u2611\n\n\u2013 Both make sense without further description in syntactic structure.\n\n  * [4.44] _Jane decided a cat with a long tail._ \u2612\n\n\u2013 Does not make sense again in syntactic correctness.\n\n  * [4.45] _Jane decided a cat with a long tail should be her next pet._ \u2611\n\n\u2013 Syntactic correct although the sentence structure is slightly complex.\n\n  * [4.46] _Jane gave Lily some food._ \u2611\n\n\u2013 Syntactic correct although most of the time it describes food.\n\n  * [4.47] _Jane decided Lily some food._ \u2612\n\n\u2013 Although syntactic structure is the same, but they have different designs to\nfurther describe food types and purposes.\n\n### 4.3.6 Verb Phrase Subcategorization\n\nThere is universal pattern or structure to classify verbs in VPs.\nSubcategories represent the ability of lexical items (usually verbs) to\nrecognize the existence and types of syntactic arguments they co-occur in\nlinguistics (Brown and Miller 2020; Gorrell 2006). Although traditional\nEnglish grammar subcategorizes verbs into transitive and intransitive\nsubcategories, modern English grammars distinguish over 100 subcategories.\nSubcategorization frames can be considered as a set of rules to generate\nsyntactic structures out of the base form. There are five major frame rules as\nshown in Fig. 4.4.\n\n  1. 1.\n\nVP with single verb as member\n\n[4.48] _He_ _talks_ _._ (VP \u2192 VB)\n\n[4.49] _I_ _laugh_ _._ (VP \u2192 VB)\n\n  2. 2.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "32043cd8-4f1b-40aa-a915-b20b24cc58b7": {"__data__": {"id_": "32043cd8-4f1b-40aa-a915-b20b24cc58b7", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77a515e5-0eb4-4703-ac62-7ce0f899bb9d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "e059842cfd463730871190963bbfd77c2185482386484d2b9b4ef93661e80f13", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc5dba57-4dba-4185-9e02-e133ce92c8ba", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3c9c42d0f08a9f9063f81159a1792b00538b9acd12d18dabc3861e5b15de3b7c", "class_name": "RelatedNodeInfo"}}, "hash": "2a35fd85d4a12361c1f1ec478ba55c8808c984228e610782f6f9f68af4781eda", "text": "Subcategories represent the ability of lexical items (usually verbs) to\nrecognize the existence and types of syntactic arguments they co-occur in\nlinguistics (Brown and Miller 2020; Gorrell 2006). Although traditional\nEnglish grammar subcategorizes verbs into transitive and intransitive\nsubcategories, modern English grammars distinguish over 100 subcategories.\nSubcategorization frames can be considered as a set of rules to generate\nsyntactic structures out of the base form. There are five major frame rules as\nshown in Fig. 4.4.\n\n  1. 1.\n\nVP with single verb as member\n\n[4.48] _He_ _talks_ _._ (VP \u2192 VB)\n\n[4.49] _I_ _laugh_ _._ (VP \u2192 VB)\n\n  2. 2.\n\nVerbal phrase requires a noun phrase (NP) as a specifier (VS)\u2014intransitive\nverbs\n\n[4.50] _He_ _finds a clue_ _._ (VP \u2192 VB + NP)\n\n[4.51] _She_ _sees Jack_ _._ (VP \u2192 VB + NP)\n\n  3. 3.\n\nVerbal phrase requires a noun phrase (NP) as a specifier (VS) and a noun\nphrase (NP) as a complement (VC)\u2014direct transitive verbs\n\n[4.52] _Please_ _show me the map_ _._ (VP \u2192 VB + NP + NP)\n\n  4. 4.\n\nVerbal phrase requires a noun phrase (NP) as a specifier (VS) and a\nprepositional phrase (PP) headed by _on_ as a complement (VC)\u2014indirect\ntransitive verbs governing _on:_\n\n[4.53] _This ingredient can_ _make six muffins depending on size_ _._\n\n(VP \u2192 VB + PP + NP)\n\n  5. 5.\n\nVerbal phrase requires a noun phrase (NP) as a specifier (VS), as a complement\n(VC), and a prepositional phrase headed by _to_ as a complement\n(VC)\u2014ditransitive verbs:\n\n[4.54] _Do you_ _mean that I need to attend the exam?_ (VP-VB + S)\n\n![](../images/533412_1_En_4_Chapter/533412_1_En_4_Fig4_HTML.png)\n\nFig. 4.4\n\nExamples of verbs with different frames of subcategorization in VP syntax\n\n### 4.3.7 The Role of Lexicon in Parsing\n\nA lexicon is the vocabulary of a language or branch of knowledge (Bender 2013;\nBrown and Miller 2020), e.g. medical, computer science domain. It is a\nlanguage\u2019s inventory of lexemes in linguistics. The word lexicon derives from\nGreek word _\u03bb\u03b5\u03be\u03b9\u03ba\u03cc\u03bd_ meaning _of or for words_.\n\nLinguists believe that all human languages compose of two major components:\n(1) _lexicon_ as the list of a language\u2019s words and vocabulary and (2)\n_grammar_ as the set of rules to allow words combinations into meaningful\nsentences.\n\nItems within a lexicon are called _lexemes_ , and groups of lexemes are called\n_lemmas_ , often used to describe the size of a lexicon.\n\n_Lexical analysis_ is the process to understand what words mean, intuit\ncontexts, and note the relationship of one word to others. It analyzes and\nconverts the sequence of words into a list of lexical tokens. A program that\nperforms such lexical analysis is called _tokenizer_ , _lexer,_ or _scanner_.\nA lexer is combined with a parser generally to analyze the syntax of\nsentences, texts, or dialogues.\n\nThe roles of lexicon in parsing are to:\n\n  1. 1.\n\nTreat as the starting point for POS tagging.\n\n  2. 2.\n\nProvide extra information such as subcategorization with frames and syntactic\nrules.\n\nFor verbs, lexicon refers to several types of subcategorizations such as\n_think_ vs. _laugh._\n\nFor adjectives:\n\n  * [4.55] _Jack is angry with Sophia_ vs. [4.56] _Jack is angry at Sophia_.\n\n  * [4.57] _Jack is mad at Sophia_ vs. [4.58] _Jack is mad with Sophia._ \u2612\n\nThere are patterns and rules.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc5dba57-4dba-4185-9e02-e133ce92c8ba": {"__data__": {"id_": "cc5dba57-4dba-4185-9e02-e133ce92c8ba", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32043cd8-4f1b-40aa-a915-b20b24cc58b7", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "2a35fd85d4a12361c1f1ec478ba55c8808c984228e610782f6f9f68af4781eda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "331462d8-74f8-42ee-a2d4-c82b8b5f6eb8", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "6e58a049a53b2e24881809dc9a1b0dc4d8d6eeba888f2d1e3ec700b95d6f491f", "class_name": "RelatedNodeInfo"}}, "hash": "3c9c42d0f08a9f9063f81159a1792b00538b9acd12d18dabc3861e5b15de3b7c", "text": "A program that\nperforms such lexical analysis is called _tokenizer_ , _lexer,_ or _scanner_.\nA lexer is combined with a parser generally to analyze the syntax of\nsentences, texts, or dialogues.\n\nThe roles of lexicon in parsing are to:\n\n  1. 1.\n\nTreat as the starting point for POS tagging.\n\n  2. 2.\n\nProvide extra information such as subcategorization with frames and syntactic\nrules.\n\nFor verbs, lexicon refers to several types of subcategorizations such as\n_think_ vs. _laugh._\n\nFor adjectives:\n\n  * [4.55] _Jack is angry with Sophia_ vs. [4.56] _Jack is angry at Sophia_.\n\n  * [4.57] _Jack is mad at Sophia_ vs. [4.58] _Jack is mad with Sophia._ \u2612\n\nThere are patterns and rules. Both are correct for [4.55] and [4.56] , but for\nthe verb _mad_ , [4.57] is correct while incorrect for [4.58] which means\nsubcategorization is acceptable for some pattern but not only on syntax.\n\nFor nouns: [4.59] _Janet has a passion for classical music_ vs. [4.60] _Janet\nhas an interest in classical music_.\n\nThey have different patterns of syntactic rules.\n\n### 4.3.8 Recursion in Grammar Rules\n\nEnglish sentences can be very large and complex in structure. A concise\nsentence usually consists of limited set of constituent types, i.e. NP, VP,\nand PP to recursion and construct grammar rules as follows:\n\n  * S \u2192 NP VP [4.61] _My good friend Jack buys a flat._\n\n  * VP \u2192 V NP [4.62] _buys a flat._\n\n  * NP \u2192 NP PP [4.63] _My good friend._\n\n  * NP \u2192 NP S [4.64] _The boy who come early today won the game._\n\n  * PP \u2192 prep NP [4.65] _The cupcake with sprinkles is yours_.\n\n## 4.4 Context-Free Grammar (CFG)\n\n### 4.4.1 What Is Context-Free Language (CFL)?\n\nContext-free language (CFL) is a superset of _Regular Language_ _(RL)_\ngenerated by context-free grammar (CFG) which means every RL is a CFL but not\nall CFL is a RL (Eisenstein 2019; Jurafsky et al. 1999). In short, CFL is:\n\n  1. 1.\n\nRecursively enumerable language as superset of language model\n\n  2. 2.\n\nContext-sensitive language, a subset of recursively enumerable language\n\n  3. 3.\n\nSubsets of context-sensitive language.\n\nThe four levels of human language are shown in Fig. 4.5.\n\n![](../images/533412_1_En_4_Chapter/533412_1_En_4_Fig5_HTML.png)\n\nFig. 4.5\n\nLevel of languages\n\nThe set of all CFLs are identical to the set of languages accepted by pushdown\nautomata, and RL is a CFL subset. An input language is accepted by a\ncomputational model if it runs through the model and ends in an acceptable\nfinal state, most arithmetic expressions generated by context-free grammar\n(CFG) is CFL.\n\nCFL is closed in a specific operation if applying a CFL language operation on\nits results. These operations include union, concatenation, Kleene closure,\nsubstitution, prefix, cycle, reversal, quotient, union, intersection,\ndifference with RL, and homomorphism. CFL and CFG have NLP and computer\nlanguage designs in computer science and linguistics.\n\n### 4.4.2 What Is Context-Free Grammar (CFG)?\n\nCFG is to describe CFL as a set of recursive rules for generating string\npatterns, because the application of production rules in a grammar is context-\nindependent, meaning they do not depend on other symbols with the rules\n(Bender 2013; Brown and Miller 2020).\n\nCFG is commonly applied in linguists and compiler design to describe\nprogramming languages and parsers that can be created automatically.\n\n### 4.4.3 Major Components of CFG\n\nCFG consists of four major components (Bender 2013; Jurafsky et al. 1999):\n\n  1. 1.\n\nA set of nonterminal symbols N are placeholders for patterns of terminal\nsymbols created by nonterminal symbols. These symbols usually located at the\nLHS (left-hand-side) of production rules (P). The strings generated by CFG\nusually consist of symbols only from nonterminal symbols.\n\n  2. 2.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "331462d8-74f8-42ee-a2d4-c82b8b5f6eb8": {"__data__": {"id_": "331462d8-74f8-42ee-a2d4-c82b8b5f6eb8", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc5dba57-4dba-4185-9e02-e133ce92c8ba", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3c9c42d0f08a9f9063f81159a1792b00538b9acd12d18dabc3861e5b15de3b7c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "379795ad-cd6e-4919-ae94-e1e957728d85", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "ccb6c2ca68ea0a57883def0065662eb5cefaa396a50936e0ff821a050c20be19", "class_name": "RelatedNodeInfo"}}, "hash": "6e58a049a53b2e24881809dc9a1b0dc4d8d6eeba888f2d1e3ec700b95d6f491f", "text": "CFG is to describe CFL as a set of recursive rules for generating string\npatterns, because the application of production rules in a grammar is context-\nindependent, meaning they do not depend on other symbols with the rules\n(Bender 2013; Brown and Miller 2020).\n\nCFG is commonly applied in linguists and compiler design to describe\nprogramming languages and parsers that can be created automatically.\n\n### 4.4.3 Major Components of CFG\n\nCFG consists of four major components (Bender 2013; Jurafsky et al. 1999):\n\n  1. 1.\n\nA set of nonterminal symbols N are placeholders for patterns of terminal\nsymbols created by nonterminal symbols. These symbols usually located at the\nLHS (left-hand-side) of production rules (P). The strings generated by CFG\nusually consist of symbols only from nonterminal symbols.\n\n  2. 2.\n\nA set of terminal symbols _\u03a3_ (disjoint from N) are characters appear in\nstrings generated by grammar. Terminal symbols usually located only at RHS\n(right-hand-side) of production rules (P).\n\n  3. 3.\n\nA set of production rules _P_ : _A \u2192 \u03b1_ ,where _A_ is a nonterminal symbol and\n_\u03b1_ is a string of symbols from the infinite set of strings ( _\u03a3_ \u222a N).\n\n  4. 4.\n\nA designated start symbol _S_ is a start symbol of the sentence/utterance.\n\n_\u03a3_ is a set of POS and N is the set of constituent types, i.e. NP, VP, and PP\nmentioned in Chap. [3](533412_1_En_3_Chapter.xhtml) and previous section,\nrespectively.\n\n### 4.4.4 Derivations Using CFG\n\nThe standard formulation of CFG is given by\n\nAssume _L_ G generated by grammar G is a set of strings composed of terminal\nsymbols which is generated from _S_ :\n\n![$$ {L}_{\\\\mathrm{G}}=\\\\left\\\\{w|w\\\\;\\\\mathrm{is}\\\\ \\\\mathrm{in}\\\\kern0.62em\n{\\\\varSigma}^{\\\\ast}\\\\kern0.62em \\\\mathrm{and}\\\\;S\\\\Rightarrow w\\\\right\\\\}\n$$](../images/533412_1_En_4_Chapter/533412_1_En_4_Chapter_TeX_Equ1.png)\n\n(4.1)\n\nLet _\u03a3_ be the set of POS, so CFG in Eq. (4.1) can create grammar like this:\n\n![$$ \\\\mathrm{N}\\\\kern0.62em \\\\mathrm{V}\\\\kern0.62em \\\\det\\\\;\\\\mathrm{N}\n$$](../images/533412_1_En_4_Chapter/533412_1_En_4_Chapter_TeX_Equ2.png)\n\n(4.2)\n\nThe definition of CFG is given by\n\n![$$ {L}_{\\\\mathrm{G}}=\\\\left\\\\{\\\\begin{array}{l}s \\\\mid w\\\\;\\\\mathrm{is}\\\\\n\\\\mathrm{in}\\\\;{\\\\varSigma}^{\\\\ast}\\\\;\\\\mathrm{and}\\\\;S\\\\Rightarrow\nw\\\\;\\\\mathrm{and}\\\\;s\\\\;\\\\mathrm{can}\\\\;\\\\mathrm{be}\\\\;\\\\mathrm{derived}\\\\\n\\\\mathrm{from}\\\\;w\\\\;\\\\mathrm{by}\\\\;\\\\mathrm{substituting}\\\\ \\\\\\\\\n{}\\\\mathrm{words}\\\\\n\\\\mathrm{for}\\\\;\\\\mathrm{POS}\\\\;\\\\mathrm{as}\\\\;\\\\mathrm{licensed}\\\\;\\\\mathrm{by}\\\\;\\\\mathrm{the}\\\\\n\\\\mathrm{lexicon}\\\\end{array}\\\\right\\\\}\n$$](../images/533412_1_En_4_Chapter/533412_1_En_4_Chapter_TeX_Equ3.png)\n\n(4.3)\n\nThis definition can generate numerous productions like this format:\n\n![$$ \\\\mathrm{S}\\\\to \\\\mathrm{NP}\\\\;\\\\mathrm{VP}\n$$](../images/533412_1_En_4_Chapter/533412_1_En_4_Chapter_TeX_Equ4.png)\n\n(4.4)\n\nEquation (4.4) is the most basic grammar rule where a sentence is generated\nfrom a NP and a VP that can be further decomposed recursively as shown in Fig.\n4.6.\n\n![](../images/533412_1_En_4_Chapter/533412_1_En_4_Fig6_HTML.png)\n\nFig.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "379795ad-cd6e-4919-ae94-e1e957728d85": {"__data__": {"id_": "379795ad-cd6e-4919-ae94-e1e957728d85", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "331462d8-74f8-42ee-a2d4-c82b8b5f6eb8", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "6e58a049a53b2e24881809dc9a1b0dc4d8d6eeba888f2d1e3ec700b95d6f491f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "723d2a3a-1b1e-4e99-a1b6-6225edfe33bf", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "d8556968af08de84eb99ccab300e52b4ea9776c9e4e9c51a894c7fab1b80d5a0", "class_name": "RelatedNodeInfo"}}, "hash": "ccb6c2ca68ea0a57883def0065662eb5cefaa396a50936e0ff821a050c20be19", "text": "[$$ \\\\mathrm{S}\\\\to \\\\mathrm{NP}\\\\;\\\\mathrm{VP}\n$$](../images/533412_1_En_4_Chapter/533412_1_En_4_Chapter_TeX_Equ4.png)\n\n(4.4)\n\nEquation (4.4) is the most basic grammar rule where a sentence is generated\nfrom a NP and a VP that can be further decomposed recursively as shown in Fig.\n4.6.\n\n![](../images/533412_1_En_4_Chapter/533412_1_En_4_Fig6_HTML.png)\n\nFig. 4.6\n\nCFG rules and corresponding parse tree for sentence [4.66] _Jane plays the\npiano_\n\nIt shows CFG rules and its corresponding parse tree for sentence/utterance\n[4.66] _Jane plays the piano_. There are four tokens in this\nsentence/utterance to form a well-defined syntactic structure generated by NP\nand VP. NP can be designated to a name pointed to token _Jane_ , and for VP,\nis decomposed into verb or NP as shown in four production rules shown in the\ntop left corner at Fig. 4.6. In this case, verb is pointed to _plays,_ NP can\nbe decomposed into a determiner and a noun pointed to _the_ and _piano,_\nrespectively.\n\n## 4.5 CFG Parsing\n\nThere are three CFG parsing levels: (1) morphological, (2) phonological, and\n(3) syntactic (Grune and Jacob 2007; Jurafsky et al. 1999).\n\n### 4.5.1 Morphological Parsing\n\n_Morphological parsing_ is the initial level to determine morphemes of a word\nbeing constructed. For example, a morphological parser can reveal that the\nword _mice_ is the plural form of noun stem _mouse_ , while _cats_ is the\nplural form of noun stem _cat_. Given the string _cats_ as input, the\nmorphological parser will interpret _cats_ as _cat_ N PL. By using FSA (Finite\nState Automata), FST (Finite State Transducer), a morphological parser can\nproduce an output with their stems and modifiers.\n\nOriginally, FST is generated by algorithmic parsing of word source such as a\ncomplete dictionary with modifier markups but can be realized by recurrent\nneural networks with training corpus upon advancement in machine learning and\nartificial neural networks.\n\n### 4.5.2 Phonological Parsing\n\n_Phonological parsing_ is the second level using the sounds of a language,\ni.e. phonemes to process sentences/utterances (Wagner and Torgesen 1987).\n\nPhonological processing includes (1) awareness, (2) working memory, and (3)\nphonological retrieval. All three components are important to speech\nproduction and written language skills development. Hence, it is necessary to\nobserve children\u2019s spoken and written language development with phonological\nprocessing difficulties.\n\nPhonological parsing is to interpret sounds into words and phrases to generate\nparser.\n\n### 4.5.3 Syntactic Parsing\n\n_Syntactic parsing_ is the third level to identify relevant components and\ncorrect grammar of a sentence. Abstract meaning representation is assigned to\ndefine legal strings of a language like CFG without recognizing the structure.\n\nParsing algorithms are applied to analyze sentences/utterances within language\nand assign appropriate syntactic structures into them. Parse trees are useful\nto study grammar, semantic analysis, machine translation, speech recognition,\nQ&A chatbots in NLP.\n\n### 4.5.4 Parsing as a Kind of Tree Searching\n\n_Syntactic parsing_ can be considered as search within a set of parse trees,\nits main purpose is to identify the right path and space through automation in\nan FSA system structure.\n\nCFG is a process to determine the right parse tree among all possible options.\nIf there is more than one possible parse tree, stochastic method (or other\nmachine learning methods) will be applied to locate a probable one. In other\nwords, it is a process to identify search space defined by grammatical rules\nso that their constraints can become inputs to perform automatic parsing and\nstudy grammars.\n\n### 4.5.5 CFG for Fragment of English\n\nEnglish grammar and lexicon simplified domains are applied to reveal CFG rules\nin an example of musical instruments as shown in Fig. 4.7. It consists of\nproduction rules from several categories S \u2192 NP VP, S \u2192 Aux NP VP, S \u2192 VP as\nwell as production rules for NP, Nom, and VP with components Det, N, V, Prep,\nand PropN.\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "723d2a3a-1b1e-4e99-a1b6-6225edfe33bf": {"__data__": {"id_": "723d2a3a-1b1e-4e99-a1b6-6225edfe33bf", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "379795ad-cd6e-4919-ae94-e1e957728d85", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "ccb6c2ca68ea0a57883def0065662eb5cefaa396a50936e0ff821a050c20be19", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c94e6ca9-20cf-4a22-abd0-227220f8ba47", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f8df0352ea0c071cda80d5740ccf09157b0fe4c4107483540f9a8351604fbd7e", "class_name": "RelatedNodeInfo"}}, "hash": "d8556968af08de84eb99ccab300e52b4ea9776c9e4e9c51a894c7fab1b80d5a0", "text": "CFG is a process to determine the right parse tree among all possible options.\nIf there is more than one possible parse tree, stochastic method (or other\nmachine learning methods) will be applied to locate a probable one. In other\nwords, it is a process to identify search space defined by grammatical rules\nso that their constraints can become inputs to perform automatic parsing and\nstudy grammars.\n\n### 4.5.5 CFG for Fragment of English\n\nEnglish grammar and lexicon simplified domains are applied to reveal CFG rules\nin an example of musical instruments as shown in Fig. 4.7. It consists of\nproduction rules from several categories S \u2192 NP VP, S \u2192 Aux NP VP, S \u2192 VP as\nwell as production rules for NP, Nom, and VP with components Det, N, V, Prep,\nand PropN.\n\n![](../images/533412_1_En_4_Chapter/533412_1_En_4_Fig7_HTML.png)\n\nFig. 4.7\n\nA simplified example on English grammar and lexicon\n\n### 4.5.6 Parse Tree for \u201cPlay the Piano\u201d for Prior CFG\n\nA parse tree of sentence/utterance [4.67] _play the piano_ is shown in Fig.\n4.8 _._ It has three tokens _play\u2014_ Verb, _the\u2014_ Det, and _piano\u2014_ Noun to\nconstruct a parse tree from top node S to generate VP, VP to generate Verb and\nNP, and NP to decompose into Det Nom, and Nom to generate Noun.\n\n![](../images/533412_1_En_4_Chapter/533412_1_En_4_Fig8_HTML.png)\n\nFig. 4.8\n\nParse tree for the simple sentence \u201cPlay the piano\u201d\n\n### 4.5.7 Top-Down Parser\n\nThere are (1) top-down and (2) bottom-up parser approaches to construct a\nparse tree. _Top-down parser_ constructs from _rootnode_ S down to\n_leavenodes_ (words in the sentence/utterance). The first step is to identify\nall trees with root S, the next step is to expand all constituents in these\ntrees based on the given production rules. The whole process is operated in\nlevel-by-level process until parse trees reach the leaves, i.e. POS tokens of\nthe sentence/utterance. For candidate parse trees that cannot match the leave\nnodes, i.e. POS tokens are discarded and considered as failed parse tree(s).\nFigure 4.9 shows first three-level construction all possible parse trees\napplying _Top-Down parser_.\n\n![](../images/533412_1_En_4_Chapter/533412_1_En_4_Fig9_HTML.jpg)\n\nFig. 4.9\n\nA three-level expansion of parse tree generation using top-down approach\n\nIt showed that the parse tree construction started from base level with S tag\n(root node). The second level has generated an additional layer with three\npossible production rules: S \u2192 NP & VP, S \u2192 Aux & NP & VP, and S \u2192 VP. The\nthird level is complex because it has decomposed into three levels, S \u2192 NP &\nVP is the first variation to decompose into Det and Nom. NP is the second\nvariation to decompose into PropN. It is noted that LHS is the expanded part\nfor demonstration purpose, but both LHS and RHS are required expansion. S \u2192\nAux & NP & VP are the second variation where NP to decompose into Det & Nom,\nand a NP decompose into PropN. VP decomposition in the first four parse tree\nis not shown as they all failed to match the leave nodes except only the fifth\ncase is correct to form a complete _play the piano_ parse tree.\n\n_Top-down approach_ by CFG on terminals and nonterminals is shown in Fig.\n4.10. It showed rule 3 as the first one to apply and rule 2 for VP decomposed\ninto V NP and V to decompose _play_ and then NP to Det and Nom, rule 4 and\nrule 5 are Det points to _the_ , and rule 6 Nom points to _an_ and final rule\npoints to end, and rule 7 points to _piano_. This will complete a top-down\napproach parsing with the fifth parse tree end-up as valid solution. Readers\ncan base on these seven-step process to complete the construction of parse\ntree for the fifth case as an exercise.\n\n![](../images/533412_1_En_4_Chapter/533412_1_En_4_Fig10_HTML.png)\n\nFig.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c94e6ca9-20cf-4a22-abd0-227220f8ba47": {"__data__": {"id_": "c94e6ca9-20cf-4a22-abd0-227220f8ba47", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "723d2a3a-1b1e-4e99-a1b6-6225edfe33bf", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "d8556968af08de84eb99ccab300e52b4ea9776c9e4e9c51a894c7fab1b80d5a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "521cdbc0-f093-4182-9fb7-675d51d8bbe8", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "a3fdc7d8c74c56e172e38b25249168642767845ac3d3813c3727eace4ca5cb00", "class_name": "RelatedNodeInfo"}}, "hash": "f8df0352ea0c071cda80d5740ccf09157b0fe4c4107483540f9a8351604fbd7e", "text": "_Top-down approach_ by CFG on terminals and nonterminals is shown in Fig.\n4.10. It showed rule 3 as the first one to apply and rule 2 for VP decomposed\ninto V NP and V to decompose _play_ and then NP to Det and Nom, rule 4 and\nrule 5 are Det points to _the_ , and rule 6 Nom points to _an_ and final rule\npoints to end, and rule 7 points to _piano_. This will complete a top-down\napproach parsing with the fifth parse tree end-up as valid solution. Readers\ncan base on these seven-step process to complete the construction of parse\ntree for the fifth case as an exercise.\n\n![](../images/533412_1_En_4_Chapter/533412_1_En_4_Fig10_HTML.png)\n\nFig. 4.10\n\nCFG rules and terminal/nonterminal nodes being used with top-down approach\nparsing\n\n### 4.5.8 Bottom-Up Parser\n\n_Bottom-up parser,_ however, starts from token words of the\nsentences/utterances to construct parser tree upward by applying the same set\nof production rules and try to generate from right-hand-side (RHS) of the\nproduction rule in reverse order. In example [4.67] _play the piano_ has two\nvariations to start in which the word _play_ can be considered either as a\nnoun (N) or a verb (V). So, there are two options: one with the _play the\npiano_ as N Det or as V Det N. Since this approach cannot indicate which one\nis the correct option so the parsing operation will continue to grow until\nthey can reach the root node S, and if they cannot match the root node, the\ntree(s) will be discarded.\n\nFigure 4.11 shows the first three-level expansion of a parse tree using\n_bottom-up approach_. So, in this case _play the piano_ has two variations\neither play is N or V. There are two parts one is _play_ consider as N and\nother as V from base level. So, at second level is to further expand the line\npointed to _play_ and tried to expand N pointed to play into N in the first\ncase. In second case is to further expand N pointed to Nom in second layer. In\nthe third level, second case is further expanded into two options, one is Nom\n\u2192 V and the other is VP \u2192 V & NP, NP \u2192 Det & Nom, and further up to S \u2192 VP to\ncomplete the whole parsing, in which other two parse tree options endedup with\ninvalid parsing as shown in Fig. 4.11.\n\n![](../images/533412_1_En_4_Chapter/533412_1_En_4_Fig11_HTML.png)\n\nFig. 4.11\n\nA three-level expansion of parse tree generation using bottom-up approach\n\nFigure 4.12 shows CFG rules for terminal and nonterminal nodes applying\nbottom-up approach parsing. Again, it consists of seven steps. Rule 1 is V\npointed to _play_ , rule 2 is Det pointed to _the_ , rule 3 is N pointed to\n_piano_ , rule 4 is Nom pointed to N, rule 5 is NP pointed to Det & Nom, rule\n6 is VP pointed to NP, and rule 7 is S point to VP to complete the whole parse\ntree until it can finally match the root/source node S.\n\n![](../images/533412_1_En_4_Chapter/533412_1_En_4_Fig12_HTML.png)\n\nFig. 4.12\n\nCFG rules and terminal/nonterminal nodes being used with bottom-up approach\nparsing\n\n### 4.5.9 Control of Parsing\n\nAlthough both top-down and bottom-up parsing are straightforward, the control\nof parsing is still needed to consider (1) which node that need to expand\nfirst and (2) select grammatical rules sequence wisely to save time as most of\nthe parse tree generation are dead-end and wastage of resources.\n\n### 4.5.10 Pros and Cons of Top-Down vs. Bottom-Up Parsing\n\n#### 4.5.10.1 Top-Down Parsing Approach\n\n##### Pros\n\nSince it starts from root/source node S, it can always generate a correct\nparse tree unless the sentence has a syntactic error. In other words, it never\nexplores the parse that will not end up to root/source node S which means it\nwill always find a solution.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "521cdbc0-f093-4182-9fb7-675d51d8bbe8": {"__data__": {"id_": "521cdbc0-f093-4182-9fb7-675d51d8bbe8", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c94e6ca9-20cf-4a22-abd0-227220f8ba47", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f8df0352ea0c071cda80d5740ccf09157b0fe4c4107483540f9a8351604fbd7e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fee58545-512c-4ca6-aeac-c39e423a6d64", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "92872574562c0b71572d29af36ef63e0025537e2e0bf414b3b4a1fac762d6614", "class_name": "RelatedNodeInfo"}}, "hash": "a3fdc7d8c74c56e172e38b25249168642767845ac3d3813c3727eace4ca5cb00", "text": "4.12\n\nCFG rules and terminal/nonterminal nodes being used with bottom-up approach\nparsing\n\n### 4.5.9 Control of Parsing\n\nAlthough both top-down and bottom-up parsing are straightforward, the control\nof parsing is still needed to consider (1) which node that need to expand\nfirst and (2) select grammatical rules sequence wisely to save time as most of\nthe parse tree generation are dead-end and wastage of resources.\n\n### 4.5.10 Pros and Cons of Top-Down vs. Bottom-Up Parsing\n\n#### 4.5.10.1 Top-Down Parsing Approach\n\n##### Pros\n\nSince it starts from root/source node S, it can always generate a correct\nparse tree unless the sentence has a syntactic error. In other words, it never\nexplores the parse that will not end up to root/source node S which means it\nwill always find a solution.\n\n##### Cons\n\nThis approach does not consider final word/token tags during parsing from the\nvery beginning, it will waste a lot of time to generate tree(s) that may be\ntotally unrelated to the correct result. _play_ should parse as V instead of N\nas shown in Fig. 4.9, this approach showed that all first fourth parts of\nparse tree using _play_ as N are invalid and waste of time to parse tree\ngeneration.\n\n#### 4.5.10.2 Bottom-Up Parsing Approach\n\n##### Pros\n\nSince it starts from sentence tokens/POS, it can always generate parse tree\nwith all tokens/POS in the sentence considered and reduced time on rules\nunrelated to these tokens which means it can sort out problems occurred in\ntop-down approach for all production rules without POS tags.\n\n##### Cons\n\nThis approach may often end up with broken tree(s) that cannot match the root\nnode S to complete parse tree as it starts from leave node instead of\nroot/source node S. It makes sense because although there are many ways to\nmatch production rules, the variations of most parse trees are syntactic\nincorrect so they cannot match the root/source node S. All parse trees in Fig.\n4.11 showed that except the last one (also the correct one), others ended up\nwith broken trees and failed to match the root/source node S again wasted time\nto parse tree generation.\n\nLet us look at lexicalized and probabilistic parsing as alternative.\n\n## 4.6 Lexical and Probabilistic Parsing\n\n### 4.6.1 Why Using Probabilities in Parsing?\n\nThere are two reasons using probabilities parsing (Eisenstein 2019; Jurafsky\net al. 1999): (1) resolve ambiguity and (2) word prediction in voice\nrecognition. For instance:\n\n  * [4.68] _I saw Jane with the telescope._ (Jane with telescope or I use telescope to see Jane?)\n\n  * [4.69] _I saw the Great Pyramid flying over Giza plateau_ vs.\n\n  * [4.70] _I saw UFO flying over Giza plateau_\n\nAlthough both situations have pragmatic problems in which [4.69] is incorrect\nbecause Great Pyramid is an unmovable architecture. It can be solved using\nprobabilities in parsing from a large corpus and knowledge base (KB) to\nidentify the frequencies of a particular term or constituent is used correctly\nwithout pragmatical analysis.\n\nFor example, in voice recognition:\n\n  * [4.71] _Jack has to go_ vs.\n\n  * [4.72] _Jack half to go_ vs.\n\n  * [4.73] _If way thought Jack wood go_\n\nNote: when analyzing N-gram probabilities in Chapter\n[2](533412_1_En_2_Chapter.xhtml) on N-gram Language Model, _I have_ , _I\nshould_ , _I would_ usage and bigram probabilities from _The Adventures of\nSherlock Holmes_ , they provided directions for one that is more probable and\nused frequently instead of understanding their exact semantic or pragmatic\nmeanings. So, such probabilistic method can also be applied to parsing.\n\n### 4.6.2 Semantics with Parsing\n\nThe following examples show how semantic meanings (Bunt et al. 2013; Goddard\n1998) affect/determine the validness of sentence/utterance in parsing:\n\n  * [4.74] _Jack drew one card from a desk_ [?] vs.\n\n  * [4.75] _Jack drew one card from a deck._\n\n  * Note: _drew_ \u2192 _deck_ is clearly a semantic concern.\n\n  * [4.76] _I saw the Great Pyramid flying over Giza plateau_. [?] vs.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fee58545-512c-4ca6-aeac-c39e423a6d64": {"__data__": {"id_": "fee58545-512c-4ca6-aeac-c39e423a6d64", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "521cdbc0-f093-4182-9fb7-675d51d8bbe8", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "a3fdc7d8c74c56e172e38b25249168642767845ac3d3813c3727eace4ca5cb00", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "96229776-9ca8-4693-8b5c-f7a5eafcadef", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f9513356d95f27630bde967d43933730054af74b2118dfcc031b067e07aadf4d", "class_name": "RelatedNodeInfo"}}, "hash": "92872574562c0b71572d29af36ef63e0025537e2e0bf414b3b4a1fac762d6614", "text": "So, such probabilistic method can also be applied to parsing.\n\n### 4.6.2 Semantics with Parsing\n\nThe following examples show how semantic meanings (Bunt et al. 2013; Goddard\n1998) affect/determine the validness of sentence/utterance in parsing:\n\n  * [4.74] _Jack drew one card from a desk_ [?] vs.\n\n  * [4.75] _Jack drew one card from a deck._\n\n  * Note: _drew_ \u2192 _deck_ is clearly a semantic concern.\n\n  * [4.76] _I saw the Great Pyramid flying over Giza plateau_. [?] vs.\n\n  * [4.77] _I saw a UFO flying over Giza plateau_.\n\n  * Note: movable vs. unmovable objects.\n\n  * [4.78] _The workers dumped sacks into a pin_. [?] vs.\n\n  * [4.79] _The workers dumped sacks into a bin_.\n\n  * Note: _dump_ looks for a locative complement.\n\n  * [4.80] _Tom hit the ball with the pen_. [?] vs.\n\n  * [4.81] _Tom hit the ball with the bat_.\n\n  * Note: which object can use to hit the ball?\n\n  * [4.82] _Visiting relatives can be boring_. [?] vs.\n\n  * [4.83] _Visiting museums can be boring_.\n\nNote: Visiting relatives are genuinely ambiguous. Visiting museums are evident\nas only animate bodies can visit. There is no need for abstraction with enough\ndata, in other words, sufficient large corpus, databank, or dialogue databank\ncan sort out ambiguity problems to work out correct syntax with semantic\nmeaning in many cases.\n\nThere are two classical approaches to add semantics into parsing: (1) cascade\nsystems to construct all parses and use semantics for rating tedious and\ncomplex; (2) do semantics incrementally.\n\nA modern approach is to forget the meaning and only based on KB and corpus. If\na corpus contains sufficient sentences and knowledge, facts about meaning\nemerge in the probability of observed sentences themselves. It is modern\nbecause constructing world models are harder than early researchers realized\nbut there are huge text corpora to construct useful statistics. Here comes\nlexical and probabilistic approach of parsing.\n\n### 4.6.3 What Is PCFG?\n\n_A probabilistic_ _context-free grammar_ _(_ _PCFG_ _)_ is a context-free\ngrammar that associates each of its production rule with a probability. It\ncreates the same set of parses for a test that traditional CFG performs but\nassigns a probability value to each parse. In other words, the probability of\na parse generated by a PCFG is the product of probability\u2019s rules.\n\nThe general format of PCFG production rule is given by\n\n![$$ A\\\\to \\\\beta \\\\left\\[p\\\\right\\]\n$$](../images/533412_1_En_4_Chapter/533412_1_En_4_Chapter_TeX_Equ5.png)\n\n(4.5)\n\nAnother way to interpret it as:\n\n![$$ P\\\\left\\(A\\\\to \\\\beta |A\\\\right\\)\n$$](../images/533412_1_En_4_Chapter/533412_1_En_4_Chapter_TeX_Equ6.png)\n\n(4.6)\n\nNote: the sum of all probabilities of rules with LHS A must be 1.\n\nPCFG extends CFG like how Markov models extend regular grammars. Each\nproduction rule is assigned with a probability. The probability of a parse is\nthe product of probabilities of productions used in that derivation. These\nprobabilities can be regarded as parameters of the model, and for large NLP\nproblems it is convenient to learn these parameters via machine learning\nmethods. A probabilistic grammar\u2019s validity is constrained by context of its\ntraining dataset.\n\nAn efficient PCFG design must weigh scalability, generality factors and issues\nsuch as grammar ambiguity must be resolved to improve system performance.\n\n### 4.6.4 A Simple Example of PCFG\n\nThis section used sentences/utterances [4.84] _buy coffee from Starbucks_ as\nexample to illustrate how PCFG works. It has simple CFG rules and\nprobabilities in a segment of an AI chatbot dialogue for food ordering at\ncampus as shown in Fig. 4.13.\n\n![](../images/533412_1_En_4_Chapter/533412_1_En_4_Fig13_HTML.png)\n\nFig.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96229776-9ca8-4693-8b5c-f7a5eafcadef": {"__data__": {"id_": "96229776-9ca8-4693-8b5c-f7a5eafcadef", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fee58545-512c-4ca6-aeac-c39e423a6d64", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "92872574562c0b71572d29af36ef63e0025537e2e0bf414b3b4a1fac762d6614", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35e62314-07b0-4bd4-ac68-c6a728a21e58", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "fb78e0c8ceb82ed916bcb25d831458e8bcfa5d1471711baa289bfd56253e7e46", "class_name": "RelatedNodeInfo"}}, "hash": "f9513356d95f27630bde967d43933730054af74b2118dfcc031b067e07aadf4d", "text": "These\nprobabilities can be regarded as parameters of the model, and for large NLP\nproblems it is convenient to learn these parameters via machine learning\nmethods. A probabilistic grammar\u2019s validity is constrained by context of its\ntraining dataset.\n\nAn efficient PCFG design must weigh scalability, generality factors and issues\nsuch as grammar ambiguity must be resolved to improve system performance.\n\n### 4.6.4 A Simple Example of PCFG\n\nThis section used sentences/utterances [4.84] _buy coffee from Starbucks_ as\nexample to illustrate how PCFG works. It has simple CFG rules and\nprobabilities in a segment of an AI chatbot dialogue for food ordering at\ncampus as shown in Fig. 4.13.\n\n![](../images/533412_1_En_4_Chapter/533412_1_En_4_Fig13_HTML.png)\n\nFig. 4.13\n\nSample CFG rules and their probabilities in AI Chatbot dialogues (food\nordering at campus)\n\nThe probability of each production rule type must sum up to 1 is one of the\nmost important basic criteria of PCFG as shown. For instance, three production\nrules of S: S \u2192 NP VP (0.82), S \u2192 Aux NP VP (0.12) and S \u2192 VP (0.06) must sum-\nup to 1. It is the same as other production rules for NP, Nom, VP, Det, N, V,\nAux, Proper-N, and Pronoun. Of course, if the corpus is very large, some of\nthese probability values will be very small, just like N-gram probability\nevaluation discussed in Chap. [2](533412_1_En_2_Chapter.xhtml).\n\nIt can apply either top-down parser or bottom-up parser approach to generate\nparse tree with the following PCFG probability evaluation scheme:\n\n![$$ P\\(T\\)=\\\\prod \\\\limits_{n\\\\in T}p\\\\left\\(r\\(n\\)\\\\right\\)\n$$](../images/533412_1_En_4_Chapter/533412_1_En_4_Chapter_TeX_Equ7.png)\n\n(4.7)\n\nwhere _p(r(n))_ is the probability that rule _r_ will be applied to expand the\nnonterminal _n_.\n\nSo, what required to achieve is\n\n![$$ \\\\hat{T}\\(S\\)={\\\\displaystyle \\\\begin{array}{c}\\\\arg \\\\max P\\(T\\)\\\\\\\\\n{}T\\\\in \\\\tau \\(S\\)\\\\end{array}}\n$$](../images/533412_1_En_4_Chapter/533412_1_En_4_Chapter_TeX_Equ8.png)\n\n(4.8)\n\nwhere _\u03c4_ ( _S_ ) denotes the set of all possible parses for _S_.\n\nFigure 4.14 depicts different meanings and two parse trees for utterances\n[4.85] _can you buy Starbucks coffee?_ The first interpretation regards\n_Starbucks_ and _coffee_ are two standalone NPs with equal significance. The\nsecond interpretation is to combine _Starbucks_ and _coffee_ into a single NP\nconstituent which is a brand name, in this case, _can you buy Starbucks\ncoffee_ can interpret to buy coffee or non-coffee items. Hence, parse tree\nprobability calculation is also different.\n\n![](../images/533412_1_En_4_Chapter/533412_1_En_4_Fig14_HTML.png)\n\nFig. 4.14\n\nTwo possible parse trees for utterance \u201cCan you buy Starbucks coffee\u201d?\n\nFigure 4.15 shows all CFG rules and associated probabilities of these two\nparse trees. So, PCFG probabilities for parse trees 1 and 2 are\n\n![$$ {\\\\displaystyle \\\\begin{array}{ll}P\\\\left\\({\\\\mathrm{PT}}_1\\\\right\\)&amp;\n=0.12\\\\times 0.36\\\\times 0.06\\\\times 0.06\\\\times 0.37\\\\times 0.72\\\\times\n0.43\\\\times 0.36\\\\times 0.41\\\\times 0.63\\\\times 0.75\\\\\\\\ {}&amp; =1.242\\\\times\n{10}^{-6}\\\\end{array}}\n$$](../images/533412_1_En_4_Chapter/533412_1_En_4_Chapter_TeX_Equa.png)\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35e62314-07b0-4bd4-ac68-c6a728a21e58": {"__data__": {"id_": "35e62314-07b0-4bd4-ac68-c6a728a21e58", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "96229776-9ca8-4693-8b5c-f7a5eafcadef", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f9513356d95f27630bde967d43933730054af74b2118dfcc031b067e07aadf4d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "632c0d18-5b4c-461d-9d75-73d9a50db254", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "87771d447172d7e30f68c93eb49a292cfddd6fddf9e7a2044f3c3b036f2f72c7", "class_name": "RelatedNodeInfo"}}, "hash": "fb78e0c8ceb82ed916bcb25d831458e8bcfa5d1471711baa289bfd56253e7e46", "text": "Figure 4.15 shows all CFG rules and associated probabilities of these two\nparse trees. So, PCFG probabilities for parse trees 1 and 2 are\n\n![$$ {\\\\displaystyle \\\\begin{array}{ll}P\\\\left\\({\\\\mathrm{PT}}_1\\\\right\\)&amp;\n=0.12\\\\times 0.36\\\\times 0.06\\\\times 0.06\\\\times 0.37\\\\times 0.72\\\\times\n0.43\\\\times 0.36\\\\times 0.41\\\\times 0.63\\\\times 0.75\\\\\\\\ {}&amp; =1.242\\\\times\n{10}^{-6}\\\\end{array}}\n$$](../images/533412_1_En_4_Chapter/533412_1_En_4_Chapter_TeX_Equa.png)\n\n![$$ {\\\\displaystyle \\\\begin{array}{ll}P\\\\left\\({\\\\mathrm{PT}}_2\\\\right\\)&amp;\n=0.12\\\\times 0.36\\\\times 0.36\\\\times 0.06\\\\times 0.05\\\\times 0.72\\\\times\n0.43\\\\times 0.36\\\\times 0.41\\\\times 0.63\\\\times 0.75\\\\\\\\ {}&amp; =1.007\\\\times\n{10}^{-6}\\\\end{array}}\n$$](../images/533412_1_En_4_Chapter/533412_1_En_4_Chapter_TeX_Equb.png)\n\n![](../images/533412_1_En_4_Chapter/533412_1_En_4_Fig15_HTML.png)\n\nFig. 4.15\n\nCFG rules and associated probabilities for two possible parse trees PT1 vs.\nPT2\n\nCFG probability algorithm parse tree 1 has a high probability. In other words,\nit is more possible the meaning is to buy coffee other than buy other things\nfrom Starbucks. It also shows an efficient solution to differentiate which\nparse tree is more probable, when there are ambiguities in two or more parse\ntrees provided with sufficient lexical probabilities and corpus to calculate\nprobabilities.\n\n### 4.6.5 Using Probabilities for Language Modeling\n\n_Probability parsing_ can be considered as the integration of N-gram\nprobability concept with parse tree formation. Since there are few grammar\nrules than word sequence for N-gram generation, applying this calculation\nmethod one can calculate results efficiently instead of N-gram frequencies\nregardless of syntactic meaning and rules.\n\nBased on this method, the probability of _S_ is the sum of probabilities of\nall possible parses given by\n\n![$$ P\\(S\\)=\\\\sum \\\\limits_{T\\\\in \\\\tau \\(S\\)}P\\(T\\)\n$$](../images/533412_1_En_4_Chapter/533412_1_En_4_Chapter_TeX_Equ9.png)\n\n(4.9)\n\nagainst N-gram probability calculation with Markov model.\n\n![$$ P\\(S\\)=P\\\\left\\({w}_1\\\\right\\)\\\\ast P\\\\left\\({w}_2|{w}_1\\\\right\\)\\\\ast\nP\\\\left\\({w}_3|{w}_1{w}_2\\\\right\\)\\\\ast\nP\\\\left\\({w}_4|{w}_1{w}_2{w}_3\\\\right\\)\\\\dots\n$$](../images/533412_1_En_4_Chapter/533412_1_En_4_Chapter_TeX_Equ10.png)\n\n(4.10)\n\n### 4.6.6 Limitations for PCFG\n\nIn many situations, it is adequate to know that one rule is used more\nfrequently than another, e.g.\n\n[4.86] _Can you buy Starbucks coffee_? vs. [4.87] _Can you buy KFC coffee_?\n\nBut often it matters what the context is.\n\nFor example:\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "632c0d18-5b4c-461d-9d75-73d9a50db254": {"__data__": {"id_": "632c0d18-5b4c-461d-9d75-73d9a50db254", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35e62314-07b0-4bd4-ac68-c6a728a21e58", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "fb78e0c8ceb82ed916bcb25d831458e8bcfa5d1471711baa289bfd56253e7e46", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f48a3feb-0a05-4043-9d55-fe9e834590c8", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "4a9faf8e819bc9f418f5c32cbbe5ee26b49d92891cc8816656919b9b12c2a98a", "class_name": "RelatedNodeInfo"}}, "hash": "87771d447172d7e30f68c93eb49a292cfddd6fddf9e7a2044f3c3b036f2f72c7", "text": "[4.86] _Can you buy Starbucks coffee_? vs. [4.87] _Can you buy KFC coffee_?\n\nBut often it matters what the context is.\n\nFor example:\n\n![$$ {\\\\displaystyle \\\\begin{array}{ll}\\\\mathrm{S}\\\\to\n\\\\mathrm{NP}\\\\;\\\\mathrm{VP}&amp; \\\\\\\\ {}\\\\mathrm{NP}\\\\to\n\\\\mathrm{Pronoun}&amp; \\\\left\\[0.80\\\\right\\]\\\\\\\\ {}\\\\mathrm{NP}\\\\to\n\\\\mathrm{LexNP}&amp; \\\\left\\[0.20\\\\right\\]\\\\end{array}}\n$$](../images/533412_1_En_4_Chapter/533412_1_En_4_Chapter_TeX_Equ11.png)\n\n(4.11)\n\nFor example, when NP is the subject, the probability of a pronoun may be\nhigher at 0.91. When NP is the direct object, the probability of a pronoun may\nbe lower at 0.34 which means it depends on NP position in a\nsentence/utterance. In other words, the probabilities also often depend on\nlexical options as shown in the following examples:\n\n  * [4.88] _I saw the Great Pyramid flying over Giza Plateau_. vs.\n\n  * [4.89] _I saw a UFO flying over Giza Plateau_.\n\n  * [4.90] _Farmer dumped sacks in the bin_. vs.\n\n  * [4.91] _Farmer dumped sacks of apples_.\n\n  * [4.92] _Jack hit the ball with the bag_. vs.\n\n  * [4.93] _Jack hit the ball with the bat._\n\n  * [4.94] _Visiting relatives can be boring_. vs.\n\n  * [4.95] _Visiting museums can be boring_.\n\n  * [4.96] _There were boys in park and girls_ vs.\n\n  * [4.97] _There were boys in park and shops_.\n\nFor instance, there are two interpretations of utterances [4.98] _boys in park\nand girls_ as shown in Fig. 4.16 showing their syntax ambiguities.\n\n![](../images/533412_1_En_4_Chapter/533412_1_En_4_Fig16_HTML.png)\n\nFig. 4.16\n\nTwo interpretations of the utterance \u201cboys in park and girls\u201d\n\nFigure 4.16 shows two possible parse trees for utterance [4.98]. The first is\n_boys in park_ is a noun clause with conjunction NP _girls_. The second is\n_park and girls_ belong to a single NP with boys as single NP. Although\nstructures are different but the mathematization result for two parse trees is\nidentical which means CFG probability calculation cannot differentiate which\nis better or popular. How to fix this problem?\n\n### 4.6.7 The Fix: Lexicalized Parsing\n\nThe lexicon can be considered as an estimation of a knowledge base (KB) a\npossible solution to the above ambiguous problem.\n\nFigure 4.17 shows [4.90] applying lexical parsing as example. Each constituent\nis a _head word,_ i.e. S using _dumped_ as _Head word_. NP and VP are\nsignified by _farmer_ and the other signified by head word _dumped_ at second\ntier. From _farmer_ it comes up with NNS _farmer_. VP from _dumped_ because\nwill come up with VBD, NP and PP and VDB is signified by _dumped_ as head word\nand NP is _sack_ and PP is _into_. So _sack_ further decomposes in NNS which\npoints to _sacks_ , for PP to further decomposes into to P and NP into _bin_.\nThis will provide information to further decomposition by combining keywords.\nSo, for NP _bin_ it will further decompose into _the_ and _the bin_ as head\nwords for DT and NN, respectively.\n\n![](../images/533412_1_En_4_Chapter/533412_1_En_4_Fig17_HTML.png)\n\nFig. 4.17\n\nLexical tree for the utterance \u201cworkers dumped sacks into a bin\u201d\n\nBy adding lexical items with production rules:\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f48a3feb-0a05-4043-9d55-fe9e834590c8": {"__data__": {"id_": "f48a3feb-0a05-4043-9d55-fe9e834590c8", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "632c0d18-5b4c-461d-9d75-73d9a50db254", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "87771d447172d7e30f68c93eb49a292cfddd6fddf9e7a2044f3c3b036f2f72c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ac5d598-6a86-4d48-98da-d1eb787a8069", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "e88ad571b1aed8de903eeb7351c3d37d24847880470a4cb26cc4e8618edb96d4", "class_name": "RelatedNodeInfo"}}, "hash": "4a9faf8e819bc9f418f5c32cbbe5ee26b49d92891cc8816656919b9b12c2a98a", "text": "VP from _dumped_ because\nwill come up with VBD, NP and PP and VDB is signified by _dumped_ as head word\nand NP is _sack_ and PP is _into_. So _sack_ further decomposes in NNS which\npoints to _sacks_ , for PP to further decomposes into to P and NP into _bin_.\nThis will provide information to further decomposition by combining keywords.\nSo, for NP _bin_ it will further decompose into _the_ and _the bin_ as head\nwords for DT and NN, respectively.\n\n![](../images/533412_1_En_4_Chapter/533412_1_En_4_Fig17_HTML.png)\n\nFig. 4.17\n\nLexical tree for the utterance \u201cworkers dumped sacks into a bin\u201d\n\nBy adding lexical items with production rules:\n\n![$$ \\\\mathrm{VP}\\\\left\\(\\\\mathrm{dumped}\\\\right\\)\\\\to\n\\\\mathrm{VBD}\\\\;\\\\left\\(\\\\mathrm{dumped}\\\\right\\)\\\\;\\\\mathrm{NP}\\\\;\\\\left\\(\\\\mathrm{sacks}\\\\right\\)\\\\;\\\\mathrm{PP}\\\\;\\\\left\\(\\\\mathrm{into}\\\\right\\)=8\\\\times\n{10}^{-10}\n$$](../images/533412_1_En_4_Chapter/533412_1_En_4_Chapter_TeX_Equ12.png)\n\n(4.12)\n\n![$$ \\\\mathrm{VP}\\\\left\\(\\\\mathrm{dumped}\\\\right\\)\\\\to\n\\\\mathrm{VBD}\\\\;\\\\left\\(\\\\mathrm{dumped}\\\\right\\)\\\\;\\\\mathrm{NP}\\\\;\\\\left\\(\\\\mathrm{cats}\\\\right\\)\\\\;\\\\mathrm{PP}\\\\;\\\\left\\(\\\\mathrm{into}\\\\right\\)=1\\\\times\n{10}^{-10}\n$$](../images/533412_1_En_4_Chapter/533412_1_En_4_Chapter_TeX_Equ13.png)\n\n(4.13)\n\n![$$ \\\\mathrm{VP}\\\\left\\(\\\\mathrm{dumped}\\\\right\\)\\\\to\n\\\\mathrm{VBD}\\\\;\\\\left\\(\\\\mathrm{dumped}\\\\right\\)\\\\;\\\\mathrm{NP}\\\\;\\\\left\\(\\\\mathrm{stones}\\\\right\\)\\\\;\\\\mathrm{PP}\\\\;\\\\left\\(\\\\mathrm{into}\\\\right\\)=2\\\\times\n{10}^{-10}\n$$](../images/533412_1_En_4_Chapter/533412_1_En_4_Chapter_TeX_Equ14.png)\n\n(4.14)\n\n![$$ \\\\mathrm{VP}\\\\left\\(\\\\mathrm{dumped}\\\\right\\)\\\\to\n\\\\mathrm{VBD}\\\\;\\\\left\\(\\\\mathrm{dumped}\\\\right\\)\\\\;\\\\mathrm{NP}\\\\;\\\\left\\(\\\\mathrm{sacks}\\\\right\\)\\\\;\\\\mathrm{PP}\\\\;\\\\left\\(\\\\mathrm{above}\\\\right\\)=1\\\\times\n{10}^{-12}\n$$](../images/533412_1_En_4_Chapter/533412_1_En_4_Chapter_TeX_Equ15.png)\n\n(4.15)\n\nwith lower probabilities means infrequency appeared in the corpus.\n\nThis determination method is more efficient as compared with N-gram\nprobability calculation, sample sentences/utterances such as:\n\n  * [4.99] _The farmer dumped sacks of apples into a bin._ vs.\n\n  * [4.100] _The famer dumped sacks of peaches into a bin._ vs.\n\n  * [4.101] _The farmer dumped all the sacks of apples into a bin_.\n\nBut there will be situations that many lexical probabilities comeup with 0\nvalues like N-gram probability evaluation.\n\nA short-cut by considering the following lexical rule as replacement instead\nof considering the whole lexical rule such as Eq. (4.12) can sort out this\nproblem:\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ac5d598-6a86-4d48-98da-d1eb787a8069": {"__data__": {"id_": "5ac5d598-6a86-4d48-98da-d1eb787a8069", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f48a3feb-0a05-4043-9d55-fe9e834590c8", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "4a9faf8e819bc9f418f5c32cbbe5ee26b49d92891cc8816656919b9b12c2a98a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74c9dd41-6daa-4924-bf7a-0b5abcd0a46f", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "62eb4d13947f889430caac4f779e17931277ca0caec008fe6340f88b08b1311e", "class_name": "RelatedNodeInfo"}}, "hash": "e88ad571b1aed8de903eeb7351c3d37d24847880470a4cb26cc4e8618edb96d4", "text": "This determination method is more efficient as compared with N-gram\nprobability calculation, sample sentences/utterances such as:\n\n  * [4.99] _The farmer dumped sacks of apples into a bin._ vs.\n\n  * [4.100] _The famer dumped sacks of peaches into a bin._ vs.\n\n  * [4.101] _The farmer dumped all the sacks of apples into a bin_.\n\nBut there will be situations that many lexical probabilities comeup with 0\nvalues like N-gram probability evaluation.\n\nA short-cut by considering the following lexical rule as replacement instead\nof considering the whole lexical rule such as Eq. (4.12) can sort out this\nproblem:\n\n![$$ \\\\mathrm{VP}\\\\left\\(\\\\mathrm{dumped}\\\\right\\)\\\\to\n\\\\mathrm{VBD}\\\\;\\\\mathrm{NP}\\\\;\\\\mathrm{PP}\\\\kern0.62em\np\\\\left\\(r\\(n\\)\\\\kern0.5em |\\\\kern0.5em n,\\\\kern0.5em h\\(n\\)\\\\right\\)\n$$](../images/533412_1_En_4_Chapter/533412_1_En_4_Chapter_TeX_Equ16.png)\n\n(4.16)\n\nBy doing so, lexical probability of certain nodes _n_ with heads _h_ is\nconsidered based on two conditions: (1) syntactic type of node _n_ and (2)\nhead of node\u2019s mother _h_ ( _m_ ( _n_ )), so lexical rule of Eq. (4.16) is\nsplit into the following:\n\n![$$ {\\\\displaystyle\n\\\\begin{array}{l}\\\\mathrm{Given}\\\\;P\\\\left\\(h\\(n\\)={\\\\mathrm{word}}_i|,\\\\kern0.5em\nn|,\\\\kern0.5em h\\\\left\\(m\\(n\\)\\\\right\\)\\\\right\\)\\\\\\\\\n{}\\\\mathrm{VP}\\\\left\\(\\\\mathrm{dumped}\\\\right\\)\\\\to\n\\\\mathrm{PP}\\\\left\\(\\\\mathrm{into}\\\\right\\),p=p1\\\\\\\\\n{}\\\\mathrm{VP}\\\\left\\(\\\\mathrm{dumped}\\\\right\\)\\\\to\n\\\\mathrm{PP}\\\\left\\(\\\\mathrm{of}\\\\right\\),p=p2\\\\\\\\\n{}\\\\mathrm{NP}\\\\left\\(\\\\mathrm{sacks}\\\\right\\)\\\\to\n\\\\mathrm{PP}\\\\left\\(\\\\mathrm{of}\\\\right\\),p=p3\\\\end{array}}\n$$](../images/533412_1_En_4_Chapter/533412_1_En_4_Chapter_TeX_Equ17.png)\n\n(4.17)\n\nNow the original lexical probability Eq. (4.7) becomes\n\n![$$ P\\(T\\)=\\\\prod \\\\limits_{n\\\\in T}p\\\\left\\(r\\(n\\)|,\\\\kern0.5em\nn|,\\\\kern0.5em h\\(n\\)\\\\right\\)\\\\ast p\\\\left\\(h\\(n\\)|,\\\\kern0.5em\nn|,\\\\kern0.5em h\\\\left\\(m\\(n\\)\\\\right\\)\\\\right\\)\n$$](../images/533412_1_En_4_Chapter/533412_1_En_4_Chapter_TeX_Equ18.png)\n\n(4.18)\n\nUsing Brown corpus as example, the probability of:\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "74c9dd41-6daa-4924-bf7a-0b5abcd0a46f": {"__data__": {"id_": "74c9dd41-6daa-4924-bf7a-0b5abcd0a46f", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ac5d598-6a86-4d48-98da-d1eb787a8069", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "e88ad571b1aed8de903eeb7351c3d37d24847880470a4cb26cc4e8618edb96d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f1080d4-7827-47b9-97ac-cc0014658a6a", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "5286d7a36d5c46c7fc0ad3457b71fada26cf9f9ff005fc1b3db32d7d1979522e", "class_name": "RelatedNodeInfo"}}, "hash": "62eb4d13947f889430caac4f779e17931277ca0caec008fe6340f88b08b1311e", "text": "(4.7) becomes\n\n![$$ P\\(T\\)=\\\\prod \\\\limits_{n\\\\in T}p\\\\left\\(r\\(n\\)|,\\\\kern0.5em\nn|,\\\\kern0.5em h\\(n\\)\\\\right\\)\\\\ast p\\\\left\\(h\\(n\\)|,\\\\kern0.5em\nn|,\\\\kern0.5em h\\\\left\\(m\\(n\\)\\\\right\\)\\\\right\\)\n$$](../images/533412_1_En_4_Chapter/533412_1_En_4_Chapter_TeX_Equ18.png)\n\n(4.18)\n\nUsing Brown corpus as example, the probability of:\n\n![$$ {\\\\displaystyle \\\\begin{array}{l}p\\\\left\\(\\\\mathrm{VP}\\\\to\n\\\\mathrm{VBD}\\\\;\\\\mathrm{NP}\\\\;\\\\mathrm{PP}|,\\\\kern0.5em\n\\\\mathrm{VP}|,\\\\kern0.5em \\\\mathrm{dumped}\\\\right\\)=0.67\\\\\\\\\n{}p\\\\left\\(\\\\mathrm{VP}\\\\right\\)\\\\to \\\\mathrm{VBD}\\\\;\\\\mathrm{NP}\\\\mid\n\\\\mathrm{VP},\\\\kern0.5em \\\\mathrm{dumped}\\\\Big\\)=0.0\\\\\\\\\n{}p\\\\left\\(\\\\mathrm{into}|,\\\\kern0.5em \\\\mathrm{PP}|,\\\\kern0.5em\n\\\\mathrm{dumped}\\\\right\\)=0.22\\\\\\\\ {}p\\\\left\\(\\\\mathrm{into}|,\\\\kern0.5em\n\\\\mathrm{PP}|,\\\\kern0.5em \\\\mathrm{sacks}\\\\right\\)=0\\\\end{array}}\n$$](../images/533412_1_En_4_Chapter/533412_1_En_4_Chapter_TeX_Equ19.png)\n\n(4.19)\n\nparse contribution of this part to the total scores for two candidates will be\n\n![$$ {\\\\displaystyle \\\\begin{array}{ll}\\\\left\\[\\\\mathrm{dumped}\\\\\n\\\\mathrm{into}\\\\right\\]\\\\;0.67\\\\times 0.22&amp; =0.147\\\\\\\\\n{}\\\\left\\[\\\\mathrm{sacks}\\\\ \\\\mathrm{into}\\\\right\\]0\\\\times 0&amp;\n=0\\\\end{array}}\n$$](../images/533412_1_En_4_Chapter/533412_1_En_4_Chapter_TeX_Equ20.png)\n\n(4.20)\n\nSo, we should consider _dumped into_ instead of _sacks into_ in this case.\n\nExercises\n\n  1. 4.1\n\nWhat is syntax and parsing in linguistic? Discuss why they are important in\nNLP?\n\n  2. 4.2\n\nWhat is syntactic rule? State and explain SEVEN commonly used syntactic\npatterns in English language, with an example each to illustrate.\n\n  3. 4.3\n\nAnswer (4.2) by applying to other language such as Chinese, French, or\nSpanish. What is (are) the different of the syntactic rules between these two\nlanguages with example to illustrate.\n\n  4. 4.4\n\nWhat are constituents in English language? State and explain three commonly\nused English constituents, with an example each to illustrate how it works.\n\n  5. 4.5\n\nWhat is context-free grammar (CFG)? State and explain the importance of CFG in\nNLP.\n\n  6. 4.6\n\nState and explain FOUR major CFG components in NLP. Use an example\nsentence/utterance to illustrate.\n\n  7. 4.7\n\nWhat are TWO major types of CFG parsing scheme? Use an example\nsentence/utterance [4.102] _Jack just brought an iPhone from Apple store_ to\nillustrate how these parsers work.\n\n  8. 4.8\n\nWhat is PCFG in NLP parsing? Use same example [4.102] _Jack just brought an\niPhone from Apple store_ to illustrate how it works. Compare with parsers used\nin (4.7), which one is better?\n\n  9. 4.9\n\nWhat are the advantages and limitations of PCFG in NLP parsing? Use some\nsample sentences/utterances to support your answers.\n\n  10. 4.10\n\nWhat is lexical parsing in NLP parsing? Discuss and explain how it works by\nusing sample sentence [4.102] _Jack just brought an iPhone from Apple store_\nfor illustration.\n\nReferences\n\n  1. Allen, J.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f1080d4-7827-47b9-97ac-cc0014658a6a": {"__data__": {"id_": "1f1080d4-7827-47b9-97ac-cc0014658a6a", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "74c9dd41-6daa-4924-bf7a-0b5abcd0a46f", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "62eb4d13947f889430caac4f779e17931277ca0caec008fe6340f88b08b1311e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab11e74c-f3ec-4c74-b401-5c065c3ad6a1", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "329dc0ee68604000d4a804e1e4eafce9c8b31d664e983c01a8d2cafcce1170f1", "class_name": "RelatedNodeInfo"}}, "hash": "5286d7a36d5c46c7fc0ad3457b71fada26cf9f9ff005fc1b3db32d7d1979522e", "text": "Use an example\nsentence/utterance [4.102] _Jack just brought an iPhone from Apple store_ to\nillustrate how these parsers work.\n\n  8. 4.8\n\nWhat is PCFG in NLP parsing? Use same example [4.102] _Jack just brought an\niPhone from Apple store_ to illustrate how it works. Compare with parsers used\nin (4.7), which one is better?\n\n  9. 4.9\n\nWhat are the advantages and limitations of PCFG in NLP parsing? Use some\nsample sentences/utterances to support your answers.\n\n  10. 4.10\n\nWhat is lexical parsing in NLP parsing? Discuss and explain how it works by\nusing sample sentence [4.102] _Jack just brought an iPhone from Apple store_\nfor illustration.\n\nReferences\n\n  1. Allen, J. (1994) Natural Language Understanding (2nd edition) Pearson\n\n  2. Bender, E. M. (2013) Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Morphology and Syntax (Synthesis Lectures on Human Language Technologies). Morgan & Claypool Publishers[Crossref](https://doi.org/10.1007/978-3-031-02150-3)\n\n  3. Brown, K. and Miller, J. (2020) Syntax: A Linguistic Introduction to Sentence Structure. Routledge.[Crossref](https://doi.org/10.4324/9781003070702)\n\n  4. Bunt, H. et al. (2013) Computing Meaning: Volume 4 (Text, Speech and Language Technology Book 47). Springer.\n\n  5. Eisenstein, J. (2019) Introduction to Natural Language Processing (Adaptive Computation and Machine Learning series). The MIT Press.\n\n  6. Goddard, C. (1998) Semantic Analysis: A Practical Introduction (Oxford Textbooks in Linguistics). Oxford University Press.\n\n  7. Gorrell, P. (2006) Syntax and Parsing (Cambridge Studies in Linguistics, Series Number 76). Cambridge University Pres.\n\n  8. Grune, D. and Jacob, C. (2007) Parsing Techniques: A Practical Guide (Monographs in Computer Science). Springer.\n\n  9. Khanam, H. M. (2022) Natural Language Processing Applications: Part of Speech Tagging. Scholars\u2019 Press.\n\n  10. Jurafsky, D., Marin, J., Kehler, A., Linden, K., Ward, N. (1999). Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition. Prentice Hall.\n\n  11. Tuchong (2022 The milky cat with long tail meowing. [https://\u200bstock.\u200btuchong.\u200bcom/\u200bimage/\u200bdetail?\u200bimageId=\u200b8968016011051664\u200b24](https://stock.tuchong.com/image/detail?imageId=896801601105166424). Accessed 20 July 2022.\n\n  12. Wagner, R. K., & Torgesen, J. K. (1987). The nature of phonological processing and its causal role in the acquisition of reading skills. Psychological Bulletin, 101(2), 192\u2013212. [https://\u200bdoi.\u200borg/\u200b10.\u200b1037/\u200b0033-2909.\u200b101.\u200b2.\u200b192](https://doi.org/10.1037/0033-2909.101.2.192).[Crossref](https://doi.org/10.1037/0033-2909.101.2.192)\n\n\n\u00a9 The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.\n2024\n\nR. S. T. Lee Natural Language Processing\n<https://doi.org/10.1007/978-981-99-1999-4_5>\n\n# 5\\. Meaning Representation\n\nRaymond S. T. Lee 1\n\n(1)\n\nUnited International College, Beijing Normal University-Hong Kong Baptist\nUniversity, Zhuhai, China\n\n## 5.1 Introduction\n\nThe understanding of sentences/utterances in terms of structure, grammar, and\nthe relationship between words using N-gram models or simple syntactic rules\nhas hitherto studied, but not on actual meaning of sentences or different\nwords in a sentence.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab11e74c-f3ec-4c74-b401-5c065c3ad6a1": {"__data__": {"id_": "ab11e74c-f3ec-4c74-b401-5c065c3ad6a1", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f1080d4-7827-47b9-97ac-cc0014658a6a", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "5286d7a36d5c46c7fc0ad3457b71fada26cf9f9ff005fc1b3db32d7d1979522e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbfcb61f-bd1b-480c-9294-75cd9ae2720b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "104c9c6de2958576c36d4bc8f64bce1adb8f3f5bd6ff7825dec1d21c9d1f18a9", "class_name": "RelatedNodeInfo"}}, "hash": "329dc0ee68604000d4a804e1e4eafce9c8b31d664e983c01a8d2cafcce1170f1", "text": "[Crossref](https://doi.org/10.1037/0033-2909.101.2.192)\n\n\n\u00a9 The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.\n2024\n\nR. S. T. Lee Natural Language Processing\n<https://doi.org/10.1007/978-981-99-1999-4_5>\n\n# 5\\. Meaning Representation\n\nRaymond S. T. Lee 1\n\n(1)\n\nUnited International College, Beijing Normal University-Hong Kong Baptist\nUniversity, Zhuhai, China\n\n## 5.1 Introduction\n\nThe understanding of sentences/utterances in terms of structure, grammar, and\nthe relationship between words using N-gram models or simple syntactic rules\nhas hitherto studied, but not on actual meaning of sentences or different\nwords in a sentence. This chapter will explore how to interpret _meaning_ and\nscientific/logical methods to process meaning called _meaning representation_.\nIt is difficult to interpret advance NLP analysis which involves semantic\nmeaning, pragmatic meaning, and discourse in subsequent chapters without it.\nLet us start with meaning representations that amalgamate knowledge linguistic\nforms of real-world to the world of linguistics.\n\n## 5.2 What Is Meaning?\n\n_Language_ is prodigious in recognizing humans encode or decode world\ndescription from experiences to ideas and interpret others\u2019 opinions. It is\nnatural but difficult to utter word strings that match the world into\nexpressions. A way to enrich this activity is to transform essences that wish\nto convey into meaningful words, clauses, phrases, or sentences/utterances by\nverbal or written forms for others to listen, understand, inference, and even\nrespond.\n\n_Meaning_ is the message to convey by words, phrases, and sentences/utterances\nwith context in linguistics. It is also called _lexical_ or _semantic\nmeanings_. Prof. W Tecumseh Fitch described semantics meaning in _The\nEvolution of Language_ (Fitch 2010) as a branch of language study that\nconsistently related with philosophy. This is because the study of semantic\nmeaning raises many fundamental philosophical problems that needed to be\nsolved and explained by philosophers.\n\nA good dictionary provides meaning explanation of a single word in details and\nmany dictionaries on concept/language translation. Nevertheless, the meanings\nof sentences/utterances are not simply the combination of individual word\u2019s\nmeaning, but usually appeared as phrasal words with specific meanings at\npragmatic level, e.g. [5.1] _off the wagon_.\n\n_Semantic meaning_ is the study of meaning assignment to minimal meaning-\nbearing elements to form complex and meaningful ideas. Some basic word groups\nmay be aggregated in content relationship called _thematic groups_ , and\nlexical or semantic fields related to _common sense_ or _world knowledge,_\ne.g. the concept of _doctor_ in English constitutes the lexical semantic field\nin two senses: a medical doctor, or a person with PhD title. Once the meaning\nof a word (word group) is decrypted or analyzed, reaction is formed as\nresponse to the event it represents. Words and their meanings are significant\ninformational cues to understand languages. Further, a person\u2019s life\nexperience and cultural difference are relevant to linguistic meaning\ndevelopment in communication process.\n\n## 5.3 Meaning Representations\n\nThis chapter will adopt similar approach as per _syntax_ and _morphology\nanalysis_ (Bender 2013) to create linguistics inputs representations and\ncapture their meanings. These linguistic representations are meanings\ncharacterization of sentences and state-of-affairs in real-world situation.\n\nUnlike parse trees, these representations are not primarily descriptions of\ninput structure, but is a kind of representation of how humans _understand,\nmean_ anything such as actions, events, and objects, etc. and try to _make\nsense_ of it in our environment\u2014 _the meaning of everything_.\n\nThere are five types of meaning representation: (1) _categories_ , (2)\n_events_ , (3) _time_ , (4) _aspect_ , and (5) _beliefs, desires_ , and\n_intentions_.\n\n  1. 1.\n\n_Categories_ refer to specific objects and entities, e.g. _company names_ ,\n_locations_ , _objects_.\n\n  2. 2.\n\n_Events_ refer to actions or phenomena experienced, e.g. _eating lunch,\nwatching a movie_. They are relevant to verbs or verb phases expressed in POS.\n\n  3. 3.\n\n_Time_ refers to exact or reference moment, e.g. _9:30 am, next week, 2023_.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cbfcb61f-bd1b-480c-9294-75cd9ae2720b": {"__data__": {"id_": "cbfcb61f-bd1b-480c-9294-75cd9ae2720b", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab11e74c-f3ec-4c74-b401-5c065c3ad6a1", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "329dc0ee68604000d4a804e1e4eafce9c8b31d664e983c01a8d2cafcce1170f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b1c2e3c2-53e3-4f2b-8b70-5a2c96ff3ea2", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8e0267d1c27de4a24f57134b7ae584ee3491e1f0390f253d64c51d7ab5d2ae6a", "class_name": "RelatedNodeInfo"}}, "hash": "104c9c6de2958576c36d4bc8f64bce1adb8f3f5bd6ff7825dec1d21c9d1f18a9", "text": "and try to _make\nsense_ of it in our environment\u2014 _the meaning of everything_.\n\nThere are five types of meaning representation: (1) _categories_ , (2)\n_events_ , (3) _time_ , (4) _aspect_ , and (5) _beliefs, desires_ , and\n_intentions_.\n\n  1. 1.\n\n_Categories_ refer to specific objects and entities, e.g. _company names_ ,\n_locations_ , _objects_.\n\n  2. 2.\n\n_Events_ refer to actions or phenomena experienced, e.g. _eating lunch,\nwatching a movie_. They are relevant to verbs or verb phases expressed in POS.\n\n  3. 3.\n\n_Time_ refers to exact or reference moment, e.g. _9:30 am, next week, 2023_.\n\n  4. 4.\n\n_Aspects_ refer to:\n\n    1. (a)\n\n_Stative\u2014_ to state facts.\n\nFor example: [5.2] _Jane knows how to run_.\n\n    2. (b)\n\n_Activity\u2014_ to describe action.\n\nFor example: [5.3] _Jane is running_.\n\n    3. (c)\n\n_Accomplishment\u2014_ to describe completed action without ending terms.\n\nFor example: [5.4] _Jane booked the room_.\n\n    4. (d)\n\n_Achievement\u2014_ to describe terminated action.\n\nFor example: [5.5] _Jane found the book_.\n\n  5. 5.\n\n_Beliefs, desire, and intention_ refer to principles such as:\n\n[5.6] _I think what you are saying is totally correct._\n\n[5.7] _Jane wants to know why she failed in the test_.\n\n[5.8] _I believe everything happens for a reason_.\n\nThese principles are complex as they involve several thoughts in philosophy.\nNevertheless, it is important to devise suitable, logical, and computational\nrepresentations in NLP to facilitate _semantic processing_ and present ideas\nin sentences/utterances.\n\n## 5.4 Semantic Processing\n\n_Semantic processing_ (Bender and Lascarides 2019; Best et al. 2000; Goddard\n1998) undertakes meaning representation to encode and interpret meanings.\nThese representations allow to:\n\n  1. 1.\n\n_Reason_ relations with the environment\n\nFor example: [5.9] _Is Jack inside the classroom_?\n\n  2. 2.\n\n_Answer questions_ based on contents\n\nFor example: [5.10] _Who got the highest grade in the test_?\n\n  3. 3.\n\n_Perform inference_ based on knowledge and determine the verity of unknown\nfact(s), thing(s), or event(s),\n\nFor example: [5.11] _If Jack is in the classroom, and Mary is sitting next to\nhim, then Mary is also in the classroom_.\n\nSemantic processing applied to typical applications includes Q&A chatbot\nsystems, it is necessary to understand meanings, i.e. the ability to answer\nquestions about context or discourse with knowledge, literal or even embedded\nmeanings for implementation. The following shows live examples in our AI Tutor\nchatbot (Cui et al. 2020) which involve different degrees of semantic\nprocessing:\n\n  * [5.12] _What is the meaning of NLP_?\n\n  * \u2013 Basic level of semantic processing for the meaning of certain concept.\n\n  * [5.13] _How does N-gram model work_?\n\n  * \u2013 Requires understandings on facts and meanings to respond.\n\n  * [5.14] _Is Turing Test still exist_?\n\n  * \u2013 Involves high-level query and inference from previous knowledge.\n\n  * [5.15] _Why do we need to study self-awareness in AI_?\n\n  * \u2013 Involves high-level information such as world knowledge or common sense aside AI terminology knowledge base to respond.\n\n  * [5.16] _Should I study AI_?\n\n  * \u2013 Involves the highest-level information about user\u2019s common sense and world knowledge aside AI concepts learnt by the book.\n\n## 5.5 Common Meaning Representation\n\nThere are four common methods of meaning representation scheme: (1) First-\nOrder Predicate Calculus (FOPC), (2) Semantic Networks (semantic net), (3)\nConceptual Dependency Diagram (CDD), and (4) Frame-based Representation. A\nsample sentence [5.17] Jack drives a Mercedes is used to illustrate how they\nperform.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1c2e3c2-53e3-4f2b-8b70-5a2c96ff3ea2": {"__data__": {"id_": "b1c2e3c2-53e3-4f2b-8b70-5a2c96ff3ea2", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbfcb61f-bd1b-480c-9294-75cd9ae2720b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "104c9c6de2958576c36d4bc8f64bce1adb8f3f5bd6ff7825dec1d21c9d1f18a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1a8a867-b916-4d1c-8344-83fc7f136d9f", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "2e267ca5307d88cff6bd53c7d3facf1032cc8f11cca33e914c4a95e8e0dc71b1", "class_name": "RelatedNodeInfo"}}, "hash": "8e0267d1c27de4a24f57134b7ae584ee3491e1f0390f253d64c51d7ab5d2ae6a", "text": "* [5.14] _Is Turing Test still exist_?\n\n  * \u2013 Involves high-level query and inference from previous knowledge.\n\n  * [5.15] _Why do we need to study self-awareness in AI_?\n\n  * \u2013 Involves high-level information such as world knowledge or common sense aside AI terminology knowledge base to respond.\n\n  * [5.16] _Should I study AI_?\n\n  * \u2013 Involves the highest-level information about user\u2019s common sense and world knowledge aside AI concepts learnt by the book.\n\n## 5.5 Common Meaning Representation\n\nThere are four common methods of meaning representation scheme: (1) First-\nOrder Predicate Calculus (FOPC), (2) Semantic Networks (semantic net), (3)\nConceptual Dependency Diagram (CDD), and (4) Frame-based Representation. A\nsample sentence [5.17] Jack drives a Mercedes is used to illustrate how they\nperform.\n\n### 5.5.1 First-Order Predicate Calculus (FOPC)\n\n_First-Order Predicate Logic_ (FOPL) (Dijkstra and Scholten 1989; Goldrei\n2005) is also known as _predicate logic_ , or _first-order predicate\ncalculus_. It is a robust language representation scheme to express the\nrelationship between information objects as _predicates_. For example, FOPC\nmeaning for [5.17] is given by\n\n![$$ {\\\\displaystyle \\\\begin{array}{l}\\\\exists x,y\\\\\n\\\\mathrm{Driving}\\(x\\)\\\\wedge \\\\mathrm{Driver}\\\\left\\( Jack,x\\\\right\\)\\\\wedge\n\\\\mathrm{DriveThing}\\\\left\\(y,x\\\\right\\)\\\\\\\\ {}\\\\kern0.75em \\\\wedge\n\\\\mathrm{CarBrand}\\\\left\\(\\\\mathrm{Mercedes},y\\\\right\\)\\\\end{array}}\n$$](../images/533412_1_En_5_Chapter/533412_1_En_5_Chapter_TeX_Equ1.png)\n\n(5.1)\n\nThis FOPC formulation consists of four predicate calculus segments (\n_predicates_ ) in logical terms.\n\n### 5.5.2 Semantic Networks\n\n_Semantic networks_ ( _semantic nets_ ) (Jackson 2019; Sowa 1991) are\nknowledge representation technique used for _propositional information_. They\nconvey knowledge meanings in a two-dimensional representation. A _semantic\nnet_ can be represented as a labelled directed graph. The logic behind is that\na _concept meaning_ is connected to other _concepts_ and can be represented as\na graph. The information in _semantic net_ is characterized as a set of\n_concept nodes_ to link up with each other by set of labelled arcs which\ncharacterized the relationship as illustrated in Fig. 5.1 for example sentence\n[5.17] _._\n\n![](../images/533412_1_En_5_Chapter/533412_1_En_5_Fig1_HTML.png)\n\nFig. 5.1\n\nSemantic net for example sentence [5.17]\n\n_Driving_ is the core concept connected to two _nodes (concepts)_ : _Driver_\nand _DriveThing_ which links to _Jack_ as the _driver_ and _Mercedes_ as\n_drivething_ respectively.\n\n### 5.5.3 Conceptual Dependency Diagram (CDD)\n\n_Conceptual Dependency_ _Diagram_ _(CDD)_ is a theory to describe how to\nrepresent the _meaning_ of sentences/utterances to draw _inferences_. It has\nbeen argued that CDD representation is independent of the language in which\nsentences are stated originally.\n\nSchank (1972) proposed Conceptual Dependency (CD) theory as a part of a\nnatural language comprehension project. Sentences/utterances applying CD can\ntranslate and express basic concepts as a small set of _semantic primitives_ ,\nwhich can be integrated to represent complex meanings\u2014 _conceptualizations_.\nFigure 5.2 shows a CD diagram for example sentence [5.17].\n\n![](../images/533412_1_En_5_Chapter/533412_1_En_5_Fig2_HTML.png)\n\nFig. 5.2\n\nConceptual dependency diagram for example sentence [5.17]\n\n_Mercedes_ and _Jack_ are two concepts linkup by main concept _Drive-by_ using\nCD representation.\n\n### 5.5.4 Frame-Based Representation\n\n_Frame-based systems_ use _frames_ and _notions_ as basic components to\ncharacterize domain knowledge introduced by Prof.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1a8a867-b916-4d1c-8344-83fc7f136d9f": {"__data__": {"id_": "a1a8a867-b916-4d1c-8344-83fc7f136d9f", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b1c2e3c2-53e3-4f2b-8b70-5a2c96ff3ea2", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8e0267d1c27de4a24f57134b7ae584ee3491e1f0390f253d64c51d7ab5d2ae6a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f4bca3b-8a2d-4b46-9e66-0ab40c3892e2", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f67fa49a5963c4469e150da7e7d4017550f3fa91a626f2055f56d284e0fc987b", "class_name": "RelatedNodeInfo"}}, "hash": "2e267ca5307d88cff6bd53c7d3facf1032cc8f11cca33e914c4a95e8e0dc71b1", "text": "Schank (1972) proposed Conceptual Dependency (CD) theory as a part of a\nnatural language comprehension project. Sentences/utterances applying CD can\ntranslate and express basic concepts as a small set of _semantic primitives_ ,\nwhich can be integrated to represent complex meanings\u2014 _conceptualizations_.\nFigure 5.2 shows a CD diagram for example sentence [5.17].\n\n![](../images/533412_1_En_5_Chapter/533412_1_En_5_Fig2_HTML.png)\n\nFig. 5.2\n\nConceptual dependency diagram for example sentence [5.17]\n\n_Mercedes_ and _Jack_ are two concepts linkup by main concept _Drive-by_ using\nCD representation.\n\n### 5.5.4 Frame-Based Representation\n\n_Frame-based systems_ use _frames_ and _notions_ as basic components to\ncharacterize domain knowledge introduced by Prof. Marvin Minsky in his\nremarkable work _A framework for representing knowledge_ published in 1975\n(Minsky 1975).\n\nA _frame_ is a knowledge configuration to characterize a _concept_ such as _a\ncar_ or _driving a car_ attached to certain definitional and descriptive\ninformation. There are several constructed knowledge representation systems\nbased on the original model. The vital successor of frame-based representation\nschemes are _description logs_ that encode the declarative part of _frames_\nusing _semantic logics_. Most of these semantic logics are components in\nfirst-order logic that related to feature logics. A frame-based representation\nfor [5.17] is shown in Fig. 5.3.\n\n![](../images/533412_1_En_5_Chapter/533412_1_En_5_Fig3_HTML.png)\n\nFig. 5.3\n\nFrame-based diagram for example sentence [5.17]\n\nThe frame-based representation is also invariance to language(s) being used\nlike other meaning representation model.\n\nIn summary, these meaning representations indicated that the linguistics\nmeaning for [5.17] describes certain _state-of-affairs_ happened in a real-\nworld. Different meaning representation models are just different ways to\nrepresent the same scenario. For example, FOPC is a kind of mathematical and\nlogical representation of meanings, while semantic nets are graphical\nrepresentation of such meaning in the form of _directed graphs_.\n\n## 5.6 Requirements for Meaning Representation\n\nThere are three factors to fulfill a meaning representation (Bunt 2013; Butler\n2015; Potts 1994): (1) Verifiability, (2) Ambiguity, and (3) Vagueness\nconsiderations.\n\n### 5.6.1 Verifiability\n\n_Verifiability_ is to determine sentence/utterance is literally _meaningful_\n(it expresses a proposition) if and only if it is either analytic or\nempirically verifiable, which means that it must provide ways for comparison\nof meaning representations to facts against knowledge base, world knowledge,\nor common sense such as:\n\n  * For example: [5.18] _Does Jack drive a Mercedes_?\n\nA verifiable meaning representation asserts to _prove_ the correctness of this\nstatement with comparison, matching, or inferencing operations.\n\nThe answer is _yes_ according to statement [5.17].\n\n### 5.6.2 Ambiguity\n\n_Ambiguity_ is a word, statement, or phrase that consists of more than one\nmeaning. Ambiguous words or phrases can cause confusion, misunderstanding, or\neven humor situations.\n\n  * For example: [5.19] _Jack rode a horse in brown outfit_.\n\nThis clause may drive readers to wonder that the horse wore brown outfit\ninstead of the rider. Likewise, same words with different meanings induce\nambiguity, e.g. _Jack took off his gun at the bank._ It is diverting to\nconfuse the meaning of _bank_ refers to a building or the land alongside of a\nriver or lake. _Context meaning_ is important to resolve ambiguity.\n\n### 5.6.3 Vagueness\n\n_Vagueness_ is to describe borderline cases, e.g. _tall_ is a vague term in\nthe sense that a person who is 1.6 m in height is neither _tall_ nor _shor_ t\nsince there is no amount of conceptual analysis or empirical investigation can\nsettle whether a 1.6 m person is tall or not without any frame of reference.\nHere is another live example:\n\n  * [5.20] _He lives somewhere in the south of US_.\n\n  * \u2013 is also vague as to the meaning of location.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f4bca3b-8a2d-4b46-9e66-0ab40c3892e2": {"__data__": {"id_": "0f4bca3b-8a2d-4b46-9e66-0ab40c3892e2", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1a8a867-b916-4d1c-8344-83fc7f136d9f", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "2e267ca5307d88cff6bd53c7d3facf1032cc8f11cca33e914c4a95e8e0dc71b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b030ab3e-69b3-4c2e-9f0a-39541bf80cf0", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "1808b3a041209555e8042b3cded0ecff601fd390cdbfb0aedb801a846a0bdcf5", "class_name": "RelatedNodeInfo"}}, "hash": "f67fa49a5963c4469e150da7e7d4017550f3fa91a626f2055f56d284e0fc987b", "text": "Likewise, same words with different meanings induce\nambiguity, e.g. _Jack took off his gun at the bank._ It is diverting to\nconfuse the meaning of _bank_ refers to a building or the land alongside of a\nriver or lake. _Context meaning_ is important to resolve ambiguity.\n\n### 5.6.3 Vagueness\n\n_Vagueness_ is to describe borderline cases, e.g. _tall_ is a vague term in\nthe sense that a person who is 1.6 m in height is neither _tall_ nor _shor_ t\nsince there is no amount of conceptual analysis or empirical investigation can\nsettle whether a 1.6 m person is tall or not without any frame of reference.\nHere is another live example:\n\n  * [5.20] _He lives somewhere in the south of US_.\n\n  * \u2013 is also vague as to the meaning of location.\n\n_Ambiguity_ and _vagueness_ are two varieties of uncertainty which are often\ndiscussed together but are distinct in essential features and significances in\nsemantic theory. _Ambiguity_ involves uncertainty about mapping between\nrepresentation levels which have more than a single meaning with different\nstructural characteristics, while _vagueness_ involves uncertainly about the\nactual meaning of terms. Hence, a good meaning representation system should\nresolve _vagueness_ and avoid _ambiguity_.\n\n### 5.6.4 Canonical Forms\n\n#### 5.6.4.1 What Is Canonical Form?\n\nA _canonical form_ refers to entities of resources which can be determined in\nmore than one way, and one of them can be considered as a favorable\n_canonical_ ( _standard) form_.\n\nThe _canonical form_ of a mathematical entity is a standard way of determining\nthat quantity in mathematical expression. For example, the canonical form of a\npositive integer in decimal form is a _number sequence_ which is not started\nfrom zero. It is a class of entity in which an equivalence relation is\ndefined. For example, a _Row Echelon Form (REF)_ and _Jordan Normal Form_ are\ntypical canonical forms for matrices interpretation in _Linear Algebra_.\n\nThere are many methods to represent canonical form of the same entity in\ncomputer science. For instance: (1) _computer algebra_ represents mathematical\nobjects and (2) _path_ concept in a _hierarchical file system_ , where a\nsingle file can be referenced in several ways.\n\n#### 5.6.4.2 Canonical Form in Meaning Representation\n\n_Canonical form_ of meaning representation in NLP refers to the phenomena of a\nsingle sentence/utterance can be assigned with multiple _meanings_ leading to\nsame meaning representation. For example:\n\n  * [5.21] _Jack eats KitKat_.\n\n  * [5.22] _KitKat, Jack likes to eat_.\n\n  * [5.23] _What Jack eats is KitKat?_\n\n  * [5.24] _It\u2019s KitKat at that Jack eats_.\n\nAll these sentences/utterances have similar meanings with minor variations in\n_tones_ and _thematic_ issues.\n\nFOPC, semantic net, conceptional dependency diagram, and frame-based\nrepresentation are good elaborations of how canonical form performs and store\nsuch representations in a knowledge base.\n\n#### 5.6.4.3 Canonical Forms: Pros and Cons\n\n##### Advantages\n\n  1. 1.\n\nSimplify reasoning and storage operations.\n\n  2. 2.\n\nNeedless to generate inference rules for all different variations with same\nmeaning.\n\n##### Disadvantages\n\nNevertheless, it may complicate semantic analysis for sentences/utterances\nwith similar meanings, but each has variance in phonemes or high-level\nsemantic meanings like examples [5.21\u20135.24].\n\n## 5.7 Inference\n\n### 5.7.1 What Is Inference?\n\n_Inference_ (Blackburn and Bos 2005) is divided into _deduction_ and\n_induction_ with origin dated back to Ancient Greece from Aristotle 300s BCE.\n_Deduction_ refers to use available information to guess or draw conclusion\nabout facts such as legendary _Sherlock Holmes_ \u2019 deductive reasoning methods\n(Doyle 2019). Examples of inference by deduction reasoning:\n\n  * [5.25] _Jack is a pilot; he travels a lot_.\n\n  * [5.26] _Jane\u2019s hair is totally soaked; it might be raining outside_.\n\n  * [5.27] _Mary has been very busy at work and may not be able to come for gathering this evening_.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b030ab3e-69b3-4c2e-9f0a-39541bf80cf0": {"__data__": {"id_": "b030ab3e-69b3-4c2e-9f0a-39541bf80cf0", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f4bca3b-8a2d-4b46-9e66-0ab40c3892e2", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f67fa49a5963c4469e150da7e7d4017550f3fa91a626f2055f56d284e0fc987b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f4b4260-f0a6-44f2-8ec5-796514392f74", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "1454b31f865de2854ecea836006853e0c26e13a3456b4e7da4de447e7ae010bf", "class_name": "RelatedNodeInfo"}}, "hash": "1808b3a041209555e8042b3cded0ecff601fd390cdbfb0aedb801a846a0bdcf5", "text": "## 5.7 Inference\n\n### 5.7.1 What Is Inference?\n\n_Inference_ (Blackburn and Bos 2005) is divided into _deduction_ and\n_induction_ with origin dated back to Ancient Greece from Aristotle 300s BCE.\n_Deduction_ refers to use available information to guess or draw conclusion\nabout facts such as legendary _Sherlock Holmes_ \u2019 deductive reasoning methods\n(Doyle 2019). Examples of inference by deduction reasoning:\n\n  * [5.25] _Jack is a pilot; he travels a lot_.\n\n  * [5.26] _Jane\u2019s hair is totally soaked; it might be raining outside_.\n\n  * [5.27] _Mary has been very busy at work and may not be able to come for gathering this evening_.\n\n_Induction_ is inference from evidence to a universal conclusion. An important\nfact is that the conclusions may be correct or incorrect.\n\nExamples of inference by inductive reasoning:\n\n  * [5.28] _The sun rose in the morning every day for the past 30 years. The sun rises every day (in human history)_.\n\n  * [5.29] _The first two kids I met at my new school were kind to me. The students at this school are kind_.\n\n  * [5.30] _Our teacher allows us to pick a piece of object out of a box. The first four students got candies. The box must be full of candies_.\n\nAn inference is valid in general if it is conformed to sound evidence(s) and\nthe conclusion follows logically from related premises.\n\n### 5.7.2 Example of Inferencing with FOPC\n\nInferencing with FOPC is to come up with valid conclusions which leaned on\ninputs meaning representation and knowledge base. For example:\n\n  * [5.31] _Does Jack eat KitKat_?\n\nIt consists of two FOPC statements:\n\n![$$ \\\\mathrm{Thing}\\\\left\\(\\\\mathrm{KitKat}\\\\right\\)\n$$](../images/533412_1_En_5_Chapter/533412_1_En_5_Chapter_TeX_Equ2.png)\n\n(5.2)\n\n![$$ \\\\mathrm{Eat}\\\\ \\\\left\\(\\\\mathrm{Jack},x\\\\right\\)\\\\wedge\n\\\\mathrm{Thing}\\(x\\)\n$$](../images/533412_1_En_5_Chapter/533412_1_En_5_Chapter_TeX_Equ3.png)\n\n(5.3)\n\nGiven the above two FOPC statements are true, it can infer the saying [5.31]\nas _yes_ by using inductive or deductive reasoning.\n\n## 5.8 Fillmore\u2019s Theory of Universal Cases\n\n_Case grammar_ (Fillmore 2020; Mazarweh 2010) is a linguistic system which\nfocuses on the association between _number of subjects_ , _objects, or\nvalence,_ etc. of a verb and grammatic context used in linguistic analysis.\nThis theory is proposed by American linguistic Prof. Charles J. Fillmore\n(1929\u20132014) in his remarkable work _The Case for Case_ in semantic analysis\npublished in 1968, also known as _Fillmore\u2019s Theory_ _of Universal Cases_\n(Fillmore 1968). He believed that there are only a restricted number of\n_semantic roles_ , called _case roles_ appeared in every sentence/utterance\nbeing constructed with the _verb_.\n\n### 5.8.1 What Is Fillmore\u2019s Theory of Universal Cases?\n\n_The_ _Fillmore\u2019s Theory_ _of Universal Cases_ (Fillmore 2020; Mazarweh 2010)\nanalyzes fundamental syntactic structure of sentences/utterances by exploring\nthe association of _semantic roles_ such as: _agent, benefactor, location,\nobject_ , _or instrument,_ etc. which are required by the verb in\nsentence/utterance. For instance, the verb _pay_ consists of semantic roles\nsuch as _agent (A)_ , _beneficiary (B), and object (O)_ for sentence\nconstruction. For example:\n\n  * [5.32] _Jane (A) pays cash (O) to Jack (B)_.\n\nAccording to _Fillmore\u2019s Case Theory_ , each verb needs a certain number of\n_case roles_ to form a _case-frame_. Thus, _case-frame_ determines the vital\naspects of semantic valency of verbs, adjectives, and nouns. _Case-frames_ are\nconformed to certain limitations such as a particular _case role_ can appear\nonly once per sentence.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f4b4260-f0a6-44f2-8ec5-796514392f74": {"__data__": {"id_": "1f4b4260-f0a6-44f2-8ec5-796514392f74", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b030ab3e-69b3-4c2e-9f0a-39541bf80cf0", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "1808b3a041209555e8042b3cded0ecff601fd390cdbfb0aedb801a846a0bdcf5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98ec463e-00aa-4b21-acec-c61ca61a2547", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "c2c6a53b4b91e191233652143f105478a349d15007e10e673eef03d1732bcfb1", "class_name": "RelatedNodeInfo"}}, "hash": "1454b31f865de2854ecea836006853e0c26e13a3456b4e7da4de447e7ae010bf", "text": "which are required by the verb in\nsentence/utterance. For instance, the verb _pay_ consists of semantic roles\nsuch as _agent (A)_ , _beneficiary (B), and object (O)_ for sentence\nconstruction. For example:\n\n  * [5.32] _Jane (A) pays cash (O) to Jack (B)_.\n\nAccording to _Fillmore\u2019s Case Theory_ , each verb needs a certain number of\n_case roles_ to form a _case-frame_. Thus, _case-frame_ determines the vital\naspects of semantic valency of verbs, adjectives, and nouns. _Case-frames_ are\nconformed to certain limitations such as a particular _case role_ can appear\nonly once per sentence. There are _mandatory_ and _optional cases_. _Mandatory\ncases_ cannot be deleted; otherwise, it will produce ungrammatical sentences.\nFor example:\n\n  * [5.33] _This form is used to provide you_.\n\nThis sentence/utterance makes no sense without an additional role that explain\n_provide you to_ or _with_ what matter or notion. One possible solution is:\n\n  * [5.34] _This form is used to provide you_ _with the necessary information_ _._\n\nThe association between nouns and their structures contains both syntactic and\nsemantic importance. The syntactic positional relationship between forms in a\nsentence varies from language to language, so grammarians can observe, examine\nsemantic values in these nouns, and provide information to consider case role\nin a specific language.\n\nOne of the major tasks of semantic analysis in _Fillmore\u2019s Theory_ is to offer\na possible mapping between syntactic constituents of a parsed clause and their\nsemantic roles associated with the verb. The term case role is widely used for\npurely semantic relations, including theta and thematic roles. The _theta role\n(\u03b8-role)_ refers to a formal device for representing syntactic argument\nstructure required syntactically by a particular verb. For instance:\n\n  * [5.35] _Jack gives the toy to Ben_.\n\nStatement [5.35] shows the verb _give_ has three arguments, whereas _Jack_ is\ndetermined as the external _theta role_ of _agent_ , _toy_ is determined as\nthe _theme role_ , and to Ben is determined as the _goal role_.\n\n_Thematic role_ , also called _semantic role_ , refers to case role that a\nnoun phase (NP) may deploy with respect to _action_ or _state_ used by the\n_main verb_. For example:\n\n  * [5.36] _Jack gets a prize_.\n\nStatement [5.36] shows _Jack_ is the _agent_ as he is _doer_ to _get_ , the\n_prize_ is the _object_ being received, so it is a _patient_.\n\n### 5.8.2 Major Case Roles in Fillmore\u2019s Theory\n\nThere are six major roles in _Fillmore\u2019s Theory_ :\n\n  1. 1.\n\n_Agent\u2014_ doer of action, attributes intention.\n\n  2. 2.\n\n_Experiencer\u2014_ doer of action without intention.\n\n  3. 3.\n\n_Theme\u2014_ thing that undergoes change or being acted upon with.\n\n  4. 4.\n\n_Instrument\u2014_ tool being used to perform the action.\n\n  5. 5.\n\n_Beneficiary\u2014_ person or thing for which the action being acted on or\nperformed to.\n\n  6. 6.\n\n_To/At/From Loc/Poss/Time\u2014_ to possess thing(s), place, location, or time.\n\nFor examples:\n\n  * [5.37] _Jack cut the meat with a knife_.\n\n  * [5.38] _The meat was cut by Jack_.\n\n  * [5.39] _The meat was cut with a knife_.\n\n  * [5.40] _A knife cut the meat_.\n\n  * [5.41] _The meat is cut_.\n\n  * [5.42] _Jack lent Jane the CD_.\n\n  * [5.43] _Jack lent the CD to Jane_.\n\nThese examples can conclude that:\n\n  1. 1.\n\n_Agent\u2014Jack_ is the _doer_ revealed in [5.37, 5.38, 5.42, and 5.43] that\nperforms the _action_.\n\n  2. 2.\n\n_Theme\u2014meat_ and _CD_ are _things (objects)_ being acted upon or undergoing\nchange as revealed in [5.38\u20135.43] accordingly.\n\n  3. 3.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "98ec463e-00aa-4b21-acec-c61ca61a2547": {"__data__": {"id_": "98ec463e-00aa-4b21-acec-c61ca61a2547", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f4b4260-f0a6-44f2-8ec5-796514392f74", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "1454b31f865de2854ecea836006853e0c26e13a3456b4e7da4de447e7ae010bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "542e3e7e-e3ca-4632-8700-4e602e99d875", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "6c01396c652b640f7356aac40c1c4373a1c607463348990f63b36f67ddfe2031", "class_name": "RelatedNodeInfo"}}, "hash": "c2c6a53b4b91e191233652143f105478a349d15007e10e673eef03d1732bcfb1", "text": "* [5.38] _The meat was cut by Jack_.\n\n  * [5.39] _The meat was cut with a knife_.\n\n  * [5.40] _A knife cut the meat_.\n\n  * [5.41] _The meat is cut_.\n\n  * [5.42] _Jack lent Jane the CD_.\n\n  * [5.43] _Jack lent the CD to Jane_.\n\nThese examples can conclude that:\n\n  1. 1.\n\n_Agent\u2014Jack_ is the _doer_ revealed in [5.37, 5.38, 5.42, and 5.43] that\nperforms the _action_.\n\n  2. 2.\n\n_Theme\u2014meat_ and _CD_ are _things (objects)_ being acted upon or undergoing\nchange as revealed in [5.38\u20135.43] accordingly.\n\n  3. 3.\n\n_Instrument\u2014knife_ is the _tool_ to complete an action as revealed in [5.37,\n5.39, and 5.40].\n\n  4. 4.\n\n_To-Poss\u2014Jane_ is the one that _possesses the CD_ as revealed in [5.42 and\n5.43] driven by Jack the giver.\n\n_Syntactic choices_ intuition is largely a reflection of underlying semantic\nrelationships which means that identical meanings can descend to articles,\ne.g. [5.37] can also be presented in [5.38\u20135.39], or simplified versions\npresented in [5.40\u20135.41]. _Syntax_ can have several syntactic options but are\nrelated to same meanings in semantic meanings. _Semantic analysis_ is a major\ntask to offer a suitable linkage between constituent of a _parsed clause_ and\nassociated semantic roles related to the _main verb_.\n\n### 5.8.3 Complications in Case Roles\n\nThere are four types of complications in case roles analysis:\n\n  1. 1.\n\nSyntactic constituents\u2019 ability to indicate semantic roles in several cases,\ne.g. subject position: _agent_ vs. _instrument_ vs. _theme_\n\n[5.44] _Jack cut the fish_.\n\n[5.45] _The knife cut the fish_.\n\n[5.46] _The fish is cut_.\n\n  2. 2.\n\nSyntactic expression options availability, e.g. _agent_ and _theme_ in\ndifferent configurations such as:\n\n[5.47] _Jack cut the fish_.\n\n[5.48] _It was the fish that Jack cut_.\n\n[5.49] _The fish was cut by Jack_.\n\n  3. 3.\n\nPrepositional ambiguity not always introduces the same role, e.g. proposition\n_by_ may indicate either _agent_ or _instrument_ such as:\n\n[5.50] _The meat was cut by Jack_.\n\n[5.51] _The meat was cut by a knife_.\n\n  4. 4.\n\nRole options in a sentence:\n\n[5.52] _Jack cut the fish with a knife_.\n\n[5.53] _The fish was cut by Jack_.\n\n[5.54] _The fish was cut with a knife_.\n\n[5.55] _A knife cut the fish_.\n\n[5.56] _The fish was cut_.\n\nIt seems that _semantic roles_ act like a _musical conductor_ in an orchestra\nwith old syntactic constituents and left them out at times, but it is not as\nbad as it seems. There are regularities to consider sets of rules which is the\nbeauty of human languages to describe the same idea in different styles and\nconfigurations.\n\nThere are possible rules in case role, such as:\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "542e3e7e-e3ca-4632-8700-4e602e99d875": {"__data__": {"id_": "542e3e7e-e3ca-4632-8700-4e602e99d875", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "98ec463e-00aa-4b21-acec-c61ca61a2547", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "c2c6a53b4b91e191233652143f105478a349d15007e10e673eef03d1732bcfb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "290e2dc9-d928-4cb4-a952-a5717c1a52f9", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f0894c2296583fb3692e9ca843bd4f28614544035970022abd7a6f76adfdb9d8", "class_name": "RelatedNodeInfo"}}, "hash": "6c01396c652b640f7356aac40c1c4373a1c607463348990f63b36f67ddfe2031", "text": "[5.51] _The meat was cut by a knife_.\n\n  4. 4.\n\nRole options in a sentence:\n\n[5.52] _Jack cut the fish with a knife_.\n\n[5.53] _The fish was cut by Jack_.\n\n[5.54] _The fish was cut with a knife_.\n\n[5.55] _A knife cut the fish_.\n\n[5.56] _The fish was cut_.\n\nIt seems that _semantic roles_ act like a _musical conductor_ in an orchestra\nwith old syntactic constituents and left them out at times, but it is not as\nbad as it seems. There are regularities to consider sets of rules which is the\nbeauty of human languages to describe the same idea in different styles and\nconfigurations.\n\nThere are possible rules in case role, such as:\n\n![$$ \\\\boxed{\\\\begin{array}{l}\\\\mathrm{If}\\\\kern0.5em \\\\exists \\\\kern0.5em\n\\\\mathrm{Agent},\\\\mathrm{it}\\\\ \\\\mathrm{becomes}\\\\ \\\\mathrm{Subject}\\\\\\\\\n{}\\\\kern3.75em \\\\mathrm{Else}\\\\ \\\\mathrm{if}\\\\kern0.5em \\\\exists \\\\kern0.5em\n\\\\mathrm{Instrument}\\\\ \\\\mathrm{it}\\\\ \\\\mathrm{becomes}\\\\\n\\\\mathrm{Subject}\\\\\\\\ {}\\\\kern4.75em \\\\mathrm{Else}\\\\ \\\\mathrm{if}\\\\kern0.5em\n\\\\exists \\\\kern0.5em \\\\mathrm{Theme}\\\\ \\\\mathrm{it}\\\\ \\\\mathrm{becomes}\\\\\n\\\\mathrm{Subject}\\\\\\\\ {}\\\\ \\\\mathrm{Agent}\\\\ \\\\mathrm{preposition}\\\\\n\\\\mathrm{is}\\\\ \\\\mathrm{BY}\\\\\\\\ {}\\\\mathrm{Instrument}\\\\\n\\\\mathrm{preposition}\\\\ \\\\mathrm{is}\\\\ \\\\mathrm{BY}\\\\ \\\\mathrm{if}\\\\\n\\\\mathrm{no}\\\\ \\\\mathrm{agent},\\\\mathrm{else}\\\\ \\\\mathrm{WITH}\\\\end{array}}\n$$](../images/533412_1_En_5_Chapter/533412_1_En_5_Chapter_TeX_Equ4.png)\n\n(5.4)\n\nNote that:\n\n  1. 1.\n\nThey are general rules, some verbs may have exception.\n\n  2. 2.\n\nEvery syntactic constituent can only fill-in one case at a time.\n\n  3. 3.\n\nNo case role can appear twice in the same rule.\n\n  4. 4.\n\nOnly NPs of same case role can be co-joined in the rule.\n\n#### 5.8.3.1 Selectional Restrictions\n\n_Selectional restrictions_ are methods to restrict types of certain roles to\nbe used for semantic consideration. For instance:\n\n  1. 1.\n\n_Agents_ must be _animate,_ i.e. a living thing such as a person, _Jack_.\n\n  2. 2.\n\n_Instruments_ must be _inanimate objects,_ i.e. non-living things such as\n_rock_.\n\n  3. 3.\n\n_Themes_ are types that may be _dependent on verbs,_ e.g. _window_ relates to\nthe verb _break_.\n\nSuch constraints can be applied to the following examples to check whether\nthey make sense or not:\n\n  * [5.57] _Someone_ _assassinated_ _the President_ vs.\n\n  * [5.58] _The spider_ _assassinated_ _the fly_. \u2612\n\nNevertheless, additional rules can be deployed to state that _assassinate_ has\n_intentional_ or _political killing_ such that [5.58] may be incorrect. In\nfact, such method is usually applied for semantic analysis to be discussed in\nChap. [6](533412_1_En_6_Chapter.xhtml).\n\n## 5.9 First-Order Predicate Calculus\n\n### 5.9.1 FOPC Representation Scheme\n\n_First-Order Predicate Calculus (_ _FOPC_ _)_ (Dijkstra and Scholten 1989;\nGoldrei 2005) can be used as a framework to derive semantic representation of\na sentence/utterance. Although it is imperfect, it is still the most\nstraightforward mechanism to interpret meanings as other alternatives are\nfinite and complex for implementation. In most cases, they become notational\nvariants in which the quintessential parts are the same regardless of variant\nto select.\n\nFOPC supports:\n\n  1. 1.\n\n_Reasoning_ in _truth_ conditions analysis to respond _yes_ or _no_ questions.\n\n  2. 2.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "290e2dc9-d928-4cb4-a952-a5717c1a52f9": {"__data__": {"id_": "290e2dc9-d928-4cb4-a952-a5717c1a52f9", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "542e3e7e-e3ca-4632-8700-4e602e99d875", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "6c01396c652b640f7356aac40c1c4373a1c607463348990f63b36f67ddfe2031", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d09852a-b4f6-41e0-a385-15ab77b3aa46", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "25a9eb3b0d4928f2afad0f35aa8f02d532a07bd58b63dd244baa2530880bcf63", "class_name": "RelatedNodeInfo"}}, "hash": "f0894c2296583fb3692e9ca843bd4f28614544035970022abd7a6f76adfdb9d8", "text": "[6](533412_1_En_6_Chapter.xhtml).\n\n## 5.9 First-Order Predicate Calculus\n\n### 5.9.1 FOPC Representation Scheme\n\n_First-Order Predicate Calculus (_ _FOPC_ _)_ (Dijkstra and Scholten 1989;\nGoldrei 2005) can be used as a framework to derive semantic representation of\na sentence/utterance. Although it is imperfect, it is still the most\nstraightforward mechanism to interpret meanings as other alternatives are\nfinite and complex for implementation. In most cases, they become notational\nvariants in which the quintessential parts are the same regardless of variant\nto select.\n\nFOPC supports:\n\n  1. 1.\n\n_Reasoning_ in _truth_ conditions analysis to respond _yes_ or _no_ questions.\n\n  2. 2.\n\n_Variables_ in general cases through variable binding at responses and\nstorage.\n\n  3. 3.\n\n_Inference_ to respond beyond knowledge base storage on new knowledge.\n\nThis choice is neither arbitrary, nor determined by practical application.\nFOPC reflects natural languages semantics as it was designed by humans.\n\n### 5.9.2 Major Elements of FOPC\n\nFOPC consists of four major elements: (1) terms, (2) predicates, (3)\nconnectives, and (4) quantifiers.\n\n  1. 1.\n\n**Terms**\n\nTerms are objects names with three representations: (a) constants, (b)\nfunctions, and (c) variables.\n\n_Constants_ refer to specific object described in sentence/utterance, e.g.\nJack, IBM.\n\n_Functions_ refer to concepts expressed as genitives such as brand name,\nlocation, e.g. _Brandname(Mercedes)_ , _LocationOf(KFC)_ can be regarded as\nsingle-argument predicate.\n\n_Variables_ refer to objects without reference which object is referred to,\nlike variables _x_ , _y and z_ used in a mathematical equation _x_ \\+ _y_ =\n_z_ (e. g. _a_ , _b_ , _c_ , _x_ , _y_ , _z_ , etc.). They are frequently used\nin FOPC for query and inferencing operations.\n\n  2. 2.\n\n**Predicates**\n\n_Predicates_ (Epstein 2012) refer to a predicate notion in traditional grammar\ntraces back to _Aristotelian logic_ (Parry and Hacker 1991). A _predicate_ is\nregarded as the property of a subject has or is characterized by. It can be\nconsidered as the expression of fact to the relations that link up some fixed\nnumber of objects in a specific domain, e.g. _he talks_ , _she cries_ , _Jack\nplays football,_ etc. _Predicates_ are often represented with capital letters\nlike _Buy_ or _Play_ in FOPC and combine with object-names to form a\nproposition, e.g. _Drive(Mercedes)_ , _Drive(Mercedes, Jack)_ ,\n_Drive(Mercedes, x)_ , _Drive(Mercedes, Jack, UIC, Starbucks)_ , _Drive(car,\nx, org, dest),_ etc.\n\n  3. 3.\n\n**Connectives**\n\n_Connectives_ refer to proposition combinations. _Conjunctions_ ( _and_ as in\nEnglish, written as _&_ or \u039b), _disjunctions_ ( _or_ as in English, written\nV), and _implications_ (as _if-then_ in English, written \u2192 or \u2283). _Negation_\n(as _not_ in English, written \u00ac or ~ ) is also regarded a _connective_ , even\nthough it operates on a single proposition.\n\n  4. 4.\n\n**Quantifiers**\n\n_Quantifiers_ refer to generalizations. There are two major kinds of\nquantifiers: _universal_ (as _all_ in English, written \u2200) and existential (as\n_some_ in English, written \u2203). The term _first-order_ in FOPC means that this\nlogic only uses quantifiers to generalize _objects_ , but never onto\n_predicates_.\n\nA FOPC _Context-Free Grammar_ (CFG) specification is shown in Fig. 5.4.\n\n![](../images/533412_1_En_5_Chapter/533412_1_En_5_Fig4_HTML.png)\n\nFig.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d09852a-b4f6-41e0-a385-15ab77b3aa46": {"__data__": {"id_": "3d09852a-b4f6-41e0-a385-15ab77b3aa46", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "290e2dc9-d928-4cb4-a952-a5717c1a52f9", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f0894c2296583fb3692e9ca843bd4f28614544035970022abd7a6f76adfdb9d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "471a4722-e519-4537-8b54-73e79cab831e", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "1ca4cce76195a3c041218157e1d7f7d31313d353b0138e1c4c6bec2f5539c7ec", "class_name": "RelatedNodeInfo"}}, "hash": "25a9eb3b0d4928f2afad0f35aa8f02d532a07bd58b63dd244baa2530880bcf63", "text": "_Negation_\n(as _not_ in English, written \u00ac or ~ ) is also regarded a _connective_ , even\nthough it operates on a single proposition.\n\n  4. 4.\n\n**Quantifiers**\n\n_Quantifiers_ refer to generalizations. There are two major kinds of\nquantifiers: _universal_ (as _all_ in English, written \u2200) and existential (as\n_some_ in English, written \u2203). The term _first-order_ in FOPC means that this\nlogic only uses quantifiers to generalize _objects_ , but never onto\n_predicates_.\n\nA FOPC _Context-Free Grammar_ (CFG) specification is shown in Fig. 5.4.\n\n![](../images/533412_1_En_5_Chapter/533412_1_En_5_Fig4_HTML.png)\n\nFig. 5.4\n\nContext-free grammar (CFG) specification of FOPC\n\n### 5.9.3 Predicate-Argument Structure of FOPC\n\nThe _semantics_ of human languages usually exhibit certain _predicate-\nargument_ structure by _variables,_ e.g. indefinites in generic cases and\ninferencing. It also uses _quantifiers,_ e.g. _every_ , _some_ to create FOPC\nflexibility for sentence structures and partial compositional semantics e.g.\n_sort of_.\n\n_Predicate-argument_ structure refers to _actions, events,_ and _relations_\nthat can be determined and represented by _predicates_ and _arguments_.\nLanguages exhibit certain division-of-labor in which words/constituents are\nserved as predicates and arguments, e.g. _predicates_ to manifest _verb_ , and\n_arguments_ to manifest _different cases of the verb_.\n\n_Predicates_ are primarily _verbs (V), verb phrases (VPs), prepositions,\nadjectives, sentences/utterances_ and sometimes can be _nouns_ and even _noun\nphrases (NPs)._ For instance:\n\n  * [5.59] _Helen cries_.\n\n  * [5.60] _Helen speaks to Mary_.\n\n  * [5.61] _Helen speaks loudly_.\n\n  * [5.62] _Helen speaks loudly in the classroom_.\n\n_Arguments_ are primarily nouns, nominals, and noun phrases (NPs), but can be\nother constituents which rely upon the actual context of sentence/utterance.\nFor instance:\n\n  * [5.63] _Jack goes to the bank_ vs.\n\n  * [5.64] _He goes to the bank_.\n\nThe following shows an FOPC formulation example:\n\n  * [5.65] _Jack gave a pen to Jane_.\n\n![$$\n\\\\mathrm{Giving}\\\\left\\(\\\\mathrm{Jack},\\\\mathrm{Jane},\\\\mathrm{Pen}\\\\right\\)\n$$](../images/533412_1_En_5_Chapter/533412_1_En_5_Chapter_TeX_Equ5.png)\n\n(5.5)\n\nNote that the corresponding FOPC formulation (5.5) is precisely in Fillmore\u2019s\ncase role theory that _gives_ conveys a three-argument predicate: (1) _Agent_\nwhich is _Jack_ as the _giver_ ; (2) _Possess_ which is _Jane_ as the\n_recipient,_ and (3) _Theme_ which is the _pen_ as the _direct object_.\n\nIt can have other configurations to describe the same predicate logic for\nexample:\n\n![$$\n\\\\mathrm{Giving}\\\\left\\(\\\\mathrm{Jack},\\\\mathrm{Pen},\\\\mathrm{Jane}\\\\right\\)\n$$](../images/533412_1_En_5_Chapter/533412_1_En_5_Chapter_TeX_Equ6.png)\n\n(5.6)\n\n![$$\n\\\\mathrm{Gave}\\\\left\\(\\\\mathrm{John},\\\\mathrm{Pen},\\\\mathrm{Jane}\\\\right\\)\n$$](../images/533412_1_En_5_Chapter/533412_1_En_5_Chapter_TeX_Equ7.png)\n\n(5.7)\n\nHere are some complex cases with additional constituents:\n\n  * [5.66] _Jack gave Jane a pen for Susan_.\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "471a4722-e519-4537-8b54-73e79cab831e": {"__data__": {"id_": "471a4722-e519-4537-8b54-73e79cab831e", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d09852a-b4f6-41e0-a385-15ab77b3aa46", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "25a9eb3b0d4928f2afad0f35aa8f02d532a07bd58b63dd244baa2530880bcf63", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a5fdbae-d8f0-49ed-ad88-651a98d525d1", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "14ceda01cc50189e4bc188986d8b1ed96fe76484a70306eabaac7976535180cb", "class_name": "RelatedNodeInfo"}}, "hash": "1ca4cce76195a3c041218157e1d7f7d31313d353b0138e1c4c6bec2f5539c7ec", "text": "It can have other configurations to describe the same predicate logic for\nexample:\n\n![$$\n\\\\mathrm{Giving}\\\\left\\(\\\\mathrm{Jack},\\\\mathrm{Pen},\\\\mathrm{Jane}\\\\right\\)\n$$](../images/533412_1_En_5_Chapter/533412_1_En_5_Chapter_TeX_Equ6.png)\n\n(5.6)\n\n![$$\n\\\\mathrm{Gave}\\\\left\\(\\\\mathrm{John},\\\\mathrm{Pen},\\\\mathrm{Jane}\\\\right\\)\n$$](../images/533412_1_En_5_Chapter/533412_1_En_5_Chapter_TeX_Equ7.png)\n\n(5.7)\n\nHere are some complex cases with additional constituents:\n\n  * [5.66] _Jack gave Jane a pen for Susan_.\n\n![$$ \\\\mathrm{Giving}\\\\\n\\\\left\\(\\\\mathrm{Jack},\\\\mathrm{Jane},\\\\mathrm{Pen},\\\\mathrm{Susan}\\\\right\\)\n$$](../images/533412_1_En_5_Chapter/533412_1_En_5_Chapter_TeX_Equ8.png)\n\n(5.8)\n\n  * [5.67] _Jack gave Jane a pen for Susan on Monday_.\n\n![$$\n\\\\mathrm{Giving}\\\\left\\(\\\\mathrm{Jack},\\\\mathrm{Jane},\\\\mathrm{Pen},\\\\mathrm{Susan},\\\\mathrm{Monday}\\\\right\\)\n$$](../images/533412_1_En_5_Chapter/533412_1_En_5_Chapter_TeX_Equ9.png)\n\n(5.9)\n\n  * [5.68] _Jack gave Jane a pen for Susan in class on Monday_.\n\n![$$\n\\\\mathrm{Giving}\\\\left\\(\\\\mathrm{Jack},\\\\mathrm{Jane},\\\\mathrm{Pen},\\\\mathrm{Susan},\\\\mathrm{in}\\\\\n\\\\mathrm{class},\\\\mathrm{Monday}\\\\right\\)\n$$](../images/533412_1_En_5_Chapter/533412_1_En_5_Chapter_TeX_Equ10.png)\n\n(5.10)\n\nNote that all these predicates should be treated individually as their\narguments have different overall meanings.\n\n### 5.9.4 Meaning Representation Problems in FOPC\n\nA _predicate_ that represents a _verb_ meaning, e.g. _give_ has the same\nargument numbers present as its syntactic categorization frame. It is still\ndifficult to (1) determine the correct _role_ numbers for an event, (2)\nmanifest facts about case role(s) associated with the event, and (3) ensure\ncorrect inference(s) is/are derived from meaning representation.\n\nAccording to above considerations, the FOPC formulation stated in Eq. (5.5) is\nnot as useful as it seems, it would be preferable if _roles_ or _cases_ are\nseparated and flexible when deciding the whole FOPC statement like this:\n\n![$$ \\\\exists x,y\\\\ \\\\mathrm{Borrowing}\\(x\\)\\\\wedge\n\\\\mathrm{Borrow}\\\\mathrm{er}\\\\left\\(\\\\mathrm{Jack},x\\\\right\\)\\\\wedge\n\\\\mathrm{Borrow}\\\\mathrm{ed}\\\\left\\(y,x\\\\right\\)\\\\wedge\n\\\\mathrm{Borrow}\\\\_\\\\mathrm{to}\\\\left\\(\\\\mathrm{Jane},x\\\\right\\)\\\\wedge\n\\\\mathrm{Isa}\\\\left\\(y,\\\\mathrm{Pen}\\\\right\\)\n$$](../images/533412_1_En_5_Chapter/533412_1_En_5_Chapter_TeX_Equ11.png)\n\n(5.11)\n\nNote: Corresponding to Fillmore\u2019s case role theory, _Borrower = Agent_ ,\n_Borrowed = Theme_ , _Borrow_to = To-Poss._\n\nAlthough the notion of predicate relation becomes more complicated, it allows\nmore flexibility for sentence/utterance construction.\n\nIt may further generalize Eq. (5.11) into the following formulation:\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a5fdbae-d8f0-49ed-ad88-651a98d525d1": {"__data__": {"id_": "3a5fdbae-d8f0-49ed-ad88-651a98d525d1", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "471a4722-e519-4537-8b54-73e79cab831e", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "1ca4cce76195a3c041218157e1d7f7d31313d353b0138e1c4c6bec2f5539c7ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b19fce5-054a-4767-be9c-c74e8662ee2b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f0bbe651fca1086351852b05e21e11e8344e1aeda96e120642ded4a70b999412", "class_name": "RelatedNodeInfo"}}, "hash": "14ceda01cc50189e4bc188986d8b1ed96fe76484a70306eabaac7976535180cb", "text": "It may further generalize Eq. (5.11) into the following formulation:\n\n![$$ \\\\exists x,y,z\\\\ \\\\mathrm{Borrowing}\\(x\\)\\\\wedge\n\\\\mathrm{Borrow}\\\\mathrm{er}\\\\left\\(w,x\\\\right\\)\\\\wedge\n\\\\mathrm{Borrow}\\\\mathrm{ed}\\\\left\\(y,x\\\\right\\)\\\\wedge\n\\\\mathrm{Borrow}\\\\_\\\\mathrm{to}\\\\left\\(z,x\\\\right\\)\n$$](../images/533412_1_En_5_Chapter/533412_1_En_5_Chapter_TeX_Equ12.png)\n\n(5.12)\n\nBy doing so, it can generate other complicated clauses by applying different\npredicates combinations. The semantics of NPs and PPS in a sentence plug into\nslots provided by the template can allow flexibility to variable arguments\nnumber associated with an event (predicate).\n\nThis event has many roles to cement input with specific category (e.g. pen)\nfor categories and instances declaration. For example:\n\n![$$\n\\\\mathrm{Isa}\\\\left\\(\\\\mathrm{MobyDick},\\\\mathrm{Novel}\\\\right\\),\\\\mathrm{AKO}\\\\left\\(\\\\mathrm{Novel},\\\\mathrm{Literature}\\\\right\\)\n$$](../images/533412_1_En_5_Chapter/533412_1_En_5_Chapter_TeX_Equ13.png)\n\n(5.13)\n\nNote: Just like _Isa()_ to serve as predicate _Is a_ , _AKO()_ is a useful\npredicate to serve as the meaning _a kind of_. In fact, FOPC materializes\nevents so that they can be quantified, related to other events and objects\nthrough a defined set of relationships, and logical connections between\nclosely related instances without meaning assumptions.\n\n### 5.9.5 Inferencing Using FOPC\n\n_Inference_ is an important process in FOPC which has the capability to\n_validate_ or _prove_ whether a proposition is _true_ or _false_ from a\nknowledge base. _Modus Ponens_ is a fundamental inferencing method used in\nFOPC.\n\n_Modus Ponens (MP)_ is a mode of reasoning from a hypothetical proposition. If\nthe _antecedent_ is _true_ , then the _consequent_ should be also _true_. In\nother words, MP is a kind of _deductive reasoning_ in the form of: _P implies\nQ, i.e. If P is true, then Q must also be true_. Its rule may be written in\nsequent notation as:\n\n![](../images/533412_1_En_5_Chapter/533412_1_En_5_Equ14_HTML.png)\n\n(5.14)\n\nwhere _P_ , _Q_ , and _P_ \u2192 _Q_ are statements or propositions in a formal\nlanguage and \u251c is a metalogical symbol, meaning that _Q_ is a syntactic\nconsequence of _P_ and _P_ \u2192 _Q_ in a logical system. MP rule justification in\na classical two-valued logic is given by a _truth table_ as shown in Fig. 5.5.\n\n![](../images/533412_1_En_5_Chapter/533412_1_En_5_Fig5_HTML.png)\n\nFig. 5.5\n\nTruth table of Modus Pones in 2-valued logic\n\nThe following example uses a _Tesla car_ to demonstrate how FOPC works in\n_logic inference_. It has three statements to process:\n\n![$$ \\\\boxed{\\\\begin{array}{l}\\\\mathrm{ElectricCar}\\\\\n\\\\left\\(\\\\mathrm{Tesla}\\\\right\\)\\\\\\\\ {}\\\\underset{\\\\_}{\\\\forall x\\\\\n\\\\mathrm{ElectricCar}\\\\ \\(x\\)\\\\to\n\\\\mathrm{Fuel}\\\\left\\(x,\\\\mathrm{Electricity}\\\\right\\)\\\\ }\\\\\\\\ {}\\\\kern1.75em\n\\\\mathrm{Fuel}\\\\left\\(\\\\mathrm{Tesla},\\\\mathrm{Electricity}\\\\right\\)\\\\end{array}}\n$$](../images/533412_1_En_5_Chapter/533412_1_En_5_Chapter_TeX_Equ15.png)\n\n(5.15)\n\nNote: First statement says _Tesla_ is an _electric car_ , second statement\nsays for all electric cars _x_ , if a car is an electric car, the fuel being\nused must be electricity.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b19fce5-054a-4767-be9c-c74e8662ee2b": {"__data__": {"id_": "1b19fce5-054a-4767-be9c-c74e8662ee2b", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a5fdbae-d8f0-49ed-ad88-651a98d525d1", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "14ceda01cc50189e4bc188986d8b1ed96fe76484a70306eabaac7976535180cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a8933ee-8098-405b-ae91-184e3ba43068", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "53b2e65c517e5946b46c2dffd7d23e2f421b62f2722163dc98b6a70a4088d917", "class_name": "RelatedNodeInfo"}}, "hash": "f0bbe651fca1086351852b05e21e11e8344e1aeda96e120642ded4a70b999412", "text": "It has three statements to process:\n\n![$$ \\\\boxed{\\\\begin{array}{l}\\\\mathrm{ElectricCar}\\\\\n\\\\left\\(\\\\mathrm{Tesla}\\\\right\\)\\\\\\\\ {}\\\\underset{\\\\_}{\\\\forall x\\\\\n\\\\mathrm{ElectricCar}\\\\ \\(x\\)\\\\to\n\\\\mathrm{Fuel}\\\\left\\(x,\\\\mathrm{Electricity}\\\\right\\)\\\\ }\\\\\\\\ {}\\\\kern1.75em\n\\\\mathrm{Fuel}\\\\left\\(\\\\mathrm{Tesla},\\\\mathrm{Electricity}\\\\right\\)\\\\end{array}}\n$$](../images/533412_1_En_5_Chapter/533412_1_En_5_Chapter_TeX_Equ15.png)\n\n(5.15)\n\nNote: First statement says _Tesla_ is an _electric car_ , second statement\nsays for all electric cars _x_ , if a car is an electric car, the fuel being\nused must be electricity.\n\nThe above predicate _Electric Car (Tesla)_ matches the _antecedent_ of the\nrule, so based on simple MP deduction to conclude that _Fuel(Tesla,\nElectricity)_ is a _True_ statement.\n\nIn fact, MP can be applied in _Forward_ and _Backward Reasoning_ modes.\n\n_Forward Reasoning (FR)_ , also called _normal mode_ is used in normal\nsituation by adding all facts into a KB to invoke all applicable implication\nrules to examine clause correctness or new knowledge addition.\n\n_Backward Reasoning (BR)_ is MP operates in reverse mode to prove specific\nproposition or called _query_ in computer science. That is, to examine if the\nquery formula is true by its presence in KB, or without negative implication\nor facts on return query results.\n\nExercises\n\n  1. 5.1\n\nWhat is _meaning representation_? Explain why _meaning representation_ is\nimportant in NLP. Give one or two live examples to support your answer.\n\n  2. 5.2\n\nState and explain FIVE major categories of _meaning representation_. For each,\ngive one live example to support your answer.\n\n  3. 5.3\n\nState and explain FOUR common types of meaning representation in NLP. For each\ntype, use the following sample sentence/utterance [5.70] [5.69] _Jack buys a\nnew flat in London_ to illustrate how they work for meaning representation.\n\n  4. 5.4\n\nWhat are the THREE basic requirements for meaning representation. For each\nrequirement, give two live examples to support your answer.\n\n  5. 5.5\n\nWhat is Canonical Form? How canonical form is applied to meaning\nrepresentation. For sample sentence/utterance [5.70] [5.69] _Jack buys a new\nflat in London_ , give five variations of this sentence and work out the\ncanonical form in the forms of FOPC and Semantic Net.\n\n  6. 5.6\n\nWhat is Inference? Explain why inference is vital to NLP and the\nimplementation of NLP applications such as Q&A chatbot.\n\n  7. 5.7\n\nWhat is Fillmore\u2019s Theory of universal cases? State and explain SIX major case\nroles of Fillmore\u2019s Theory in meaning representation. Use a live example for\nillustration.\n\n  8. 5.8\n\nWhat is the complication of Fillmore\u2019s Theory in meaning representation by\nusing several live examples, explain how it can be solved.\n\n  9. 5.9\n\nWhat are FOUR basic components of First-Order Predicate Calculus (FOPC)? State\nand explain their roles and function in FOPC formulation.\n\n  10. 5.10\n\nWhat is Modus Ponens (MP) in inferencing? In addition to MP, state and explain\nother possible inferencing methods that can be applied to FOPC in meaning\nrepresentation.\n\nReferences\n\n  1. Bender, E. M. (2013) Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Morphology and Syntax (Synthesis Lectures on Human Language Technologies). Morgan & Claypool Publishers[Crossref](https://doi.org/10.1007/978-3-031-02150-3)\n\n  2. Bender, E. M. and Lascarides, A. (2019) Linguistic Fundamentals for Natural Language Processing II: 100 Essentials from Semantics and Pragmatics (Synthesis Lectures on Human Language Technologies). Springer.\n\n  3. Best, W., Bryan, K. and Maxim, J. (2000) Semantic Processing: Theory and Practice. Wiley.\n\n  4.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a8933ee-8098-405b-ae91-184e3ba43068": {"__data__": {"id_": "9a8933ee-8098-405b-ae91-184e3ba43068", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b19fce5-054a-4767-be9c-c74e8662ee2b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f0bbe651fca1086351852b05e21e11e8344e1aeda96e120642ded4a70b999412", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "630f713f-df93-4c00-9741-54633ff17775", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "275f608b627fabee65c8dc0578fff977b6a83c08b3b12a0c6252ffd00d0ce37d", "class_name": "RelatedNodeInfo"}}, "hash": "53b2e65c517e5946b46c2dffd7d23e2f421b62f2722163dc98b6a70a4088d917", "text": "References\n\n  1. Bender, E. M. (2013) Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Morphology and Syntax (Synthesis Lectures on Human Language Technologies). Morgan & Claypool Publishers[Crossref](https://doi.org/10.1007/978-3-031-02150-3)\n\n  2. Bender, E. M. and Lascarides, A. (2019) Linguistic Fundamentals for Natural Language Processing II: 100 Essentials from Semantics and Pragmatics (Synthesis Lectures on Human Language Technologies). Springer.\n\n  3. Best, W., Bryan, K. and Maxim, J. (2000) Semantic Processing: Theory and Practice. Wiley.\n\n  4. Blackburn, P and Bos, J. (2005) Representation and Inference for Natural Language: A First Course in Computational Semantics (Studies in Computational Linguistics). Center for the Study of Language and Information.\n\n  5. Bunt, H. (2013) Computing Meaning: Volume 4 (Text, Speech and Language Technology Book 47). Springer.\n\n  6. Butler, A. (2015) Linguistic Expressions and Semantic Processing: A Practical Approach. Springer.[Crossref](https://doi.org/10.1007/978-3-319-18830-0)[zbMATH](http://www.emis.de/MATH-item?1323.68002)\n\n  7. Cui, Y., Huang, C., Lee, Raymond (2020). AI Tutor: A Computer Science Domain Knowledge Graph-Based QA System on JADE platform. World Academy of Science, Engineering and Technology, Open Science Index 168, International Journal of Industrial and Manufacturing Engineering, 14(12), 543 - 553.\n\n  8. Dijkstra, E. W. and Scholten, C. S. (1989) Predicate Calculus and Program Semantics (Monographs in Computer Science). Springer. Advanced Reasoning Forum.\n\n  9. Doyle, A. C. (2019) The Adventures of Sherlock Holmes (AmazonClassics Edition). AmazonClassics.\n\n  10. Epstein, R. (2012) Predicate Logic. Advanced Reasoning Forum.\n\n  11. Fillmore, C. J. (1968) The Case for Case. In Bach and Harms (Ed.): Universals in Linguistic Theory. New York: Holt, Rinehart, and Winston, 1-88.\n\n  12. Fillmore, C. J. (2020) Form and Meaning in Language, Volume III: Papers on Linguistic Theory and Constructions (Volume 3). Center for the Study of Language and Information.\n\n  13. Goddard, C. (1998) Semantic Analysis: A Practical Introduction (Oxford Textbooks in Linguistics). Oxford University Press.\n\n  14. Goldrei, D. (2005) Propositional and Predicate Calculus: A Model of Argument. Springer.[zbMATH](http://www.emis.de/MATH-item?1082.03001)\n\n  15. Jackson, P. C. (2019) Toward Human-Level Artificial Intelligence: Representation and Computation of Meaning in Natural Language (Dover Books on Mathematics). Dover Publications.\n\n  16. Mazarweh, S. (2010) Fillmore Case Grammar: Introduction to the Theory. GRIN Verlag.\n\n  17. Minsky, M. (1975). A framework for representing knowledge. In P. Winston, Ed., The Psychology of Computer Vision. New York: McGraw-Hill, pp. 211-277.\n\n  18. Parry, W. T. and Hacker, E. A. (1991) Aristotelian logic. Suny Press.[zbMATH](http://www.emis.de/MATH-item?0769.03002)\n\n  19. Potts, T. C. (1994) Structures and Categories for the Representation of Meaning. Cambridge University Press.[Crossref](https://doi.org/10.1017/CBO9780511554629)[zbMATH](http://www.emis.de/MATH-item?0836.68090)\n\n  20. Schank, R. C. (1972). Conceptual dependency: A theory of natural language processing. Cognitive Psychology, 3, 552\u2013631.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "630f713f-df93-4c00-9741-54633ff17775": {"__data__": {"id_": "630f713f-df93-4c00-9741-54633ff17775", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a8933ee-8098-405b-ae91-184e3ba43068", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "53b2e65c517e5946b46c2dffd7d23e2f421b62f2722163dc98b6a70a4088d917", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d607e9a-32d4-4498-a209-81f17a445317", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "c3a0c13858d2d798a8528d00ee05ac5945620b054bc6d4eb715201bc4ce9bed2", "class_name": "RelatedNodeInfo"}}, "hash": "275f608b627fabee65c8dc0578fff977b6a83c08b3b12a0c6252ffd00d0ce37d", "text": "In P. Winston, Ed., The Psychology of Computer Vision. New York: McGraw-Hill, pp. 211-277.\n\n  18. Parry, W. T. and Hacker, E. A. (1991) Aristotelian logic. Suny Press.[zbMATH](http://www.emis.de/MATH-item?0769.03002)\n\n  19. Potts, T. C. (1994) Structures and Categories for the Representation of Meaning. Cambridge University Press.[Crossref](https://doi.org/10.1017/CBO9780511554629)[zbMATH](http://www.emis.de/MATH-item?0836.68090)\n\n  20. Schank, R. C. (1972). Conceptual dependency: A theory of natural language processing. Cognitive Psychology, 3, 552\u2013631.[Crossref](https://doi.org/10.1016/0010-0285\\(72\\)90022-9)\n\n  21. Sowa, J. (1991) Principles of Semantic Networks: Explorations in the Representation of Knowledge (Morgan Kaufmann Series in Representation and Reasoning). Morgan Kaufmann Publication.[zbMATH](http://www.emis.de/MATH-item?0758.68018)\n\n\n\u00a9 The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.\n2024\n\nR. S. T. Lee Natural Language Processing\n<https://doi.org/10.1007/978-981-99-1999-4_6>\n\n# 6\\. Semantic Analysis\n\nRaymond S. T. Lee 1\n\n(1)\n\nUnited International College, Beijing Normal University-Hong Kong Baptist\nUniversity, Zhuhai, China\n\n## 6.1 Introduction\n\n### 6.1.1 What Is Semantic Analysis?\n\n_Semantic analysis_ (Cruse 2011; Goddard 1998; Kroeger 2019) can be considered\nas the process of identifying meanings from texts and utterances by analyzing\ngrammatic structures relationships between words, tokens of written texts, or\nverbal communications in NLP.\n\nSemantic analysis tools can assist organizations to extract meaningful\ninformation from unstructured data automatically such as emails,\nconversations, and customers feedbacks. There are many ways ranging from\ncomplete ad-hoc domain-oriented techniques to some theoretical but impractical\nmethods. It is a sophisticated task for a machine to perform interpretation\ndue to complexity and subjectivity involved in human languages. Semantic\nanalysis on natural language captures text meaning with contexts, sentences,\nand grammar logical structures (Bender and Lascarides 2019; Butler 2015).\n\nSemantic analysis is a process to transform linguistic inputs to meaning\nrepresentation and stamina for machine learning tools like text analysis,\nsearch engines, and chatbots. From computer science perspective, semantics can\nbe considered as group of words, phrases, or clauses that provide concern\nspecific context to language, or clues to word meanings and relationships. For\ninstance, a successful semantic analysis will base on quantity methods such as\nword frequency and context on location to generate cognitive connection\nbetween the clause _giant panda is a portly folivore found in China_ and its\nsemantic meaning instead of just the name ( _panda_ ) it stands for.\n\n### 6.1.2 The Importance of Semantic Analysis in NLP\n\n_Semantic analysis_ (Goddard 1998; Sowa 1991) is important to conscious of\nknowledge relevance and information about _meaning,_ e.g., _giant panda_\ncharacteristics, comparisons with other panda species, evolution history,\nrelated news, and information.\n\nIt ensures that the contents are relevant to the understanding of (1) user,\n(2) contents, and (3) context presence in NLP. The problem with establishing\nrelationships between contents and context is that most data-driven technology\ncannot comprehend contextual message of the sentence (phrase or clause) it\nconveys. If the understandings on context and user\u2019s behavior have deep\nsemantic level, it can produce contents relevance and resonant experience.\n\nThere are many automatic classification systems today with purely _bag of\nwords_ approach to identify relevant features and determine documents\nmeanings. Few uses _correlation_ and _collocation_ to account for words that\nhave several meanings based on context. Nevertheless, none uses full semantic\nanalysis for words meanings. But this is very much needed to interpret a\ndocument correctness because language, especially English language is\nambiguous. English nouns have an average of five to eight synonyms, e.g.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d607e9a-32d4-4498-a209-81f17a445317": {"__data__": {"id_": "2d607e9a-32d4-4498-a209-81f17a445317", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "630f713f-df93-4c00-9741-54633ff17775", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "275f608b627fabee65c8dc0578fff977b6a83c08b3b12a0c6252ffd00d0ce37d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a325d46-66cf-4c0e-bc5b-d24307d91452", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "2a8b3b0b85464be19cefd49e631443821045678bd477fe6d4c54bee582be4a87", "class_name": "RelatedNodeInfo"}}, "hash": "c3a0c13858d2d798a8528d00ee05ac5945620b054bc6d4eb715201bc4ce9bed2", "text": "It ensures that the contents are relevant to the understanding of (1) user,\n(2) contents, and (3) context presence in NLP. The problem with establishing\nrelationships between contents and context is that most data-driven technology\ncannot comprehend contextual message of the sentence (phrase or clause) it\nconveys. If the understandings on context and user\u2019s behavior have deep\nsemantic level, it can produce contents relevance and resonant experience.\n\nThere are many automatic classification systems today with purely _bag of\nwords_ approach to identify relevant features and determine documents\nmeanings. Few uses _correlation_ and _collocation_ to account for words that\nhave several meanings based on context. Nevertheless, none uses full semantic\nanalysis for words meanings. But this is very much needed to interpret a\ndocument correctness because language, especially English language is\nambiguous. English nouns have an average of five to eight synonyms, e.g. _run_\nhas more than 100 common meanings like _running towards the finish line_ ,\n_run to a meeting_ , _run a company_ , _the machine is running, tears ran down\nher face_ , _ran for president_ , _run him a couple thousand dollars,_ etc. If\na bag of words is used as features, the software will never be able to\ndistinct between important facts and irrelevant information leading to\nimprecise classification results and ambiguities.\n\n### 6.1.3 How Human Is Good in Semantic Analysis?\n\nHumans extract abstract ideas and notions like breathing without awareness.\nUse the meaning of _apple_ as example, when discussing about the concept of\n_apple_ is referred to _fruit_ consume regularly. But now, a majority is\nreferred to brand name _Apple_ that dominates mobile phones and computers\nindustry. In other words, humans are competent to extract context surrounding\n_words, phrases, objects, scenarios_ and compare information with _prior\nexperience,_ _common sense_ , and _world knowledge_ to construct overall\n_meanings_ in a text or conversation. These analyses outputs will be used to\npredict outcome with incredible accuracy, but algorithms and computers\ncapacity upgrades had modified habitual practices to fit in with machine\nlearning and NLP allowing machine-driven semantic analysis becomes reality.\nSuch machine learning based semantic analysis schemes can help to reveal the\nmeanings in online messages and conversations, determine answers to questions\nwithout manually extracting relevant information from large volumes of\nunstructured data. The truth is semantic analysis aims to _make sense of\neverything_ from words to languages in daily life.\n\n## 6.2 Lexical Vs Compositional Semantic Analysis\n\n### 6.2.1 What Is Lexical Semantic Analysis?\n\n_Lexical semantic analysis_ (Cruse 1986) is a subfield of linguistic semantics\nto study word\u2019s compositionality, grammar, structure mechanisms, and the\nrelationships between word senses and their usages.\n\nThe analytical unit in _lexical semantics_ is called _lexical unit_ , which\nincludes not only words, but also partial words, affixes (subunits), compound\nwords, and phrases, collectively referred to _lexical terms_. _Lexical units_\nare catalogue of words called _lexicon_ of a language. _Lexical semantics_ can\nbe interpreted as the relationship between _lexical terms_ ,\n_sentence/utterance syntax_ and _its meaning_.\n\n_Lexical semantics_ analyzes the meaning of lexical items that correlate with\nlanguage or syntax structure. They classify and decompose lexical terms and\ntokens; examine the similarities and differences in lexical semantic structure\nacross languages; review correlation between sentence\u2019s lexical and syntactic\nmeaning with its semantic meaning.\n\n_Lexical relation_ in lexical semantic involves the analysis of meaning or\nwords relevance in lexical level that include homonymy, polysemy, metonymy,\nsynonyms, antonyms, hyponymy, and hypernymy to be studied in word sense and\nrelation section.\n\n### 6.2.2 What Is Compositional Semantic Analysis?\n\n_Compositionality_ is a notion in philosophy of language: If the meaning of\neach complex expression _E_ in the sentence/utterance depends not only on the\nmeaning of a single word, but also on syntactic structure and the arrangement\nof different words, then sentence/utterance can be considered as\n_compositional_ in linguistic perspective.\n\nThe meaning of a sentence/utterance in a compositional language relies only on\nthe meaning of words that construct sentence/utterance and how those group of\nwords are syntactically linkup to each other. Thus, composition semantics is\nto investigate the meaning of sentence/utterance with its syntactic structure\ninstead of individual word.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a325d46-66cf-4c0e-bc5b-d24307d91452": {"__data__": {"id_": "2a325d46-66cf-4c0e-bc5b-d24307d91452", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2d607e9a-32d4-4498-a209-81f17a445317", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "c3a0c13858d2d798a8528d00ee05ac5945620b054bc6d4eb715201bc4ce9bed2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb765245-f3ad-4357-9964-f2f027ba17f4", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "de980e51641f7683f89c8630bd0539a1e98e1845c26bb7323c029dbb435bd59f", "class_name": "RelatedNodeInfo"}}, "hash": "2a8b3b0b85464be19cefd49e631443821045678bd477fe6d4c54bee582be4a87", "text": "### 6.2.2 What Is Compositional Semantic Analysis?\n\n_Compositionality_ is a notion in philosophy of language: If the meaning of\neach complex expression _E_ in the sentence/utterance depends not only on the\nmeaning of a single word, but also on syntactic structure and the arrangement\nof different words, then sentence/utterance can be considered as\n_compositional_ in linguistic perspective.\n\nThe meaning of a sentence/utterance in a compositional language relies only on\nthe meaning of words that construct sentence/utterance and how those group of\nwords are syntactically linkup to each other. Thus, composition semantics is\nto investigate the meaning of sentence/utterance with its syntactic structure\ninstead of individual word. The logic behind is words conduct the overall\nsentence/utterance meaning but not the combination of their own meaning. For\nexample:\n\n  * [6.1] _Andrew likes Jane = > likes (Andrew, Jane)_ vs\n\n  * [6.2] _Jane likes Andrew = > likes (Jane, Andrew)_\n\nAlthough individual meaning of every single word in these sentences/utterances\nis the same but due to different words arrangement, their meanings and\npredicate logics can be different.\n\n_Compositional semantics_ is to study the meaning of complex language units\nsuch as sentences, paragraphs, or documents. It is vital to transform the\ninformation represented by language units into a formal representation which\nconsists of (1) symbolic and (2) vectorial representations.\n\n_Symbolic representations_ are meanings expressed as a logical formula by\ninferential mechanisms, or graph-based representations expressed by graphical\ntransformation.\n\n_Vectorial representation_ are methods based on _distributional semantics_\nsuch as word embeddings to represent meaning as word vectors in multi-\ndimensional space.\n\nAt present, only _vectorial representations_ are applied in large-scale as it\nis difficult to guarantee large sets of logical propositions consistency based\non textual input to problematic inferential mechanisms. There is neither a\nconsensus on suitable graph-based representations such as semantic nets to\nexpress linguistic entities meaning, nor proper operations apply to these\nrepresentations.\n\n## 6.3 Word Senses and Relations\n\n### 6.3.1 What Is Word Sense?\n\n_Word sense_ is a crucial concept to interpret meaning of words in\nlinguistics. For instance, there are over 20 different word senses of the word\n_bank_ in a dictionary, each has a distinct notion based on context and\nsyntactic structure of the word being used, such as:\n\n  1. 1.\n\nFinancial organizations that accept deposits and use funds for lending\noperations (Noun).\n\n[6.3] _Jack goes to the bank and withdraws some money_.\n\n  2. 2.\n\nInventory or stock that keeps for emergencies (Noun).\n\n[6.4] _Jack goes to the food bank to acquire some food_.\n\n  3. 3.\n\nA container with an opening on top to store money (Noun).\n\n[6.5] _His coin bank was empty_.\n\n  4. 4.\n\nA sloping land besides a slope or body of water (Noun).\n\n[6.6] _Jack stands beside the bank of a river (Noun)_.\n\n  5. 5.\n\nA long plie or ridge (Noun).\n\n[6.7] _Jack digs a bank of earth_.\n\n  6. 6.\n\nEnclose with a bank (Verb).\n\n[6.8] _bank roads_\n\n  7. 7.\n\nCover with ashes to control the flames (Verb).\n\n[6.9] _Bank a fire_\n\n  8. 8.\n\nTip laterally (Verb).\n\n[6.10] _The pilot had to bank the aircraft_.\n\n  9. 9.\n\nA fighter maneuvers the aircraft to tip laterally (Noun).\n\n[6.11] _F19 fighter went into a steep bank_.\n\n  10. 10.\n\nSimilar Objects Are Arranged in a Row (Noun).\n\n[6.12] _He operated a bank of switches_.\n\n### 6.3.2 Types of Lexical Semantics\n\nThere are six types of commonly used lexical semantics: (1) homonymy, (2)\npolysemy, (3) metonymy, (4) synonyms, (5) antonyms, (6) hyponymy and\nhypernymy.\n\n#### 6.3.2.1 Homonymy\n\n_Homophones_ are words that are spelled and pronounced the same but have\ndifferent meanings. The word _homonym_ comes from prefix _homo-_ stands for\n_same_ and suffix _-nym_ stands for _name_.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cb765245-f3ad-4357-9964-f2f027ba17f4": {"__data__": {"id_": "cb765245-f3ad-4357-9964-f2f027ba17f4", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a325d46-66cf-4c0e-bc5b-d24307d91452", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "2a8b3b0b85464be19cefd49e631443821045678bd477fe6d4c54bee582be4a87", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3dd270c-7cb5-43ae-98e5-689dbb7629b6", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "69d0ecf644b3c732cee28bd5bb54c5adca907ad9937e0053ae1ac8fba4d63149", "class_name": "RelatedNodeInfo"}}, "hash": "de980e51641f7683f89c8630bd0539a1e98e1845c26bb7323c029dbb435bd59f", "text": "9. 9.\n\nA fighter maneuvers the aircraft to tip laterally (Noun).\n\n[6.11] _F19 fighter went into a steep bank_.\n\n  10. 10.\n\nSimilar Objects Are Arranged in a Row (Noun).\n\n[6.12] _He operated a bank of switches_.\n\n### 6.3.2 Types of Lexical Semantics\n\nThere are six types of commonly used lexical semantics: (1) homonymy, (2)\npolysemy, (3) metonymy, (4) synonyms, (5) antonyms, (6) hyponymy and\nhypernymy.\n\n#### 6.3.2.1 Homonymy\n\n_Homophones_ are words that are spelled and pronounced the same but have\ndifferent meanings. The word _homonym_ comes from prefix _homo-_ stands for\n_same_ and suffix _-nym_ stands for _name_.\n\nExample 1: bank1: financial institution vs bank2: slopping land:\n\n[6.13] _He went to the bank and withdrew some cash_.\n\n[6.14] _He was standing at the bank of the lake in the forest_.\n\nExample 2: bat1: a sporting club for ball hitting vs bat2: a kind of flying\nmammal:\n\n[6.15] _He handles his bat skillfully during the game_.\n\n[6.16] _Bats live the longest as compared with other species of similar size._\n\nExample 3: play1: light-hearted recreational activity for amusement vs\n\nplay2: the activity of doing something in an agreed succession.\n\n[6.17] _This Shakespeare play is excellent_.\n\n[6.18] _It is still my play_.\n\nThere are two related concepts with _homonymy_ : (1) _homographs_ are usually\ndefined as words that have the same spelling with different pronunciations and\n(2) _homophones_ are words that share same pronunciation regardless of\nspellings as examples above. Further, _homographs_ are words with same\nspellings and _heterographs_ are words that share same pronunciation but\ndifferent spellings, e.g. _chart vs chat,_ peace vs piece, right vs write,\netc.\n\n_Homonymy_ often causes problems in the following NLP applications:\n\n  1. 1.\n\ninformation retrieval confusion e.g., _cat scan._\n\n  2. 2.\n\nmachine translation confuses foreign languages\u2019 meanings:\n\ne.g. bank1\u2014Financial institution, bank (English) \u2192 la banque (French)\n\n[6.19] _He goes to the bank and withdraws some cash_. (English)\n\n[6.20] _Il va \u00e0 la banque et retire de l\u2019argent_. (French)\n\ne.g. bank2\u2014sloping land, bank (English) \u2192 la rive (French)\n\n[6.21] _He lived by the bank of the lake_. (English)\n\n[6.22] _Il habitait au bord du lac_. (French)\n\n  1. 3.\n\ntext-to-speech confusion\n\ne.g., _bass_ (string instrument) vs _bass_ (fish)\n\n#### 6.3.2.2 Polysemy\n\n_Polysemy_ are words with same spellings but different in meanings and\ncontext. The difference between _homonymy_ and _polysemy_ is delicate and\nsubjective.\n\ne.g., bank\n\n[6.23] _The bank was built in 1866_. (financial building)\n\n[6.24] _He withdrew some money from the bank early this morning_. (financial\norganization)\n\nIn fact, many commonly words are polysemy with multiple contexts and meanings\nin different sentence situations.\n\ne.g., _get_ is a commonly used word that has at least three distinct meanings.\n\n[6.25] _I get an apple from the basket_. (have something)\n\n[6.26] _I get it_. (understand)\n\n[6.27] _She gets thinner_. (reach or cause to a specified state or condition)\n\n#### 6.3.2.3 Metonymy\n\n_Metonymy_ is a kind of _figure-of-speech_ in which one word or phrase is\nreplaced by another association.\n\n_It_ is also a rhetorical strategy to describe the periphery of nucleus\nindirectly, as in describing someone\u2019s outfit to individual\u2019s characteristics.\nIt is regarded as a systematic relationship between senses, or systematic\npolysemy, e.g.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3dd270c-7cb5-43ae-98e5-689dbb7629b6": {"__data__": {"id_": "b3dd270c-7cb5-43ae-98e5-689dbb7629b6", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb765245-f3ad-4357-9964-f2f027ba17f4", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "de980e51641f7683f89c8630bd0539a1e98e1845c26bb7323c029dbb435bd59f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8d803e0-c547-4d0c-ac84-b50dcbd3721d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "be03fde7a651d172bae779d6a9ff9a697c8be11b64af1c5e3b1785f20ddae149", "class_name": "RelatedNodeInfo"}}, "hash": "69d0ecf644b3c732cee28bd5bb54c5adca907ad9937e0053ae1ac8fba4d63149", "text": "(financial\norganization)\n\nIn fact, many commonly words are polysemy with multiple contexts and meanings\nin different sentence situations.\n\ne.g., _get_ is a commonly used word that has at least three distinct meanings.\n\n[6.25] _I get an apple from the basket_. (have something)\n\n[6.26] _I get it_. (understand)\n\n[6.27] _She gets thinner_. (reach or cause to a specified state or condition)\n\n#### 6.3.2.3 Metonymy\n\n_Metonymy_ is a kind of _figure-of-speech_ in which one word or phrase is\nreplaced by another association.\n\n_It_ is also a rhetorical strategy to describe the periphery of nucleus\nindirectly, as in describing someone\u2019s outfit to individual\u2019s characteristics.\nIt is regarded as a systematic relationship between senses, or systematic\npolysemy, e.g. _college_ , _hospital_ , and _museum_ can all stand for\nbuilding with semantic relationship between that building and an institution.\n\n_Metonymy_ and _metaphor_ have fundamental differences in functions.\n_Metonymy_ is about referring a method of designation or component\nidentification or symbolic linkage with association, e.g., _crown_ for\nmonarchy or _royalty_. _Metaphor_ is about understanding and interpretation in\ncontract. It is a means to understand or explain a phenomenon by another\ndescription. For instance:\n\n[6.28] _Her business rises like phoenix_.\n\n#### 6.3.2.4 Zeugma Test\n\n_Zeugma_ is the usage of a word(s) that makes sense in one way but not the\nother. Examples of _zeugma_ that caused conflicts in semantics:\n\n[6.29] _Wage neither war nor peace_.\n\n  * There is a term to _wage war_ but literally incorrect to say to _wage peace_.\n\n[6.30] _He watched the brightness of lightning and the pounding of\nthunderstorm_.\n\n  * He can only _watch lightning_ but not _thunder_.\n\nThe _zeugma test_ in semantic analysis consists of using a putatively\nambiguous expression in a sentence in which several of its putative meanings\nare crowded together, whether it make sense or not. Let us use the word\n_serve_ as example:\n\n  * [6.31] _Which United Airlines flights serve dinner_?\n\n  * [6.32] _Does Jack serve the Army_?\n\n  * [6.33] _Do United Airlines flights serve dinner and the Army_?\n\nIt showed that there are two difference senses of _serve_ though [6.33] may\nsound odd.\n\n#### 6.3.2.5 Synonyms\n\n_Synonyms_ are words with same meaning in some or all contexts. They usually\nappear in language in different contexts, such as formal and informal\nlanguage, daily conversations, and business correspondence. Synonyms have\nmodest meaning when used, although they have the same meaning, e.g.,\n_create/make_ , _start/begin_ , _big/huge_ , _attempt/try_ , _house/mansion_ ,\n_pretty/beauty,_ etc. _Synonyms_ have two l _exemes_ if they are\ninterchangeable in all cases and retain the same meaning.\n\nHowever, there are very few _truths_ synonymy in the real-world situation as\nto whether two words are truly _synonyms_. The logic behind is if they are\ndifferent words then they must mean something else or have some context\ndifferences in usage and cannot be the same in all situations. In many cases,\ntwo words are not exactly interchangeable when they appear, even though many\naspects of the meaning are the same. These words are used and mean differently\ndue to concepts of politeness, slang, register, and genre etc.\n\ne.g., _large vs big_ (are they _exactly_ the same?)\n\n[6.34] _This building is very big_ vs\n\n[6.35] _This building is very large_.\n\n[6.36] _Janet is her big sister_ vs\n\n[6.37] _Janet is her large sister_.\n\nAlthough both words have same meanings in the description of _size_ , the word\n_big_ has an additional notion of older in terms of _seniority_ description.\n\n#### 6.3.2.6 Antonyms\n\n_Antonyms_ is the word sense between words with opposite context meanings.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8d803e0-c547-4d0c-ac84-b50dcbd3721d": {"__data__": {"id_": "a8d803e0-c547-4d0c-ac84-b50dcbd3721d", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b3dd270c-7cb5-43ae-98e5-689dbb7629b6", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "69d0ecf644b3c732cee28bd5bb54c5adca907ad9937e0053ae1ac8fba4d63149", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48bb1a59-e55c-44ea-b0a0-d0e02c94b641", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "91b052d0b70539bce6ff08bea5eadf593b88e7ae2c303511d065e0e8ba8f2b5a", "class_name": "RelatedNodeInfo"}}, "hash": "be03fde7a651d172bae779d6a9ff9a697c8be11b64af1c5e3b1785f20ddae149", "text": "In many cases,\ntwo words are not exactly interchangeable when they appear, even though many\naspects of the meaning are the same. These words are used and mean differently\ndue to concepts of politeness, slang, register, and genre etc.\n\ne.g., _large vs big_ (are they _exactly_ the same?)\n\n[6.34] _This building is very big_ vs\n\n[6.35] _This building is very large_.\n\n[6.36] _Janet is her big sister_ vs\n\n[6.37] _Janet is her large sister_.\n\nAlthough both words have same meanings in the description of _size_ , the word\n_big_ has an additional notion of older in terms of _seniority_ description.\n\n#### 6.3.2.6 Antonyms\n\n_Antonyms_ is the word sense between words with opposite context meanings. It\nhas the notion in which other sense relations do not occupy synonym regardless\nof human tendency to categorize experience in dichotomous contrast is not\neasily judged.\n\nHowever, the notion of antonyms is immeasurable. Humans understand the concept\nof _opposite_ from childhood, encounter them in daily life, and even use\n_antonyms_ as a kind of cognitive method to organize notions, concepts, and\nexperiences, e.g. _big vs small, dark vs bright, hot vs cold, in vs out,_ etc.\nAntonyms can also use to interpret binary, scale, or position _opposition_\nsuch as _long vs short, fast vs slow and up vs down,_ etc.\n\n#### 6.3.2.7 Hyponymy and Hypernymy\n\n_Hyponym_ is a word sense of another word if the first word sense is specific,\ndenoting a subclass of the other sense in linguistics, e.g. _truck_ is a\nhyponym of _vehicle_ , _mango_ is a hyponym of _fruit,_ and _chair_ is a\nhyponym of _furniture_ ; or conversely _hypernym/superordinate_ (hyper is\nsuper), e.g. _vehicle_ is a hypernym of _truck_ , _fruit_ is a hypernym of\n_mango_ , _furniture_ is hypernym of _chair_.\n\nIt is interesting to know that hyponymy is not only limited to nouns but it\ncan also be found in verbs, e.g. _gaze_ , _glimpse,_ or _stare_ are all\nregarded to specific moment of _seeing_.\n\n_Hyponymy_ and _hypernymy_ relationship between word sense and relation are\nregarded as the relationship between class and subclass concepts in _object-\noriented programming_ from computer science perspective, e.g. the class of\n_vehicle_ have three subclasses: _car, lorry,_ and _bus_ , while the class of\n_fruit_ can have numerous subclasses such as _apple, orange,_ and _mango_ ; or\nin reverse manner, the concept _vehicle_ is the superclass of _car,_ and the\nconcept _fruit_ is the superclass of _mango_.\n\nFurthermore, words have _hyponyms_ of same broader term are hypernym known as\n_co-hyponyms,_ e.g. _daisy_ and _rose_ and broader term of _flower_ called\n_hyponymy_ or _inclusion_ , which has the same situation for word sense\nrelation of _co-hypernymy_.\n\n_Hyponymy_ has (1) _extensional_ , (2), _entailment_ , (3) _transitive,_ and\n(4) _IS-A hierarchy_ characteristics:\n\n  1. 1.\n\n_Extensional_ is the class represented by the parent extension, including the\nclass represented by hyponym, e.g. the relations between vehicle and truck.\n\n  2. 2.\n\n_Entailment_ is a hyponym sense A of sense B if A entails B.\n\n  3. 3.\n\n_Transitive means if_ A entails B and B entails C, then A entails C, e.g.\n_truck, vehicle_ , _transport_ where _truck_ is a hyponymy of _vehicle_ ,\n_vehicle_ is a hyponymy of _transpor_ t, so _truck_ is a hyponymy of\n_transport_.\n\n  4. 4.\n\n_IS-A hierarchy_ where _A IS-A B (or A IsA B)_ , and _B subsumes A_ in object-\noriented programming (OOP).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48bb1a59-e55c-44ea-b0a0-d0e02c94b641": {"__data__": {"id_": "48bb1a59-e55c-44ea-b0a0-d0e02c94b641", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8d803e0-c547-4d0c-ac84-b50dcbd3721d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "be03fde7a651d172bae779d6a9ff9a697c8be11b64af1c5e3b1785f20ddae149", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63a4bd6d-e2f5-4fb9-933b-39662c7c812c", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8a557d39a10c704bfa33e37ed142a0b4ab7e2ee3b050b6c0303bc7902a60de0f", "class_name": "RelatedNodeInfo"}}, "hash": "91b052d0b70539bce6ff08bea5eadf593b88e7ae2c303511d065e0e8ba8f2b5a", "text": "1.\n\n_Extensional_ is the class represented by the parent extension, including the\nclass represented by hyponym, e.g. the relations between vehicle and truck.\n\n  2. 2.\n\n_Entailment_ is a hyponym sense A of sense B if A entails B.\n\n  3. 3.\n\n_Transitive means if_ A entails B and B entails C, then A entails C, e.g.\n_truck, vehicle_ , _transport_ where _truck_ is a hyponymy of _vehicle_ ,\n_vehicle_ is a hyponymy of _transpor_ t, so _truck_ is a hyponymy of\n_transport_.\n\n  4. 4.\n\n_IS-A hierarchy_ where _A IS-A B (or A IsA B)_ , and _B subsumes A_ in object-\noriented programming (OOP).\n\n#### 6.3.2.8 Hyponyms and Instances\n\n_Hyponyms_ has notions of _instance_ and _class_. In linguistics, an\n_instance_ can be considered as proper noun with unique entity, e.g. _New\nYork_ is an instance of _city_ ; _USA_ is an instance of _country_. It is\nregarded as the relationship between _class vs object_ in object programming.\n\nIn short, _class_ is the notion of things and objects, whereas _object_ is the\ninstance of class, e.g. _person_ is a _class_ concept to describe an\nindividual person while _Jack_ is an _object_ , which is an _instance_ of that\n_class_ concept.\n\nA simple test: the relationship between _car_ and _Tesla_ , are they _class-\nobject_ relationship or _class-subclass_ relationship?\n\n## 6.4 Word Sense Disambiguation\n\n### 6.4.1 What Is Word Sense Disambiguation (WSD)?\n\n_Word sense_ _disambiguation (WSD)_ (Agirre and Edmonds 2007) is a well-known\nchallenge in computational linguistics that involves the identification for\n_correct semantic meaning_ of words used in sentences/utterances. WSD is the\nability to determine which meaning of a word is activated when a word is used\nin a specific context of NLP _._\n\n_Lexical ambiguity_ is one of the initial problems that any NLP system may\nencounter. In summary, POS tagging is applied to resolve _syntactic\nambiguity,_ while WSD is applied to resolve _semantic ambiguity_. However, it\nis always difficult to resolve semantic than syntactic ambiguity. Consider\ndistinct sense for the word _bass_ examples:\n\n  * [6.38] _Jane hates to hear the bass sound_.\n\n  * [6.39] _Jack is eating fried bass_.\n\nIt has completely different word sense in which [6.38] represents a musical\ninstrument and [6.39] represents a type of fish. So, by using WSD the two\nsentences can be interpreted like this:\n\n  * [6.40] _Jane hates to hear the bass/instrument sound_.\n\n  * [6.41] _Jack is eating fried bass/fish_.\n\n### 6.4.2 Difficulties in Word Sense Disambiguation\n\nThere are five major concerns in WSD: (1) difference meaning across\ndictionaries, (2) POS tagging, (3) inter-judge variance, (4) pragmatic\n(discourse), and (5) senses discreteness.\n\n  1. 1.\n\n**Meaning across dictionaries**\n\nA problem with WSD is senses decision as dictionaries and thesauri offer\nseveral words divisions into senses. Many WSD research are commonly used\nWordNet (WordNet 2022a) as the reference word sense corpus for English. It can\nbe considered as a comprehensive lexicon that composes of word concepts and\ntheir semantic relations with other concepts (e.g., synonyms). For example,\nthe concept of _car_ is interpreted as _{car, auto, automobile, machine,\nmotorcar}_. BabelNet (2022) is a recent multilingual encyclopedic dictionary\nwith multilingual WSD.\n\n  2. 2.\n\n**POS Tagging**\n\nWSD and POS tagging involve disambiguation or tagging with words. However,\nalgorithms used for one tend not to work well for the other, mainly because a\nword's POS is primarily determined by adjacent one to three words versus word\nsense determined by more distant words in many cases.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63a4bd6d-e2f5-4fb9-933b-39662c7c812c": {"__data__": {"id_": "63a4bd6d-e2f5-4fb9-933b-39662c7c812c", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48bb1a59-e55c-44ea-b0a0-d0e02c94b641", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "91b052d0b70539bce6ff08bea5eadf593b88e7ae2c303511d065e0e8ba8f2b5a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6aec1e8b-3df6-4662-92ea-3828d287524e", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "e7f81a2dea54e88f1c8547e7bd3e62e198e55e45d206ce77c11d4fda5444bf85", "class_name": "RelatedNodeInfo"}}, "hash": "8a557d39a10c704bfa33e37ed142a0b4ab7e2ee3b050b6c0303bc7902a60de0f", "text": "Many WSD research are commonly used\nWordNet (WordNet 2022a) as the reference word sense corpus for English. It can\nbe considered as a comprehensive lexicon that composes of word concepts and\ntheir semantic relations with other concepts (e.g., synonyms). For example,\nthe concept of _car_ is interpreted as _{car, auto, automobile, machine,\nmotorcar}_. BabelNet (2022) is a recent multilingual encyclopedic dictionary\nwith multilingual WSD.\n\n  2. 2.\n\n**POS Tagging**\n\nWSD and POS tagging involve disambiguation or tagging with words. However,\nalgorithms used for one tend not to work well for the other, mainly because a\nword's POS is primarily determined by adjacent one to three words versus word\nsense determined by more distant words in many cases. For example, the success\nrate of POS tagging algorithms is around 96% versus 75% in WSD with supervised\nlearning current research and findings (Agirre and Edmonds 2007).\n\n  3. 3.\n\n**Inter-judge variance**\n\nWSD system test results on a task are usually compared to ones by humans.\nWhile it is easy to attribute POS to texts, it is difficult in training to\nmark word senses. Since human performance serves as the standard, it is an\nupper limit for computer performance. However, humans fared much better at\ncoarse-grained discrimination than at fine-grained discrimination and is the\nreason for research of the former put to the test in recent WSD evaluation\nexercises.\n\n  4. 4.\n\n**Pragmatic** **(Discourse)**\n\n_Pragmatic_ and _discours_ e are complex problems in NLP. Many AI researchers\nbelieve that one cannot analyze meanings of words without some form of\nsensible _ontology analysis_ and _world knowledge_ at a pragmatic level. Also\n_,_ _common sense_ is sometimes required to distinguish words such as pronouns\nin anaphors or cataphors of the text.\n\n  5. 5.\n\n**Senses discreteness**\n\nThe _notion_ of word sense is sometimes unpredictable and controversial. Most\ncan agree on semantic interpretation at the level of _coarse-grained_\n_homographs_ but going down to _fine-grained polysemy_ can lead to\ndisagreement. For example, Senseval-2 (Preiss 2006) uses fine-grained sensory\ndistinctions, with only 85% of the annotated words that can agree with. Word\nmeanings are infinitely variable, in principle dependent on context, and\ncannot be easily broken down into distinct or separate sub-meanings.\n\n### 6.4.3 Method for Word Sense Disambiguation\n\nCommonly used WSD methods include: (1) knowledge base, (2) supervised\nlearning, (3) semi-supervised, and (4) unsupervised learning methods for WSD\n(Agirre and Edmonds 2007; Preiss 2006).\n\n  1. 1.\n\n_Knowledge-based_ _(KB)_ is a method mainly based on dictionaries, thesauri,\nand lexical knowledge databases. They do not need corpus evidence for\ndisambiguation. The Lesk method (Lesk 1986) is a pioneering dictionary-based\nmethod introduced by Prof. Michael Lesk in 1986. The Lesk definition and its\nalgorithm aim to measure the overlap between the meaning definitions of all\nwords in a context. Kilgarriff and Rosenzweig (2000) simplified the Lesk\ndefinition to measure the overlap between meaning definition of a word and\ncurrent context, meaning the correctness of identifying one word at a time,\nthe current context being the set of words in surrounding sentence/utterance\nor paragraph (Ayetiran and Agbele 2016).\n\n  2. 2.\n\n_Supervised learning_ _(SL)_ methods are standard machine learning techniques\nthat use semantically annotated corpora to train disambiguation. These methods\nassume that context alone can provide sufficient evidence to clarify meaning,\nso verbal knowledge and reasoning are considered unnecessary. A context is\ninterpreted as a set of word features that contains information about\nsurrounding words. Support vector machines (SVM) and memory-based learning are\ncommonly used SL methods for WSD. However, they are usually computationally\nintensive and require large manually labeled corpora to produce satisfactory\nresults.\n\n  3. 3.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6aec1e8b-3df6-4662-92ea-3828d287524e": {"__data__": {"id_": "6aec1e8b-3df6-4662-92ea-3828d287524e", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63a4bd6d-e2f5-4fb9-933b-39662c7c812c", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8a557d39a10c704bfa33e37ed142a0b4ab7e2ee3b050b6c0303bc7902a60de0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbc92233-4a9c-4d7d-813f-dd94a2fe8f04", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "7e7d8770ac7095e603fbcb28d71257dab1ccca292eb41e3b7cd08f00371fcb98", "class_name": "RelatedNodeInfo"}}, "hash": "e7f81a2dea54e88f1c8547e7bd3e62e198e55e45d206ce77c11d4fda5444bf85", "text": "2. 2.\n\n_Supervised learning_ _(SL)_ methods are standard machine learning techniques\nthat use semantically annotated corpora to train disambiguation. These methods\nassume that context alone can provide sufficient evidence to clarify meaning,\nso verbal knowledge and reasoning are considered unnecessary. A context is\ninterpreted as a set of word features that contains information about\nsurrounding words. Support vector machines (SVM) and memory-based learning are\ncommonly used SL methods for WSD. However, they are usually computationally\nintensive and require large manually labeled corpora to produce satisfactory\nresults.\n\n  3. 3.\n\nSince many WSD problems lack training corpora, _semi-supervised methods_ are\napplied to both labeled and unlabeled data, which require only amount of\nannotated text and a large amount of plain unannotated text as well as\nbootstrapping from starting data. The bootstrapping method starts with a small\namount of starting data for each word, either with manually labeled training\nexamples, or with a small set of triggering decision rules. The seed value is\nintended to train an initial classifier with some supervised method. This\nclassifier is then used on the unlabeled portion of corpus to extract a larger\ntraining set with the safest classification. This process is repeated to train\neach new classifier until the entire corpus is exhausted or the maximum number\nof iterations is reached. Other semi-supervised techniques use large unlabeled\ncorpora to provide co-occurrence information to complement labeled corpus\nperspectives to help supervised models adapt to different domains.\n\n  4. 4.\n\n_Unsupervised Learning (UL)_ methods assume that similar meanings appear in\nsimilar contexts, that is why _perceptions_ can be induced from texts by\nclustering word occurrences using a similarity measure of context. This task\nis called _word sense induction_ or _discrimination_. UL methods can overcome\nknowledge acquisition bottlenecks due to their independence from manual work.\nAlthough the performance is lower than other methods abovementioned, fair\ncomparison is hard as the induced senses should link up to a known word senses\ndictionary.\n\n## 6.5 WordNet and Online Thesauri\n\n### 6.5.1 What Is WordNet?\n\n_WordNet_ (WordNet 2022a) is a lexical corpus of words with over 200 languages\nwith adjectives, adverbs, nouns, and verbs grouped into a set of synonyms\nwhere each word in WordNet has a distinct concept. It is organized by concepts\nand meanings against a dictionary in alphabets. Since traditional dictionaries\nwere created by humans, a lexical resource is required for computers effecting\nWordNet is applicable in NLP. It is available for public access and free\ndownload with statistical information as shown in Fig. 6.1.\n\n![](../images/533412_1_En_6_Chapter/533412_1_En_6_Fig1_HTML.png)\n\nFig. 6.1\n\nWordNet basic statistical information\n\nWordNet\u2019s structure is vital tool for computational linguistics and NLP\nimplementations. It resembles a thesaurus and group words by meanings.\nHowever, they have basic differences: (a) WordNet indicates _word senses_ in\naddition to _word forms_. As a result, words that are found near one another\nin network are semantically related or even synonym with each other, (b)\nWordNet encodes semantic relations among words, whereas words in a thesaurus\ndoes not follow a distinct pattern other than the similarity in surface\nmeaning.\n\n### 6.5.2 What Is Synsets?\n\n_WordNet_ can be considered as a network of words connected by lexical and\nsemantic relations. Nouns, verbs, adjectives, and adverbs are combined into a\ngroup of cognitive synonyms called _synsets_ with each expresses a specific\nconcept. _Synsets_ are associated with conceptual semantics and lexical\nrelationships such as hyponyms and antonyms. WordNet contains over 117,000\n_synsets_. Each of these _synset_ is associated with other in a small number\nof conceptual relationships.\n\nA synset contains a short definition called a _gloss_ , and one or more short\nsentences describing how members of synset are used in most contexts. Word\nforms with many different meanings are represented in different synsets. This\nis the form of each form-meaning pair in WordNet. Each synonym group is a\nsynset within a WordNet term, and synonyms that are part of a synset are\nlexical variants of that concept.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cbc92233-4a9c-4d7d-813f-dd94a2fe8f04": {"__data__": {"id_": "cbc92233-4a9c-4d7d-813f-dd94a2fe8f04", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6aec1e8b-3df6-4662-92ea-3828d287524e", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "e7f81a2dea54e88f1c8547e7bd3e62e198e55e45d206ce77c11d4fda5444bf85", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc6b2c27-0fde-4321-8ceb-45f8d5221ab8", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8c1774665141439f5185fdbd07560adb4615ca117509306b1a10918c9cb6a725", "class_name": "RelatedNodeInfo"}}, "hash": "7e7d8770ac7095e603fbcb28d71257dab1ccca292eb41e3b7cd08f00371fcb98", "text": "Nouns, verbs, adjectives, and adverbs are combined into a\ngroup of cognitive synonyms called _synsets_ with each expresses a specific\nconcept. _Synsets_ are associated with conceptual semantics and lexical\nrelationships such as hyponyms and antonyms. WordNet contains over 117,000\n_synsets_. Each of these _synset_ is associated with other in a small number\nof conceptual relationships.\n\nA synset contains a short definition called a _gloss_ , and one or more short\nsentences describing how members of synset are used in most contexts. Word\nforms with many different meanings are represented in different synsets. This\nis the form of each form-meaning pair in WordNet. Each synonym group is a\nsynset within a WordNet term, and synonyms that are part of a synset are\nlexical variants of that concept. Figure 6.2 shows a synset tree for synset\nconcept _book_ and all concept relationships with all other related synsets.\nMeaningful related words and concepts in the generated network can be browsed\nfrom WordNet browser (WordNet 2022b).\n\n![](../images/533412_1_En_6_Chapter/533412_1_En_6_Fig2_HTML.png)\n\nFig. 6.2\n\nThe synsets concept of _book_ in WordNet\n\n### 6.5.3 Knowledge Structure of WordNet\n\nA WordNet structure is concepts of words relationship in a WordNet network to\narrange same concepts in similar interchange contexts in Fig. 6.3. These words\nare unordered sets grouped into synsets and linked with small conceptual\nrelations. An example of synsets structure _benefit_ arrayed synonyms _profit_\nwith definitions and examples as shown in Fig. 6.4. Benefit(profit) is defined\nas an advantage or profit gain from something, e.g. _He receives benefits of\ncomputers trade_.\n\n![](../images/533412_1_En_6_Chapter/533412_1_En_6_Fig3_HTML.png)\n\nFig. 6.3\n\nBasic knowledge structure of WordNet\n\n![](../images/533412_1_En_6_Chapter/533412_1_En_6_Fig4_HTML.png)\n\nFig. 6.4\n\nAn example of knowledge structure of synset _benefit_\n\n### 6.5.4 What Are Major Lexical Relations Captured in WordNet?\n\n_Super-subordinate_ relation, also called _hypernymy, hyponymy,_ or _IS-A\nrelation_ is a frequently used relation among synsets. It links generic\nsynsets such as { _furniture, piece_of_furniture_ } to sub-concepts like {\n_chair_ } and { _armchair_ }. Thus, WordNet indicates that synset _furniture_\nconsists of synset _chair_ , which in turn includes synset _armchair_ ;\nconversely, synsets like _chair_ or _armchair_ make up the synset _furniture_.\nIn fact, the synset tree goes up to _root node_ { _entity_ }.\n\nAs said, such hyponym relation is transitive in nature, e.g. if an _armchair_\nis a kind of _chair_ and if a _chair_ is a kind of _furniture_ , then an\n_armchair_ is a kind of _furniture_. WordNet distinguishes between types\n(general nouns) and instances (specific people, countries, and geographic\nentities), e.g. _book_ is a type of _publication_ , _Abraham Lincoln_ is an\ninstance of _President._ Instances are always denoted as _leaves (terminal\nnodes_ ) in synset tree hierarchies.\n\nMajor lexical relations include:\n\n  * _Synonymy_ : words with similar meaning\n\n  * _Polysemy_ : words with more than single sense\n\n  * _Hyponymy_ _/_ _Hypernymy_ : _IS-A_ relation between words\n\n  * _Meronymy/Holonymy_ : _part-whole_ relation between words\n\n  * _Antonymy_ : opposite meanings between words\n\n  * _Troponymy_ : applicable for verbs, e.g. whisper is toponym of speak.\n\nFigure 6.5 shows the major lexical relations capture in WordNet with examples.\n\n![](../images/533412_1_En_6_Chapter/533412_1_En_6_Fig5_HTML.png)\n\nFig.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc6b2c27-0fde-4321-8ceb-45f8d5221ab8": {"__data__": {"id_": "dc6b2c27-0fde-4321-8ceb-45f8d5221ab8", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbc92233-4a9c-4d7d-813f-dd94a2fe8f04", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "7e7d8770ac7095e603fbcb28d71257dab1ccca292eb41e3b7cd08f00371fcb98", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8dd7916e-c09d-4190-a57e-8dd1f2eeef37", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "00b93056e3bb6b1c7863a59bec6534be6b5a4f3c34db498ccae5571560a02fe3", "class_name": "RelatedNodeInfo"}}, "hash": "8c1774665141439f5185fdbd07560adb4615ca117509306b1a10918c9cb6a725", "text": "Major lexical relations include:\n\n  * _Synonymy_ : words with similar meaning\n\n  * _Polysemy_ : words with more than single sense\n\n  * _Hyponymy_ _/_ _Hypernymy_ : _IS-A_ relation between words\n\n  * _Meronymy/Holonymy_ : _part-whole_ relation between words\n\n  * _Antonymy_ : opposite meanings between words\n\n  * _Troponymy_ : applicable for verbs, e.g. whisper is toponym of speak.\n\nFigure 6.5 shows the major lexical relations capture in WordNet with examples.\n\n![](../images/533412_1_En_6_Chapter/533412_1_En_6_Fig5_HTML.png)\n\nFig. 6.5\n\nMajor lexical relations captured in WordNet with examples\n\n### 6.5.5 Applications of WordNet and Thesauri?\n\n_WordNet_ and _Thesauri_ applications include information extraction,\ninformation retrieval, question answering, medical informatics, and machine\ntranslation. WordNet has another common usage to determine words similarity\nwith algorithms proposed, including to measure the distance(s) among words in\nWordNet synset graphs (trees), e.g. counting the number of edges among\nsynsets. Intuitive words or synonyms are close to meaning. Many WordNet-based\nworld similarity algorithms are implemented in a Perl package called\nWordNet::Similarity and a Python package using NLTK and SpaCy will be explored\nin the second part of NLP workshops.\n\n## 6.6 Other Online Thesauri: MeSH\n\n### 6.6.1 What Is MeSH?\n\n_Medical Subject_ _Thesaurus_ _, aka._ _MeSH_ (MeSH 2022) is a hierarchically\norganized vocabulary for indexing, cataloging, and searching biomedical and\nhealth-related information created by U.S. National Library of Medicine (NLM).\nMeSH contains subject headings that appear in MEDLINE/PubMed, NLM catalog, and\nother NLM databases. It consists of 177,000 entries, 26,142 biomedical titles\nand continues to soar as the literature expands. 2020 edition contains more\nthan 25,000 subject headings, 4,400 approximately more since its launch in\n1960. These headings are organized into an 11-level hierarchy with 83\nsubheadings. MeSH can be freely used via US NLM\u2019s online MeSH browser (MeSH\n2022). MeSH headings are organized in a knowledge tree with 16 major branches:\n\nA. Anatomy, B. Organisms, C. Diseases, D. Chemicals and Drugs, E. Analytical\nDiagnostics and Therapeutic Techniques and Equipment, F. Psychiatry and\nPsychology, G. Phenomena and Processes, H. Disciplines and Occupations, I.\nAnthropology, Education, Sociology and Social Phenomena, J. Technology,\nIndustry, Agriculture, K. Humanities, L. Information Science, M. Named Groups,\nN. Health Care, V. Publication Characteristics and Z. Geographicals.\n\nMeSH glossary contains several entry terms intended to be synonyms for\ncanonical title terms in addition to hierarchical set of canonical terms.\n\n### 6.6.2 Uses of the MeSH Ontology\n\nMeSH ontology usage includes:\n\n  1. 1.\n\nsynonyms as entry terms, e.g. _sucrose and saccharose_\n\n  2. 2.\n\nhypernyms from hierarchy, e.g. _sucrose is a glycosyl glycoside_\n\n  3. 3.\n\nindex in MEDLINE/PubMED databases such as bibliographic database NLM contains\n20 million journal articles with 10\u201320 MeSH terms manually assigned to each\narticle.\n\n## 6.7 Word Similarity and Thesaurus Methods\n\n## 6.8 Introduction\n\nA _synonym_ can be considered as a binary relationship between two synonyms or\nnon-synonyms. _Similarity_ or distance is a looser measure when two words\nshare more semantic features with each other. _Similarity_ is a relationship\nbetween sensations, e.g. _bank_ is usually not similar to _slope_ , but in\nsome cases, they may have the same meaning, e.g. _bank_ _1_ is similar to\n_fund_ _3_ , and _bank_ _2_ is similar to _slope_ _5_ , in which the\nsimilarity can be calculated by word sense relationship in a sentence.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8dd7916e-c09d-4190-a57e-8dd1f2eeef37": {"__data__": {"id_": "8dd7916e-c09d-4190-a57e-8dd1f2eeef37", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc6b2c27-0fde-4321-8ceb-45f8d5221ab8", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8c1774665141439f5185fdbd07560adb4615ca117509306b1a10918c9cb6a725", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b9692f0-02f5-4fd7-8ec0-6321c3b7f203", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "15a7b67ec6978451a653f3b5ecdfd54e63b8162827abe51841e221a7fe3d196c", "class_name": "RelatedNodeInfo"}}, "hash": "00b93056e3bb6b1c7863a59bec6534be6b5a4f3c34db498ccae5571560a02fe3", "text": "3.\n\nindex in MEDLINE/PubMED databases such as bibliographic database NLM contains\n20 million journal articles with 10\u201320 MeSH terms manually assigned to each\narticle.\n\n## 6.7 Word Similarity and Thesaurus Methods\n\n## 6.8 Introduction\n\nA _synonym_ can be considered as a binary relationship between two synonyms or\nnon-synonyms. _Similarity_ or distance is a looser measure when two words\nshare more semantic features with each other. _Similarity_ is a relationship\nbetween sensations, e.g. _bank_ is usually not similar to _slope_ , but in\nsome cases, they may have the same meaning, e.g. _bank_ _1_ is similar to\n_fund_ _3_ , and _bank_ _2_ is similar to _slope_ _5_ , in which the\nsimilarity can be calculated by word sense relationship in a sentence.\n\nWord similarity is important because a good measure can be used in information\nretrieval, question answering, machine translation, natural language\ngeneration, language modeling, automatic paper scoring, and even plagiarism\ndetection.\n\nThe difference between _word similarity_ and _word relation_ is that similar\nwords are almost synonyms, e.g. _car, bicycle_ are similar in concept but not\na kind of _Is-A_ relation, whereas related words can be related in any way,\ne.g. _car, gasoline_ are highly related but not similar in semantic meaning.\n\nThere are two types of similarity algorithms: (1) _Thesaurus_ _-based\nalgorithms_ and (2) _Distributional algorithms_. _Thesaurus-based algorithms_\nare designed to examine adjacent words in a hypernym hierarchy with similar\nannotations or definitions. _Distribution algorithms_ are designed to examine\nwords with similar _distributional contexts_.\n\n### 6.8.1 Path-based Similarity\n\n_Path-based similarity_ aims to examine two concepts in general. Two concepts\nare similar if they are in the vicinity of thesaurus hierarchy. Synset tree\n(graph), the distance (path) between two synset nodes can provide a good\nindication of semantic similarity between two concepts. This evaluation method\nis known as _path-based similarity_ measurement. Figure 6.6 depicts an example\nof path-based similarity for concept _car_. Note that all concepts have path\nvalue 1 point to themselves.\n\n![](../images/533412_1_En_6_Chapter/533412_1_En_6_Fig6_HTML.png)\n\nFig. 6.6\n\nPath-based similarity for concept related to _car_\n\nFor example:\n\n_pathlen(car, car) = 1_\n\n_pathlen(car, automotive) = 2_\n\n_pathlen(car, truck) = 3_\n\n_pathlen(car, minibike) = 5_\n\n_pathlen(car, transport) = 5_\n\n_pathlen(car, artifact) = 7_\n\n_pathlen(care, tableware) = 10_\n\n_pathlen(car, fork) = 12_\n\nIn general:\n\n![$$ {\\\\displaystyle\n\\\\begin{array}{l}\\\\mathrm{pathlen}\\\\left\\({c}_1,{c}_2\\\\right\\)=1+\\\\mathrm{nos}.\\\\mathrm{of}\\\\\n\\\\mathrm{edges}\\\\ \\\\mathrm{in}\\\\ \\\\mathrm{the}\\\\ \\\\mathrm{shortest}\\\\\n\\\\mathrm{path}\\\\;\\\\mathrm{at}\\\\;\\\\mathrm{hypernym}\\\\ \\\\mathrm{graph}\\\\ \\\\\\\\\n{}\\\\mathrm{between}\\\\ \\\\mathrm{sense}\\\\ \\\\mathrm{nodes}\\\\;c1\\\\  and\\\\\nc2.\\\\end{array}}\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ1.png)\n\n(6.1)\n\nwhere pathlen( _c_ _1_ , _c_ _2_ ) ranges from 0 to 1.\n\nThe path-based similarity simpath( _c_ _1_ , _c_ _2_ ) of two nodes (concepts)\nis given by\n\n![$$\n\\\\mathrm{simpath}\\\\left\\({c}_1,{c}_2\\\\right\\)=\\\\frac{1}{\\\\mathrm{pathlen}\\\\left\\({c}_1,{c}_2\\\\right\\)}\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ2.png)\n\n(6.2)\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b9692f0-02f5-4fd7-8ec0-6321c3b7f203": {"__data__": {"id_": "5b9692f0-02f5-4fd7-8ec0-6321c3b7f203", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8dd7916e-c09d-4190-a57e-8dd1f2eeef37", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "00b93056e3bb6b1c7863a59bec6534be6b5a4f3c34db498ccae5571560a02fe3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "828b251c-7fc7-4b90-9ccd-d9e0900f2061", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "c84f994b1cf6a720d04b74a532817ed5bf868ec16c3a67763daf1f3b9b1e49dd", "class_name": "RelatedNodeInfo"}}, "hash": "15a7b67ec6978451a653f3b5ecdfd54e63b8162827abe51841e221a7fe3d196c", "text": "The path-based similarity simpath( _c_ _1_ , _c_ _2_ ) of two nodes (concepts)\nis given by\n\n![$$\n\\\\mathrm{simpath}\\\\left\\({c}_1,{c}_2\\\\right\\)=\\\\frac{1}{\\\\mathrm{pathlen}\\\\left\\({c}_1,{c}_2\\\\right\\)}\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ2.png)\n\n(6.2)\n\n![$$\n\\\\mathrm{wordsim}\\\\;\\\\left\\({w}_1,{w}_2\\\\right\\)=\\\\max\\\\;\\\\left\\(\\\\mathrm{simpath}\\\\left\\({c}_1,{c}_2\\\\right\\)\\\\right\\)\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equa.png)\n\n![$$ \\\\forall {c}_1\\\\in \\\\mathrm{senses}\\\\ \\\\left\\({w}_1\\\\right\\),{c}_2\\\\in\n\\\\mathrm{senses}\\\\ \\\\left\\({w}_2\\\\right\\)\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ3.png)\n\n(6.3)\n\nUsing _car_ concept as example:\n\n_simpath(car, car) = 1/1 = 1.0_\n\n_simpath(car, automotive) = 1/2 = 0.50_\n\n_simpath(car, truck) = 1/3 = 0.33_\n\n_simpath(car, minibike) = 1/5 = 0.20_\n\n_simpath(car, transport) = 1/5 = 0.20_\n\n_simpath(car, artifact) = 1/7 = 0.14_\n\n_simpath(car, tableware) = 1/10 = 0.10_\n\n_pathlen(car, fork) = 1/12 = 0.08_\n\n### 6.8.2 Problems with Path-based Similarity\n\nLet us assume every link denotes a uniform distance. It seems that _car_ to\n_minibike_ is closer than _car_ to _transport_ because higher synsets are more\n_abstract_ in synset tree, e.g. _object_ is abstract than _artifact_ ,\n_transport_ is abstract than _vehicle,_ etc.\n\nDespite _simpath(car, minibike)_ and _simpath(car, transport)_ have identical\nvalues, their _semantic relationship_ between each other is different,\nnaturally synsets in other branch of the synset tree are less related in\nconcept, e.g. _car_ vs _tableware_ or even _fork_.\n\nThus, it is suggested to have a metric that can represent the cost of each\nedge independently, so that words associated with abstract nodes should have\nless similarity scores.\n\n### 6.8.3 Information Content Similarity\n\nThe _Information Content Similarity_ metric uses information content to assess\nsemantic similarity in taxonomy which was first proposed by Prof. Philip\nResnik, whose distinguished work _Using information content to evaluate\nsematic similarity in taxonomy_ published in 1995 (Resnik 1995).\n\nLet us define _P(c)_ as the probability of a random word in corpus for an\ninstance of concept _c._ There is a unique random variable ranging from words\nformally associated with each concept in the hierarchy. For a given concept,\neach observed noun is either a member of the concept with probability _P(c)_\nor is not a member of the concept with probability _1-P(c)_. All words are\nmembers of the root node entity, i.e. _P(root)=1_ , lower nodes in the\nhierarchy have lower probability.\n\nInformation content similarity is generally determined by counting against the\ncorpus. When applying to _car_ concept example, each instance of _car_ counts\ntoward frequency of _automotive, wheeled vehicle, vehicle,_ etc. So given\n_word(c)_ is the collection of all words that are children of node _c_ , the\nprobability of information content similarity _P(c)_ in a corpus is given by\nEq. (6.4):\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "828b251c-7fc7-4b90-9ccd-d9e0900f2061": {"__data__": {"id_": "828b251c-7fc7-4b90-9ccd-d9e0900f2061", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b9692f0-02f5-4fd7-8ec0-6321c3b7f203", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "15a7b67ec6978451a653f3b5ecdfd54e63b8162827abe51841e221a7fe3d196c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a068b35-98d5-43e2-9717-a01bfd64426c", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3a054464d24b33346ff22c4506d997deee9d210277165d40862b8de8647e2d71", "class_name": "RelatedNodeInfo"}}, "hash": "c84f994b1cf6a720d04b74a532817ed5bf868ec16c3a67763daf1f3b9b1e49dd", "text": "For a given concept,\neach observed noun is either a member of the concept with probability _P(c)_\nor is not a member of the concept with probability _1-P(c)_. All words are\nmembers of the root node entity, i.e. _P(root)=1_ , lower nodes in the\nhierarchy have lower probability.\n\nInformation content similarity is generally determined by counting against the\ncorpus. When applying to _car_ concept example, each instance of _car_ counts\ntoward frequency of _automotive, wheeled vehicle, vehicle,_ etc. So given\n_word(c)_ is the collection of all words that are children of node _c_ , the\nprobability of information content similarity _P(c)_ in a corpus is given by\nEq. (6.4):\n\n![$$ P\\(c\\)=\\\\frac{\\\\sum_{w\\\\in \\\\mathrm{words}\\(c\\)}\\\\\n\\\\mathrm{count}\\(w\\)}{N}\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ4.png)\n\n(6.4)\n\nThus, (1) words(transport) _=_ {transport, wheeled vehicle, automotive, car,\ntruck, motorcycle, minibike} and (2) words(automotive) _=_ {car, truck}.\n\nA synset tree of car associated with _P_ ( _c_ ) up to transport level in each\ncorpus is shown in Fig. 6.7.\n\n![](../images/533412_1_En_6_Chapter/533412_1_En_6_Fig7_HTML.png)\n\nFig. 6.7\n\nSynset tree of \u201c _car_ \u201d with associated _P(c)_ (up to _transport_ level in\nthe corpus)\n\nInformation content (IC) is given by\n\n![$$ IC\\(c\\)=-\\\\log\\\\;P\\(c\\)\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ5.png)\n\n(6.5)\n\nwhere the lowest common subsume (LCS) is given by\n\n![$$ \\\\mathrm{LCS}\\\\left\\({c}_1,{c}_2\\\\right\\)=\\\\mathrm{the}\\\\\n\\\\mathrm{lowest}\\\\ \\\\mathrm{common}\\\\ \\\\mathrm{subsumer}\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ6.png)\n\n(6.6)\n\ni.e. the lower node in hierarchy that subsumes (is a hypernym of) both _c_ 1\nand _c_ 2 is ready to apply _information content_ as a similarity metric.\n\n### 6.8.4 The Resnik Method\n\n_Resnik Method_ (Resnik 1995, 1999) refers to the similarity between two words\nthat are in the vicinity of their common information. It is defined to measure\nthe most informative common information contents, i.e. (lowest) subsumer\n(MIS/LCS) of two nodes, given by\n\n![$$\n{\\\\mathrm{sim}}_{\\\\mathrm{resnik}}\\\\;\\\\left\\({c}_1,{c}_2\\\\right\\)=-\\\\log\\\\;P\\\\left\\(\\\\mathrm{LCS}\\\\left\\({C}_1,{C}_2\\\\right\\)\\\\right\\)\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ7.png)\n\n(6.7)\n\n### 6.8.5 The Dekang Lin Method\n\n_Dekang Lin method_ was proposed by Prof. Dekang Lin with his work\n_Information-Theoretic Definition of Similarity_ at ICML in 1998 (Lin 1998).\nIt determines the similarity between concepts A and B is not only what they\nhave in common but also the differences between them. It concerns with (1)\n_commonality_ and (2) _difference_. _Commonality_ , denoted by _IC(_ common\n_(A,B))_ means A and B are more in common that has more _similarity_.\n_Difference_ , denoted by _IC(_ description _(A,B) \u2212 IC(_ common _(A,B))_\nmeans more differences between A and B that has less _similarity_.\n\n_Similarity_ theorem is similarity between A and B measured by the ratio\nbetween amount of information required to state commonality of A and B, the\ninformation required to describe what _A_ and _B_ are given by\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a068b35-98d5-43e2-9717-a01bfd64426c": {"__data__": {"id_": "1a068b35-98d5-43e2-9717-a01bfd64426c", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "828b251c-7fc7-4b90-9ccd-d9e0900f2061", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "c84f994b1cf6a720d04b74a532817ed5bf868ec16c3a67763daf1f3b9b1e49dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "965acd25-5015-4688-a3e9-d37f06a5a82a", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "6e6b98a8400eeff1e6e1d17dd2da24d1897aaa44ce0892a118025a9b4c618832", "class_name": "RelatedNodeInfo"}}, "hash": "3a054464d24b33346ff22c4506d997deee9d210277165d40862b8de8647e2d71", "text": "Dekang Lin with his work\n_Information-Theoretic Definition of Similarity_ at ICML in 1998 (Lin 1998).\nIt determines the similarity between concepts A and B is not only what they\nhave in common but also the differences between them. It concerns with (1)\n_commonality_ and (2) _difference_. _Commonality_ , denoted by _IC(_ common\n_(A,B))_ means A and B are more in common that has more _similarity_.\n_Difference_ , denoted by _IC(_ description _(A,B) \u2212 IC(_ common _(A,B))_\nmeans more differences between A and B that has less _similarity_.\n\n_Similarity_ theorem is similarity between A and B measured by the ratio\nbetween amount of information required to state commonality of A and B, the\ninformation required to describe what _A_ and _B_ are given by\n\n![$$ \\\\mathrm{simLin}\\\\left\\(A,B\\\\right\\)\\\\hbox{--}\n\\\\log\\\\;P\\\\left\\(\\\\mathrm{common}\\\\left\\(A,B\\\\right\\)\\\\right\\)/\\\\log\\\\;P\\\\left\\(\\\\mathrm{description}\\\\;\\\\left\\(A,B\\\\right\\)\\\\right\\)\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ8.png)\n\n(6.8)\n\nHe further modified _Resnik method_ demonstrating that information in common\nis twice the LCS information content given by:\n\n![$$ \\\\mathrm{SimLin}\\\\left\\({c}_1,{c}_2\\\\right\\)=\\\\frac{2x\\\\log\nP\\\\left\\(\\\\mathrm{LCS}\\\\left\\({c}_1,{c}_2\\\\right\\)\\\\right\\)}{\\\\log\nP\\\\left\\({c}_1\\\\right\\)+\\\\log P\\\\left\\({c}_2\\\\right\\)}\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ9.png)\n\n(6.9)\n\nUsing _car_ concept as example:\n\n![$$ {\\\\displaystyle\n\\\\begin{array}{l}\\\\mathrm{SimLin}\\\\left\\(\\\\mathrm{car},\\\\mathrm{minibike}\\\\right\\)=\\\\frac{2x\\\\log\nP\\\\left\\(w\\\\mathrm{heeled}\\\\ \\\\mathrm{vehicle}\\\\right\\)}{\\\\log\nP\\\\left\\(\\\\mathrm{car}\\\\right\\)+\\\\log\nP\\\\left\\(\\\\mathrm{minibike}\\\\right\\)}\\\\\\\\ {}\\\\kern5.52em =\\\\frac{2x\\\\log\nP\\(0.102\\)}{\\\\log P\\(0.00872\\)+\\\\log P\\(0.000537\\)}=0.372\\\\end{array}}\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equb.png)\n\n![$$\n\\\\mathrm{SimLin}\\\\left\\(\\\\mathrm{car},\\\\mathrm{truck}\\\\right\\)=\\\\frac{2x\\\\log\nP\\\\left\\(\\\\mathrm{automotive}\\\\right\\)}{\\\\log\nP\\\\left\\(\\\\mathrm{car}\\\\right\\)+\\\\log P\\\\left\\(\\\\mathrm{truck}\\\\right\\)}\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equc.png)\n\n![$$ =\\\\frac{2x\\\\log P\\(0.0172\\)}{\\\\log P\\(0.00872\\)+\\\\log P\\(0.00117\\)}=0.707\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equd.png)\n\nThis calculation showed that _car_ is related to _truck_ than _minibike_ at\nhierarchy tree in Fig. 6.10.\n\n### 6.8.6 The (Extended) Lesk Algorithm\n\n_The (extended)_ _Lesk Algorithm_ uses a thesaurus-based to measure glosses\nwhich contain similar words for concept similarity. For instance, _drawing\npaper_ is a type of paper for _drafting, including the art of transferring\ndesigns from specially prepared paper to a glass, wood, or even metal\nsurface_.\n\nFor all n-word phrases which appear in two glosses:\n\n  1. 1.\n\nAdd a score of _n_ 2\n\n  2. 2.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "965acd25-5015-4688-a3e9-d37f06a5a82a": {"__data__": {"id_": "965acd25-5015-4688-a3e9-d37f06a5a82a", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a068b35-98d5-43e2-9717-a01bfd64426c", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3a054464d24b33346ff22c4506d997deee9d210277165d40862b8de8647e2d71", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77ad28f1-2c3c-4dd4-b3ac-b673719a7a46", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "639dbc653f836e477093cfed16dee10bdfe5447628e311a6f83b915a3f0d4a3f", "class_name": "RelatedNodeInfo"}}, "hash": "6e6b98a8400eeff1e6e1d17dd2da24d1897aaa44ce0892a118025a9b4c618832", "text": "6.10.\n\n### 6.8.6 The (Extended) Lesk Algorithm\n\n_The (extended)_ _Lesk Algorithm_ uses a thesaurus-based to measure glosses\nwhich contain similar words for concept similarity. For instance, _drawing\npaper_ is a type of paper for _drafting, including the art of transferring\ndesigns from specially prepared paper to a glass, wood, or even metal\nsurface_.\n\nFor all n-word phrases which appear in two glosses:\n\n  1. 1.\n\nAdd a score of _n_ 2\n\n  2. 2.\n\n_Paper_ and specially prepared for 1 + 22 = 5\n\n  3. 3.\n\nEvaluate the overlaps for other relations which define glosses of hypernyms\nand hyponyms\n\nThe extended Lesk for similarity (sim _e_ Lesk) is given by\n\n![$$ {\\\\mathrm{sim}}_{e\\\\mathrm{Lesk}}\\\\left\\({c}_1,{c}_2\\\\right\\)=\\\\sum\n\\\\limits_{r,q\\\\in RELS} overlap\\\\left\\(\ngloss\\\\left\\(r\\\\left\\({c}_1\\\\right\\)\\\\right\\),\\\\kern0.5em\ngloss\\\\left\\(q\\\\left\\({c}_2\\\\right\\)\\\\right\\)\\\\right\\)\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ10.png)\n\n(6.10)\n\n## 6.9 Distributed Similarity\n\n### 6.9.1 Distributional Models of Meaning\n\n_Distributional models_ of meaning can be considered as a kind of _vector\nspace models_ of meaning. Prof. Zellig Harris (1909\u20131992) claimed that\n_oculist and eye-doctor\u2026 occur in almost the same environments\u2026_ (Harris 1954)\nwhich means A & B is _synonym_ if _A and B have almost identical environment_\ns. Sir John R Firth (1890\u20131960) also stated that _you shall know a word by the\ncompany it keeps!_ (Firth 1957):\n\n  * [6.41] _A bottle of Baileys is on the table_.\n\n  * [6.42] _Many coffee drinkers like Baileys_.\n\n  * [6.43] _Baileys will make you drunk_.\n\n  * [6.44] _We make Baileys out of Irish whiskey and cream_.\n\nHumans can guess _Baileys_ from context words is an _alcoholic coffee beverage\nflavored with cream and Irish whiskey_. This means that two words are\n_semantically similar_ if they are similar in context of the word being used\nfor algorithm interpretation.\n\n### 6.9.2 Word Vectors\n\n_Word vector_ is a vector of _weights_. In a simple _1-of-N_ encoding every\nelement in the vector is associated with a word in vocabulary. Word encoding\nis vector where the corresponding element is set to one, and other elements\nare zero.\n\nGiven a target word _w_ , assume there is a binary feature _f_ _i_ for each\n_N_ words in lexicon _v_ _i_ , the word vector is given by:\n\n![$$ W=\\\\left\\({f}_1,{f}_2,{f}_3,\\\\dots {f}_N\\\\right\\)\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ11.png)\n\n(6.11)\n\nApply to above _Baileys\u2019_ example, if _w = Baileys_ , _f_ _1_ _= coffee_ , _f_\n_2_ _= whiskey_ , _f_ _3_ _= beer_ , _f_ _4_ _= cream_ ,\u2026\n\n![$$ w=\\\\left\\(1,1,0,1,\\\\dots \\\\right\\)\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ12.png)\n\n(6.12)\n\n### 6.9.3 Term-Document Matrix\n\nText data is denoted as a matrix in this method. The rows represent sentences\nfrom the data to be analyzed and columns represent words of the matrix. Each\ncell is the counting of term _t_ in a document _d:tf_ _t,d_ , and each\ndocument is a counter vector in **\u2115\u036e** .", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77ad28f1-2c3c-4dd4-b3ac-b673719a7a46": {"__data__": {"id_": "77ad28f1-2c3c-4dd4-b3ac-b673719a7a46", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "965acd25-5015-4688-a3e9-d37f06a5a82a", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "6e6b98a8400eeff1e6e1d17dd2da24d1897aaa44ce0892a118025a9b4c618832", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "641a4acd-647f-4461-9c3b-665b15f08f74", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "b8b5d72187e4aad4ecd7a7f7435789fcbecd5c1c9098d8979e1df5e05c0308f8", "class_name": "RelatedNodeInfo"}}, "hash": "639dbc653f836e477093cfed16dee10bdfe5447628e311a6f83b915a3f0d4a3f", "text": "[$$ w=\\\\left\\(1,1,0,1,\\\\dots \\\\right\\)\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ12.png)\n\n(6.12)\n\n### 6.9.3 Term-Document Matrix\n\nText data is denoted as a matrix in this method. The rows represent sentences\nfrom the data to be analyzed and columns represent words of the matrix. Each\ncell is the counting of term _t_ in a document _d:tf_ _t,d_ , and each\ndocument is a counter vector in **\u2115\u036e** .\n\nFigure 6.8 shows a _term-document matrix_ to investigate the relationships of\nfour important words: _battle_ , _soldier_ , _fool,_ and _trick_ from six\nfamous literatures: _As you like it, Henry V, Julius Caesar,_ and _Twelfth\nNight_ extracted from _The Complete Works of Shakespeare_ by William\nShakespeare (1564\u20131616) (Shakespeare 2021), _The Adventures of Sherlock\nHolmes_ (Doyle 2019), and _Moby Dick_ by Herman Melville (1819\u20131891) (Melville\n2012).\n\n![](../images/533412_1_En_6_Chapter/533412_1_En_6_Fig8_HTML.png)\n\nFig. 6.8\n\nTerm-document matrix of 6 famous English literature\n\nIt showed that:\n\n  1. 1.\n\ntwo documents _Julius Caesar_ and _Henry V_ are similar if their term-document\nvectors are similar as in Fig. 6.9.\n\n![](../images/533412_1_En_6_Chapter/533412_1_En_6_Fig9_HTML.png)\n\nFig. 6.9\n\nTerm-document matrix comparison by document vectors\n\n  1. 2.\n\neach word is a count vector in **\u2115** D as a row, Fig. 6.10 shows row vector\nfor the word _fool_ across these six documents.\n\n![](../images/533412_1_En_6_Chapter/533412_1_En_6_Fig10_HTML.png)\n\nFig. 6.10\n\nIllustration of count vector for six documents domain\n\n  1. 3.\n\ntwo words are semantically similar if their word vectors are similar, e.g.\n_fool_ and _trick_. It makes sense because they are related to each other\n_semantically_ as compared with _battle_ and _soldier_ , as shown in Fig.\n6.11.\n\n![](../images/533412_1_En_6_Chapter/533412_1_En_6_Fig11_HTML.png)\n\nFig. 6.11\n\nSample of two similar word by vector comparison across six documents\n\nA term-context matrix can be formed using smaller context, e.g. _a set of 10\nsuccessive words from a paragraph or search engine_. A word is now defined by\na vector over the number of context words, which can be an entire document,\nliterature, or a list of words in a search engine, etc.\n\nThere is an argument as to whether raw counts can be used. _tf-idf (term\nfrequency and inverse document frequency)_ are commonly used in place of raw\nterm counts for _term-document matrix_ whereas _Positive Point-wise Mutual\nInformation_ _(_ _PPMI_ _)_ method for _term-context matrix,_ respectively.\n\n### 6.9.4 Point-wise Mutual Information (PMI)\n\nPoint-wise Mutual Information (PMI) is to evaluate whether events x and y co-\noccur more than if they are independent given by\n\n![$$\nPMI\\\\left\\(X,Y\\\\right\\)={\\\\log}_2\\\\frac{P\\\\left\\(x,y\\\\right\\)}{P\\(x\\)\\(y\\)}\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ13.png)\n\n(6.13)\n\nFor word similarity measurement application, Church and Hanks (1990) proposed\nPMI between two words given by\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "641a4acd-647f-4461-9c3b-665b15f08f74": {"__data__": {"id_": "641a4acd-647f-4461-9c3b-665b15f08f74", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77ad28f1-2c3c-4dd4-b3ac-b673719a7a46", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "639dbc653f836e477093cfed16dee10bdfe5447628e311a6f83b915a3f0d4a3f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d2ddf35-1b06-46dc-8939-020e2683c7ea", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "ecc75bdf663b210c4e3e051053c3c1939dcdb7c1df98282a28ed0a371dac5a68", "class_name": "RelatedNodeInfo"}}, "hash": "b8b5d72187e4aad4ecd7a7f7435789fcbecd5c1c9098d8979e1df5e05c0308f8", "text": "### 6.9.4 Point-wise Mutual Information (PMI)\n\nPoint-wise Mutual Information (PMI) is to evaluate whether events x and y co-\noccur more than if they are independent given by\n\n![$$\nPMI\\\\left\\(X,Y\\\\right\\)={\\\\log}_2\\\\frac{P\\\\left\\(x,y\\\\right\\)}{P\\(x\\)\\(y\\)}\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ13.png)\n\n(6.13)\n\nFor word similarity measurement application, Church and Hanks (1990) proposed\nPMI between two words given by\n\n![$$\nPMI\\\\left\\({\\\\mathrm{word}}_1,{\\\\mathrm{word}}_2\\\\right\\)={\\\\log}_2\\\\frac{P\\\\left\\({\\\\mathrm{word}}_1,{\\\\mathrm{word}}_2\\\\right\\)}{P\\\\left\\({\\\\mathrm{word}}_1\\\\right\\)P\\\\left\\({\\\\mathrm{word}}_2\\\\right\\)}\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ14.png)\n\n(6.14)\n\nNiwa and Nitta (1994) proposed Positive PMI (or PPMI) by replacing all PMI\nvalues less than zero into zero values, which is now commonly used in PMI\ncalculations for document similarity comparison.\n\n### 6.9.5 Example of Computing PPMI on a Term-Context Matrix\n\nGiven matrix _F_ with _C_ columns (contexts), _W_ rows (words) and _f_ _ij_ is\nthe number of times _w_ i occurs in context cj, Positive PMI(PPMI) between\n_word_ _1_ and _word_ _2_ is given by:\n\n![$$\n\\\\mathrm{PPMI}\\\\left\\({\\\\mathrm{word}}_1,{\\\\mathrm{word}}_2\\\\right\\)=\\\\max\n\\\\left\\({\\\\log}_2\\\\frac{P\\\\left\\({\\\\mathrm{word}}_1,{\\\\mathrm{word}}_2\\\\right\\)}{P\\\\left\\({\\\\mathrm{word}}_1\\\\right\\)P\\\\left\\({\\\\mathrm{word}}_2\\\\right\\)},0\\\\right\\)\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ15.png)\n\n(6.15)\n\nwhere:\n\n![$$ \\\\left\\\\{\\\\begin{array}{c} PMI\\\\left\\(W,C\\\\right\\),\\\\kern0.5em if\\\\\nPMI\\\\left\\(W,C\\\\right\\)&gt;0\\\\\\\\ {}\\\\kern4.75em 0,\\\\kern0.75em if\\\\\nPMI\\\\left\\(W,C\\\\right\\)&lt;0\\\\end{array}\\\\right.\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ16.png)\n\n(6.16)\n\n![$$ p\\\\left\\(W,C\\\\right\\)=\\\\frac{f_{ij}}{\\\\sum_{i=1}^W\\\\kern0.24em\n{\\\\sum}_{j=1}^C\\\\;{f}_{ij}},p\\\\left\\({W}_i\\\\right\\)=\\\\frac{\\\\sum_{j=1}^C\\\\;{f}_{ij}}{N},\\\\kern0.62em\np\\\\left\\({C}_j\\\\right\\)=\\\\frac{\\\\sum_{i=1}^W\\\\;{f}_{ij}}{N}\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ17.png)\n\n(6.17)\n\nin which: _p(W, C)_ is the probability of considering _target word W_ and\n_context word C_ together. _p(W)_ and _p(C)_ is the probability of occurring\n_target word W_ and _context word C_ , if they are independent _f_ _ij_ is the\nnumber of times _W_ _i_ occurs in context _C_ _j_.\n\nLet us use the previous document term matrix of six English literatures as\nexample to calculate word and context of total counts and probabilities as\nshown in Figs. 6.12 and 6.13.\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d2ddf35-1b06-46dc-8939-020e2683c7ea": {"__data__": {"id_": "5d2ddf35-1b06-46dc-8939-020e2683c7ea", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "641a4acd-647f-4461-9c3b-665b15f08f74", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "b8b5d72187e4aad4ecd7a7f7435789fcbecd5c1c9098d8979e1df5e05c0308f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de394b31-815e-41b6-866b-4254de6f1fcb", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "08e105e548eb383382fb5cc222bc0402ec8603c66f6dcbad76c63dfa45b06d03", "class_name": "RelatedNodeInfo"}}, "hash": "ecc75bdf663b210c4e3e051053c3c1939dcdb7c1df98282a28ed0a371dac5a68", "text": "_p(W)_ and _p(C)_ is the probability of occurring\n_target word W_ and _context word C_ , if they are independent _f_ _ij_ is the\nnumber of times _W_ _i_ occurs in context _C_ _j_.\n\nLet us use the previous document term matrix of six English literatures as\nexample to calculate word and context of total counts and probabilities as\nshown in Figs. 6.12 and 6.13.\n\n![$$\nP\\\\left\\(W=\\\\mathrm{fool},C=\\\\mathrm{As}\\\\;\\\\mathrm{You}\\\\;\\\\mathrm{Like}\\\\;\\\\mathrm{It}\\\\right\\)=37/225=0.164\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Eque.png)\n\n![$$ P\\\\left\\(W=\\\\mathrm{fool}\\\\right\\)=111/225=0.493\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equf.png)\n\n![$$\nP\\\\left\\(C=\\\\mathrm{As}\\\\;\\\\mathrm{You}\\\\;\\\\mathrm{Like}\\\\;\\\\mathrm{It}\\\\right\\)=41/225=0.182\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equg.png)\n\n![](../images/533412_1_En_6_Chapter/533412_1_En_6_Fig12_HTML.png)\n\nFig. 6.12\n\nTerm-context matrix of six contexts with word and context total counts\n\n![](../images/533412_1_En_6_Chapter/533412_1_En_6_Fig13_HTML.png)\n\nFig. 6.13\n\nTerm-context matrix of six contexts with word and context total probabilities\n\nLet us calculate PMI score for the word _fool_ co-occurred with context from\nC1 = As You Like It based on the above information from Fig. 6.13.\n\nUsing  ![$$ \\\\mathrm{PMI}\\\\left\\(W,C\\\\right\\)=\\\\log\n\\\\frac{p\\\\left\\(W,C\\\\right\\)}{p\\(W\\)p\\(C\\)}\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_IEq1.png)\n\n![$$ PMI\\\\left\\(\\\\mathrm{fool},C1\\\\right\\)=\\\\log \\\\frac{0.164}{0.493\\\\ast\n0.182}=0.604\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ18.png)\n\n(6.18)\n\nSimilarly, the rest of PMI values for this term-context matrix are calculated\nas follows in Fig. 6.14:\n\n![](../images/533412_1_En_6_Chapter/533412_1_En_6_Fig14_HTML.png)\n\nFig. 6.14\n\nTerm-context matrix of six contexts with PPMI values\n\nNote that:  ![$$\n\\\\mathrm{PPMI}\\\\left\\(W,C\\\\right\\)=\\\\left\\\\{\\\\begin{array}{c}\\\\mathrm{PMI}\\\\left\\(W,C\\\\right\\),\\\\mathrm{if}\\\\kern0.24em\n\\\\mathrm{PMI}\\\\left\\(W,C\\\\right\\)&gt;0\\\\\\\\ {}0, if\\\\kern0.24em\n\\\\mathrm{PMI}\\\\left\\(W,C\\\\right\\)&lt;0\\\\end{array}\\\\right\\\\}\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_IEq2.png) from\n(6.16)\n\n### 6.9.6 Weighing PMI Techniques\n\nIt is noted that PMI is biased toward infrequent events from above matrix such\nas rare words have high PMI values. There are two possible methods to improve\nPMI values: (1) apply add-k smoothing, e.g. _add-1_ _Smoothing_ and (2) assign\nrare words with higher probabilities.\n\n### 6.9.7 K-Smoothing in PMI Computation\n\nSince PMI is usually biased with infrequent events, K-smoothing method can be\nsolution.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de394b31-815e-41b6-866b-4254de6f1fcb": {"__data__": {"id_": "de394b31-815e-41b6-866b-4254de6f1fcb", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d2ddf35-1b06-46dc-8939-020e2683c7ea", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "ecc75bdf663b210c4e3e051053c3c1939dcdb7c1df98282a28ed0a371dac5a68", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c4c6450-2aad-447c-aa48-7db5816d05fc", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "d167d187ab6e59c44fcbd04f56040ad6437f088d2fee1b1b107fda71af50f96a", "class_name": "RelatedNodeInfo"}}, "hash": "08e105e548eb383382fb5cc222bc0402ec8603c66f6dcbad76c63dfa45b06d03", "text": "There are two possible methods to improve\nPMI values: (1) apply add-k smoothing, e.g. _add-1_ _Smoothing_ and (2) assign\nrare words with higher probabilities.\n\n### 6.9.7 K-Smoothing in PMI Computation\n\nSince PMI is usually biased with infrequent events, K-smoothing method can be\nsolution. For example, apply _add 2_ _Smoothing_ (i.e., set _k_ = 2) in every\ncell of co-occurrence matrix as in Fig. 6.15 and see how it works.\n\n![](../images/533412_1_En_6_Chapter/533412_1_En_6_Fig15_HTML.png)\n\nFig. 6.15\n\nTerm-context matrix of six contexts with word and context total count with\nAdd-2 Smoothing\n\nThe corresponding probabilities matrix after Add-2 Smoothing is shown in Fig.\n6.16.\n\n![](../images/533412_1_En_6_Chapter/533412_1_En_6_Fig16_HTML.png)\n\nFig. 6.16\n\nTerm-context matrix of six contexts with word and context total prob. with\nAdd-2 Smoothing\n\nThe Term-context matrix with PPMI values after applying _Add-2_ _Smoothing_ is\nshown in Fig. 6.17.\n\n![](../images/533412_1_En_6_Chapter/533412_1_En_6_Fig17_HTML.png)\n\nFig. 6.17\n\nTerm-context matrix of six contexts with PPMI values with Add-2 smoothing\n\nIt may have certain improvement in PPMI values giving the rate context words\ntheoretically.\n\nHowever, there were not many improvements in this case.\n\nAnother method to achieve is by raising context probabilities to a certain\nfactor \u03b1, say 0.8.\n\n![$$ PPM{I}_{\\\\alpha}\\\\left\\(w,c\\\\right\\)=\\\\max \\\\left\\(\\\\log\n\\\\frac{P\\\\left\\(w,c\\\\right\\)}{P\\(w\\){P}_{\\\\alpha }\\(c\\)},0\\\\right\\)\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ19.png)\n\n(6.19)\n\nwhere:  ![$$ {P}_{\\\\alpha }\\(c\\)=\\\\frac{\\\\mathrm{count}{\\(c\\)}^{\\\\alpha\n}}{\\\\sum_c\\\\ \\\\mathrm{count}{\\(c\\)}^{\\\\alpha }}\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_IEq3.png)\n\nFor example: say _P(a) = 0.95_ and _P(b) = 0.05_ :\n\n![$$ {P}_{\\\\alpha\n}\\(a\\)=\\\\frac{0.95^{0.8}}{0.95^{0.8}+{0.05}^{0.8}}=0.913,{P}_{\\\\alpha\n}\\(b\\)=\\\\frac{0.05^{0.8}}{0.95^{0.8}+{0.05}^{0.8}}=0.083.\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ20.png)\n\n(6.20)\n\nResults using _\u03b1_ = 0.8 and 0.9 are shown in Figs. 6.18 and 6.19,\nrespectively.\n\n![](../images/533412_1_En_6_Chapter/533412_1_En_6_Fig18_HTML.png)\n\nFig. 6.18\n\nTerm-context matrix of six contexts with PPMI values with _\u03b1_ = 0.80\n\n![](../images/533412_1_En_6_Chapter/533412_1_En_6_Fig19_HTML.png)\n\nFig. 6.19\n\nTerm-context matrix of six contexts with PPMI values with _\u03b1_ = 0.90\n\n### 6.9.8 Context and Word Similarity Measurement\n\nWhen applying context and world similarity measurement against context and\nword vector, remember that cosine for computing similarity is given by\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c4c6450-2aad-447c-aa48-7db5816d05fc": {"__data__": {"id_": "8c4c6450-2aad-447c-aa48-7db5816d05fc", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de394b31-815e-41b6-866b-4254de6f1fcb", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "08e105e548eb383382fb5cc222bc0402ec8603c66f6dcbad76c63dfa45b06d03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f54b124a-b23b-48c8-bc72-74d818994e78", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "5fc7637f05fd82b846d173cf523774ccc8d13589135c0ece2d997dd1f04d27f6", "class_name": "RelatedNodeInfo"}}, "hash": "d167d187ab6e59c44fcbd04f56040ad6437f088d2fee1b1b107fda71af50f96a", "text": "6.18 and 6.19,\nrespectively.\n\n![](../images/533412_1_En_6_Chapter/533412_1_En_6_Fig18_HTML.png)\n\nFig. 6.18\n\nTerm-context matrix of six contexts with PPMI values with _\u03b1_ = 0.80\n\n![](../images/533412_1_En_6_Chapter/533412_1_En_6_Fig19_HTML.png)\n\nFig. 6.19\n\nTerm-context matrix of six contexts with PPMI values with _\u03b1_ = 0.90\n\n### 6.9.8 Context and Word Similarity Measurement\n\nWhen applying context and world similarity measurement against context and\nword vector, remember that cosine for computing similarity is given by\n\n![$$ \\\\cos\n\\\\left\\(\\\\overrightarrow{v},\\\\overrightarrow{w}\\\\right\\)=\\\\frac{\\\\overrightarrow{v}\\\\cdotp\n\\\\overrightarrow{w}}{\\\\left|\\\\overrightarrow{v}\\\\right|\\\\left|\\\\overrightarrow{w}\\\\right|}=\\\\frac{\\\\sum_{i=1}^N\\\\;{v}_i{w}_i}{\\\\sqrt{\\\\sum_{i=1}^N\\\\;{v_i}^2}\\\\sqrt{\\\\sum_{i=1}^N\\\\;{w_i}^2}}\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ21.png)\n\n(6.21)\n\nwhere _v_ _i_ is PPMI value for _word v_ in _context i_ ; _w_ _i_ is PPMI\nvalue for _word w_ in _context I_ , _cos(v,w)_ is the cosine similarity of _v_\nand _w_.\n\n_Context_ and _word similarity_ measurement of six literatures is shown in\nFig. 6.20.\n\n![](../images/533412_1_En_6_Chapter/533412_1_En_6_Fig20_HTML.png)\n\nFig. 6.20\n\nContext and Word Similarity from six sample literature\n\nFor context comparison, cosine similarity measurement is performed _between C1\nAs You Like It_ and other five literatures, in which _cosine (C1, C2)_ have\nthe highest _0.453_ as compared with others ranging from _0.044 (C3:Julius\nCaesar)_ and _0.157 (C6:Moby Dick)_. It showed that it makes sense as the\ncontext of _As You Like It_ has theme similarity with _Twelfth Night_ than\nother literatures.\n\nFor word comparison, comparison is performed at _W4: trick_ with three other\nwords across six literatures, in which cosine _W4:trick, W3:fool_ have the\nhighest similarities among other two words _W1:battle_ and _W2:Solder_ which\nin fact they are related in meanings and English usage.\n\nIt also showed other possible similarity measurements include _Jaccard, Dice_\n, and _JS_ s methods given by\n\n![$$\n{\\\\mathrm{sim}}_{\\\\mathrm{Jaccard}}\\\\left\\(\\\\overrightarrow{v},\\\\overrightarrow{w}\\\\right\\)=\\\\frac{\\\\sum_{i=1}^N\\\\kern0.24em\n\\\\min \\\\left\\({v}_i,{w}_i\\\\right\\)}{\\\\sum_{i=1}^N\\\\kern0.24em \\\\max\n\\\\left\\({v}_i,{w}_i\\\\right\\)}\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ22.png)\n\n(6.22)\n\n![$$\n{\\\\mathrm{sim}}_{\\\\mathrm{Dice}}\\\\left\\(\\\\overrightarrow{v},\\\\overrightarrow{w}\\\\right\\)=\\\\frac{2x{\\\\sum}_{i=1}^N\\\\min\n\\\\left\\({v}_i,\\\\kern0.62em\n{w}_i\\\\right\\)}{\\\\sum_{i=1}^N\\\\left\\({v}_i+{w}_i\\\\right\\)}\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ23.png)\n\n(6.23)\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f54b124a-b23b-48c8-bc72-74d818994e78": {"__data__": {"id_": "f54b124a-b23b-48c8-bc72-74d818994e78", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c4c6450-2aad-447c-aa48-7db5816d05fc", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "d167d187ab6e59c44fcbd04f56040ad6437f088d2fee1b1b107fda71af50f96a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d951d46-71ee-4450-81a8-249a3d1d379c", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "30020a89967eebbc80adcf9e12b17c2c91f9df6b5896ab995e57256da09bc0b2", "class_name": "RelatedNodeInfo"}}, "hash": "5fc7637f05fd82b846d173cf523774ccc8d13589135c0ece2d997dd1f04d27f6", "text": "[$$\n{\\\\mathrm{sim}}_{\\\\mathrm{Dice}}\\\\left\\(\\\\overrightarrow{v},\\\\overrightarrow{w}\\\\right\\)=\\\\frac{2x{\\\\sum}_{i=1}^N\\\\min\n\\\\left\\({v}_i,\\\\kern0.62em\n{w}_i\\\\right\\)}{\\\\sum_{i=1}^N\\\\left\\({v}_i+{w}_i\\\\right\\)}\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equ23.png)\n\n(6.23)\n\n![$$ {\\\\mathrm{sim}}_{\\\\mathrm{JS}}\\\\left\\(\\\\overrightarrow{v}\\\\parallel\n\\\\overrightarrow{w}\\\\right\\)=D\\\\left\\(\\\\overrightarrow{v\\\\;}\\\\;|\\\\kern0.24em\n\\\\frac{\\\\overrightarrow{v}+\\\\overrightarrow{w}}{2}\\\\right\\)+D\\\\left\\(\\\\overrightarrow{w\\\\;}\\\\;|\\\\kern0.24em\n\\\\frac{\\\\overrightarrow{v}+\\\\overrightarrow{w}}{2}\\\\right\\)\n$$](../images/533412_1_En_6_Chapter/533412_1_En_6_Chapter_TeX_Equh.png)\n\n### 6.9.9 Evaluating Similarity\n\nLike N-grams, similarity methods have (1) intrinsic and (2) extrinsic\nevaluation schemes. Intrinsic evaluation refers to the correlation between\nsimilarity scores of algorithms and human words. Extrinsic evaluation, also\ncalled task-based or end-to-end evaluation, refers to detect misspellings,\nword sense disambiguation (WSD), and use in grading essays or TOEFL multiple\nchoice vocabulary tests.\n\nExercises\n\n  1. 6.1\n\nWhat is semantic analysis? State and explain the importance of semantic\nanalysis in NLP. Give 2 live examples for illustration.\n\n  2. 6.2\n\nState and explain how humans are good in semantic analysis. Give 2 daily life\nexamples to support your answers.\n\n  3. 6.3\n\nWhat is the difference between lexical vs compositional semantic analysis?\nEach of them gives 2 examples to support your answers.\n\n  4. 6.4\n\nWhat is word sense in linguistic? State and explain any 5 basic types of\nlexical semantics and their word senses. For each of them, give 2 examples for\nillustration.\n\n  5. 6.5\n\nWhat is Zeugma is linguistic and why is important in NLP? Give 2 live example\nto illustrate how Zeugma Test for testing semantic correctness of\nsentences/utterances.\n\n  6. 6.6\n\nWhat are the major concerns and difficulties encountered in word sense\ndisambiguation (WSD). Give one example for each concern to support your\nanswers.\n\n  7. 6.7\n\nState and explain FOUR major methods to tackle word sense disambiguation\n(WSD). Which one(s) is(are) commonly used in NLP application nowadays to\ntackle WSD? Why?\n\n  8. 6.8\n\nWhat is Synsets in WordNet framework? Give 2 examples on how it works to\nsupport your answers.\n\n  9. 6.9\n\nWhat is Path-based Similarity in Semantic Analysis? Use _book_ as the basic\nsynset to construct a synset tree like Fig. 6.9 and calculate all the related\nPath-based Similarity between different concepts related to _book_.\n\n  10. 6.10\n\nBased on the synset tree created in question 6.9, calculate the similarity\nvalues by using: (1) Resnik Method and (2) Dekang Lin Method and compare them\nwith the ones calculated in 6.9.\n\n  11. 6.11\n\nWhat is distributed similarity? State and explain methods used for distributed\nsimilarity measurement.\n\n  12.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d951d46-71ee-4450-81a8-249a3d1d379c": {"__data__": {"id_": "3d951d46-71ee-4450-81a8-249a3d1d379c", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f54b124a-b23b-48c8-bc72-74d818994e78", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "5fc7637f05fd82b846d173cf523774ccc8d13589135c0ece2d997dd1f04d27f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c10f578-9617-4ff9-a27e-0c9f487a398c", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "2a2cbf831bfacbc3d54fe7e33de2e415362fc11bddd4ab43eac4b88714f6e54b", "class_name": "RelatedNodeInfo"}}, "hash": "30020a89967eebbc80adcf9e12b17c2c91f9df6b5896ab995e57256da09bc0b2", "text": "Why?\n\n  8. 6.8\n\nWhat is Synsets in WordNet framework? Give 2 examples on how it works to\nsupport your answers.\n\n  9. 6.9\n\nWhat is Path-based Similarity in Semantic Analysis? Use _book_ as the basic\nsynset to construct a synset tree like Fig. 6.9 and calculate all the related\nPath-based Similarity between different concepts related to _book_.\n\n  10. 6.10\n\nBased on the synset tree created in question 6.9, calculate the similarity\nvalues by using: (1) Resnik Method and (2) Dekang Lin Method and compare them\nwith the ones calculated in 6.9.\n\n  11. 6.11\n\nWhat is distributed similarity? State and explain methods used for distributed\nsimilarity measurement.\n\n  12. 6.12\n\nUse four famous literatures: (1) Moby Dick (Melville 2012), (2) Little Women\nby Louisa Mary Alcott (1832\u20131888) (Alcott 2017), (3) The Adventures of\nSherlock Holmes (Doyle 2019), and (4) War and Peace by Leo Tolstoy (1828\u20131910)\n(Tolstoy 2019) as context documents, and select ANY 4 words (wisely) to\nillustrate how term-context matrix, PMI and PPMI for document and word\nsimilarity measurement in semantic analysis.\n\n  13. 6.13\n\nRepeat question 6.12 by using K-smoothing method for PMI/PPMI calculations\n(with _k_ = 1 and 2) and different values of \u03b1 and compare them with results\nfound in 6.12. Explain why it can/cannot be improved.\n\nReferences\n\n  1. Agirre, E. and Edmonds, P. (Eds) (2007) Word Sense Disambiguation: Algorithms and Applications (Text, Speech and Language Technology Book 33). Springer.\n\n  2. Alcott, L. M. (2017) Little Women. AmazonClassics.\n\n  3. Ayetiran, E. F., & Agbele, K. (2016). An optimized Lesk-based algorithm for word sense disambiguation. Open Computer Science, 8(1), 165-172.[Crossref](https://doi.org/10.1515/comp-2018-0015)\n\n  4. BabelNet. 2022. BabelNet official site. [https://\u200bbabelnet.\u200borg/\u200b](https://babelnet.org/). Accessed 25 July 2022.\n\n  5. Bender, E. M. and Lascarides, A. (2019) Linguistic Fundamentals for Natural Language Processing II: 100 Essentials from Semantics and Pragmatics (Synthesis Lectures on Human Language Technologies). Springer.\n\n  6. Butler, A. (2015) Linguistic Expressions and Semantic Processing: A Practical Approach. Springer.[Crossref](https://doi.org/10.1007/978-3-319-18830-0)[zbMATH](http://www.emis.de/MATH-item?1323.68002)\n\n  7. Church, K. W., & Hanks, P. (1990). Word association norms, mutual information, and lexicography. Computational Linguistics - Association for Computational Linguistics, 16(1), 22-29.\n\n  8. Cruse, A. (2011) Meaning in Language: An Introduction to Semantics and Pragmatics (Oxford Textbooks in Linguistics). Oxford University Press\n\n  9. Cruse, A. (1986) Lexical Semantics (Cambridge Textbooks in Linguistics). Cambridge University Press.\n\n  10. Doyle, A. C. (2019) The Adventures of Sherlock Holmes (AmazonClassics Edition). AmazonClassics.\n\n  11. Firth, J. R. (1957). Papers in Linguistics 1934-1951. Oxford University Press.\n\n  12. Goddard, C. (1998) Semantic Analysis: A Practical Introduction (Oxford Textbooks in Linguistics). Oxford University Press.\n\n  13. Harris, Z. S. (1954). Distributional structure.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c10f578-9617-4ff9-a27e-0c9f487a398c": {"__data__": {"id_": "4c10f578-9617-4ff9-a27e-0c9f487a398c", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d951d46-71ee-4450-81a8-249a3d1d379c", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "30020a89967eebbc80adcf9e12b17c2c91f9df6b5896ab995e57256da09bc0b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0417ac08-df82-4822-81bc-4c0b75fcb71b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "603720836885381ef94ebf9e4ee91561d809eea468f4c611a01f2e346e66ba80", "class_name": "RelatedNodeInfo"}}, "hash": "2a2cbf831bfacbc3d54fe7e33de2e415362fc11bddd4ab43eac4b88714f6e54b", "text": "Cruse, A. (2011) Meaning in Language: An Introduction to Semantics and Pragmatics (Oxford Textbooks in Linguistics). Oxford University Press\n\n  9. Cruse, A. (1986) Lexical Semantics (Cambridge Textbooks in Linguistics). Cambridge University Press.\n\n  10. Doyle, A. C. (2019) The Adventures of Sherlock Holmes (AmazonClassics Edition). AmazonClassics.\n\n  11. Firth, J. R. (1957). Papers in Linguistics 1934-1951. Oxford University Press.\n\n  12. Goddard, C. (1998) Semantic Analysis: A Practical Introduction (Oxford Textbooks in Linguistics). Oxford University Press.\n\n  13. Harris, Z. S. (1954). Distributional structure. Word (Worcester), 10(2-3), 146-162. doi: [https://\u200bdoi.\u200borg/\u200b10.\u200b1080/\u200b00437956.\u200b1954.\u200b11659520](https://doi.org/10.1080/00437956.1954.11659520).[Crossref](https://doi.org/10.1080/00437956.1954.11659520)\n\n  14. Kilgarriff, A. and Rosenzweig, J. (2000). Framework and results for English SENSEVAL. Computers and the Humanities, 34(1/2), 15-48.[Crossref](https://doi.org/10.1023/A:1002693207386)\n\n  15. Kroeger, P. (2019) Analyzing Meaning: An Introduction to Semantics and Pragmatics (Textbooks in Language Sciences). Freie Universit\u00e4t\n\n  16. Lesk, M. 1986. Automatic Sense Disambiguation Using Machine Readable Dictionaries: How to Tell a Pine Cone from an Ice Cream Cone. ACM Special Interest Group for Design of Communication: Proceedings of the 5th Annual International Conference on Systems Documentation. ACM; 24\u201326. [https://\u200bdoi.\u200borg/\u200b10.\u200b1145/\u200b318723.\u200b318728](https://doi.org/10.1145/318723.318728).[Crossref](https://doi.org/10.1145/318723.318728)\n\n  17. Lin, D. K. (1998) An Information-Theoretic Definition of Similarity. In Proceedings of the Fifteenth International Conference on Machine Learning (ICML '98). Morgan Kaufmann Publishers Inc., 296\u2013304.\n\n  18. Melville, H. 2012. Moby-dick. Penguin English Library.\n\n  19. MESH. 2022. MeSH browser official site. [https://\u200bwww.\u200bnim.\u200bnih.\u200bgov/\u200bmesh/\u200bmeshome.\u200bhtml](https://www.nim.nih.gov/mesh/meshome.html). Accessed 25 July 2022.\n\n  20. Niwa, Y. and Nitta. Y. 1994. Co-occurrence Vectors from Corpora vs. Distance Vectors from Dictionaries. In COLING 1994 Volume 1: The 15th International Conference on Computational Linguistics, Kyoto, Japan. [https://\u200baclanthology.\u200borg/\u200bC94-1049.\u200bpdf](https://aclanthology.org/C94-1049.pdf).\n\n  21. Preiss, J. (2006). A detailed comparison of WSD systems: An analysis of the system answers for the SENSEVAL-2 English all words task. Natural Language Engineering, 12(3), 209-228.[Crossref](https://doi.org/10.1017/S1351324906004281)\n\n  22. Resnik, P. (1995). Using Information Content to Evaluate Semantic Similarity in a Taxonomy. Cornell University Library. [https://\u200barxiv.\u200borg/\u200babs/\u200bcmp-lg/\u200b9511007](https://arxiv.org/abs/cmp-lg/9511007).\n\n  23. Resnik, P. (1999). Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language. JAIR 11, 95-130.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0417ac08-df82-4822-81bc-4c0b75fcb71b": {"__data__": {"id_": "0417ac08-df82-4822-81bc-4c0b75fcb71b", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c10f578-9617-4ff9-a27e-0c9f487a398c", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "2a2cbf831bfacbc3d54fe7e33de2e415362fc11bddd4ab43eac4b88714f6e54b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c34d5d6-e083-4f0a-bd38-a4f1a55402bd", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "b46f6200dbbb5fb731f59c6c75324e5fc3b72c028d7bb1fb127eb32533ac2d88", "class_name": "RelatedNodeInfo"}}, "hash": "603720836885381ef94ebf9e4ee91561d809eea468f4c611a01f2e346e66ba80", "text": "21. Preiss, J. (2006). A detailed comparison of WSD systems: An analysis of the system answers for the SENSEVAL-2 English all words task. Natural Language Engineering, 12(3), 209-228.[Crossref](https://doi.org/10.1017/S1351324906004281)\n\n  22. Resnik, P. (1995). Using Information Content to Evaluate Semantic Similarity in a Taxonomy. Cornell University Library. [https://\u200barxiv.\u200borg/\u200babs/\u200bcmp-lg/\u200b9511007](https://arxiv.org/abs/cmp-lg/9511007).\n\n  23. Resnik, P. (1999). Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language. JAIR 11, 95-130.[Crossref](https://doi.org/10.1613/jair.514)[zbMATH](http://www.emis.de/MATH-item?0924.68155)\n\n  24. Shakespeare, W. (2021) The Complete Works of Shakespeare (AmazonClassics Edition). AmazonClassics.\n\n  25. Sowa, J. (1991) Principles of Semantic Networks: Explorations in the Representation of Knowledge (Morgan Kaufmann Series in Representation and Reasoning). Morgan Kaufmann Publication.[zbMATH](http://www.emis.de/MATH-item?0758.68018)\n\n  26. Tolstoy, L. (2019) War and Peace. AmazonClassics.\n\n  27. WordNet. 2022a. WordNet official site. [https://\u200bwordnet.\u200bprinceton.\u200bedu/\u200b](https://wordnet.princeton.edu/). Accessed 25 July 2022.\n\n  28. WordNet. 2022b. WordNet browser official site: [http://\u200bwordnetweb.\u200bprinceton.\u200bedu/\u200bparl/\u200bwebwn](http://wordnetweb.princeton.edu/parl/webwn). Accessed 25 July 2022.\n\n\n\u00a9 The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.\n2024\n\nR. S. T. Lee Natural Language Processing\n<https://doi.org/10.1007/978-981-99-1999-4_7>\n\n# 7\\. Pragmatic Analysis and Discourse\n\nRaymond S. T. Lee 1\n\n(1)\n\nUnited International College, Beijing Normal University-Hong Kong Baptist\nUniversity, Zhuhai, China\n\n## 7.1 Introduction\n\n_Pragmatics_ and _discourse_ _analysis_ (Bender and Lascarides 2019; Cruse\n2011; Goddard 1998; Kroeger 2019) refer to the study of language in context\nmeaning of sentences/utterances unlike word layers, syntax, grammatic\nrelationship, semantic and meaning presentations learnt in previous chapters.\n\n_Pragmatics analysis_ focuses on _context meaning_. Discourse analysis studies\nsocial context in written and spoken language. They consist of structured,\ncoherent, and cohesive sets of sentences/utterances to reflect what\nconstitutes an utterance versus a set of unrelated sentences and how the text\nis related.\n\nThere are two types of discourse in daily life: (1) _monologu_ e and (2)\n_dialogue_. A _monologue_ is a one way communication between a _speaker\n(writer)_ and an _audience (reader)_ , e.g. read or write a book, watch a TV\nshow or a play, listen to a speech, attend a presentation, or a lecture\ndepends on the deposition of dialogue. _Dialogue_ refers to participation in\nturn to speaker and hearer. It has a two-way or multiple ways of\ncommunications.\n\nThere are also two types of dialogue: (1) _human-to-human_ , e.g. daily\nconversations, group discussions, and (2) (a) _human-to-computer interaction_\n(HCI), e.g. conversational agent, chatbot in NLP, and (b) _computer-to-\ncomputer interaction_ (CCI), e.g. cross machines verbal communication in smart\ncity and intelligent transportation system, multi-agent based bargain and\nnegotiation systems, etc.\n\n## 7.2 Discourse Phenomena\n\nThere are many discourse phenomena solved by humans naturally, but some like\nconference resolution required a lot of effort by machine to solve.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c34d5d6-e083-4f0a-bd38-a4f1a55402bd": {"__data__": {"id_": "5c34d5d6-e083-4f0a-bd38-a4f1a55402bd", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0417ac08-df82-4822-81bc-4c0b75fcb71b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "603720836885381ef94ebf9e4ee91561d809eea468f4c611a01f2e346e66ba80", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f296ff5-5e78-4b0c-85fe-d2f0aa9dfd36", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "a29e0b74954f0eb497349132e08e5dc0a9e9a929dc00e24167ec37332730a558", "class_name": "RelatedNodeInfo"}}, "hash": "b46f6200dbbb5fb731f59c6c75324e5fc3b72c028d7bb1fb127eb32533ac2d88", "text": "_Dialogue_ refers to participation in\nturn to speaker and hearer. It has a two-way or multiple ways of\ncommunications.\n\nThere are also two types of dialogue: (1) _human-to-human_ , e.g. daily\nconversations, group discussions, and (2) (a) _human-to-computer interaction_\n(HCI), e.g. conversational agent, chatbot in NLP, and (b) _computer-to-\ncomputer interaction_ (CCI), e.g. cross machines verbal communication in smart\ncity and intelligent transportation system, multi-agent based bargain and\nnegotiation systems, etc.\n\n## 7.2 Discourse Phenomena\n\nThere are many discourse phenomena solved by humans naturally, but some like\nconference resolution required a lot of effort by machine to solve.\n\n### 7.2.1 Coreference Resolution\n\n_Coreference_ _resolution_ (Bender and Lascarides 2019; Goddard 1998) is a\ntask to identify all linguistics expressions, also known as _mentions_ which\ncorrespond to real-word entity described in the text. These mentions are\nassembled and replaced by correct pronouns and noun phrases. It is simple by\nhumans but always mistaken by machines. For example:\n\n  * [7.1] _Jack saw Andrew in the examination hall. He looked nervous._\n\n  * [7.2] _Jack saw the student in the examination hall. He looked nervous._\n\nHuman and machine will likely consider the first subject mentioned in\nforegoing sentence/utterance as reference to pronoun of the following\nsentence. For instance, _He_ in [7.1] will refer to _Jack_. However,\ncoreference resolution by human perspective in [7.2] will consider _He_ may\nnot refer to _Jack_ but _the student_ as it is natural and logical to relate\n_student_ with examination.\n\nExample below is more obvious:\n\n  * [7.3] _Jane talked to Amy about her examination result. She looked worry._\n\n  * [7.4] _Jane talked to Amy about her examination result. She felt sorry about it._\n\n_She_ in [7.3] should refer probably to _Amy_ is worried as she participated\nthe examination instead of _Jane_.\n\n_She_ in [7.4] should probably refer to _Jane_ instead of _Amy_ participated\nthe examination but _Jane_ is more likely to _feel sorry_ as empathy to _Amy_.\n\nHumans can discern the above naturally by context and common sense or world\nknowledge but confound computers to develop judgement.\n\n### 7.2.2 Why Is it Important?\n\nLet us look at some standard situations prior to complex coreference\nresolution cases:\n\n  * [7.5] _Jack gives Ian 1,000 dollars. He is generous._ (original sentence)\n\n  * [7.6] _Jack gives Ian 1,000 dollars. Jack is generous_. (with coreference resolution)\n\nor compact cases handled by computer satisfactorily:\n\n  * [7.7] _\u201cI voted for Jack as he is more aligned to my values\u201d, Ian said_ (original sentence)\n\n  * [7.8] _\u201cIan voted for Jack as Jack is more aligned to Ian\u2019s values\u201d, Ian said_ (with coreference resolution)\n\nFrom The Adventures of Sherlock Holmes (Doyle 2019):\n\n  * [7.9] _I was seized with a keen desire to see Holmes again, and to know how was employing his extraordinary powers._ (original sentence)\n\n  * [7.10] _Watson was seized with a keen desire to see Holmes again, and to know how Holmes was employing Holmes\u2019 extraordinary powers_ (with coreference resolution)\n\nor more challenging sentences of famous discourse from _A Scandal in Bohemia_\n:\n\n  * [7.11] _To Sherlock Holmes, she is always \u201c the woman\u201d. I have seldom heard him mention her under any other name._ (original sentence)\n\n  * [7.12] _To Sherlock Holmes, Irene Adler is always \u201cthe woman\u201d. Watson has seldom heard Holmes mention Irene Adler under any other name._ (with coreference resolution)\n\n[7.11] is more challenging as the reference name _Irene Adler_ for _she_ did\nnot occur, but after two sentences, there is no emotion akin to affection for\n_Irene Adler_.\n\nThis phenomenon is called _cataphor_ to acquire meaning from a subsequent word\nor phrase in linguistics.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f296ff5-5e78-4b0c-85fe-d2f0aa9dfd36": {"__data__": {"id_": "3f296ff5-5e78-4b0c-85fe-d2f0aa9dfd36", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c34d5d6-e083-4f0a-bd38-a4f1a55402bd", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "b46f6200dbbb5fb731f59c6c75324e5fc3b72c028d7bb1fb127eb32533ac2d88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f5b45c0c-780a-4c07-9329-57e41c1e3700", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "c2518e6953b6c7a8858520d9a99ed16d7ceef9e9f2232b496f19ab84c4078b4e", "class_name": "RelatedNodeInfo"}}, "hash": "a29e0b74954f0eb497349132e08e5dc0a9e9a929dc00e24167ec37332730a558", "text": "I have seldom heard him mention her under any other name._ (original sentence)\n\n  * [7.12] _To Sherlock Holmes, Irene Adler is always \u201cthe woman\u201d. Watson has seldom heard Holmes mention Irene Adler under any other name._ (with coreference resolution)\n\n[7.11] is more challenging as the reference name _Irene Adler_ for _she_ did\nnot occur, but after two sentences, there is no emotion akin to affection for\n_Irene Adler_.\n\nThis phenomenon is called _cataphor_ to acquire meaning from a subsequent word\nor phrase in linguistics.\n\nThe subsequent phrase (or word group) is called _antecedent_ or a referent\nagainst anaphora, a rhetorical term for a phrase (or word group) repetition at\nthe start of consecutive sentences/utterances used in many English sentences\u2019\nconstruction, i.e. [7.5], [7.7], [7.9] are reference terms mentioned\nrepetitively prior pronouns replacement.\n\nCoreference resolution is a versatile tool applied in many NLP applications\nincluding text understanding and analysis, information retrieval and\nextraction, text summarization, machine translation, and even sentiment\nanalysis. It is a great way to obtain unambiguous sentences comprehensible by\ncomputers.\n\n### 7.2.3 Coherence and Coreference\n\n#### 7.2.3.1 What Is Coherence?\n\nIn linguistics, _coherence_ (Bender and Lascarides 2019; Goddard 1998) refers\nto meaning relationships between individual units, which can be sentences\n(discourses) or textual statements. Texts appear to have logical and\nsemantical consistency for reader or hearer due to these relations.\n\n_Coherence-oriented_ text analysis is primarily concerned with the\nconstruction and configuration of meaning in a text, that is, how various\ncomponents are connected to make the text meaningful to recipient as a random\nsequence of disjointed phrases and clauses.\n\nIn other words, if a text has coherence, its parts are well-connected and head\nfor the same direction. Without coherence, a discussion or utterance may\nneither make sense nor follow by audience. It has both verbal and written\nlanguages significance.\n\nHere are some coherence examples:\n\n  * [7.13] _History_ _reveals that_ _humans_ _have come a long way from birth._ _They_ _have invented many_ _new technologies_ _that improve the standard of living. However,_ _technologies_ _that are supposed to provide us a better world are sometimes end up to disaster, such as the invention of_ _nuclear weapons_ _, environmental pollution, and the extinction of some animal species._\n\nIn [7.13], coherence terms _History\u2192 humans\u2192 They\u2192 technologies\u2192 nuclear\nweapons_ with repetitive terms and concepts provide a stream of ideas flow and\nknowledge for hearer or reader to understand the message conveyed in this\npassage.\n\n#### 7.2.3.2 What Is Coreference?\n\n_Coreference_ ( _co-reference_ ) appears when two (or group of) terms refer to\nthe same person or thing with a unified reference to achieve linguistic\ncoherence. For example:\n\n  * [7.14] _Jack said Helen would arrive soon, and she did._\n\n  * _\u2013 Helen_ and _she_ refer to the same person.\n\nConference is not always trivial to determine, e.g.\n\n  * [7.15] _Jack said he would join the term_ vs\n\n  * [7.16] _Jack told Ian to come, he smiled_.\n\nWhen comparing [7.15] vs [7.16], [7.15] is trivial as there is only a subject\n(noun) that _he_ can refer to (i.e. _Jack_ ), while _he_ in [7.16] can refer\nto either _Jack_ or _Ian_.\n\nDetermining coreference expressions are important in many NLP applications\nsuch as information retrieval and extraction, text summarization and dialogues\nunderstanding in Q&A chatbot systems.\n\n### 7.2.4 Importance of Coreference Relations\n\nTo understand the meaning of a coreference relationship, let us look at how to\nextract key information or summarize the following text:\n\n  * [7.17] _XYZ bank is continuing to struggle with severe financial problems. According to finance news report, their CEO Charles Smith will announce to step-down at the press conference tomorrow morning_.\n\nThe texts in [7.17] are coherent with well-structured coreference in a typical\nnews article.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5b45c0c-780a-4c07-9329-57e41c1e3700": {"__data__": {"id_": "f5b45c0c-780a-4c07-9329-57e41c1e3700", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f296ff5-5e78-4b0c-85fe-d2f0aa9dfd36", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "a29e0b74954f0eb497349132e08e5dc0a9e9a929dc00e24167ec37332730a558", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80e5d739-3b00-4259-9797-6d2278fbfb14", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "30be05431a5cae6f6c0f77a21b98060ea2dc97ff416612873545106e8a3f923d", "class_name": "RelatedNodeInfo"}}, "hash": "c2518e6953b6c7a8858520d9a99ed16d7ceef9e9f2232b496f19ab84c4078b4e", "text": "_Jack_ ), while _he_ in [7.16] can refer\nto either _Jack_ or _Ian_.\n\nDetermining coreference expressions are important in many NLP applications\nsuch as information retrieval and extraction, text summarization and dialogues\nunderstanding in Q&A chatbot systems.\n\n### 7.2.4 Importance of Coreference Relations\n\nTo understand the meaning of a coreference relationship, let us look at how to\nextract key information or summarize the following text:\n\n  * [7.17] _XYZ bank is continuing to struggle with severe financial problems. According to finance news report, their CEO Charles Smith will announce to step-down at the press conference tomorrow morning_.\n\nThe texts in [7.17] are coherent with well-structured coreference in a typical\nnews article. Coherence concept terms are also used to extract information:\n\n  * [XYZ bank] \u2192 [financial problem] \u2192 [CEO] \u2192 [Charles Smith] \u2192 [step down] \u2192 [press conference] \u2192 [tomorrow morning]\n\nA reasonable text summary may be:\n\n  * [7.18] _The CEO of XYZ bank Charles Smith will announce his step-down at tomorrow morning\u2019s press conference_.\n\nThis example shows the coherent relationships between text segments, where the\nfirst sentence provides context weights of the second sentence.\n\nRemarks: A well-structured text summarization/information extraction case will\nand should match with _Fillmore\u2019s Case Roles Theory_ with well-defined: agent,\npatient, location, time, purpose, beneficiary, possessor, instrument etc., in\nother words, a well coherence text message and utterance regards the first\nsentence as the _opening_ of a speech followed by _elaboration_ of an open\nstatement in coreference relation with a _thematic relation_ like watching a\nmovie or a TV show.\n\nFurther to elaboration and thematic relation, _coreference relation_ has\nanother type called _inference-type_. It regards first sentence/utterance as\n_claims_ followed by _explanation_ of claims sentence. For inference argument,\nthe first sentence is the _effect_ followed by _cause(s)_ of the following\nsentences:\n\n  * [7.19] _Jack keeps Ian\u2019s car key. He was drunk last night_. (coherence) vs\n\n  * [7.20] _Jack keeps Ian\u2019s car key. He wants to see movie tonight_. (without coherence)\n\nCoherence occurred in [7.19] as the first statement has relevance to the\nsecond statement with pragmatic meaning, whereas the second statement is\n_probably_ an explanation, or a _cause_ of the event where _Jack_ keeps\n_Ian\u2019s_ car key because _Ian_ was drunk by common sense/world knowledge. Thus,\n_He_ should be _Ian_ instead of _Jack_ by inference.\n\nWhile two statements in [7.20] have neither coherence nor logic _cause\u2013effect_\nrelationship between them, it is difficult to judge whether _He_ in the second\nstatement should refer to _Jack_ or _Ian_. Thus, _Jack_ regards as the subject\nand the referent _He_ in usage of English although it may be incorrect.\n\n### 7.2.5 Entity-Based Coherence\n\nLet\u2019s look at the following examples:\n\n  * [7.21] _Helen went to the superstore to buy a cello_.\n\n  * [7.22] _She had frequented the store for a long time_.\n\n  * [7.23] _She was delighted to buy the cello finally_.\n\n  * [7.24] _She just discovered that the store is closed_.\n\n  * [7.25] _It was the store Helen had frequented for a long time_.\n\n  * [7.26] _She was delighted to buy that cello_.\n\n  * [7.27] _The music generated by it is beautiful._\n\n  * [7.28] _It was closed when Helen arrived_.\n\nEntity-based coherence models measure coherence to track salient central\nentities across utterances. _Centralization theory_ (Grosz et al. 1995) is a\nremarkable entity-based coherence theory for tracking whether entities (the\nso-called _Central Entity_ , CE) are prominent at each point in a discourse\nmodel. For examples from [7.21] to [7.23] are _Helen_ who will be the\nreference for _she_ in these statements naturally.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80e5d739-3b00-4259-9797-6d2278fbfb14": {"__data__": {"id_": "80e5d739-3b00-4259-9797-6d2278fbfb14", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5b45c0c-780a-4c07-9329-57e41c1e3700", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "c2518e6953b6c7a8858520d9a99ed16d7ceef9e9f2232b496f19ab84c4078b4e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9a8004a-abac-48c4-b129-a20461b0a3e8", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "6929717d2e85bfedac61b4aa8f6a654acd699cdc87cf9e01ac57d450d0988f3c", "class_name": "RelatedNodeInfo"}}, "hash": "30be05431a5cae6f6c0f77a21b98060ea2dc97ff416612873545106e8a3f923d", "text": "* [7.24] _She just discovered that the store is closed_.\n\n  * [7.25] _It was the store Helen had frequented for a long time_.\n\n  * [7.26] _She was delighted to buy that cello_.\n\n  * [7.27] _The music generated by it is beautiful._\n\n  * [7.28] _It was closed when Helen arrived_.\n\nEntity-based coherence models measure coherence to track salient central\nentities across utterances. _Centralization theory_ (Grosz et al. 1995) is a\nremarkable entity-based coherence theory for tracking whether entities (the\nso-called _Central Entity_ , CE) are prominent at each point in a discourse\nmodel. For examples from [7.21] to [7.23] are _Helen_ who will be the\nreference for _she_ in these statements naturally. While CE in [7.25] is\nshifted from _the superstore_ to _cello_ in [7.26] and [7.27], CE is shifted\nback to _the store_ in [7.28] to make it more complex.\n\n## 7.3 Discourse Segmentation\n\n### 7.3.1 What Is Discourse Segmentation?\n\n_Discourse segmentation_ is the task of determining the smallest non-\noverlapping discourse units, known as _elementary discourse units (EDUs),_\nwhich can be further categorized into (1) sentence segmentation and (2)\nsentence-level discourse segmentation.\n\nThe main purpose of discourse segmentation is to divide a text document (set\nof utterances) into a list of subtopics. This is often a higher-level\nsimplification structure of a discourse. For example, an academic article is\nusually segmented into _abstract, introduction, methodology, implementation,\nresults, discussion_ , _conclusion,_ etc. to comprehend.\n\nThere are (1) _unsupervised_ and (2) _supervised_ _discourse_ _segmentation_\nmethods. The applications of automatic discourse segmentation include (1)\ninformation extraction or retrieval and (2) text summarization on each segment\nseparately.\n\n### 7.3.2 Unsupervised Discourse Segmentation\n\n_Unsupervised_ _discourse_ _segmentation_ is a class usually presented as a\nlinear segmentation of raw data and segmentation into multiple paragraph\nsubtopics. _Unsupervised_ means that the task is not given training data as\nexamples to understand linear segmentation task. These examples involve\nsplitting the text into multi-paragraph units to represent paragraphs of the\noriginal text. These algorithms rely on cohesion, which can be defined as the\nlinguistic means of linking units of text together.\n\n_Cohesion_ _-based approach_ involves dividing text into subtopics, where\nsentences or paragraphs cohere to each other and reveal the relationship\nbetween two or more words in two units like synonyms.\n\n_Cohesion_ is the linking of text units based on linguistic means. Lexical\ncohesion is the use of similar words to link units of text with the same word,\nsynonym or hypernym. For instance:\n\n  * [7.29] _Yesterday was Jane\u2019s birthday. Betty and Mary went to buy a present from the gift shop. Mary intended to buy a purse. \u201cDon\u2019t do that.\u201d, mentioned Betty. \u201cJane already got one. She will ask you to return it.\u201d_\n\nNon-lexical cohesion approach using anaphora.\n\n  * [7.30] _Peel, core and slice_ _peaches_ _and_ _pineapples_ _, then place_ _these fruits_ _in the skillet._\n\nUnsupervised discourse segmentation was proposed by Prof. Marti Hearst in his\nclassical works on TextTiling in early 1990.\n\n### 7.3.3 Hearst\u2019s TextTiling Method\n\n_Hearst\u2019s TextTiling_ (Hearst 1997) is a typical discourse segmentation\nalgorithm to subdivide explanatory text into multiple paragraphs or\nautomatically grouped subtopic segments representing in the original text.\n\nHearst\u2019s TextTiling method is a typical unsupervised method that no training\ndataset and prior knowledge base are required. Hearst\u2019s original work used\narticles from _Stargazers_ , a science magazine with _TextTiling method_ to\ncharacterize article text messages into subtopics.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e9a8004a-abac-48c4-b129-a20461b0a3e8": {"__data__": {"id_": "e9a8004a-abac-48c4-b129-a20461b0a3e8", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80e5d739-3b00-4259-9797-6d2278fbfb14", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "30be05431a5cae6f6c0f77a21b98060ea2dc97ff416612873545106e8a3f923d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63e2e665-c350-4422-a3ef-cc5fc2712a93", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "170a5d6b5069c314690c3a38898c7924362a8d4e508c8a94156d49eee0ed1170", "class_name": "RelatedNodeInfo"}}, "hash": "6929717d2e85bfedac61b4aa8f6a654acd699cdc87cf9e01ac57d450d0988f3c", "text": "* [7.30] _Peel, core and slice_ _peaches_ _and_ _pineapples_ _, then place_ _these fruits_ _in the skillet._\n\nUnsupervised discourse segmentation was proposed by Prof. Marti Hearst in his\nclassical works on TextTiling in early 1990.\n\n### 7.3.3 Hearst\u2019s TextTiling Method\n\n_Hearst\u2019s TextTiling_ (Hearst 1997) is a typical discourse segmentation\nalgorithm to subdivide explanatory text into multiple paragraphs or\nautomatically grouped subtopic segments representing in the original text.\n\nHearst\u2019s TextTiling method is a typical unsupervised method that no training\ndataset and prior knowledge base are required. Hearst\u2019s original work used\narticles from _Stargazers_ , a science magazine with _TextTiling method_ to\ncharacterize article text messages into subtopics.\n\nFor example, consider a 21-paragraph science news article extracted from the\nmagazine with a topic focused on reports of life on Earth and other plants,\nits contents are characterized into the following subtopic discussions (Hearst\n1997):\n\n  * [Para 1\u20133] _Introduction \u2013 the search of life in space_\n\n  * [Para 4\u20135] _The moon\u2019s chemical composition_\n\n  * [Para 6\u20138] _How early earth-moon proximity shaped the moon_\n\n  * [Para 9\u201312] _How the moon helped life evolve on earth_\n\n  * [Para 13] _Improbability of the earth-moon system_\n\n  * [Para 14\u201316] _Binary/trinary star systems make life unlikely_\n\n  * [Para 17\u201318] _The low probability of nonbinary/trinary systems_\n\n  * [Para 19\u201320] _Properties of earth\u2019s sun that facilitate life_\n\n  * [Para 21] _Summary_\n\n_TextTiling_ is a technique to divide a full-length text document into\ncoherent multi-paragraph units that correspond to a series of subtopic\nparagraphs as shown in example above. The algorithm assumes that during a\nsubtopic discussion, a set of words is used, and subtopics changed significant\nparts of vocabulary accordingly.\n\nThe distribution of terms extracted from Stargazer text is assigned with a\nsingle-digit frequency for each sentence number, with spaces for zero\nfrequencies (Hearst 1997) as shown in Fig. 7.1. It revealed that terms:\n\n  1. 1.\n\noccurred frequently throughout the text such as _moon_ and _planet_ are often\nindicative of main topic(s) of the text,\n\n  2. 2.\n\nless common but evenly distributed, such as _scientists_ and _form_ are both\ngeneric to create a subtopic title,\n\n  3. 3.\n\nlike _space_ and _star_ occurred more frequent from sentences 5 to 20 and 60\nto 90, while term _life_ to _planet_ occurred more frequent from sentences 58\nto 78 which may create two distinct clusters of subtopic discussion, and\n\n  4. 4.\n\nlike _life_ to _species_ have similar phenomena occurred to create a natural\ncluster between sentences 35 and 55 and conform with human judgement as\nsubtopic discussion of _How the moon helped life evolve on earth_.\n\n![](../images/533412_1_En_7_Chapter/533412_1_En_7_Fig1_HTML.png)\n\nFig. 7.1\n\nDistribution of selected terms in Stargazer text (blanks mean zero frequency)\n\nThese results suggested that the logic behind sentences or paragraphs in\nsubtopics are consistent with each other but not with paragraphs in adjacent\ntopics.\n\n### 7.3.4 TextTiling Algorithm\n\n_TextTiling algorithm_ (Hearst 1997) for discourse segmentation and subtopic\nstructure characterization using term repetition consists of three processes:\n(1) _tokenization_ , (2) _lexical score determination,_ and (3) _boundary\nidentification_.\n\n_Tokenization_ includes converting words to lowercase, removing stop-words and\nroot-words, and converting words into pseudo-sentences with the same length\nsuch as 15 words.\n\n_Lexical score determination_ includes calculating lexical cohesion scores for\neach gap between pseudo-sentences. This lexical cohesion score represents\nwords similarity. For instance, take 10 pseudo-sentences each before and after\n_gap_ , followed by the computation of cosine similarity between word vectors\nwhich is given by\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63e2e665-c350-4422-a3ef-cc5fc2712a93": {"__data__": {"id_": "63e2e665-c350-4422-a3ef-cc5fc2712a93", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9a8004a-abac-48c4-b129-a20461b0a3e8", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "6929717d2e85bfedac61b4aa8f6a654acd699cdc87cf9e01ac57d450d0988f3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ec573f0-4669-4a2c-acac-0a589bb21e2a", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "4e0f1a0bc8867eccc55361c02b88005920839f8e0eece753d51b8d646d0db95f", "class_name": "RelatedNodeInfo"}}, "hash": "170a5d6b5069c314690c3a38898c7924362a8d4e508c8a94156d49eee0ed1170", "text": "### 7.3.4 TextTiling Algorithm\n\n_TextTiling algorithm_ (Hearst 1997) for discourse segmentation and subtopic\nstructure characterization using term repetition consists of three processes:\n(1) _tokenization_ , (2) _lexical score determination,_ and (3) _boundary\nidentification_.\n\n_Tokenization_ includes converting words to lowercase, removing stop-words and\nroot-words, and converting words into pseudo-sentences with the same length\nsuch as 15 words.\n\n_Lexical score determination_ includes calculating lexical cohesion scores for\neach gap between pseudo-sentences. This lexical cohesion score represents\nwords similarity. For instance, take 10 pseudo-sentences each before and after\n_gap_ , followed by the computation of cosine similarity between word vectors\nwhich is given by\n\n![$$\n{\\\\mathrm{sim}}_{\\\\mathrm{cosine}}\\\\left\\(\\\\overrightarrow{b},\\\\overrightarrow{a}\\\\right\\)=\\\\frac{\\\\overrightarrow{b}\\\\cdot\n\\\\overrightarrow{a}}{\\\\left|\\\\overrightarrow{b}\\\\;\\\\right|\\\\left|\\\\overrightarrow{a}\\\\right|}=\\\\frac{\\\\sum_{i=1}^N\\\\kern0.24em\n{b}_i\\\\times\n{a}_i}{\\\\sqrt{\\\\sum_{i=1}^N\\\\;{b}_i^2}\\\\sqrt{\\\\sum_{i=1}^N\\\\kern0.24em\n{a}_i^2}}\n$$](../images/533412_1_En_7_Chapter/533412_1_En_7_Chapter_TeX_Equ1.png)\n\n(7.1)\n\n_Boundary identification_ involves assigning a boundary distance to identify a\nnew segment. Similarity is first created, and the depth value of similarity\nvalley ( _a_ - _b_ ) + ( _c_ - _b_ ) is calculated as shown in Fig. 7.2, then\nperformed segmentation if the depth score value is greater than the threshold\nas shown in Fig. 7.3.\n\n![](../images/533412_1_En_7_Chapter/533412_1_En_7_Fig2_HTML.png)\n\nFig. 7.2\n\nLexical score determination with similarity valleys\n\n![](../images/533412_1_En_7_Chapter/533412_1_En_7_Fig3_HTML.png)\n\nFig. 7.3\n\nBoundary identification with discourse segments\n\n### 7.3.5 Supervised Discourse Segmentation\n\nIt is relatively easy to collect bounded training data using _supervised_\n_discourse_ _segmentation_ such as news reports from TV shows, paragraph\nsegmentation in text or dialogue to find paragraphs in speech recognition\noutput.\n\nSeveral classifiers can be used to achieve supervised segmentation, one is\ncalled _feature set_ which is a superset for unsupervised segmentation with\noften domain-specific utterance tokens and keywords.\n\nSupervised discourse segmentation is also a model. It is (1) a classification\ntask that uses one of the supervised classifier methods, such as SVM, Na\u00efve\nBayer, maximum entropy, etc. to distinguish whether sentence boundaries have\nparagraph boundaries, or (2) a sequence labeling task to label sentences with\nor without paragraph borders. It uses cohesive features including word\noverlap, word cosine similarity, anaphora, and additional features such as\ndiscourse markers or keywords.\n\nDiscourse tokens or keywords/phrases indicate discourse structure, e.g. _good\nevening, join our broadcast news now_ , _or join the company at the\nbeginning/end of the segment,_ etc. They can be manual codes or automatically\ndetermined by feature selection.\n\nHowever, measuring _precision_ , _recall_ , and _F-measure_ are not always\ngood evaluation ideas as they are insensitive to near misses. Pevzner and\nHearst (2002) proposed a good and effective evaluation metric for text\nsegmentation called _WindowDiff_ method.\n\n## 7.4 Discourse Coherence\n\n### 7.4.1 What Makes a Text Coherent?\n\nA _text coherent_ refers to the application of:\n\n  1. 1.\n\nA coherent relationship between a subfield of discourse called _rhetorical\nstructure_ and a whole theory called _Rhetorical Structure Theory_ (RST). It\nis a text organization theory that describes the relationships exist between\nparts of a text.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ec573f0-4669-4a2c-acac-0a589bb21e2a": {"__data__": {"id_": "9ec573f0-4669-4a2c-acac-0a589bb21e2a", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63e2e665-c350-4422-a3ef-cc5fc2712a93", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "170a5d6b5069c314690c3a38898c7924362a8d4e508c8a94156d49eee0ed1170", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6a8a432-83e6-41f8-9083-1f5431eff637", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "73de9289b8bc72fdaeb3848394731388e2f231c55fe659d49373bfd3861ad104", "class_name": "RelatedNodeInfo"}}, "hash": "4e0f1a0bc8867eccc55361c02b88005920839f8e0eece753d51b8d646d0db95f", "text": "They can be manual codes or automatically\ndetermined by feature selection.\n\nHowever, measuring _precision_ , _recall_ , and _F-measure_ are not always\ngood evaluation ideas as they are insensitive to near misses. Pevzner and\nHearst (2002) proposed a good and effective evaluation metric for text\nsegmentation called _WindowDiff_ method.\n\n## 7.4 Discourse Coherence\n\n### 7.4.1 What Makes a Text Coherent?\n\nA _text coherent_ refers to the application of:\n\n  1. 1.\n\nA coherent relationship between a subfield of discourse called _rhetorical\nstructure_ and a whole theory called _Rhetorical Structure Theory_ (RST). It\nis a text organization theory that describes the relationships exist between\nparts of a text. It was proposed by Mann and Thompson (1988) in their\nremarkable paper _Rhetorical structure theory: toward a functional theory of\ntext organization_ , published in 1988. The theory was developed as part of\nresearch on computer-aided text generation in text summarization and\napplications used by NLP researchers.\n\n  2. 2.\n\nThe ordering of subsections of discourse called _discourse topic structure_.\nIt is the key to discourse cohesion and embodies the essence of discourse\nanalysis. It has been extensively adopted by past decades and become a key\ncomponent in text analysis. Linearly segmenting text into appropriate topic\nstructures can reveal valuable information such as the overall topic structure\nof the text, which can be used for text analysis tasks such as text\nsummarization, information retrieval, and discourse analysis.\n\n  3. 3.\n\nA _Referring Expression_ _(RE)_ is any noun phrase or a substitute for a noun\nphrase whose function in spoken, signed, or written text is to single out a\nsingle person, place, object or group of people, places, objects, etc.\n\n### 7.4.2 What Is Coherence Relation?\n\n_Coherence relation_ refers to discourse properties that make each discourse\nmeaningful (or have appropriate meaning) in the context. It refers to common\ndenominator to identify possible connections between utterances in a series of\nstatements or discourses about the same topic.\n\nThese sense relations in discourse analysis named _Coherence Relations_ by\nProf. Jerry R. Hobbs in his works _Coherence and_ _Coreference_ published by\n_Cognitive Science_ in 1979 (Hobbs 1979) had further developed by other\nlinguistics including Sanders et al. (1992) and Kehler (2002) into a well-\ndefined theory.\n\nThese meaning relationships, called _propositional relations_ defined by Mann\nand Thompson (1986), are encoded in text recognized by the reader trying to\nunderstand the text and its components, and to see why the speaker or author\nadded the sentence. Coherent relationships are sometimes referred to as types\nof thematic development such as the narrative of a movie or TV show involving\n_cause-and-effect_ story type in sense relations development.\n\n### 7.4.3 Types of Coherence Relations\n\nThere are five major types of coherence relations (1) _parallel_ , (2)\n_elaboration_ , (3) _cause-and-effect_ , (4) _contrast,_ and (5) _occasion_.\n\n  1. 1.\n\n_Parallel_ infers _p_ ( _a_ 1, _a_ 2, \u2026) from the assertion of _S_ 0 and _p_ (\n_b_ 1, _b_ 2\u2026) from the assertion of _S_ 1, where _a_ _i_ and _b_ _i_ are\nsimilar for all _i_.\n\n[7.31] _Rich man wants more power. Poor man wants more food._\n\nThey are frequently used in describing two sense relations with similar\nsituation (meaning) but different in object, reference, and scenario.\n\n  2. 2.\n\n_Elaboration_ infers the same proposition _P_ from the assertions of _S_ 0 and\n_S_ 1.\n\n[7.32] _Dorothy was from Kansas. She lived in the great Kansas prairies._\n\n[7.33] _Nicolas Telsa was a genius. He invented over hundreds of things in his\nlife._\n\nThey are frequently used in discourse construction, the successive\nsentences/utterances are further elaboration of the previous one.\n\n  3. 3.\n\n_Cause-and-effect_ are _S_ 0 and _S_ 1 if _S_ 1 infers _S_ 0, i.e.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6a8a432-83e6-41f8-9083-1f5431eff637": {"__data__": {"id_": "e6a8a432-83e6-41f8-9083-1f5431eff637", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ec573f0-4669-4a2c-acac-0a589bb21e2a", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "4e0f1a0bc8867eccc55361c02b88005920839f8e0eece753d51b8d646d0db95f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40eb9039-c2f3-4d30-9b7d-7a8ef047469d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "1736f8927f6c2249c30a9f348d96c3ab601e086a62a7f178cb0dd09dd5a4f8bd", "class_name": "RelatedNodeInfo"}}, "hash": "73de9289b8bc72fdaeb3848394731388e2f231c55fe659d49373bfd3861ad104", "text": "[7.31] _Rich man wants more power. Poor man wants more food._\n\nThey are frequently used in describing two sense relations with similar\nsituation (meaning) but different in object, reference, and scenario.\n\n  2. 2.\n\n_Elaboration_ infers the same proposition _P_ from the assertions of _S_ 0 and\n_S_ 1.\n\n[7.32] _Dorothy was from Kansas. She lived in the great Kansas prairies._\n\n[7.33] _Nicolas Telsa was a genius. He invented over hundreds of things in his\nlife._\n\nThey are frequently used in discourse construction, the successive\nsentences/utterances are further elaboration of the previous one.\n\n  3. 3.\n\n_Cause-and-effect_ are _S_ 0 and _S_ 1 if _S_ 1 infers _S_ 0, i.e. _S_ 1 \u2192 _S_\n0\n\n[7.34] _Jack cannot afford to buy the car. He lost his job._\n\n[7.35] _Nicolas Tesla invented over hundreds of things in his life. He was a\ngenius._\n\n_Cause-and-effect_ discourse relation can refer to animate or inanimate\nsubjects in [7.35] which is the reverse of elaboration statement [7.33] but do\nnot always occur.\n\n  4. 4.\n\n_Contrast_ in _S_ 0 and _S_ 1 if _P_ 0 and _P_ 1 infer from _S_ 0 and _S_ 1\nwith one pair of elements that are contrast with each other, where other\nelements are similar in context.\n\n[7.36] _Hope for the best. Prepare for the worst._\n\n[7.37] _Jack is meticulous while Bob is sloppy._\n\nContrast coherence relations can exist within a sentence, or in successive\nsentences/utterances. It often refers to two subjects, or events with contrast\nsense relations.\n\n  5. 5.\n\n_Occasion_ is the alteration of state that can infer from the assertion of _S_\n0, where final state can infer from _S_ 1, or the alteration of state can\ninfer from the assertion of _S_ 1, whose initial state can infer from _S_ 0.\n\n[7.38] _Jane put the books into a schoolbag, she left the classroom with\nHelen._\n\n[7.39] _Jack failed in the exam. He started to work hard._\n\nState change invokes new action.\n\n### 7.4.4 Hierarchical Structure of Discourse Coherence\n\nDiscourse coherence can also be revealed by the hierarchy between coherent\nrelations. For example:\n\n  * [7.40] _Jack went to town to buy a toy_.\n\n  * [7.41] _He took a bus to the shopping mall_.\n\n  * [7.42] _He needed to buy a toy for his child_.\n\n  * [7.43] _It is Jane\u2019s birthday_.\n\n  * [7.44] _He also wanted to buy some books for weekend reading_.\n\nA hierarchical structure of discourse coherence is shown in Fig. 7.4.\n[7.40]\u2013[7.44] can be organized in a hierarchy tree structure, e.g. _Occasion_\nconsists of two expressions, one is expression e1 (statement [7.40]) and the\nother is an explanatory clause which in turn consists of expression e2\n(statement [7.41]) and a parallel clause which consists of two entities, one\nis explanatory expression e3 and the other is expression e5 (statement\n[7.44]), e3 is further divided into statements [7.42] and [7.43],\nrespectively.\n\n![](../images/533412_1_En_7_Chapter/533412_1_En_7_Fig4_HTML.png)\n\nFig. 7.4\n\nHierarchical structures in discourse coherence\n\n### 7.4.5 Types of Referring Expressions\n\n_Referring expression_ _(RE)_ is a surrogate for any noun phrase or noun\nphrase whose function in utterance is to identify some discrete objects. There\nare five frequently used REs in discourse coherence: (1) _indefinite noun\nphrases_ , (2) _definite noun phrases_ , (3) _pronouns_ , (4)\n_demonstratives,_ and (5) _names_.\n\n  1. 1.\n\n_Indefinite noun phrases_ introduce entities into context that are new to\nlistener, e.g., _a policeman, some apples, a new iPad,_ etc.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40eb9039-c2f3-4d30-9b7d-7a8ef047469d": {"__data__": {"id_": "40eb9039-c2f3-4d30-9b7d-7a8ef047469d", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6a8a432-83e6-41f8-9083-1f5431eff637", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "73de9289b8bc72fdaeb3848394731388e2f231c55fe659d49373bfd3861ad104", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c35795ee-c5d2-473e-8726-ccc51ee9d9eb", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "b46af15972205e3440847b2e651e321af83abb56f2e541f701d1b4acc33a83d4", "class_name": "RelatedNodeInfo"}}, "hash": "1736f8927f6c2249c30a9f348d96c3ab601e086a62a7f178cb0dd09dd5a4f8bd", "text": "![](../images/533412_1_En_7_Chapter/533412_1_En_7_Fig4_HTML.png)\n\nFig. 7.4\n\nHierarchical structures in discourse coherence\n\n### 7.4.5 Types of Referring Expressions\n\n_Referring expression_ _(RE)_ is a surrogate for any noun phrase or noun\nphrase whose function in utterance is to identify some discrete objects. There\nare five frequently used REs in discourse coherence: (1) _indefinite noun\nphrases_ , (2) _definite noun phrases_ , (3) _pronouns_ , (4)\n_demonstratives,_ and (5) _names_.\n\n  1. 1.\n\n_Indefinite noun phrases_ introduce entities into context that are new to\nlistener, e.g., _a policeman, some apples, a new iPad,_ etc.\n\n[7.45] _I go to the electronic store to buy a new notebook computer._\n\n  2. 2.\n\n_Definite noun phrases_ refer to entities recognizable by listener such as\nabovementioned, combination of beliefs about the world, e.g., _a furry white\ncat, the cat,_ etc.\n\n[7.46] _Don\u2019t look at the sun directly with bare eyes, it will hurt yourself._\n\n  3. 3.\n\n_Pronouns_ are another form of definite designation, usually with stronger\nrestrictions than standard designation, e.g., s/ _he, it_ , _they,_ etc.\n\n[7.47] _I go to the electronic store to buy a new notebook computer. This\ncomputer is rather light and fast_.\n\n  4. 4.\n\n_Demonstratives_ are pronouns that can act alone or as determiners, e.g.\n_this, that._\n\n[7.48] _That book seems to be very interesting and worth buying it_.\n\n  5. 5.\n\n_Names_ are common methods to refer people, organizations, and locations.\n\n[7.49] _I bought lunch at KFC today._\n\n### 7.4.6 Features for Filtering Potential Referents\n\nThere are four common features to filter potential references in discourse\ncoherence: (1) _number agreement_ , (2) _person agreement_ , (3) _gender\nagreement,_ and (4) _binding theory constraints_.\n\n  1. 1.\n\n_Number agreement_ are pronouns and references must agree in number, e.g.,\n_single or plural_.\n\n[7.50] _The children are playing in the park. They look happy_.\n\n  2. 2.\n\n_Person agreement_ refers to the first, second, or third person.\n\n[7.51] _Jane and Helen got up early. They needed to take an exam this\nmorning._\n\n  3. 3.\n\n_Gender agreement_ refers to male, female, or non-person, e.g. _he, she, or\nit_.\n\n[7.52] _Jack looked tired. He didn\u2019t sleep last night._\n\n  4. 4.\n\n_Binding theory constraints_ refer to constraints imposed by syntactic\nrelations between denotative expressions and possible preceding noun phrases\nin the same sentence.\n\n[7.53] _Jane purchased herself an iPad_. ( _herself_ should be _Jane_ )\n\n[7.54] _Jane purchased her an iPad_. ( _her_ may not be _Jane_ )\n\n[7.55] _She claimed that she purchased Mary a iPad_. ( _She_ and _she_ may not\nbe _Mary_ )\n\n### 7.4.7 Preferences in Pronoun Interpretation\n\nThere are six types of preferences in pronoun interpretation: (1) recency, (2)\ngrammatical role, (3) repeated mention, (4) parallelism, (5) verb semantics,\nand (6) selected restrictions.\n\n_Recency_ refers to entities from recent utterances:\n\n  * [7.56] _Tim went to see a doctor at the clinic. He felt sick. It might be influenza._\n\n_Grammatical role_ is to emphasize the hierarchy of entities according to\ngrammatical position of the terms that represent them, e.g. subject, object,\netc.\n\n  * [7.57] _Jane went to Starbucks to meet Jackie. She ordered a hot mocha_. ( _She_ should be _Jane_ )\n\n  * [7.58] _Jane discussed with Jackie about her exam results. She felt so nervous about it._ ( _She_ should be _Jackie_ instead of _Jane_ )\n\n  * [7.59] _Jane discussed with Jackie about her exam results.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c35795ee-c5d2-473e-8726-ccc51ee9d9eb": {"__data__": {"id_": "c35795ee-c5d2-473e-8726-ccc51ee9d9eb", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40eb9039-c2f3-4d30-9b7d-7a8ef047469d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "1736f8927f6c2249c30a9f348d96c3ab601e086a62a7f178cb0dd09dd5a4f8bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c4e0ddc-12fe-4500-b878-b6b87efed231", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "d936b6f2f57480f657a76ca11a11831ee573357b94ce6f1b17bcf92a1b7cb493", "class_name": "RelatedNodeInfo"}}, "hash": "b46af15972205e3440847b2e651e321af83abb56f2e541f701d1b4acc33a83d4", "text": "_Recency_ refers to entities from recent utterances:\n\n  * [7.56] _Tim went to see a doctor at the clinic. He felt sick. It might be influenza._\n\n_Grammatical role_ is to emphasize the hierarchy of entities according to\ngrammatical position of the terms that represent them, e.g. subject, object,\netc.\n\n  * [7.57] _Jane went to Starbucks to meet Jackie. She ordered a hot mocha_. ( _She_ should be _Jane_ )\n\n  * [7.58] _Jane discussed with Jackie about her exam results. She felt so nervous about it._ ( _She_ should be _Jackie_ instead of _Jane_ )\n\n  * [7.59] _Jane discussed with Jackie about her exam results. She felt so sorry about it_. ( _She_ should be _Jane_ instead of _Jackie_ )\n\n_Repeated mention_ refers to mentioning about the same thing.\n\n  * [7.60] _Jane went to supermarket to buy some food. It turned out it was closed_.\n\n_Parallelism_ refers to subject-to-subject or object-to-object kind of\nexpression:\n\n  * [7.61] _Mary went with Jane to Starbucks. Ian went with her to the bookstore afterwards._ ( _her_ should probably be _Jane_ instead of _Mary_ )\n\n_Verb semantics_ are _verbs that seem to emphasize one of their argument\npositions:_\n\n  * [7.62] _Jane warned Mary. She might fail the test_.\n\n  * [7.63] _Jane blamed Mary. She lost the watch_.\n\nIn [7.62] _She_ should be _Mary_ as _Mary_ is the one being warned about\nfailing the test. For [7.63] _She_ should be _Jane_ who suffered. It is a\npragmatic phenomenon because it involves common sense by word meaning _blamed_\nto understand correct coreference in the second statement.\n\n_Selectional restrictions_ refer to another semantic knowledge playing a role:\n\n  * [7.64] _Mary lost her iPhone in the shopping mall after carrying it the whole afternoon_.\n\nNote that [7.64] involves high-level semantic or common sense understanding of\n_it_ can mean iPhone or shopping mall but it has been carried for the whole\nafternoon, so it cannot be an unmovable object except iPhone.\n\n## 7.5 Algorithms for Coreference Resolution\n\n### 7.5.1 Introduction\n\n_Coreference_ _resolution_ (CR) is the task of finding all linguistic\nexpressions (called mentions) in any text involving real-world entities. After\nfinding these mentions and grouping them, they can be resolved by replacing\npronouns with noun phrases.\n\nThere are three fundamental algorithms for conference resolution: (1) Hobbs\nalgorithm, (2) Centering algorithm, and (3) Log-linear model.\n\n### 7.5.2 Hobbs Algorithm\n\n#### 7.5.2.1 What Is Hobbs Algorithm?\n\nHobbs algorithm was one of the early approaches to pronoun resolution proposed\nby Prof. Jerry R. Hobbs in 1978 (Hobbs 1978) and further consolidated as well-\nknown algorithm for coreference resolution in his remarkable work _Coherence\nand Coreferences_ published in Cognitive Science 1979 (Hobbs 1979).\n\nHe original work proposed two CR algorithms, a simple algorithm based purely\non grammar, and a complex algorithm that incorporated semantics into parsing\nmethods (Hobbs 1978, 1979).\n\nUnlike other algorithms, Hobbs' algorithm does not turn to a discourse model\nfor parsing because its parse tree and grammar rules are the only information\nused in pronoun parsing. Let us look at how it works.\n\n#### 7.5.2.2 Hobbs\u2019 Algorithm\n\nHobbs\u2019 algorithm assumes a parse tree where each NP node has an N type node\nbelow it as the parent of a lexical object. It operates as follows:\n\n  1. 1.\n\nStart with the node of noun phrase (NP) that directly dominates the pronoun.\n\n  2. 2.\n\nGo up tree to the first NP or sentence (S) node visited, denote this node as\nX, and name the path being applied to reach it as p.\n\n  3. 3.\n\nVisit all branches under node X to the left of path p, breadth-first, from\nleft to right, taking any NP node found as an antecedent, there is an NP or\nS-node between it and X.\n\n  4. 4.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c4e0ddc-12fe-4500-b878-b6b87efed231": {"__data__": {"id_": "7c4e0ddc-12fe-4500-b878-b6b87efed231", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c35795ee-c5d2-473e-8726-ccc51ee9d9eb", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "b46af15972205e3440847b2e651e321af83abb56f2e541f701d1b4acc33a83d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3a8ed41-5133-4ae7-923d-a565af99fa7e", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "fa651f04b3ddebe1d488b1eb242b1dd99ecc31f0d1cdf0cd2c217cc038a0676e", "class_name": "RelatedNodeInfo"}}, "hash": "d936b6f2f57480f657a76ca11a11831ee573357b94ce6f1b17bcf92a1b7cb493", "text": "Let us look at how it works.\n\n#### 7.5.2.2 Hobbs\u2019 Algorithm\n\nHobbs\u2019 algorithm assumes a parse tree where each NP node has an N type node\nbelow it as the parent of a lexical object. It operates as follows:\n\n  1. 1.\n\nStart with the node of noun phrase (NP) that directly dominates the pronoun.\n\n  2. 2.\n\nGo up tree to the first NP or sentence (S) node visited, denote this node as\nX, and name the path being applied to reach it as p.\n\n  3. 3.\n\nVisit all branches under node X to the left of path p, breadth-first, from\nleft to right, taking any NP node found as an antecedent, there is an NP or\nS-node between it and X.\n\n  4. 4.\n\nIf node X is the highest S-node in sentence, visit the surface parse trees of\nprevious sentences in the text with the most recent first, each tree is then\nvisited in a left-to-right and breadth-first manner. When an NP node is\nencountered, it is recommended as an antecedent. If X is not the first S-node\nin the set, go to step 5.\n\n  5. 5.\n\nClimb up from node X to the first NP or S-node encountered, denote this new\nnode as X and name the path as p.\n\n  6. 6.\n\nIf X is an NP vertex, and if the path p to X does not pass through a nominal\nvertex immediately dominated by X, then denote X as an antecedent.\n\n  7. 7.\n\nVisit all branches under node X to the left of path p, breadth-first manner,\nfrom left to right, denoting each NP node encountered as an antecedent.\n\n  8. 8.\n\nIf X is an S-node, visit all branches of node X to the right of path p from\nleft to right and breadth-first manner, but do not visit below any NP or S\nbeing encountered as the antecedent.\n\n  9. 9.\n\nReturn to Step 4.\n\n#### 7.5.2.3 Example of Using Hobbs\u2019 Algorithm\n\nStatement [7.65] is a classic example stated in Hobbs\u2019 original paper (Hobbs\n1978) to demonstrate how Hobbs\u2019 algorithm works as shown in Fig. 7.5.\n\n  * [7.65] _The castle in Camelot remained the residence of the king until 536 when he moved it to London_.\n\n![](../images/533412_1_En_7_Chapter/533412_1_En_7_Fig5_HTML.png)\n\nFig 7.5\n\nParse Tree for statement [7.65]\n\nExample\u2014What does _it_ stand for?\n\n  1. 1.\n\nStart with node NP1, step 2 climbs up to node S1.\n\n  2. 2.\n\nStep 3 searches the left part of S1\u2019s tree but fails to locate any eligible NP\nnode.\n\n  3. 3.\n\nStep 4 fails to apply.\n\n  4. 4.\n\nStep 5 climbs up to NP2 which step 6 proposes 536 as antecedent of _it_.\n\n  5. 5.\n\nThe algorithm can be further improved by applying simple selectional\nconstraints, such as\n\n    * _Date can\u2019t move_ ;\n\n    * _Places can\u2019t move_ ;\n\n    * _Large or fixed objects can\u2019t move_.\n\n  6. 6.\n\nAfter NP2 is rejected, steps 7 and 8 turn up nothing, and control is returned\nto step 4 which fails to apply.\n\n  7. 7.\n\nStep 5 climbs up to S2 which step 6 fails to apply.\n\n  8. 8.\n\nIn step 7, the breadth-first search recommends that NP3 _the castle_ is\nrejected by the constraint number 3.\n\n  9. 9.\n\nThe algorithm continues to visit NP4 where it correctly recommends _the\nresidence_ as antecedent.\n\nExercise: How to check coreference resolution of _he_ as _the king_?\n\n#### 7.5.2.4 Performance of Hobbs\u2019 Algorithm\n\nIn the original work, Hobbs manually analyzed 100 consecutive examples from\nthree different texts, assuming correct parsing was available, and the\nalgorithm was 72.7% correct (Hobbs 1978); which is quite impressive for such\nsimple algorithm. If the algorithm is integrated with syntactic constraints\nwhen resolving pronouns as shown in Fig. 7.5, the performance can be even\nhigher.\n\nHowever, Hobbs\u2019 algorithm experiences two major problems.\n\n  1. 1.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3a8ed41-5133-4ae7-923d-a565af99fa7e": {"__data__": {"id_": "e3a8ed41-5133-4ae7-923d-a565af99fa7e", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c4e0ddc-12fe-4500-b878-b6b87efed231", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "d936b6f2f57480f657a76ca11a11831ee573357b94ce6f1b17bcf92a1b7cb493", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a4d1058-b028-41b4-98ad-72370fe1093b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8e43abe89deb0aea20f904e52ec1eaa15cfa1d26c675e698b8e500f1e51270e6", "class_name": "RelatedNodeInfo"}}, "hash": "fa651f04b3ddebe1d488b1eb242b1dd99ecc31f0d1cdf0cd2c217cc038a0676e", "text": "8. 8.\n\nIn step 7, the breadth-first search recommends that NP3 _the castle_ is\nrejected by the constraint number 3.\n\n  9. 9.\n\nThe algorithm continues to visit NP4 where it correctly recommends _the\nresidence_ as antecedent.\n\nExercise: How to check coreference resolution of _he_ as _the king_?\n\n#### 7.5.2.4 Performance of Hobbs\u2019 Algorithm\n\nIn the original work, Hobbs manually analyzed 100 consecutive examples from\nthree different texts, assuming correct parsing was available, and the\nalgorithm was 72.7% correct (Hobbs 1978); which is quite impressive for such\nsimple algorithm. If the algorithm is integrated with syntactic constraints\nwhen resolving pronouns as shown in Fig. 7.5, the performance can be even\nhigher.\n\nHowever, Hobbs\u2019 algorithm experiences two major problems.\n\n  1. 1.\n\nWhen looking for the antecedent of a pronoun within a sentence, it goes\nsequentially further up the tree to the left of pronoun, such an error is\nlooked for in the previous sentence.\n\n  2. 2.\n\nThis algorithm does not assume a discourse segmentation structure and may\nrevert to arbitrarily far of the text to find an antecedent.\n\nNevertheless, as he concluded in his original paper, na\u00efve-based approach on\nco-reference resolution did provide a high baseline and works in many usual\nsituations in discourse analysis, and still being used as a benchmark in\nrelated CR research nowadays (Cornish 2009; Kehler et al. 2008; Lata et al.\n2022; Wolna et al. 2022).\n\n### 7.5.3 Centering Algorithm\n\n_Centering Theory_ _(CT)_ was proposed by Profs Barbara J. Grosz and Candace\nL. Sidner in their distinguished work _Attention, Intentions, and the\nStructure of Dis-Course_ , as part of its main theory of discourse analysis\n(Grosz and Sidner 1986). It is a theory of discourse structure that models the\ninterrelationships between foci or centers as the choice of reference terms\nand the perceived coherence of discourse.\n\nThe basic idea is:\n\n  1. 1.\n\na discourse has a focus, or center,\n\n  2. 2.\n\nthe center typically remains the same for a few sentences, then shifts to a\nnew object,\n\n  3. 3.\n\nthe center of a sentence is typically pronominalized,\n\n  4. 4.\n\nonce a center is determined, there is a strong inclination for subsequent\npronouns to continue referring to it.\n\nIn centering algorithm, utterances from a discourse have a _backward-looking\ncenter_ ( _C_ _b_ ) and a set of _forward-looking centers_ ( _C_ _f_ ). The Cf\nset of an utterance _U_ 0 is the set of utterance units elicited by that\nutterance. _C_ _f_ set is ranked by discourse emphasis, the most accepted\nranking is by grammatical role. The highest-ranked element in this list is\ncalled the _preferred center_ ( _C_ _p_ ), which represents the highest-ranked\nelement among previous utterances found in the current utterance and serves as\na link between these utterances. Any sudden shifts in the topic of utterances\nare reflected in changes in _C_ _b_ between utterances.\n\n#### 7.5.3.1 What Is Centering Algorithm?\n\nCentering algorithm (Grosz and Sidner 1986; Tetreault 2001) consists of three\nparts: (1) initial settings, (2) constraints, (3) rules and algorithm.\n\n#### 7.5.3.2 Part I: Initial Setting\n\n  * Let _U_ _n_ , _U_ _n_ +1 be 2 successive utterances.\n\n  * Backward-looking center of _U_ _n_ , written as _C_ _b_ ( _U_ _n_ ), denotes focus after _U_ _n_ is interpreted.\n\n  * Forward-looking centers of _U_ _n_ , written as _C_ _f_ ( _U_ _n_ ), forms ordered list of entities in _U_ _n_ that can serve as _C_ _b_ ( _U_ _n_ +1).\n\n  * _C_ _b_ ( _U_ _n_ +1) is the highest-ranking element of _C_ _f_ ( _U_ _n_ ) mentioned in _U_ _n_ +1.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a4d1058-b028-41b4-98ad-72370fe1093b": {"__data__": {"id_": "6a4d1058-b028-41b4-98ad-72370fe1093b", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3a8ed41-5133-4ae7-923d-a565af99fa7e", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "fa651f04b3ddebe1d488b1eb242b1dd99ecc31f0d1cdf0cd2c217cc038a0676e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b740b58a-71cd-4b92-bd5a-b07e2b25dc70", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "ebe11657f36ffd4b857a4bb71bb2592123b958d4ba9ea4808ddace3f14c095c1", "class_name": "RelatedNodeInfo"}}, "hash": "8e43abe89deb0aea20f904e52ec1eaa15cfa1d26c675e698b8e500f1e51270e6", "text": "#### 7.5.3.2 Part I: Initial Setting\n\n  * Let _U_ _n_ , _U_ _n_ +1 be 2 successive utterances.\n\n  * Backward-looking center of _U_ _n_ , written as _C_ _b_ ( _U_ _n_ ), denotes focus after _U_ _n_ is interpreted.\n\n  * Forward-looking centers of _U_ _n_ , written as _C_ _f_ ( _U_ _n_ ), forms ordered list of entities in _U_ _n_ that can serve as _C_ _b_ ( _U_ _n_ +1).\n\n  * _C_ _b_ ( _U_ _n_ +1) is the highest-ranking element of _C_ _f_ ( _U_ _n_ ) mentioned in _U_ _n_ +1.\n\n  * Order of entities in _C_ _f_ ( _U_ _n_ ): in which subject > existential predicate nominal > object > indirect object > demarcated adverbial PP.\n\n  * Let _C_ _p_ ( _U_ _n_ +1) be the highest-ranked forward-looking center.\n\n#### 7.5.3.3 Part II: Constraints\n\nFor each utterance _U_ _i_ ( _i_ = \u2026 _m_ ) in a discourse segment _D_ :\n\n  * There is precisely one _C_ _b_.\n\n  * Every element of _C_ _f_ -list for _U_ _i_ must be realized in _U_ _i_.\n\n  * The center, _C_ _b_ ( _U_ _i_ , _D_ ) is the highest-ranked element of _C_ _f_ ( _U_ _i_ -1, _D_ ) realized by _U_ _i_.\n\n#### 7.5.3.4 Part III: Rules and Algorithm\n\nFor each utterance _U_ _i_ ( _i_ = \u2026 _m_ ) in a discourse segment _D_ :\n\n  * Rule 1: If some elements of _C_ _f_ ( _U_ _i_ -1, _D_ ) is realized as a pronoun in _U_ _i_ , then so is _C_ _b_ ( _U_ _i_ , _D_ ).\n\n  * Rule 2: Transition states, defined as follows, are ordered such that the sequence of _Continue_ is preferred over the sequence of _Retains_ , which are preferred over _Smooth-Shift_ and then _Rough-Shift_.\n\nThe relationship between _C_ _b_ and _C_ _p_ of two utterances determines\ncoherence between words. Centering theory ranks the coherence of adjacent\nutterances with transitions determined by:\n\n  1. 1.\n\n_C_ _b_ is the same from _U_ _n_ -1 to _U_ _n_ or not\n\n  2. 2.\n\nthis entity coincides with _C_ _p_ of _U_ _n_ or not\n\nFigure 7.6 shows the criteria for each transition in centering algorithm.\n\n![](../images/533412_1_En_7_Chapter/533412_1_En_7_Fig6_HTML.png)\n\nFig. 7.6\n\nThe criteria for each transition in Centering Algorithm\n\nThe algorithm based on these rules and conditions is defined as follows:\n\n  1. 1.\n\nCreate all possible _C_ _b_ \u2013 _C_ _f_ combinations.\n\n  2. 2.\n\nFilter these combinations by constraints and centering rules.\n\n  3. 3.\n\nRank remaining combinations by transitions.\n\n#### 7.5.3.5 Example of Centering Algorithm\n\n  * _U_ 1: _Jane heard some beautiful music at the CD store._\n\n  * _U_ 2: _Jane played it to Mary._\n\n  * _U_ 3: _She bought it._\n\nBy applying grammatical role hierarchy to construct Cf. So, for U1 will have:\n\n  * _C_ _f_ ( _U_ 1): _{Jane, music, CD store}_\n\n  * _C_ _p_ ( _U_ 1): _Jane_\n\n  * _C_ _b_ ( _U_ 1): _Undefined_\n\n_U_ 2 has two pronouns: _She_ and _it_. _She_ is compatible (in syntax) with\n_Jane_ , while _it_ is compatible with either _music_ or _CD store_.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b740b58a-71cd-4b92-bd5a-b07e2b25dc70": {"__data__": {"id_": "b740b58a-71cd-4b92-bd5a-b07e2b25dc70", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a4d1058-b028-41b4-98ad-72370fe1093b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8e43abe89deb0aea20f904e52ec1eaa15cfa1d26c675e698b8e500f1e51270e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12bba127-6227-417f-a8a8-0b71e26447df", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "e22230ef541ee3f17c50490a85ac4742d4dc19912918e38e695ef71236c4d782", "class_name": "RelatedNodeInfo"}}, "hash": "ebe11657f36ffd4b857a4bb71bb2592123b958d4ba9ea4808ddace3f14c095c1", "text": "3.\n\nRank remaining combinations by transitions.\n\n#### 7.5.3.5 Example of Centering Algorithm\n\n  * _U_ 1: _Jane heard some beautiful music at the CD store._\n\n  * _U_ 2: _Jane played it to Mary._\n\n  * _U_ 3: _She bought it._\n\nBy applying grammatical role hierarchy to construct Cf. So, for U1 will have:\n\n  * _C_ _f_ ( _U_ 1): _{Jane, music, CD store}_\n\n  * _C_ _p_ ( _U_ 1): _Jane_\n\n  * _C_ _b_ ( _U_ 1): _Undefined_\n\n_U_ 2 has two pronouns: _She_ and _it_. _She_ is compatible (in syntax) with\n_Jane_ , while _it_ is compatible with either _music_ or _CD store_.\n\nSince _Jane_ is the highest _C_ _f_ ( _U_ 1) ranked member, _C_ _b_ ( _U_ 2)\nshould be referred to _Jane_ by comparing result transitions for every\npossible referent of _it_.\n\nIf _it_ is assumed to _music_ , the result will be:\n\n  * _C_ _f_ ( _U_ 2): _{Jane, music, Mary}_\n\n  * _C_ _p_ ( _U_ 2): _Jane_\n\n  * _C_ _b_ ( _U_ 2): _Jane_\n\nResult: _Continue_ (since _C_ _p_ ( _U_ 2) = _C_ _b_ ( _U_ 2) and _C_ _b_ (\n_U_ 1) is _undefined_ ).\n\nConversely, if _it_ is assumed to _CD store_ , the result will be:\n\n  * _C_ _f_ ( _U_ 2): _{Jane, CD store, Mary}_\n\n  * _C_ _p_ ( _U_ 2): _Jane_\n\n  * _C_ _b_ ( _U_ 2): _Jane_\n\nResult: _Continue_ (since _C_ _p_ ( _U_ 2) = _C_ _b_ ( _U_ 2) and _C_ _b_ (\n_U_ 1) is _undefined_ )\n\nAs both are _Continue_ , it will be set referring to _music_ instead of _CD\nstore_.\n\nNext, let us look at _U_ 3.\n\nFor U3, _She_ is compatible with either _Jane_ or _Mary_ , while _it_ is\ncompatible with _music_. So, if _she_ refers to _Jane_ , i.e., _C_ _b_ ( _U_\n3) = _Jane,_ the result will be:\n\n  * _C_ _f_ ( _U_ 3): _{Mary, music}_\n\n  * _C_ _p_ ( _U_ 3): _Mary_\n\n  * _C_ _b_ ( _U_ 3): _Mary_\n\nResult: _Smooth-Shift_ (since _C_ _p_ ( _U_ 3) = _C_ _b_ ( _U_ 3) but _C_ _b_\n( _U_ 3)\u2260 _C_ _b_ ( _U_ 2)).\n\nSince _Continue_ is preferred to _Smooth-shift_ using Rule 2, _Jane_ should be\nassigned as the referent, so Centering algorithm works in this situation.\n\n#### 7.5.3.6 Performance of Centering Algorithm\n\nClearly, centering algorithm implicitly accounts for grammatical roles,\nrecency, and repeated-mention preference in pronoun interpretation.\n\nHowever, the grammatical role hierarchy affects emphasis indirectly because\nthe final conversion type specifically determines the final reference\nassignment. Confusion can arise if the former lead to a high-level\ntransformation in this case, where a referent in a low-level grammatical role\nprefers a referent in a high-level role. For instance:\n\n  * _U_ 1: _Jane opened a new music store in the city._\n\n  * _U_ 2: _Mary entered the store and looked at some CDs._\n\n  * _U_ 3: _She finally bought some._\n\nIn this example, common sense indicates that _She_ in U3 should refer to\n_Mary_ instead of _Jane_.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12bba127-6227-417f-a8a8-0b71e26447df": {"__data__": {"id_": "12bba127-6227-417f-a8a8-0b71e26447df", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b740b58a-71cd-4b92-bd5a-b07e2b25dc70", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "ebe11657f36ffd4b857a4bb71bb2592123b958d4ba9ea4808ddace3f14c095c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d09f8ab-0693-484b-9233-805f358333f9", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "a25974ba9dbc6cc01b75894c7e308dfd17b8634d4b93e7a53977b3ecd1d2b8d9", "class_name": "RelatedNodeInfo"}}, "hash": "e22230ef541ee3f17c50490a85ac4742d4dc19912918e38e695ef71236c4d782", "text": "#### 7.5.3.6 Performance of Centering Algorithm\n\nClearly, centering algorithm implicitly accounts for grammatical roles,\nrecency, and repeated-mention preference in pronoun interpretation.\n\nHowever, the grammatical role hierarchy affects emphasis indirectly because\nthe final conversion type specifically determines the final reference\nassignment. Confusion can arise if the former lead to a high-level\ntransformation in this case, where a referent in a low-level grammatical role\nprefers a referent in a high-level role. For instance:\n\n  * _U_ 1: _Jane opened a new music store in the city._\n\n  * _U_ 2: _Mary entered the store and looked at some CDs._\n\n  * _U_ 3: _She finally bought some._\n\nIn this example, common sense indicates that _She_ in U3 should refer to\n_Mary_ instead of _Jane_. However, by applying Centering algorithm in this\ncase, it will assign _she_ to _Jane_ incorrectly because _C_ _b_ ( _U_ 2) =\n_Jane_ becomes _Continue_ while _Mary_ becomes a _Smooth-shift_. While if\napplying Hobbs\u2019 algorithm, _Mary_ will still be assigned as the referent.\n\nObviously, such situation occurs usually depended on situation and thematic\nscenario. As Prof. Marilyn A. Walker in her study _A corpus-based evaluation\nof centering and pronoun resolution_ (Walker 1989) compared a version of\nCentering to Hobbs on 281 examples from three genres of text in 1989 with\n77.6% and 81.8% accuracy, respectively.\n\n### 7.5.4 Machine Learning Method\n\n#### 7.5.4.1 What is Machine Learning Method?\n\n_Machine learning (ML)_ method is a simple supervised machine learning by\neither using stochastic or AI approach. It trains classifier by using manual\nlabelled corpus markers: (1) positive samples are antecedents marked with each\npronoun and (2) negative (derived) samples are pairing pronouns with non-\nantecedent NPs.\n\nIn a typical supervised ML scenario, ML system trains on a set of features and\nproduces a pro-antecedent pair to predict _1_ if they co-refer and 0\notherwise. A typical example by applying _Log-Linear model_ for pronominal\nanaphora resolution is introduced with the following features:\n\n  * Strict number [ _true or false_ ]\n\n  * Compatible number [ _true or false_ ]\n\n  * Strict gender [ _true or false_ ]\n\n  * Compatible gender [ _true or false_ ]\n\n  * Sentence distance [ _0, 1, 2, 3, \u2026_ ] from pronoun\n\n  * Hobbs\u2019 distance [ _0, 1, 2, 3, \u2026_ ] (non-groups)\n\n  * Grammatical role [ _subject, object, PP_ ] (taken by potential antecedent)\n\n  * Linguistic form [ _definite, indefinite and proper pronouns_ ]\n\nExample for Pronominal Anaphora Resolution:\n\n  * _U_ 1: _Jack saw a beautiful Mercedes GLB300 at a used car dealership_.\n\n  * _U_ 2: _He showed it to Jim_.\n\n  * _U_ 3: _He bought it_.\n\nA table of feature vector values for sentence U2 is shown in Fig. 7.7.\n\n![](../images/533412_1_En_7_Chapter/533412_1_En_7_Fig7_HTML.png)\n\nFig 7.7\n\nTable of feature vector values for sentence _U_ _2_ _: He showed it to Jim_\n\n#### 7.5.4.2 Performance of Log-Linear Model\n\nA _Log-Linear model_ trains on vectors and filters out pleonastic _it_ as in\n_it is raining_. It results in weights for each and the combination of\nfeatures. Most of the time it is rigid, harder and must decide if any two noun\nphrases co-refer.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d09f8ab-0693-484b-9233-805f358333f9": {"__data__": {"id_": "9d09f8ab-0693-484b-9233-805f358333f9", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12bba127-6227-417f-a8a8-0b71e26447df", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "e22230ef541ee3f17c50490a85ac4742d4dc19912918e38e695ef71236c4d782", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2792e8e0-2019-403f-9636-67e2cc02a0cc", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "552549b39481dddd97005535cf190fb8b43dc89b8594d16562c64f4265e1c091", "class_name": "RelatedNodeInfo"}}, "hash": "a25974ba9dbc6cc01b75894c7e308dfd17b8634d4b93e7a53977b3ecd1d2b8d9", "text": "* _U_ 2: _He showed it to Jim_.\n\n  * _U_ 3: _He bought it_.\n\nA table of feature vector values for sentence U2 is shown in Fig. 7.7.\n\n![](../images/533412_1_En_7_Chapter/533412_1_En_7_Fig7_HTML.png)\n\nFig 7.7\n\nTable of feature vector values for sentence _U_ _2_ _: He showed it to Jim_\n\n#### 7.5.4.2 Performance of Log-Linear Model\n\nA _Log-Linear model_ trains on vectors and filters out pleonastic _it_ as in\n_it is raining_. It results in weights for each and the combination of\nfeatures. Most of the time it is rigid, harder and must decide if any two noun\nphrases co-refer.\n\nNew features can be added to improve model performance such as:\n\n  * Anaphor edits distance\n\n  * Antecedent edits distance\n\n  * Alias [ _true or false_ ] (based on the named entity tagger)\n\n  * Appositive [ _true or false_ ]\n\n  * Linguistic form [ _proper, definite, indefinite, pronoun_ ]\n\n#### 7.5.4.3 Other Advanced Machine Learning Models\n\nBig data and AI offer advancement for current machine learning models CR\nresearch focus on Convolutional Neural Networks (CNN) (Auliarachman and\nPurwarianti 2019), Recurrent Neural Networks (RNN) (Afsharizadeh et al. 2021),\nLong-short Term Memory Networks (LSTM) (Li et al. 2021), Transformers and BERT\nModels (Joshi et al. 2019).\n\n## 7.6 Evaluation\n\nFrom performance perspective, commonly used methods emphasis on coreference\nchains evaluation as forming a set of facts _A_ , _B_ , and _C_ are assigned\nwith _A_ , _B_ , and _C_ classes. They consist of two data types: (1)\nreference/true chain is correct or true coreference chain occurred in an\nentity and (2) hypothesis chain/class is assigned with the entity by a\ncoreference algorithm.\n\nFor instance, _Precision_ of the system can be evaluated according to:\n\n![$$ \\\\frac{\\\\mathrm{weighted}\\\\;\\\\mathrm{sum}\\\\;\\\\mathrm{of}\\\\\n\\\\mathrm{correct}\\\\ \\\\mathrm{elments}\\\\ \\\\mathrm{in}\\\\ \\\\mathrm{hypothesis}\\\\\n\\\\mathrm{chain}}{\\\\mathrm{Number}\\\\ \\\\mathrm{of}\\\\ \\\\mathrm{elements}\\\\\n\\\\mathrm{in}\\\\ \\\\mathrm{hypothesis}\\\\ \\\\mathrm{chain}}\n$$](../images/533412_1_En_7_Chapter/533412_1_En_7_Chapter_TeX_Equ2.png)\n\n(7.2)\n\nand _Recall_ can be evaluated according to:\n\n![$$ \\\\frac{\\\\mathrm{Number}\\\\ \\\\mathrm{of}\\\\ \\\\mathrm{correct}\\\\\n\\\\mathrm{elements}\\\\ \\\\mathrm{in}\\\\ \\\\mathrm{hypothesis}\\\\\n\\\\mathrm{chain}}{\\\\mathrm{Number}\\\\ \\\\mathrm{of}\\\\ \\\\mathrm{elements}\\\\\n\\\\mathrm{in}\\\\ \\\\mathrm{reference}\\\\ \\\\mathrm{chain}}\n$$](../images/533412_1_En_7_Chapter/533412_1_En_7_Chapter_TeX_Equ3.png)\n\n(7.3)\n\nLike previous chapters on N-gram and Semantic Analysis, CR model evaluation\ncan be achieved by using: (1) intrinsic (using prototype and model itself) vs\n(2) extrinsic (task-based, end-to-end) evaluation schemes.\n\nExercises\n\n  1. 7.1\n\nWhat is _pragmatic analysis_ and _discourse_ in linguistics? Discuss their\nroles and importance in NLP.\n\n  2. 7.2\n\nWhat is the difference between _pragmatic analysis_ and _semantic analysis_ in\nterms of their functions and roles in NLU ( _Natural Language Understanding_\n)?\n\n  3. 7.3\n\nWhat is _coreference resolution_ in linguistics? Why it is important in NLP?\nUse two live examples as illustration to support your answer.\n\n  4. 7.4\n\nState and explain the differences between the concept of _coherence_ vs\n_coreference_ in _pragmatic analysis_. Give two live examples to support your\nanswer.\n\n  5. 7.5\n\nWhat is _discourse_ _segmentation_?", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2792e8e0-2019-403f-9636-67e2cc02a0cc": {"__data__": {"id_": "2792e8e0-2019-403f-9636-67e2cc02a0cc", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d09f8ab-0693-484b-9233-805f358333f9", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "a25974ba9dbc6cc01b75894c7e308dfd17b8634d4b93e7a53977b3ecd1d2b8d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1fbf18af-77b0-48ad-a125-081ab07d30ce", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "78543d133bf9313ac8f573d75e22e1084750cb3510d4c158ed79bf2e07106350", "class_name": "RelatedNodeInfo"}}, "hash": "552549b39481dddd97005535cf190fb8b43dc89b8594d16562c64f4265e1c091", "text": "Exercises\n\n  1. 7.1\n\nWhat is _pragmatic analysis_ and _discourse_ in linguistics? Discuss their\nroles and importance in NLP.\n\n  2. 7.2\n\nWhat is the difference between _pragmatic analysis_ and _semantic analysis_ in\nterms of their functions and roles in NLU ( _Natural Language Understanding_\n)?\n\n  3. 7.3\n\nWhat is _coreference resolution_ in linguistics? Why it is important in NLP?\nUse two live examples as illustration to support your answer.\n\n  4. 7.4\n\nState and explain the differences between the concept of _coherence_ vs\n_coreference_ in _pragmatic analysis_. Give two live examples to support your\nanswer.\n\n  5. 7.5\n\nWhat is _discourse_ _segmentation_? State and explain why it is vital to\n_pragmatic analysis_ and the implementation of NLP application such _Q &A\nchatbot_. Give two examples to support your answer.\n\n  6. 7.6\n\nState and explain _Hearst\u2019s TextTiling_ technique on _discourse_\n_segmentation_. How can it be further improved by using nowadays\u2019 AI and\nmachine learning technology?\n\n  7. 7.7\n\nWhat is _coherence_ _relation_? State and explain five basic types of\n_coherence relations_. For each type, give a live example for illustration.\n\n  8. 7.8\n\nWhat is _referencing expression_ in _pragmatic analysis_? State and explain\nfive basic types of _referencing expressions_. For each type, please provide a\nlive example for illustration.\n\n  9. 7.9\n\nState and explain _Hobbs\u2019 algorithm_ for coreference resolution. Use a sample\nsentence/utterance (other than the one given in the book) to illustrate how it\nworks.\n\n  10. 7.10\n\nState and explain the pros and cons of _Hobbs\u2019 algorithms_ for _coreference\nresolution_. Use live example(s) to support your answer.\n\n  11. 7.11\n\nState and explain _Centering algorithm_ for coreference resolution. Use a\nsample sentence/utterance (other than the one given in the book) to illustrate\nhow it works.\n\n  12. 7.12\n\nCompare pros and cons between _Hobbs\u2019 algorithm_ vs _Centering algorithm_. Use\nlive example(s) to support your answer.\n\n  13. 7.13\n\nWhat is _machine learning_? State and explain how _machine learning_ can be\nused for coreference resolution. Use live example(s) to support your answer.\n\n  14. 7.14\n\nName any three types of _machine learning_ models for _coreference\nresolution_. State and explain how they work.\n\n  15. 7.15\n\nName any two types of evaluation method/metrics for _coreference resolution_\nmodel in _pragmatic analysis_. State and explain how they work.\n\nReferences\n\n  1. Afsharizadeh, M., Ebrahimpour-Komleh, H., and Bagheri, A. (2021). Automatic text summarization of COVID-19 research articles using recurrent neural networks and coreference resolution. Frontiers in Biomedical Technologies. [https://\u200bdoi.\u200borg/\u200b10.\u200b18502/\u200bfbt.\u200bv7i4.\u200b5321](https://doi.org/10.18502/fbt.v7i4.5321)\n\n  2. Auliarachman, T., & Purwarianti, A. (2019). Coreference resolution system for Indonesian text with mention pair method and singleton exclusion using convolutional neural network. Paper presented at the 1-5. [https://\u200bdoi.\u200borg/\u200b10.\u200b1109/\u200bICAICTA.\u200b2019.\u200b8904261](https://doi.org/10.1109/ICAICTA.2019.8904261)\n\n  3. Bender, E. M. and Lascarides, A. (2019) Linguistic Fundamentals for Natural Language Processing II: 100 Essentials from Semantics and Pragmatics (Synthesis Lectures on Human Language Technologies). Springer.\n\n  4. Cornish, F. (2009). Inter-sentential anaphora and coherence relations in discourse: A perfect match. Language Sciences (Oxford), 31(5), 572-592.\n\n  5. Cruse, A.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1fbf18af-77b0-48ad-a125-081ab07d30ce": {"__data__": {"id_": "1fbf18af-77b0-48ad-a125-081ab07d30ce", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2792e8e0-2019-403f-9636-67e2cc02a0cc", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "552549b39481dddd97005535cf190fb8b43dc89b8594d16562c64f4265e1c091", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "623f002c-a899-46c5-9873-74a19770c592", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446523457480f2072ae02491c02e4de22dff72490f5591d2894a37837daf91e4", "class_name": "RelatedNodeInfo"}}, "hash": "78543d133bf9313ac8f573d75e22e1084750cb3510d4c158ed79bf2e07106350", "text": "Coreference resolution system for Indonesian text with mention pair method and singleton exclusion using convolutional neural network. Paper presented at the 1-5. [https://\u200bdoi.\u200borg/\u200b10.\u200b1109/\u200bICAICTA.\u200b2019.\u200b8904261](https://doi.org/10.1109/ICAICTA.2019.8904261)\n\n  3. Bender, E. M. and Lascarides, A. (2019) Linguistic Fundamentals for Natural Language Processing II: 100 Essentials from Semantics and Pragmatics (Synthesis Lectures on Human Language Technologies). Springer.\n\n  4. Cornish, F. (2009). Inter-sentential anaphora and coherence relations in discourse: A perfect match. Language Sciences (Oxford), 31(5), 572-592.\n\n  5. Cruse, A. (2011) Meaning in Language: An Introduction to Semantics and Pragmatics (Oxford Textbooks in Linguistics). Oxford University Press\n\n  6. Doyle, A. C. (2019) The Adventures of Sherlock Holmes (AmazonClassics Edition). AmazonClassics.\n\n  7. Goddard, C. (1998) Semantic Analysis: A Practical Introduction (Oxford Textbooks in Linguistics). Oxford University Press.\n\n  8. Grosz, B. J., Joshi, A. K., and Weinstein, S. (1995). Centering: A framework for modeling the local coherence of discourse. Computational Linguistics - Association for Computational Linguistics, 21(2), 203-225.\n\n  9. Grosz, B. J., and Sidner, C. L. (1986). Attention, intentions, and the structure of discourse. Computational Linguistics - Association for Computational Linguistics, 12(3), 175-204.\n\n  10. Hearst, M. A. (1997). TextTiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics - Association for Computational Linguistics, 23(1), 33-64.\n\n  11. Hobbs, J. R. (1979) Coherence and Coreference. Cognitive Science 3, 67-90.\n\n  12. Hobbs, J. R. (1978) Resolving pronoun references. Lingua, 44:311\u2013338.\n\n  13. Joshi, M., Levy, O., Weld, D.S., and Zettlemoyer, L. (2019) BERT for Coreference Resolution: Baselines and Analysis. In Proc. of Empirical Methods in Natural Language Processing (EMNLP) 2019. [https://\u200bdoi.\u200borg/\u200b10.\u200b48550/\u200barXiv.\u200b1908.\u200b09091](https://doi.org/10.48550/arXiv.1908.09091)\n\n  14. Kehler, A. (2002) Coherence, Reference, and the Theory of Grammar. Stanford, Calif.: CSLI Publishers.\n\n  15. Kehler, A., Kertz, L., Rohde, H., and Elman, J. L. (2008). Coherence and coreference revisited. Journal of Semantics (Nijmegen), 25(1), 1-44.\n\n  16. Kroeger, P. (2019) Analyzing meaning: An introduction to semantics and pragmatics (Textbooks in Language Sciences). Freie Universit\u00e4t Berlin.\n\n  17. Lata, K., Singh, P., & Dutta, K. (2022). Mention detection in coreference resolution: Survey. Applied Intelligence (Dordrecht, Netherlands), 52(9), 9816-9860.\n\n  18. Li, Y., Ma, X., Zhou, X., Cheng, P., He, K. and Li, C. (2021). Knowledge enhanced LSTM for coreference resolution on biomedical texts. Bioinformatics, 37(17), 2699-2705. [https://\u200bdoi.\u200borg/\u200b10.\u200b1093/\u200bbioinformatics/\u200bbtab153](https://doi.org/10.1093/bioinformatics/btab153)\n\n  19. Mann, W. C. and Thompson, S. A.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "623f002c-a899-46c5-9873-74a19770c592": {"__data__": {"id_": "623f002c-a899-46c5-9873-74a19770c592", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1fbf18af-77b0-48ad-a125-081ab07d30ce", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "78543d133bf9313ac8f573d75e22e1084750cb3510d4c158ed79bf2e07106350", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ffda6ceb-fd17-4d46-a30c-fd937a1d9d9d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "84bc7e46be319f66cf0e6df21aab064f0959967c32f23135ece1d07de63591f3", "class_name": "RelatedNodeInfo"}}, "hash": "446523457480f2072ae02491c02e4de22dff72490f5591d2894a37837daf91e4", "text": "Freie Universit\u00e4t Berlin.\n\n  17. Lata, K., Singh, P., & Dutta, K. (2022). Mention detection in coreference resolution: Survey. Applied Intelligence (Dordrecht, Netherlands), 52(9), 9816-9860.\n\n  18. Li, Y., Ma, X., Zhou, X., Cheng, P., He, K. and Li, C. (2021). Knowledge enhanced LSTM for coreference resolution on biomedical texts. Bioinformatics, 37(17), 2699-2705. [https://\u200bdoi.\u200borg/\u200b10.\u200b1093/\u200bbioinformatics/\u200bbtab153](https://doi.org/10.1093/bioinformatics/btab153)\n\n  19. Mann, W. C. and Thompson, S. A. (1988) Rhetorical Structure Theory: Toward a functional theory of text organization. Text & Talk, 8, 243 - 281.\n\n  20. Mann, W. C. and Thompson S. A. (1986) Relational Propositions in Discourse. Discourse Processes 9: 57-90.\n\n  21. Pevzner, L., and Hearst, M. A. (2002). A critique and improvement of an evaluation metric for text segmentation. Computational Linguistics - Association for Computational Linguistics, 28(1), 19-36.\n\n  22. Sanders, T., Spooren, W. and Noordman, L.G. (1992). Toward a taxonomy of coherence relations. Discourse Processes, 15, 1-35.\n\n  23. Tetreault, J. R. (2001). A corpus-based evaluation of centering and pronoun resolution. Computational Linguistics. Association for Computational Linguistics, 27(4), 507-520.\n\n  24. Walker, Marilyn A. (1989). Evaluating discourse processing algorithms. In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, pp. 251-261.\n\n  25. Wolna, A., Durlik, J., and Wodniecka, Z. (2022). Pronominal anaphora resolution in polish: Investigating online sentence interpretation using eye-tracking. PloS One, 17(1), e0262459-e0262459.\n\n\n\u00a9 The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.\n2024\n\nR. S. T. Lee Natural Language Processing\n<https://doi.org/10.1007/978-981-99-1999-4_8>\n\n# 8\\. Transfer Learning and Transformer Technology\n\nRaymond S. T. Lee 1\n\n(1)\n\nUnited International College, Beijing Normal University-Hong Kong Baptist\nUniversity, Zhuhai, China\n\n## 8.1 What Is Transfer Learning?\n\nTransfer learning focuses in solving a problem from acquired knowledge and\napplying such knowledge to solve another related problem(s) (Pan and Yang\n2009; Weiss et al. 2016; Zhuang et al. 2020). It is like two students learn to\nplay guitar. One has musical knowledge and the other has not. It is natural\nfor the one to transfer background knowledge to the learning process. Every\ntask has its isolated datasets and trained model in traditional ML, whereas\nlearning a new task in TL relies on previous learned tasks to acquire\nknowledge with larger datasets as shown in Fig. 8.1.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig1_HTML.png)\n\nFig. 8.1\n\nTraditional machine learning vs transfer learning\n\n## 8.2 Motivation of Transfer Learning\n\nTraditional ML datasets and trained model parameters cannot reuse. They\ninvolve enormous, rare, inaccessible, time-consuming, and costly training\nprocess in NLP tasks and computer vision. For example, if a task is text\nsentiment reviews predictions on laptops, there are large amounts of labeled\ndata, target data, and training data from these reviews.\n\nTraditional ML can work well on correlated domains, but when there are large\namounts of target data like food reviews, the inference results will be\nunsatisfactory due to domains differences. Nevertheless, these domains are\ncorrelated in some sense to bear same domain reviews as language\ncharacteristics and terminology expressions, which makes TL possible to apply\nin a high-level approach on prediction task.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ffda6ceb-fd17-4d46-a30c-fd937a1d9d9d": {"__data__": {"id_": "ffda6ceb-fd17-4d46-a30c-fd937a1d9d9d", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "623f002c-a899-46c5-9873-74a19770c592", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446523457480f2072ae02491c02e4de22dff72490f5591d2894a37837daf91e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbcf7042-6fcb-4267-8a07-f3e784a867af", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "261cce08ebf8277ac4a7ea335c1c3aa213f63853a7d2e67e02fee4b163b40850", "class_name": "RelatedNodeInfo"}}, "hash": "84bc7e46be319f66cf0e6df21aab064f0959967c32f23135ece1d07de63591f3", "text": "8.1\n\nTraditional machine learning vs transfer learning\n\n## 8.2 Motivation of Transfer Learning\n\nTraditional ML datasets and trained model parameters cannot reuse. They\ninvolve enormous, rare, inaccessible, time-consuming, and costly training\nprocess in NLP tasks and computer vision. For example, if a task is text\nsentiment reviews predictions on laptops, there are large amounts of labeled\ndata, target data, and training data from these reviews.\n\nTraditional ML can work well on correlated domains, but when there are large\namounts of target data like food reviews, the inference results will be\nunsatisfactory due to domains differences. Nevertheless, these domains are\ncorrelated in some sense to bear same domain reviews as language\ncharacteristics and terminology expressions, which makes TL possible to apply\nin a high-level approach on prediction task. This approach enables source\ndomains to become a target domain and determine its sub-domains correlations\nas shown in Fig. 8.2.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig2_HTML.png)\n\nFig. 8.2\n\nTransfer learning\n\nTL has been implemented to several machine learning applications such as image\nand text sentiment classifications.\n\n### 8.2.1 Categories of Transfer Learning\n\nDomain is to be assigned with a definition by feature space _X_ and marginal\nprobability distribution _P_ ( _X_ ) where _X_ = { _x_ 1, _x_ 2, _x_ 3, \u2026, _x_\n_n_ } \u2208 _X_.\n\nIf a feature space _X_ and distribution _P_ ( _X_ ) between two domains are\ndifferent, they are different domains.\n\nIf a task is defined by a label space _Y_ with a predictive function _f_ (\u22c5),\n_f_ (\u22c5) is represented by a conditional probability distribution given by\n(8.1)\n\n![$$ f\\\\left\\({x}_i\\\\right\\)=P\\\\left\\({y}_i|{x}_i\\\\right\\)\n$$](../images/533412_1_En_8_Chapter/533412_1_En_8_Chapter_TeX_Equ1.png)\n\n(8.1)\n\nIf a function _f_ (\u22c5) and label space _Y_ between two tasks are different,\nthey are different tasks.\n\nNow TL can give a new representation by above definitions that have _D_ _s_ as\nsource domain and _T_ _s_ as source learning task. _D_ _t_ represents target\ndomain and _T_ _t_ represents target learning task. Given two domains are\nunidentical or has two different tasks, TL aim is to improve the results _P_ (\n_Y_ _t_ | _X_ _t_ ) of _D_ _t_ when _T_ _s_ and _D_ _s_ knowledge can be\nobtained.\n\nThere are two types of TL (1) heterogeneous and (2) homogeneous as shown in\nFig. 8.3.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig3_HTML.png)\n\nFig. 8.3\n\nTwo categories of transfer learning\n\nHeterogeneous Transfer Learning\n\nWhen source feature space and feature space are different which means that _Y_\n_t_ \u2260 _Y_ _s_ and/or _X_ _t_ \u2260 _X_ _s_. Under the condition of same domain\ndistributions, the strategy of resolution is to adjust feature space smaller\nand transform it to homogeneous so that the differences between marginal or\nconditional of source and target domains will be reduced.\n\nHomogeneous Transfer Learning\n\nWhen there are conditions _X_ _t_ = _X_ _s_ and _Y_ _t_ = _Y_ _s_ , the\ndifference of two domains lies on data distributions. Three strategies are\ncommonly used to tackle homogenous TL problems: (1) reduction the differences\nof _P_ ( _X_ _t_ ) \u2260 _P_ ( _X_ _s_ ), (2) reduction the differences of _P_ (\n_Y_ _t_ | _X_ _t_ ) \u2260 _P_ ( _Y_ _s_ | _X_ _s_ ), and (3) the combination of\nstrategies (1) and (2).\n\n## 8.3 Solutions of Transfer Learning\n\nThere are four methods to solve problems produced by homogeneous and\nheterogeneous TL: (1) instance-based, (2) feature-based, (3) parameter-based,\nand (4) relational-based.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fbcf7042-6fcb-4267-8a07-f3e784a867af": {"__data__": {"id_": "fbcf7042-6fcb-4267-8a07-f3e784a867af", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ffda6ceb-fd17-4d46-a30c-fd937a1d9d9d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "84bc7e46be319f66cf0e6df21aab064f0959967c32f23135ece1d07de63591f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5b7195c-ce65-4f18-9596-aa26e8a7dadf", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "2cd292d4102ecec48e090d846f042f3abc3f78cde757feee27fd9d89c6633ce7", "class_name": "RelatedNodeInfo"}}, "hash": "261cce08ebf8277ac4a7ea335c1c3aa213f63853a7d2e67e02fee4b163b40850", "text": "Three strategies are\ncommonly used to tackle homogenous TL problems: (1) reduction the differences\nof _P_ ( _X_ _t_ ) \u2260 _P_ ( _X_ _s_ ), (2) reduction the differences of _P_ (\n_Y_ _t_ | _X_ _t_ ) \u2260 _P_ ( _Y_ _s_ | _X_ _s_ ), and (3) the combination of\nstrategies (1) and (2).\n\n## 8.3 Solutions of Transfer Learning\n\nThere are four methods to solve problems produced by homogeneous and\nheterogeneous TL: (1) instance-based, (2) feature-based, (3) parameter-based,\nand (4) relational-based.\n\n  1. 1.\n\nInstance-based\n\nThis method reweights samples from source domains and uses them as target\ndomain data to bridge the gap of marginal distribution differences which works\nbest when conditional distributions of two tasks are equal.\n\n  2. 2.\n\nFeature-based\n\nThis method works for both heterogeneous and homogeneous TL problems. For\nhomogeneous types is to bridge the gap between conditional and marginal\ndistributions of target and source domains. For heterogeneous types is to\nreduce the differences between source and target features spaces. It has two\napproaches (a) asymmetric and (b) symmetric.\n\n    1. (a)\n\nAsymmetric feature transformation aims to modify the source domain and reduce\nthe gap between source and target instances by transforming one of the source\nand target domains to the other as shown in Fig. 8.4. It can be applied when\n_Y_ _s_ and _Y_ _t_ are identical.\n\n    2. (b)\n\nSymmetric feature transformation aims to transform source and target domains\ninto their shared feature space, starting from the idea of discovering\nmeaningful structures between domains. The feature space they share is usually\nlow-dimensional. The purpose of this approach is to reduce the marginal\ndistribution distance between destination and source. The difference between\nsymmetric and symmetric feature transformation is shown in Fig. 8.5.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig4_HTML.png)\n\nFig. 8.4\n\nAsymmetric feature transformation\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig5_HTML.png)\n\nFig. 8.5\n\nSymmetric feature transformation (left) and asymmetric feature transformation\n(right)\n\n  1. 3.\n\nParameter-based\n\nThis method transfers learnt knowledge by sharing parameters common to the\nmodels of source and target learners. It applies to the idea that two related\ntasks have similarity in model structure. The trained model is transferred\nfrom source domain to target domain with parameters. This approach has a huge\nadvantage because the parameters are usually trained from randomly initialized\nparameters as training process can be time-consuming for models trained from\nthe beginning. This approach can train more than one model on the source data\nand combine parameters learnt from all models to improve results of the target\nlearner. It is often used in deep learning applications as shown in Fig. 8.6.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig6_HTML.png)\n\nFig. 8.6\n\nParameter-based methods\n\n  1. 4.\n\nRelational-based\n\nThis method transfers learnt knowledge by sharing its learnt relations between\ndifferent samples parts of source and target domains as shown in Fig. 8.7.\nFood and movie domains are a related domain example. Although the reviews\ntexts are different, sentence structures are similar. It aims to transfer\nlearnt relations of different review sentences parts from these domains to\nimprove text sentiment analysis results.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig7_HTML.png)\n\nFig. 8.7\n\nRelational-based approaches: an example of learning sentence structure of food\nreviews to help with movie reviews\u2019 sentiment analysis\n\n## 8.4 Recurrent Neural Network (RNN)\n\n### 8.4.1 What Is RNN?\n\n_Recurrent neural network (_ _RNN_ _)_ is a class of artificial neural\nnetworks (ANNs) to consider time series or sequential data as input and use\nthem prior inputs to produce current input and output (Cho et al. 2014;\nSherstinsky 2020; Yin et al. 2017).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a5b7195c-ce65-4f18-9596-aa26e8a7dadf": {"__data__": {"id_": "a5b7195c-ce65-4f18-9596-aa26e8a7dadf", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fbcf7042-6fcb-4267-8a07-f3e784a867af", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "261cce08ebf8277ac4a7ea335c1c3aa213f63853a7d2e67e02fee4b163b40850", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb6798d7-574f-47f4-a964-fbe2cf227d40", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "4a2c2106698004b3c1568347bb7b3f5d785d2413dbdd153ccb67323611c18766", "class_name": "RelatedNodeInfo"}}, "hash": "2cd292d4102ecec48e090d846f042f3abc3f78cde757feee27fd9d89c6633ce7", "text": "Although the reviews\ntexts are different, sentence structures are similar. It aims to transfer\nlearnt relations of different review sentences parts from these domains to\nimprove text sentiment analysis results.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig7_HTML.png)\n\nFig. 8.7\n\nRelational-based approaches: an example of learning sentence structure of food\nreviews to help with movie reviews\u2019 sentiment analysis\n\n## 8.4 Recurrent Neural Network (RNN)\n\n### 8.4.1 What Is RNN?\n\n_Recurrent neural network (_ _RNN_ _)_ is a class of artificial neural\nnetworks (ANNs) to consider time series or sequential data as input and use\nthem prior inputs to produce current input and output (Cho et al. 2014;\nSherstinsky 2020; Yin et al. 2017). RNN has _memory_ which means its output is\ninfluenced by prior elements of the sequence against traditional Feedforward\nNeural Network (FNN) with independent inputs and outputs as shown in Fig. 8.8.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig8_HTML.png)\n\nFig. 8.8\n\nRecurrent neural network(left) vs. feedforward neural network(right)\n\n### 8.4.2 Motivation of RNN\n\nThere are many learning tasks required: sequential data processing which\nincludes speech recognition, image captioning, and synced sequence in video\nclassification. Sentiment analysis and machine translation model outputs are\nsequences, but tasks inputs are time or space related that cannot be modeled\nby traditional neural networks to assume that test and training data are\nindependent.\n\nFor example, a language translation task aims to translate a phrase _feel\nunder the weather_ means _unwell_. This phrase makes sense only when it is\nexpressed in that specific order. Thus, the positions of each word in sentence\nmust be considered when model predicts the next word.\n\nThere are five major categories of RNN architecture corresponding to different\ntasks: (1) simple one to one model for image classification task, (2) one to\nmany for image captioning tasks, (3) many to one model for sentiment analysis\ntasks, (4) many to many models for machine translation, and (5) complex many\nto many models for video classification tasks as shown in Fig. 8.9.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig9_HTML.png)\n\nFig. 8.9\n\n5 major types of RNNs\n\n### 8.4.3 RNN Architecture\n\nRNN is like standard neural networks consists of input, hidden, and output\nlayers as shown in Fig. 8.10.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig10_HTML.png)\n\nFig. 8.10\n\nBasic architecture of RNN\n\nAn unfolded RNN architecture is narrated by _x_ _t_ as the input at time step\n_t_ , _s_ _t_ stores the values of hidden units/states at time _t_ and _o_ _t_\nis the output of network at time-step _t_. _U_ are weights of inputs, _Ws_ are\nweights of hidden units, _V_ is bias as shown in Fig. 8.11.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig11_HTML.png)\n\nFig. 8.11\n\nUnfolded RNN architecture\n\nwith the activation function _f_ , the hidden states _s_ _t_ is calculated by\nequation:\n\n![$$ {s}_t=f\\\\left\\(U{x}_t+W{s}_{t-1}\\\\right\\)\n$$](../images/533412_1_En_8_Chapter/533412_1_En_8_Chapter_TeX_Equ2.png)\n\n(8.2)\n\nthe output of each recurrent layer _o_ _t_ is calculated by equation:\n\n![$$ {o}_t= softmax\\\\left\\(V{s}_t\\\\right\\)\n$$](../images/533412_1_En_8_Chapter/533412_1_En_8_Chapter_TeX_Equ3.png)\n\n(8.3)\n\nThe hidden states _s_ _t_ are considered as network memory units which\nconsists of hidden states from several former layers. Each layer\u2019s output is\nonly related to hidden states of the current layer.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb6798d7-574f-47f4-a964-fbe2cf227d40": {"__data__": {"id_": "fb6798d7-574f-47f4-a964-fbe2cf227d40", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5b7195c-ce65-4f18-9596-aa26e8a7dadf", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "2cd292d4102ecec48e090d846f042f3abc3f78cde757feee27fd9d89c6633ce7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1942889-c301-4630-a8e0-b56f5bacfa05", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f38dd483b453309a71045a65cf6ba2686dd925d0f922be42d31a8315fb4afd88", "class_name": "RelatedNodeInfo"}}, "hash": "4a2c2106698004b3c1568347bb7b3f5d785d2413dbdd153ccb67323611c18766", "text": "[$$ {s}_t=f\\\\left\\(U{x}_t+W{s}_{t-1}\\\\right\\)\n$$](../images/533412_1_En_8_Chapter/533412_1_En_8_Chapter_TeX_Equ2.png)\n\n(8.2)\n\nthe output of each recurrent layer _o_ _t_ is calculated by equation:\n\n![$$ {o}_t= softmax\\\\left\\(V{s}_t\\\\right\\)\n$$](../images/533412_1_En_8_Chapter/533412_1_En_8_Chapter_TeX_Equ3.png)\n\n(8.3)\n\nThe hidden states _s_ _t_ are considered as network memory units which\nconsists of hidden states from several former layers. Each layer\u2019s output is\nonly related to hidden states of the current layer. A significant difference\nbetween RNN and traditional neural networks is that weights and bias _U_ , _W_\n, and _V_ are shared among layers.\n\nThere will be an output at each step of the network but unnecessary. For\ninstance, if inference is applied for sentiment expressed by a sentence, only\nan output is required when the last word is input, and none after each word\nfor input. The key to RNNs is the hidden layer to capture sequence\ninformation.\n\nFor RNN feedforward process, if the number of time steps is _k_ , then hidden\nunit values and output will be computed after _k_ \\+ 1 time steps. For\nbackward process, RNN applies an algorithm called backpropagation through time\n(BPTT).\n\nRNN topologies range from partly to fully recurrent. Partly recurrent is a\nlayered network with distinct output and input layers where recurrence is\nlimited to the hidden layer. Fully recurrent neural network (FRNN) connects\nall neurons\u2019 outputs to inputs as shown in Fig. 8.12.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig12_HTML.png)\n\nFig. 8.12\n\nA simple recurrent neural network right (left) and fully connected recurrent\nneural network (right)\n\n### 8.4.4 Long Short-Term Memory (LSTM) Network\n\n#### 8.4.4.1 What Is LSTM?\n\nLong short-term memory (LSTM) network (Staudemeyer and Morris 2019; Yu et al.\n2019) is a type of RNN with special hidden layers to deal with gradient\nexplosion and disappearance problems during long sequence training process\nproposed by Hochreiter and Schmidhuber (1997). LSTM has better performance\nwith training longer sequences against Na\u00efve RNNs.\n\nStructure frameworks of LSTM and na\u00efve RNN are shown in Fig. 8.13.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig13_HTML.png)\n\nFig. 8.13\n\nStandard RNNs (left) and LSTM (right)\n\nLSTM has two hidden layers as RNN where a memory cell in the layer is to\nreplace hidden node. RNN has only one transfer state _h_ _t_ as compared with\nRNN. There are two transfer states, _c_ _t_ (cell state) and _h_ _t_ (hidden\nstate) in LSTM. RNN\u2019s _h_ _t_ corresponds to LSTM\u2019s _c_ _t_. _c_ _t_ passed\ndown information among them, output _c_ _t_ is produced by adding _c_ _t-1_\npassed from state and values of previous step. RNN\u2019s _h_ _t_ has larger\ndifference among nodes usually.\n\n#### 8.4.4.2 LSTM Architecture\n\n_x_ _t_ _and h_ _t_ \u2212 1 are concatenated inputs from the state of previous\nstep to train with activations for four states as shown in Fig. 8.14.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig14_HTML.png)\n\nFig. 8.14\n\nFour states of LSTM\n\n_**z**_ is input calculated by multiplying the concatenate vector with weights\n_**w**_ and converted into values _0-1_ through activation function _tanh_.\n**z** **f** , **z** **i** , **z** **o** are calculated by multiplying the\nconcatenate vector with corresponding weights and converting to values _0-1_\nby a sigmoid function \u03c3 to generate gate states.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1942889-c301-4630-a8e0-b56f5bacfa05": {"__data__": {"id_": "e1942889-c301-4630-a8e0-b56f5bacfa05", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb6798d7-574f-47f4-a964-fbe2cf227d40", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "4a2c2106698004b3c1568347bb7b3f5d785d2413dbdd153ccb67323611c18766", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43a14e95-5be9-433c-be93-d04baad41ab6", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "bb9b8515e26e4a6c0e7ae98feafc61883946f04d305a1e69addf2b43a5e2987a", "class_name": "RelatedNodeInfo"}}, "hash": "f38dd483b453309a71045a65cf6ba2686dd925d0f922be42d31a8315fb4afd88", "text": "#### 8.4.4.2 LSTM Architecture\n\n_x_ _t_ _and h_ _t_ \u2212 1 are concatenated inputs from the state of previous\nstep to train with activations for four states as shown in Fig. 8.14.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig14_HTML.png)\n\nFig. 8.14\n\nFour states of LSTM\n\n_**z**_ is input calculated by multiplying the concatenate vector with weights\n_**w**_ and converted into values _0-1_ through activation function _tanh_.\n**z** **f** , **z** **i** , **z** **o** are calculated by multiplying the\nconcatenate vector with corresponding weights and converting to values _0-1_\nby a sigmoid function \u03c3 to generate gate states. _**z**_ _ **f**_ represents\nforget gate, _**z**_ _ **i**_ represents input gate, _**z**_ _ **o**_\nrepresents output gate. A memory cell of LSTM calculation is shown in Fig.\n8.15.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig15_HTML.png)\n\nFig. 8.15\n\nCalculations in memory cell of LSTM\n\nMemory cells _**c**_ _ **t**_ _ **,**_ _ **h**_ _ **t**_ _ **,**_ _ **y**_ _\n**t**_ are calculated by gate states as equations below: (\u2a01 is matrix\naddition, \u2299 is Hadamard Product)\n\n![$$ {\\\\displaystyle \\\\begin{array}{l}{c}^t={z}^f\\\\odot {c}^{t-1}+{z}^1\\\\odot\nz\\\\\\\\ {}{h}^t={z}^o\\\\odot \\\\tanh \\\\left\\({c}^t\\\\right\\)\\\\\\\\ {}{y}^t=\\\\sigma\n\\\\left\\({W}^{\\\\hbox{'}}\\\\ {h}^t\\\\right\\)\\\\end{array}}\n$$](../images/533412_1_En_8_Chapter/533412_1_En_8_Chapter_TeX_Equ4.png)\n\n(8.4)\n\nLSTM has: (1) forget, (2) memory select, and (3) output stages.\n\n  1. 1.\n\nForget stage\n\nThis stage retains important information passed in by previous node _**c**_ _\n**t**_ \u2010 _ **1**_ (the previous cell state) and discards unimportant ones. The\ncalculated **z** **f** is used as a forget gate to control what type of\n_**c**_ _ **t**_ \u2010 _ **1**_ information should be retained or discarded.\n\n  2. 2.\n\nMemory select stage\n\nThis stage _remembers_ input _**x**_ _ **t**_ selectively to record important\ninformation. _**z**_ refers to present input. _**z**_ _ **i**_ is the input\ngate to control gating signals.\n\n  3. 3.\n\nOutput stage\n\nThis stage determines what is considered as _**h**_ _ **t**_ (the current\nstate) to be passed down to the next layer. _**z**_ _ **o**_ is output gate to\ncontrol this process prior _**c**_ _ **t**_ is scaled from memory select stage\n(convert through a _tanh_ function).\n\nEach layer output _**y**_ _ **t**_ is calculated by multiplying weights with\n_**h**_ _ **t**_ and converted the product through an activation function like\nRNN, the cell state _**c**_ _ **t**_ is passed to next layer at the end of\neach layer.\n\n### 8.4.5 Gate Recurrent Unit (GRU)\n\n#### 8.4.5.1 What Is GRU?\n\nGate Recurrent Unit (GRU) can be considered as a kind of RNN like LSTM but to\nmanage backpropagation gradients problems (Chung et al. 2014; Dey and Salem\n2017). GRU proposed in 2014 and LSTM proposed in 1997 had similar performances\nin many cases but the former is often exercised due to simple calculation with\ncomparable results than the latter.\n\nGRU\u2019s input and output structures are like RNN.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43a14e95-5be9-433c-be93-d04baad41ab6": {"__data__": {"id_": "43a14e95-5be9-433c-be93-d04baad41ab6", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1942889-c301-4630-a8e0-b56f5bacfa05", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f38dd483b453309a71045a65cf6ba2686dd925d0f922be42d31a8315fb4afd88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21d6d0ae-27af-43d5-946e-648883050065", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "985aab847f4e757ef8e181a29d7ecac0a01f6f9407082dfb9a7c8089b7a9742c", "class_name": "RelatedNodeInfo"}}, "hash": "bb9b8515e26e4a6c0e7ae98feafc61883946f04d305a1e69addf2b43a5e2987a", "text": "Each layer output _**y**_ _ **t**_ is calculated by multiplying weights with\n_**h**_ _ **t**_ and converted the product through an activation function like\nRNN, the cell state _**c**_ _ **t**_ is passed to next layer at the end of\neach layer.\n\n### 8.4.5 Gate Recurrent Unit (GRU)\n\n#### 8.4.5.1 What Is GRU?\n\nGate Recurrent Unit (GRU) can be considered as a kind of RNN like LSTM but to\nmanage backpropagation gradients problems (Chung et al. 2014; Dey and Salem\n2017). GRU proposed in 2014 and LSTM proposed in 1997 had similar performances\nin many cases but the former is often exercised due to simple calculation with\ncomparable results than the latter.\n\nGRU\u2019s input and output structures are like RNN. There are inputs _**x**_ _\n**t**_ and _**h**_ _ **t**_ \u2010 _ **1**_ to contain relevant information of the\nprior node. Current outputs _**y**_ _ **t**_ and _**h**_ _ **t**_ are\ncalculated by combining _**x**_ _ **t**_ and _**h**_ _ **t**_ \u2010 _ **1**_. A\nGRU architecture is shown in Fig. 8.16.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig16_HTML.png)\n\nFig. 8.16\n\nGeneral architecture of GRU\n\n#### 8.4.5.2 GRU Inner Architecture\n\n_**r**_ is reset gate and _**z**_ is update gate. They are concatenated with\ninput _**x**_ _ **t**_ and hidden state _**h**_ _ **t**_ \u2010 _ **1**_ from the\nprior node and multiply results with weights as shown in Fig. 8.17.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig17_HTML.png)\n\nFig. 8.17\n\nReset and update gates of GRU\n\nWhen a gate control signal is available, apply _**r**_ reset gate to obtain\ndata _**h**_ _ **t**_ \u2010 _ **1**_ _**= h**_ _ **t**_ \u2010 _ **1**_ _ **er**_ after\nreset, _**h**_ _ **t**_ \u2010 _ **1**_ are concatenated with _**x**_ _ **t**_ and\napply a tanh function to generate data that lies within range (\u22121, 1) as shown\nin Fig. 8.18.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig18_HTML.png)\n\nFig. 8.18\n\nComputation of **h**\n\nAt this point, _**h**_ _ **\u2032**_ contains current input _**x**_ _ **t**_ , its\nselection memory stage is like LSTM.\n\nFinally, update memory stage is the most critical step where forget and\nremember steps are performed simultaneously. The gate **z** obtained earlier\nis applied as:\n\n![$$ {h}_t=\\\\left\\(1-z\\\\right\\)\\\\odot {h}_{t-1}+z\\\\odot {h}^{\\\\prime }\n$$](../images/533412_1_En_8_Chapter/533412_1_En_8_Chapter_TeX_Equ5.png)\n\n(8.5)\n\nwhere _**z**_ (gate signal) is within range 0\u20131. If it is close to 1 or 0, it\nsignifies more data is remained or forgotten, respectively.\n\n( _1_ \u2010 _z_ ) \u2299 _h_ _t_ \u2010 _1_ represents to forget the original hidden state\nselectively. (1\u2212 _z_ ) is considered as a forget gate to forget _**h**_ _\n**t**_ \u2010 _ **1**_ unimportant information.\n\n_**z**_ \u2299 _**h**_ \u2032 represents _**h**_ \u2032 memory selective information of\npresent node. Like ( _1_ \u2010 _z_ ), it will forget **h** **\u2032** unimportant\ninformation or is considered as selective _**h**_ \u2032 information.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "21d6d0ae-27af-43d5-946e-648883050065": {"__data__": {"id_": "21d6d0ae-27af-43d5-946e-648883050065", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "43a14e95-5be9-433c-be93-d04baad41ab6", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "bb9b8515e26e4a6c0e7ae98feafc61883946f04d305a1e69addf2b43a5e2987a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c4d99ed-5036-40e9-af5b-37117135d1ba", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "d1af6d3160af450a3dd19b129b0175c1ef6bc20752700203782bde65f6ee85be", "class_name": "RelatedNodeInfo"}}, "hash": "985aab847f4e757ef8e181a29d7ecac0a01f6f9407082dfb9a7c8089b7a9742c", "text": "If it is close to 1 or 0, it\nsignifies more data is remained or forgotten, respectively.\n\n( _1_ \u2010 _z_ ) \u2299 _h_ _t_ \u2010 _1_ represents to forget the original hidden state\nselectively. (1\u2212 _z_ ) is considered as a forget gate to forget _**h**_ _\n**t**_ \u2010 _ **1**_ unimportant information.\n\n_**z**_ \u2299 _**h**_ \u2032 represents _**h**_ \u2032 memory selective information of\npresent node. Like ( _1_ \u2010 _z_ ), it will forget **h** **\u2032** unimportant\ninformation or is considered as selective _**h**_ \u2032 information.\n\n_**h**_ _ **t**_ _**=**_ ( _ **1**_ \u2010 _ **z**_ ) \u2299 _**h**_ _ **t**_ \u2010 _ **1**_\n_**\\+ z**_ \u2299 _**h**_ \u2032 is the calculation to forget _**h**_ _ **t**_ \u2010 _\n**1**_ information from passed down and add information from the current node.\n\nIt is noted that forget _**z**_ and select _(1-z)_ factors are linked, which\nmeans it will forget the passed in information selectively. When weights _(_ _\n**z**_ _)_ are forgotten, it will apply weights in _**h**_ \u2032 to configurate\n_(1-z)_ at a constant state.\n\nGRU\u2019s input and output structures are like RNN, its internal concept is like\nLSTM. GRU has one less internal gate as compared with LSTM and fewer\nparameters but can achieve comparable satisfactory results with reduced time\nand computational resources. A GRU computation module is shown in Fig. 8.19.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig19_HTML.png)\n\nFig. 8.19\n\nComputation module of GRU\n\n### 8.4.6 Bidirectional Recurrent Neural Networks (BRNNs)\n\n#### 8.4.6.1 What Is BRNN?\n\nBidirectional Recurrent Neural Network (BRNN) is a type with RNN layers in two\ndirections (Singh et al. 2016). It links with previous and subsequent\ninformation outputs to perform inference against both RNN and LSTM to possess\ninformation from previous one. For example, in text summarization, it is\ninsufficient to consider the information from previous content, sometimes it\nalso requires subsequent text information for words prediction of a sentence.\nBRNN is proposed to deal with these circumstances.\n\nBRNN consists of two RNNs superimposed on top of each other. The output is\nmutually generated by two RNNs states. A BRNN structure is shown in Fig. 8.20.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig20_HTML.png)\n\nFig. 8.20\n\nStructure of BRNN\n\nBRNN training process is as follows:\n\n  1. 1.\n\nbegin forward propagation from time step _1_ to time step _T_ to calculate\nhidden layer\u2019s output and save at each time step,\n\n  2. 2.\n\nproceed from time step _T_ to time step _t_ to calculate backward hidden layer\noutput and save at each time step,\n\n  3. 3.\n\nobtain each moment final output according to forward and backward hidden\nlayers after calculating all input moments from both forward and backward\ndirections.\n\n## 8.5 Transformer Technology\n\n### 8.5.1 What Is Transformer?\n\n_Transformer_ is a network based on attention mechanism without recurrent and\nconvolution units (Vaswani et al. 2017). Transformer and LSTM have different\ntraining processes. LSTM is serial and iterative; it cannot proceed until the\nword before is processed against transformer is parallel which means that all\nwords are trained simultaneously to improve computational efficiency. A\ntransformer system structure is shown in Fig. 8.21.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig21_HTML.png)\n\nFig. 8.21\n\nTransformer architecture\n\n### 8.5.2 Transformer Architecture\n\nA transformer model has two parts (1) encoder and (2) decoder. Language\nsequence extracts as input, encoder maps it into a hidden layer, and decoder\nmaps the hidden layer inversely to a sequence as output.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c4d99ed-5036-40e9-af5b-37117135d1ba": {"__data__": {"id_": "0c4d99ed-5036-40e9-af5b-37117135d1ba", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21d6d0ae-27af-43d5-946e-648883050065", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "985aab847f4e757ef8e181a29d7ecac0a01f6f9407082dfb9a7c8089b7a9742c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f5f9482-81fa-496d-aa5d-a4b501471495", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "aaa6d1bbf6a7ec2804aff496588134d1cef8663ecebabbe1b033fe5bc47179f0", "class_name": "RelatedNodeInfo"}}, "hash": "d1af6d3160af450a3dd19b129b0175c1ef6bc20752700203782bde65f6ee85be", "text": "_Transformer_ is a network based on attention mechanism without recurrent and\nconvolution units (Vaswani et al. 2017). Transformer and LSTM have different\ntraining processes. LSTM is serial and iterative; it cannot proceed until the\nword before is processed against transformer is parallel which means that all\nwords are trained simultaneously to improve computational efficiency. A\ntransformer system structure is shown in Fig. 8.21.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig21_HTML.png)\n\nFig. 8.21\n\nTransformer architecture\n\n### 8.5.2 Transformer Architecture\n\nA transformer model has two parts (1) encoder and (2) decoder. Language\nsequence extracts as input, encoder maps it into a hidden layer, and decoder\nmaps the hidden layer inversely to a sequence as output.\n\n#### 8.5.2.1 Encoder\n\nThere are six identical encoder layers in the transformer with two sublayers:\n(1) self-attention and (2) feedforward in each encoder layer. Self-attention\nlayer is the first sublayer to exercise attention mechanism, and a simple\nfully connected feedforward network is the second sublayer. There follows a\nresidual connection and layer normalization from each of the sublayers. An\nencoder layer architecture is shown in Fig. 8.22.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig22_HTML.png)\n\nFig. 8.22\n\nArchitecture of an encoder layer\n\n#### 8.5.2.2 Decoder\n\nThere are 6 identical encoder layers in the transformer. In addition to\nidentical two sublayers as each encoding layer, a third sublayer is added to\nthe decoder to perform multi-head attention, taking the output of last encoder\nlayer as input. Residual connections and layer normalization are used\nsequentially for all sublayers, which is the same as the encoder. The\ndecoder's self-awareness is modified by the mask to ensure that inference of\nthe position can only use information from a known position, or in other\nwords, its previous position.\n\n### 8.5.3 Deep Into Encoder\n\n#### 8.5.3.1 Positional Encoding\n\nSince transformer has no iterative process, each word\u2019s position information\nmust be provided to ensure that it can recognize the position relationship in\nlanguage. Linear transformation of _sin_ and _cos_ functions is applied to\nprovide model position information as equation:\n\n![$$ \\\\mathrm{PE}\\\\left\\(\\\\mathrm{pos},2i\\\\right\\)=\\\\sin\n\\\\left\\(\\\\mathrm{pos}/10,{000}^{2i/{d}_{\\\\mathrm{model}}}\\\\right\\)\n$$](../images/533412_1_En_8_Chapter/533412_1_En_8_Chapter_TeX_Equa.png)\n\n![$$ \\\\mathrm{PE}\\\\left\\(\\\\mathrm{pos},2i+1\\\\right\\)=\\\\cos\n\\\\left\\(\\\\mathrm{pos}/10,{000}^{2i/{d}_{\\\\mathrm{model}}}\\\\right\\)\n$$](../images/533412_1_En_8_Chapter/533412_1_En_8_Chapter_TeX_Equ6.png)\n\n(8.6)\n\nwhere _pos_ represents to a word\u2019s position in a sentence, _i_ represents to\nword vector\u2019s dimension number, _d_ model represents to embedded dimension\u2019s\nvalue. There is a set of formulas such as sets of 0, 1, or 2, 3 processed with\nthe above sum function, respectively. As the dimension number increases, the\nperiodic changes moderately to generate a texture containing position\ninformation.\n\n#### 8.5.3.2 Self-Attention Mechanism\n\nFor input sentence, the word vector of each word is obtained through word\nembedding, and the position vector of all words is obtained in same dimensions\nthrough positional encoding that can be added directly to obtain the true\nvector representation. _i_ th word\u2019s vector is written as _x_ _i_ , _X_ is\ninput matrix combined by all word vectors. _i_ th row refers to _i_ th word\nvector.\n\n_W_ _Q_ , _W_ _K_ , _W_ _V_ are matrices defined to perform three linear\ntransformations with _**X**_ to generate three matrices _Q_ (queries), _K_\n(keys), and _V_ (values), respectively.\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2f5f9482-81fa-496d-aa5d-a4b501471495": {"__data__": {"id_": "2f5f9482-81fa-496d-aa5d-a4b501471495", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c4d99ed-5036-40e9-af5b-37117135d1ba", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "d1af6d3160af450a3dd19b129b0175c1ef6bc20752700203782bde65f6ee85be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64827f10-25d6-4752-9e3e-9ffbffd3dfca", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "68ca75897ad6c82d00f59877be3d74d4d6bff49f2184838618cbb2c1feec9b39", "class_name": "RelatedNodeInfo"}}, "hash": "aaa6d1bbf6a7ec2804aff496588134d1cef8663ecebabbe1b033fe5bc47179f0", "text": "As the dimension number increases, the\nperiodic changes moderately to generate a texture containing position\ninformation.\n\n#### 8.5.3.2 Self-Attention Mechanism\n\nFor input sentence, the word vector of each word is obtained through word\nembedding, and the position vector of all words is obtained in same dimensions\nthrough positional encoding that can be added directly to obtain the true\nvector representation. _i_ th word\u2019s vector is written as _x_ _i_ , _X_ is\ninput matrix combined by all word vectors. _i_ th row refers to _i_ th word\nvector.\n\n_W_ _Q_ , _W_ _K_ , _W_ _V_ are matrices defined to perform three linear\ntransformations with _**X**_ to generate three matrices _Q_ (queries), _K_\n(keys), and _V_ (values), respectively.\n\n![$$ \\\\mathrm{Q}=X\\\\cdot {W}_{\\\\mathrm{Q}}\n$$](../images/533412_1_En_8_Chapter/533412_1_En_8_Chapter_TeX_Equb.png)\n\n![$$ K=X\\\\cdot {W}_K\n$$](../images/533412_1_En_8_Chapter/533412_1_En_8_Chapter_TeX_Equc.png)\n\n![$$ V=X\\\\cdot {W}_V\n$$](../images/533412_1_En_8_Chapter/533412_1_En_8_Chapter_TeX_Equ7.png)\n\n(8.7)\n\nAttention mechanism computation can be described as:\n\n![$$\n\\\\mathrm{Attention}\\\\left\\(Q,K,V\\\\right\\)=\\\\mathrm{softmax}\\\\left\\(\\\\frac{Q{K}^T}{\\\\sqrt{d_k}}\\\\right\\)V\n$$](../images/533412_1_En_8_Chapter/533412_1_En_8_Chapter_TeX_Equ8.png)\n\n(8.8)\n\nThe dot products are calculated by multiplying query _Q_ by keys _K_ ,\ndividing the result by  ![$$ \\\\sqrt{d_k}\n$$](../images/533412_1_En_8_Chapter/533412_1_En_8_Chapter_TeX_IEq1.png) , and\napplying a softmax function to obtain value scores _V_.\n\n#### 8.5.3.3 Multi-Head Attention\n\nThe previously defined set of _Q_ , _K_ , _V_ allows a word that uses the\ninformation of related words. Multiple _Q_ , _K_ , _V_ defined groups can\nenable a word to represent sub-spaces at different positions with identical\ncalculation process, except that the matrix of linear transformation has\nchanged from one group ( _W_ _Q_ , _W_ _K_ , _W_ _V_ ) to multiple groups\n![$$\n\\\\left\\({\\\\mathrm{W}}_{\\\\mathrm{Q}}^0,{\\\\mathrm{W}}_{\\\\mathrm{K}}^0,{\\\\mathrm{W}}_{\\\\mathrm{V}}^0\\\\right\\)\n$$](../images/533412_1_En_8_Chapter/533412_1_En_8_Chapter_TeX_IEq2.png) ,\n![$$\n\\\\left\\({\\\\mathrm{W}}_{\\\\mathrm{Q}}^1,{\\\\mathrm{W}}_{\\\\mathrm{K}}^1,{\\\\mathrm{W}}_{\\\\mathrm{V}}^1\\\\right\\)\n$$](../images/533412_1_En_8_Chapter/533412_1_En_8_Chapter_TeX_IEq3.png) ... as\nequation:\n\n![$$\n\\\\mathrm{MultiHead}\\\\;\\\\left\\(Q,K,V\\\\right\\)=\\\\mathrm{Concat}\\\\;\\\\left\\(\\\\mathrm{head}1,\\\\dots,\n\\\\mathrm{head}\\\\ h\\\\right\\)\\\\cdot WO\n$$](../images/533412_1_En_8_Chapter/533412_1_En_8_Chapter_TeX_Equd.png)\n\nwhere\n\n![$$ head\\\\;i= Attention\\\\left\\(X{W}_Q^i,X{W}_K^i,X{W}_V^i\\\\right\\)\n$$](../images/533412_1_En_8_Chapter/533412_1_En_8_Chapter_TeX_Equ9.png)\n\n(8.9)\n\nwhere _W_ _O_ is the weights of concatenated results.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64827f10-25d6-4752-9e3e-9ffbffd3dfca": {"__data__": {"id_": "64827f10-25d6-4752-9e3e-9ffbffd3dfca", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f5f9482-81fa-496d-aa5d-a4b501471495", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "aaa6d1bbf6a7ec2804aff496588134d1cef8663ecebabbe1b033fe5bc47179f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30820258-a51a-4ed7-9749-4f0e55a08c77", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "5117c6481a881a993be55c0301b898278ee577237def3af9471c05201c30f4e3", "class_name": "RelatedNodeInfo"}}, "hash": "68ca75897ad6c82d00f59877be3d74d4d6bff49f2184838618cbb2c1feec9b39", "text": "[$$\n\\\\mathrm{MultiHead}\\\\;\\\\left\\(Q,K,V\\\\right\\)=\\\\mathrm{Concat}\\\\;\\\\left\\(\\\\mathrm{head}1,\\\\dots,\n\\\\mathrm{head}\\\\ h\\\\right\\)\\\\cdot WO\n$$](../images/533412_1_En_8_Chapter/533412_1_En_8_Chapter_TeX_Equd.png)\n\nwhere\n\n![$$ head\\\\;i= Attention\\\\left\\(X{W}_Q^i,X{W}_K^i,X{W}_V^i\\\\right\\)\n$$](../images/533412_1_En_8_Chapter/533412_1_En_8_Chapter_TeX_Equ9.png)\n\n(8.9)\n\nwhere _W_ _O_ is the weights of concatenated results.\n\nAdding input with a sublayer (self-attention layer, for example) to generate\nresidual connections as equation:\n\n![$$\n{X}_{\\\\mathrm{attention}}={X}_{\\\\mathrm{embedding}}+\\\\mathrm{Attention}\\\\;\\\\left\\(Q,K,V\\\\right\\)\n$$](../images/533412_1_En_8_Chapter/533412_1_En_8_Chapter_TeX_Equ10.png)\n\n(8.10)\n\n#### 8.5.3.4 Layer Normalization of Attention Sublayer\n\nLayer normalization is to standardize the distribution of hidden layers\nindependently to improve convergence and training processes effectively.\n\n![$$\n{X}_{\\\\mathrm{attention}}=\\\\mathrm{LayerNorm}\\\\left\\({X}_{\\\\mathrm{attention}}\\\\right\\)\n$$](../images/533412_1_En_8_Chapter/533412_1_En_8_Chapter_TeX_Equ11.png)\n\n(8.11)\n\n#### 8.5.3.5 Feedforward Layer\n\nIt is a two-layer linear map with an activation function, i.e. _ReLU_.\n\n![$$ {X}_{\\\\mathrm{hidden}}=\\\\mathrm{Linear}\\\\left\\(\nReLU\\\\left\\(\\\\mathrm{Linear}\\\\;\\\\left\\({X}_{\\\\mathrm{attention}}\\\\right\\)\\\\right\\)\\\\right\\)\n$$](../images/533412_1_En_8_Chapter/533412_1_En_8_Chapter_TeX_Eque.png)\n\nfollowed by residual connection and layer normalization scheme:\n\n![$$ {X}_{\\\\mathrm{hidden}}={X}_{\\\\mathrm{attention}}+{X}_{\\\\mathrm{hidden}}\n$$](../images/533412_1_En_8_Chapter/533412_1_En_8_Chapter_TeX_Equf.png)\n\n![$$\n{X}_{\\\\mathrm{hidden}}=\\\\mathrm{LayerNorm}\\\\left\\({X}_{\\\\mathrm{hidden}}\\\\right\\)\n$$](../images/533412_1_En_8_Chapter/533412_1_En_8_Chapter_TeX_Equ12.png)\n\n(8.12)\n\n## 8.6 BERT\n\n### 8.6.1 What Is BERT?\n\nBERT is a pre-trained model of language representation called Bidirectional\nEncoder Representation from Transformers (Devlin et al. 2018). It uses MLM\n(masked language model) to generate deep bidirectional linguistic\nrepresentation instead of traditional one-direction model or concatenate two\none-direction models to pre-train language.\n\n### 8.6.2 Architecture of BERT\n\nBERT models are pre-trained either by left-to-right or right-to-left language\nmodels previously, this unidirectional property restricts model structure to\nobtain unidirectional context information only and propensity for\nrepresentation. BERT adopted MLM in pre-training stage and a bidirectional\ntransformer with deep layers to build the entire model, the representation\ngenerated integrates both left and right content information. A BERT system\narchitecture is shown in Fig. 8.23.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig23_HTML.png)\n\nFig. 8.23\n\nSystem architecture of BERT\n\n### 8.6.3 Training of BERT\n\nBERT has two training process steps: (1) pre-training and (2) fine-tuning.\n\n#### 8.6.3.1 Pre-training BERT\n\nBERT is not constrained by a one-way language model because it randomly\nreplaces tokens in each training sequence with mask tokens ([MASK]) with 15%\nprobability to predict the original word at position [MASK].", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30820258-a51a-4ed7-9749-4f0e55a08c77": {"__data__": {"id_": "30820258-a51a-4ed7-9749-4f0e55a08c77", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64827f10-25d6-4752-9e3e-9ffbffd3dfca", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "68ca75897ad6c82d00f59877be3d74d4d6bff49f2184838618cbb2c1feec9b39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90a10781-a82b-4a14-a197-24cc7a5bcd76", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "1be0f476fd331550faffef227231e0fdd08ff7eab2a877e0073820ef08d04211", "class_name": "RelatedNodeInfo"}}, "hash": "5117c6481a881a993be55c0301b898278ee577237def3af9471c05201c30f4e3", "text": "BERT adopted MLM in pre-training stage and a bidirectional\ntransformer with deep layers to build the entire model, the representation\ngenerated integrates both left and right content information. A BERT system\narchitecture is shown in Fig. 8.23.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig23_HTML.png)\n\nFig. 8.23\n\nSystem architecture of BERT\n\n### 8.6.3 Training of BERT\n\nBERT has two training process steps: (1) pre-training and (2) fine-tuning.\n\n#### 8.6.3.1 Pre-training BERT\n\nBERT is not constrained by a one-way language model because it randomly\nreplaces tokens in each training sequence with mask tokens ([MASK]) with 15%\nprobability to predict the original word at position [MASK]. [MASK] does not\nappear in fine-tuning of downstream tasks, leading to differences in pre-\ntraining and fine-tuning stages, because the pre-training objective improves\nlanguage representation, being sensitive to [MASK] and to other insensitive\ntokens. BERT applies the following strategies:\n\nFirst, in each training sequence, a token position is randomly selected for\nprediction with a probability of 15%. If _i_ th token is selected, it will be\nreplaced by one of the following tokens:\n\n  1. 1.\n\n80% is [MASK]. For instance, the cat is **adorable** \u2192 the cat is **[MASK].**\n\n  2. 2.\n\n10% is a random token. For instance, the cat is **adorable** \u2192 the cat is\nginger.\n\n  3. 3.\n\n10% is the original token (no change). For instance, his cat is **adorable** \u2192\nhis cat is adorable.\n\nSecond, apply _T_ _i_ corresponding to the position, predict the original\ntoken through full connection, then apply softmax to output the probability of\neach token, and finally apply cross-entropy to evaluate loss.\n\nThis method causes BERT sensitive to [MASK] and all tokens to extract\nrepresentative information.\n\n#### 8.6.3.2 Next Sentence Prediction (NSP)\n\nThere are tasks such as question answering and natural language reasoning to\nunderstand the relationship between two sentences. Sentence-level\nrepresentations cannot be captured directly, as MLM tasks tend to extract\ntoken-level representations. BERT applies NSP pre-training task to let the\nmodel understands the relationships between sentences and predict whether they\nare connected.\n\nFor every training sample, select Set A and B from corpus to create a sample,\nwhere Set A is 50% of Set B (labeled \u201cIsNext\u201d), and Set B is 50% random. Next,\ntraining examples are put into BERT model to generate binary classification\npredictions.\n\n#### 8.6.3.3 Fine-tuning BERT\n\nIt is necessary to add an additional output layer to fine-tune downstream\ntasks for satisfactory performance. It does not require task-specific\nstructural modification in this process.\n\n## 8.7 Other Related Transformer Technology\n\n### 8.7.1 Transformer-XL\n\n#### 8.7.1.1 Motivation\n\nTransformers are widely used as a feature extractor in NLP but required to set\na fixed length input sequence, i.e. the default length for BERT is 512. If\ntext sequence length is shorter than fixed length, it must be solved by\npadding. If text sequence length exceeds fixed length, it can be divided into\nmultiple segments. Each segment is processed at training separately as shown\nin Fig. 8.24.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig24_HTML.png)\n\nFig. 8.24\n\nSegment training of standard transformer\n\nNevertheless, there are two problems: (1) segments are trained independently,\nthe largest dependency between different tokens depends on the segment length;\n(2) segments are separated according to a fixed length without sentences\u2019\nnatural boundaries consideration to produce semantically incomplete segments.\nThus, transformer-XL (Dai et al. 2019) is proposed.\n\n#### 8.7.1.2 Transformer-XL technology\n\n  1. 1.\n\n**Segment-level recurrence:** When processing the current segment,\nTransformer-XL caches and applies hidden vector sequence to all layers from\nprevious segment. These sequences only participate in forward calculation\nwithout backpropagation called segment-level recurrence. Figure 8.25 shows the\nsegment training of Transformer-XL.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig25_HTML.png)\n\nFig.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90a10781-a82b-4a14-a197-24cc7a5bcd76": {"__data__": {"id_": "90a10781-a82b-4a14-a197-24cc7a5bcd76", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30820258-a51a-4ed7-9749-4f0e55a08c77", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "5117c6481a881a993be55c0301b898278ee577237def3af9471c05201c30f4e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2738c963-3b32-45a6-b316-65230187332d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "11f750b78a36b66981b38760a19668c160dac72522ee6dd3444d86ae248d740b", "class_name": "RelatedNodeInfo"}}, "hash": "1be0f476fd331550faffef227231e0fdd08ff7eab2a877e0073820ef08d04211", "text": "8.24\n\nSegment training of standard transformer\n\nNevertheless, there are two problems: (1) segments are trained independently,\nthe largest dependency between different tokens depends on the segment length;\n(2) segments are separated according to a fixed length without sentences\u2019\nnatural boundaries consideration to produce semantically incomplete segments.\nThus, transformer-XL (Dai et al. 2019) is proposed.\n\n#### 8.7.1.2 Transformer-XL technology\n\n  1. 1.\n\n**Segment-level recurrence:** When processing the current segment,\nTransformer-XL caches and applies hidden vector sequence to all layers from\nprevious segment. These sequences only participate in forward calculation\nwithout backpropagation called segment-level recurrence. Figure 8.25 shows the\nsegment training of Transformer-XL.\n\n![](../images/533412_1_En_8_Chapter/533412_1_En_8_Fig25_HTML.png)\n\nFig. 8.25\n\nSegment training of transformer-XL\n\n  1. 2.\n\n**Relative position encodings** : Each token has an embedding position to\nrepresent positions relationship in standard transformer. This embedding\nposition encoding is either generated by sin/cos function or learning, but it\nis impractical in Transformer-XL because positional relationship of different\nsegments is unidentified if the same positional code is added to each segment.\nTransformer-XL applies relative position encoding instead of absolute position\nencoding, so when calculating the hidden vector of current position, it\nconsiders tokens\u2019 relative position relationships to calculate attention\nscore.\n\n### 8.7.2 ALBERT\n\nBERT model has many parameters, but it is limited by GPU/TPU memory size as\nmodel size increases. Google proposed _A Lite BERT (_ _ALBERT_ _)_ to solve\nthis problem (Lan et al. 2019). ALBERT applies two techniques to reduce\nparameters and improve NSP pre-training task, which include:\n\n  1. 1.\n\nparameter sharing\u2014apply same weights to all 12-layers,\n\n  2. 2.\n\nfactorize embeddings\u2014shorten initial embeddings to 128 features,\n\n  3. 3.\n\npretrain by LAMB optimizer\u2014replace ADAM Optimizer,\n\n  4. 4.\n\nSentence Order Prediction (SOP)\u2014replace BERT\u2019s Next Sentence Prediction (NSP)\ntask,\n\n  5. 5.\n\nN-gram masking\u2014modify Masked Language Model (MLM) task to mask out words\u2019\nN-grams instead of single words.\n\nExercises\n\n  1. 8.1.\n\nWhat is Transfer Learning (TL)? Compare the major differences between Transfer\nLearning (TL) and traditional Machine Learning (ML) in AI.\n\n  2. 8.2.\n\nDescribe and explain how Transfer Learning (TL) can be applied to NLP. Give\ntwo NLP applications as examples to support your answer.\n\n  3. 8.3.\n\nCompare the major differences between Heterogeneous vs. Homogeneous Transfer\nLearning. Give two NLP applications/systems as examples for illustration.\n\n  4. 8.4.\n\nWhat is Recurrent Neural Network (RNN)? State and explain why RNN is important\nfor the building of NLP applications. Give 2 NLP applications as example to\nsupport your answer.\n\n  5. 8.5.\n\nState and explain FIVE major categories of Recurrent Neural Networks (RNN).\nFor each type, give a live example for illustration.\n\n  6. 8.6.\n\nWhat is LSTM network? State and explain how it works by using NLP application\nsuch as Text Summarization.\n\n  7. 8.7.\n\nWhat is Gate Recurrent Unit (GRU)? Use an NLP application as example, state\nand explain the major differences between GRU and standard RNN.\n\n  8. 8.8.\n\nState and explain the key functions and architecture of Transformer\ntechnology. Use NLP application as example, state briefly how it works.\n\n  9. 8.9.\n\nWhat is BERT model? Use NLP application such as Q&A chatbot as example, state\nand explain briefly how it works.\n\nReferences\n\n  1. Cho, K., Van Merri\u00ebnboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.\n\n  2. Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2738c963-3b32-45a6-b316-65230187332d": {"__data__": {"id_": "2738c963-3b32-45a6-b316-65230187332d", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90a10781-a82b-4a14-a197-24cc7a5bcd76", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "1be0f476fd331550faffef227231e0fdd08ff7eab2a877e0073820ef08d04211", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72d52d59-bb01-48dc-8a54-3dfbc699be5d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3dd730aeb4d4846bff6331addd55a3bccadd00db348b14e907e6c22804361008", "class_name": "RelatedNodeInfo"}}, "hash": "11f750b78a36b66981b38760a19668c160dac72522ee6dd3444d86ae248d740b", "text": "8. 8.8.\n\nState and explain the key functions and architecture of Transformer\ntechnology. Use NLP application as example, state briefly how it works.\n\n  9. 8.9.\n\nWhat is BERT model? Use NLP application such as Q&A chatbot as example, state\nand explain briefly how it works.\n\nReferences\n\n  1. Cho, K., Van Merri\u00ebnboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.\n\n  2. Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.\n\n  3. Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., & Salakhutdinov, R. (2019). Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860.\n\n  4. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n\n  5. Dey, R., & Salem, F. M. (2017, August). Gate-variants of gated recurrent unit (GRU) neural networks. In 2017 IEEE 60th international midwest symposium on circuits and systems (MWSCAS) (pp. 1597-1600). IEEE.\n\n  6. Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780.\n\n  7. Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2019). Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942.\n\n  8. Pan, S. J., & Yang, Q. (2009). A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10), 1345-1359.\n\n  9. Sherstinsky, A. (2020). Fundamentals of recurrent neural network (RNN) and long short-term memory (LSTM) network. Physica D: Nonlinear Phenomena, 404, 132306.\n\n  10. Singh, B., Marks, T. K., Jones, M., Tuzel, O., & Shao, M. (2016). A multi-stream bi-directional recurrent neural network for fine-grained action detection. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1961-1970).\n\n  11. Staudemeyer, R. C., & Morris, E. R. (2019). Understanding LSTM--a tutorial into long short-term memory recurrent neural networks. arXiv preprint arXiv:1909.09586.\n\n  12. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.\n\n  13. Weiss, K., Khoshgoftaar, T. M., & Wang, D. (2016). A survey of transfer learning. Journal of Big data, 3(1), 1-40.\n\n  14. Yin, W., Kann, K., Yu, M., & Schutze, H. (2017). Comparative study of CNN and RNN for natural language processing. arXiv preprint arXiv:1702.01923.\n\n  15. Yu, Y., Si, X., Hu, C., & Zhang, J. (2019). A review of recurrent neural networks: LSTM cells and network architectures.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72d52d59-bb01-48dc-8a54-3dfbc699be5d": {"__data__": {"id_": "72d52d59-bb01-48dc-8a54-3dfbc699be5d", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2738c963-3b32-45a6-b316-65230187332d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "11f750b78a36b66981b38760a19668c160dac72522ee6dd3444d86ae248d740b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a3484a5-3d67-485f-b143-70f5f1aaa459", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f72549613d70c86bf6c734cf8d60bb4229d2c4114d6c3f9f078b48b1b7a8823c", "class_name": "RelatedNodeInfo"}}, "hash": "3dd730aeb4d4846bff6331addd55a3bccadd00db348b14e907e6c22804361008", "text": "(2017). Attention is all you need. Advances in neural information processing systems, 30.\n\n  13. Weiss, K., Khoshgoftaar, T. M., & Wang, D. (2016). A survey of transfer learning. Journal of Big data, 3(1), 1-40.\n\n  14. Yin, W., Kann, K., Yu, M., & Schutze, H. (2017). Comparative study of CNN and RNN for natural language processing. arXiv preprint arXiv:1702.01923.\n\n  15. Yu, Y., Si, X., Hu, C., & Zhang, J. (2019). A review of recurrent neural networks: LSTM cells and network architectures. Neural computation, 31(7), 1235-1270.\n\n  16. Zhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y., Zhu, H., & He, Q. (2020). A comprehensive survey on transfer learning. Proceedings of the IEEE, 109(1), 43-76.\n\n\n\u00a9 The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.\n2024\n\nR. S. T. Lee Natural Language Processing\n<https://doi.org/10.1007/978-981-99-1999-4_9>\n\n# 9\\. Major NLP Applications\n\nRaymond S. T. Lee 1\n\n(1)\n\nUnited International College, Beijing Normal University-Hong Kong Baptist\nUniversity, Zhuhai, China\n\n## 9.1 Introduction\n\nThis chapter will study three major NLP applications: (1) Information\nRetrieval Systems (IR), (2) Text Summarization Systems (TS), and (3)\nQuestion-&-Answering Chatbot System (QA Chatbot).\n\nInformation retrieval is the process of obtaining the required information\nfrom large-scale unstructured data relative to traditional structured database\nrecords from texts, images, audios, and videos. Information retrieval systems\nare not only common search engines but also recommendation systems like\ne-commerce sites, question and answer, or interactive systems.\n\nText Summarization is the process of diminishing a set of data\ncomputationally, creating a subset or summary to represent relevant\ninformation for NLP tasks such as text classification, question answering,\nlegal texts, news summarization, and headlines generation.\n\nQuestion-Answer (QA) system represents human\u2013machine interaction system with\nhuman natural language is the communication medium. It is a task-oriented\nsystem to deal with objectives or answer specific questions through dialogues\nwith sentiment analysis.\n\n## 9.2 Information Retrieval Systems\n\n### 9.2.1 Introduction to IR Systems\n\nNLP used AI techniques like N-gram, rule-based approaches, Word2vec to\nretrieve information but encountered computational limitations to process\nlarge amount of corpus information, define text and model frameworks for\ndomain specifics, GPU clusters, and induce high costs to maintain rule sets\ndue to standard modifications.\n\nCorpora cater for IR in open machine-readable standard format had grown\nexponentially due to pre-trained models\u2019 technological advancements. IR models\nfor generic language that combines generic terms with domain-specific terms,\ne.g. _lease_ can be a place or a leasehold, its objectives can be organized by\nabstract, formal, or colloquial language in a large narrative component based\non document type to improve retrieval results.\n\nText or document classification and clustering in IR research focuses on two\naspects: 1) text representation and 2) clustering algorithms. Text\nrepresentation is to convert unstructured text into a computer-processable\ndata format. During text representation process, it is necessary to extract\nand mining textual information. Semantic similarity computation is the link\nbetween text modelling and representation with application on potential\ninformation text layer. Clustering algorithms are to extract semantic\ninformation to facilitate similarity calculation for text classification and\nclustering effectiveness.\n\n### 9.2.2 Vector Space Model in IR\n\nVector Space Model (Salton et al. 1975) was a leading IR method from 1960 to\n1970. Queries and retrieved documents are represented as vectors with\ndimensionality related to word list size in this model. A retrieved document D\ncan be represented as a vector of lexical items: _D_ _i_ = ( _d_ 1, _d_ 2, , ,\n, _d_ _n_ ), where _d_ _i_ is the weight of a _i_ th lexical item in _D_ _i_.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a3484a5-3d67-485f-b143-70f5f1aaa459": {"__data__": {"id_": "0a3484a5-3d67-485f-b143-70f5f1aaa459", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72d52d59-bb01-48dc-8a54-3dfbc699be5d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3dd730aeb4d4846bff6331addd55a3bccadd00db348b14e907e6c22804361008", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26ffa5ee-477a-45b9-81e6-397321ab72ad", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "180428aee136c91183b01c1952fb0d2d8449912bddc0c217f64836185e96dfc8", "class_name": "RelatedNodeInfo"}}, "hash": "f72549613d70c86bf6c734cf8d60bb4229d2c4114d6c3f9f078b48b1b7a8823c", "text": "During text representation process, it is necessary to extract\nand mining textual information. Semantic similarity computation is the link\nbetween text modelling and representation with application on potential\ninformation text layer. Clustering algorithms are to extract semantic\ninformation to facilitate similarity calculation for text classification and\nclustering effectiveness.\n\n### 9.2.2 Vector Space Model in IR\n\nVector Space Model (Salton et al. 1975) was a leading IR method from 1960 to\n1970. Queries and retrieved documents are represented as vectors with\ndimensionality related to word list size in this model. A retrieved document D\ncan be represented as a vector of lexical items: _D_ _i_ = ( _d_ 1, _d_ 2, , ,\n, _d_ _n_ ), where _d_ _i_ is the weight of a _i_ th lexical item in _D_ _i_.\nQuery Q is expressed as a lexical item vector: _Q_ = ( _q_ 1, _q_ 2, , , , _q_\n_n_ ), where _q_ _i_ is the weight of _i_ th lexical item in query term. The\nrelevance is determined by computing the distance between lexical item vectors\nof the retrieved document and query based on this representation. Although it\ncannot prove cosine relevance is superior to other similarity methods, but it\nachieved satisfactory performance according to search engines evaluation\nresults. Cosine similarity for angle between retrieved document and query\ncalculation is expressed as:\n\n![$$ \\\\mathrm{sim}\\\\left\\({D}_i,Q\\\\right\\)=\\\\frac{\\\\overrightarrow{d_i}\\\\cdot\n\\\\overrightarrow{q}}{\\\\left|\\\\overrightarrow{d_i}\\\\right|\\\\times\n\\\\left|\\\\overrightarrow{q}\\\\right|}=\\\\frac{\\\\sum_{j=1}^n{d}_{ij}\\\\cdot\n{q}_j}{\\\\sqrt{\\\\sum_{j=1}^n{d}_{ij}^2\\\\cdot {\\\\sum}_{j=1}^n{q}_j^2}}\n$$](../images/533412_1_En_9_Chapter/533412_1_En_9_Chapter_TeX_Equ1.png)\n\n(9.1)\n\nEquation (9.1) is the weights for dot or inner product of all word terms in\nquery matching documents. There are many words item weights for vector space\nmodels. Most of the weighting methods are based on TF (Term-Frequency)\nvariation. Inverted document frequency (IDF) (Aizawa 2003) represents the\nnumber of term occurrences in retrieved document and reveals lexical term\nsignificance in the entire document dataset. A lexical item is insignificant\nwith high occurrence frequency in multiple retrieved documents.\n\nThere are other text representations methods in addition to vector space\nmodel, e.g. phrase or concept representations. Although phrase representation\ncan improve semantic contents, the reduced statistical quality of feature\nvector becomes sparse and difficult to extract statistical properties applying\nmachine learning algorithms. Figures 9.1 and 9.2 show a text is encoded by\nSentence Transformers (Reimers and Gurevych 2019) to demonstrate and compute\ncosine similarity between embeddings. It uses a pre-trained model to encode\ntwo sentences and outperform other pre-train model like BERT (Vaswani et al.\n2017).\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig1_HTML.png)\n\nFig. 9.1\n\nSentence transformers frame\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig2_HTML.png)\n\nFig. 9.2\n\nBERT frame\n\nIt is natural to identify the combination with the highest cosine similarity\nscore. By doing so, an intense ranking scheme is used as shown in Fig. 9.3 to\nidentify the highest scoring pair with a secondary complexity. However, it may\nnot work for long lists of sentences.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig3_HTML.png)\n\nFig. 9.3\n\nThe singer example of vector space model\n\nA chunking concept to divide corpus into smaller parts is shown in Figs. 9.4\nand 9.5. For example, parse 1000 sentences at a time to search the rest (all\nother sentences) of corpus or search a list of 20k sentences to divide into 20\n\u00d7 1000 sentences.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26ffa5ee-477a-45b9-81e6-397321ab72ad": {"__data__": {"id_": "26ffa5ee-477a-45b9-81e6-397321ab72ad", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a3484a5-3d67-485f-b143-70f5f1aaa459", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f72549613d70c86bf6c734cf8d60bb4229d2c4114d6c3f9f078b48b1b7a8823c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ebcec406-cfa4-4318-bf26-b41ff5f72485", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "5ca7c59a0a69b8a2d5a1c9506290a46c2fa96274bf7849a5ae0bcf9f49e7c630", "class_name": "RelatedNodeInfo"}}, "hash": "180428aee136c91183b01c1952fb0d2d8449912bddc0c217f64836185e96dfc8", "text": "9.2\n\nBERT frame\n\nIt is natural to identify the combination with the highest cosine similarity\nscore. By doing so, an intense ranking scheme is used as shown in Fig. 9.3 to\nidentify the highest scoring pair with a secondary complexity. However, it may\nnot work for long lists of sentences.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig3_HTML.png)\n\nFig. 9.3\n\nThe singer example of vector space model\n\nA chunking concept to divide corpus into smaller parts is shown in Figs. 9.4\nand 9.5. For example, parse 1000 sentences at a time to search the rest (all\nother sentences) of corpus or search a list of 20k sentences to divide into 20\n\u00d7 1000 sentences. Each query is compared with 0\u201310k sentences first and\n10k\u201320k sentences to reduce memory storage. The increases of these two values\nintensified speed and memory storage, then identified pair with the highest\nsimilarity to extract top K scores for each query as opposed to extract and\nsort scores for all _n_ 2 pairs.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig4_HTML.png)\n\nFig. 9.4\n\nMultiple examples of vector space model\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig5_HTML.png)\n\nFig. 9.5\n\nChunk multiple examples of vector space model\n\nSuch method is faster than brute force methods due to fewer samples. In\npractical industrial scenarios, more attention is paid to the speed of pre-\ntrained models, encoding methods, and data retrieval. For example, two-tower\nmodel (Yang et al. 2020) and Wide&Deep model (Cheng et al. 2016) are shown in\nFigs. 9.6 and 9.7.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig6_HTML.png)\n\nFig. 9.6\n\nTwo-tower model (Yang et al. 2020)\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig7_HTML.png)\n\nFig. 9.7\n\nWide&Deep model (Cheng et al. 2016)\n\n### 9.2.3 Term Distribution Models in IR\n\nProbabilistic Ranking Principle (PRP) models were firstly proposed by Croft\nand Harper in 1979 (Croft and Harper 1979) to compute query relevance degrees\nand retrieval. PRP regards IR as a process of statistical inference, where an\nIR system predicts query relevance from retrieved documents and sorts in\ndescending order based on predicted relevance scores. This approach is like\nBayesian model machine learning. A PRP model combines relevant feedback\ninformation with IDF and estimate each item\u2019s probabilities to optimize search\nengine retrieval performance. However, it is a difficult task to estimate each\nprobability accurately in practical applications. Okapi BM25 (Whissell and\nClarke 2011) retrieval model had solved the difficulties encountered by PRP\nmodel with satisfactory performance in TREC retrieval experiments and\ncommercial search engines. Many IR researchers had modifications based on BM25\nmodel resulting in many variations, the most common form is as follows:\n\n![$$ {\\\\displaystyle \\\\begin{array}{c}\\\\mathrm{sim}\\\\left\\(Q,D\\\\right\\)=\\\\sum\n\\\\limits_{q\\\\in Q}\\\\log\n\\\\frac{\\\\left\\({r}_i+0.5\\\\right\\)/\\\\left\\(R-{r}_i+0.5\\\\right\\)}{\\\\left\\({n}_i-{r}_i+0.5\\\\right\\)/\\\\left\\(N-{n}_i-R+{r}_i+0.5\\\\right\\)}\\\\\\\\\n{}\\\\cdot \\\\frac{\\\\left\\({k}_1+1\\\\right\\){f}_i}{K+{f}_i}\\\\cdot\n\\\\frac{\\\\left\\({k}_2+1\\\\right\\)q{f}_i}{k_2+q{f}_i}\\\\end{array}}\n$$](../images/533412_1_En_9_Chapter/533412_1_En_9_Chapter_TeX_Equ2.png)\n\n(9.2)\n\nThere are two approaches to consider which is the best BM25 method:\n\n  1. 1.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ebcec406-cfa4-4318-bf26-b41ff5f72485": {"__data__": {"id_": "ebcec406-cfa4-4318-bf26-b41ff5f72485", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26ffa5ee-477a-45b9-81e6-397321ab72ad", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "180428aee136c91183b01c1952fb0d2d8449912bddc0c217f64836185e96dfc8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f90492d-4f3d-453b-ba4b-d46faef3e44c", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "ace3d295cfd89bc86b0ad7ecb19fcc480563654f9d5a552bfcffa19e544a2854", "class_name": "RelatedNodeInfo"}}, "hash": "5ca7c59a0a69b8a2d5a1c9506290a46c2fa96274bf7849a5ae0bcf9f49e7c630", "text": "1.\n\nBM25 + Word2Vec embedding across all documents.\n\n  2. 2.\n\nBM25 + BERT \\+ Word2Vec embedding for each top-k documents, select the most\nsimilar sentence embedding across top-k paragraphs.\n\nWord2vec (Church 2017) is about word occurrences proportions in relations\nholding in general over large text corpora and combine vectors of similar\nwords into a vector space called distributional hypothesis. Word2vec\nembeddings are to compare query with sentence embeddings to select the one\nwith higher cosine similarity.\n\nTransformer-based neural network models are popular NLP research areas on\nenhanced parallelized processing capabilities. BERT is among one that uses\nTransformer-based deep bidirectional encoders to learn contextual semantic\nrelationships between lexical items and performed satisfactory in many NLP\ntasks.\n\nIt began to retrieve document with the most relevant documents followed by\nparagraphs and extract sentences from selected paragraphs. BERT embeddings are\nused to compare query with paragraphs and select the one with higher cosine\nsimilarity. Once relevant paragraphs are available, select sentence with\nanswer by comparing sentence embeddings based on Word2Vec embeddings trained\non the whole dataset, then average word embeddings in the paragraph with BM25\nscore calculation are shown in Fig. 9.8.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig8_HTML.png)\n\nFig. 9.8\n\nSample code for Word2vec embeddings with BM25 score calculation\n\nCommon words queries occurred rarely in documents with a higher number of\noccurrences produce sparse distribution. Contrarily, there will be similar\nscores at many documents if common words with same frequency occurred across\ndocuments. Documents distribution with scores and codes are shown in Figs. 9.9\nand 9.10.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig9_HTML.png)\n\nFig. 9.9\n\nDocuments distribution with scores and codes\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig10_HTML.png)\n\nFig. 9.10\n\nBM25 results\n\nSince word2vec relies heavily on each occurrence frequency, thus, it may\nproduce satisfactory performance on specific queries while the same for BERT\non general queries.\n\nThe results of two selected queries showed that query (Sentence 1) achieved\nsatisfactory performance on specific/rare terminology, while second query\n(Sentence 2) achieved satisfactory performance on normal terminology. They\ndepend on words specification level in the query. For queries which have\nspecific/rare terminology performed satisfactorily with the most similar\nsentences across all documents. For queries which have general terms, e.g.\nage, human, climate performed satisfactorily with the most relevant documents\ninstead of embeddings comparison across all of them. Thus, it is reasonable to\ncompare each time the results of two approaches and select the appropriate one\nbased on words distribution for each query.\n\n### 9.2.4 Latent Semantic Indexing in IR\n\nTerm Distribution Models in IR is a rapid and effective model. It uses topics\nto express implicit semantics of a document as index to replace incomplete,\nunreliable search terms by reliable indicants based on two assumptions:\n\n  1. 1.\n\nWords have common topics in document.\n\n  2. 2.\n\nWords not in document less likely to be related.\n\nTopic is filtered out by keywords in the Doc. Thus, _P_ = ( _\u03c9_ / _Doc_ )\nprobability distribution table is introduced: the statistics of word frequency\n(frequency) in the document, i.e. the law of large numbers.\n\n![$$ P\\\\left\\(\\\\omega |{\\\\mathrm{Topic}}_D\\\\right\\)\\\\approx P\\\\left\\(\\\\omega\n|D\\\\right\\)= tf\\\\left\\(\\\\omega, D\\\\right\\)/ len\\(D\\)\n$$](../images/533412_1_En_9_Chapter/533412_1_En_9_Chapter_TeX_Equ3.png)\n\n(9.3)\n\n_Topic_ is regarded as a language model, and _P_ = ( _\u03c9_ / _Doc_ ) is the\nprobability of word generation in this language model so the word not only\noccur in topic, but has probability generated.\n\nThere are two methods of sorting according to statistical language model when\nquery _Q_ is given, which are (1) _Query-likelihood_ and (2) _Document-\nlikelihood_ methods.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f90492d-4f3d-453b-ba4b-d46faef3e44c": {"__data__": {"id_": "4f90492d-4f3d-453b-ba4b-d46faef3e44c", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ebcec406-cfa4-4318-bf26-b41ff5f72485", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "5ca7c59a0a69b8a2d5a1c9506290a46c2fa96274bf7849a5ae0bcf9f49e7c630", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b367f9e5-a1b5-4174-9aa3-b7fce1d27b22", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "e167f456e9b82aca384efeaf155ebf2b2b4cce18931c422dbe142a89c35175c6", "class_name": "RelatedNodeInfo"}}, "hash": "ace3d295cfd89bc86b0ad7ecb19fcc480563654f9d5a552bfcffa19e544a2854", "text": "the law of large numbers.\n\n![$$ P\\\\left\\(\\\\omega |{\\\\mathrm{Topic}}_D\\\\right\\)\\\\approx P\\\\left\\(\\\\omega\n|D\\\\right\\)= tf\\\\left\\(\\\\omega, D\\\\right\\)/ len\\(D\\)\n$$](../images/533412_1_En_9_Chapter/533412_1_En_9_Chapter_TeX_Equ3.png)\n\n(9.3)\n\n_Topic_ is regarded as a language model, and _P_ = ( _\u03c9_ / _Doc_ ) is the\nprobability of word generation in this language model so the word not only\noccur in topic, but has probability generated.\n\nThere are two methods of sorting according to statistical language model when\nquery _Q_ is given, which are (1) _Query-likelihood_ and (2) _Document-\nlikelihood_ methods.\n\n#### 9.2.4.1 Query-Likelihood\n\nDetermine _M_ _D_ , corresponding to each Doc, user\u2019s Query is denoted as _Q_\n= ( _q_ 1, _q_ 2, , , , _q_ _n_ ). Query probability will be generated under\nthe _language model_ of each document that can be calculated as follows\n(Zhuang and Zuccon 2021):\n\n![$$ P\\\\left\\({q}_1\\\\dots {q}_k\\\\;|{M}_D\\\\right\\)=\\\\prod\n\\\\limits_{i=1}^kP\\\\left\\({q}_i|{M}_D\\\\right\\)\n$$](../images/533412_1_En_9_Chapter/533412_1_En_9_Chapter_TeX_Equ4.png)\n\n(9.4)\n\nSearch results are obtained by sorting all computed results. However, this\nmethod calculates the probability for each Doc independently from other Docs,\nand the relevant documents are not utilized.\n\n#### 9.2.4.2 Document-Likelihood\n\nDetermine each Query corresponding _M_ _Q_. Calculate the probability that any\ngiven document will be generated under the query\u2019s _language model_ (Zhuang\nand Zuccon 2021):\n\n![$$ P\\\\left\\(D|{M}_Q\\\\right\\)=\\\\prod \\\\limits_{\\\\omega \\\\in\nD}P\\\\left\\(\\\\omega |{M}_Q\\\\right\\)\n$$](../images/533412_1_En_9_Chapter/533412_1_En_9_Chapter_TeX_Equ5.png)\n\n(9.5)\n\nThe object of one-mode factor analysis traditionally is a matrix composed of\nidentical object-pair types of relationships. An example is a\ndocument\u2013document matrix. The matrix elements may be evaluated for similarity\nbetween documents manually. This symmetric square matrix is decomposed into\ntwo matrices by eigen-analysis. The decomposed matrix is composed of linearly\nindependent factors. Many of the factors are tiny that can be ignored usually\nproducing an original matrix approximation.\n\nTwo-mode factor analysis object is a matrix consisting of object\u2013pair\nrelationships. This matrix can be decomposed into term\u2013term,\ndocument\u2013document, and term\u2013document matrices using singular value\ndecomposition (SVD) (Aharon et al. 2006). SVD reconstructs spatial response to\nthe main patterns association between data by ignoring less significant\neffects. Thus, a term which does not occur in a document may be immediately\nadjacent to that document in semantic space based on identified association\npatterns. The information located in semantic space has a role in semantic\nindex. SVD model test and lean with results in Latent Semantic Indexing in IR\nis shown in Fig. 9.11.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig11_HTML.png)\n\nFig. 9.11\n\nSVD frame\n\nSingular-value decomposition and corresponding validation results are shown in\nFigs. 9.12 and 9.13.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig12_HTML.png)\n\nFig. 9.12\n\nExample of singular-value decomposition\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig13_HTML.png)\n\nFig.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b367f9e5-a1b5-4174-9aa3-b7fce1d27b22": {"__data__": {"id_": "b367f9e5-a1b5-4174-9aa3-b7fce1d27b22", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f90492d-4f3d-453b-ba4b-d46faef3e44c", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "ace3d295cfd89bc86b0ad7ecb19fcc480563654f9d5a552bfcffa19e544a2854", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f5036856-90ce-4a0e-a95b-e49576288d61", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9199a75cdc66cfb6dfc59120d2529608b36dae951bf2e5bfc8049470eff9f5a1", "class_name": "RelatedNodeInfo"}}, "hash": "e167f456e9b82aca384efeaf155ebf2b2b4cce18931c422dbe142a89c35175c6", "text": "The information located in semantic space has a role in semantic\nindex. SVD model test and lean with results in Latent Semantic Indexing in IR\nis shown in Fig. 9.11.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig11_HTML.png)\n\nFig. 9.11\n\nSVD frame\n\nSingular-value decomposition and corresponding validation results are shown in\nFigs. 9.12 and 9.13.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig12_HTML.png)\n\nFig. 9.12\n\nExample of singular-value decomposition\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig13_HTML.png)\n\nFig. 9.13\n\nValidation results\n\n### 9.2.5 Discourse Segmentation in IR\n\nDocument contents combine with articulated parts such as paragraphs exalt\nautomatic documents segmentation according to meanings using machine learning\nmethods to compare two adjacent sentences similarity in turn and generate\nsegmentation point with the lowest similarity. This unsupervised method is\ncalled TextTiling (Hearst 1997) as shown in Fig. 9.14. Further, supervised\nlearning methods can also be used such as classifiers constructions (Florian\n2002) or sequence models (Keneshloo et al. 2019) to detect segmentation point.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig14_HTML.png)\n\nFig. 9.14\n\nExamples of discourse segmentation\n\nRhetorical Structure Theory (RST) framework (Taboada and Mann 2006) is a\ncommonly used framework for parsing discourse as shown in Fig. 9.15. RST\ncommon relations in English are conjunction, justify, concession, elaboration,\netc. as shown in Figs. 9.16 and 9.17.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig15_HTML.png)\n\nFig. 9.15\n\nExample of rhetorical structure theory\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig16_HTML.png)\n\nFig. 9.16\n\nExamples of relations\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig17_HTML.png)\n\nFig. 9.17\n\nAttention map\n\nThere are two approaches to identify relationships: (1) rule-based on iconic\nwords such as _but, so, for example_ , and (2) machine learning with commonly\nfeatures such as bag of words (Zhang et al. 2010), Discourse markers (Fraser\n1999), Starting/ending N-grams (Robertson and Willett 1998), Location in the\ntext (Rothkopf 1971), Syntax features (Sadler and Spencer 2001), Lexical and\ndistributional similarities (Weeds et al. 2004).\n\nDiscourse segmentation task is a significant evaluation indicator for NLP\ndevelopment directions. From the application perspective, discourse\nsegmentation can assist users rely on intelligence to improve productivity,\nits technology core value can convert semi-structured and unstructured data to\nspecific description structured in turn to support substantial downstream\napplications.\n\n## 9.3 Text Summarization Systems\n\n### 9.3.1 Introduction to Text Summarization Systems\n\n#### 9.3.1.1 Motivation\n\nThere is excess information from copious sources to obtain the latest\ninformation daily. Although automatic and accurate summarization systems can\nassist users to simplify, identify, and understand key information in the\nshortest possible time but they remain challenging as new words and complex\ntext structure documents are available constantly.\n\n#### 9.3.1.2 Task Definition\n\nText summarization process generates text (document or document) summaries by\nrewriting and summarizing long text into short form (Mahalakshmi and Fatima\n2022). It refers to extract or refine text or text set key points through\ntechnologies to display original text or text set main contents or general\nidea. Text generation task is an information compression technique, whereas a\nsummarization process is considered as a function where input is a document or\ndocuments, and output is an input texts summary. Hence, input and output are\nquintessential types to classify summary tasks.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5036856-90ce-4a0e-a95b-e49576288d61": {"__data__": {"id_": "f5036856-90ce-4a0e-a95b-e49576288d61", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b367f9e5-a1b5-4174-9aa3-b7fce1d27b22", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "e167f456e9b82aca384efeaf155ebf2b2b4cce18931c422dbe142a89c35175c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3411dae-27c7-4320-b6e1-9e4139c1c9eb", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "6649493e5af683d41430c36b476763e8153df24002237595fdb47b01193f77dd", "class_name": "RelatedNodeInfo"}}, "hash": "9199a75cdc66cfb6dfc59120d2529608b36dae951bf2e5bfc8049470eff9f5a1", "text": "Although automatic and accurate summarization systems can\nassist users to simplify, identify, and understand key information in the\nshortest possible time but they remain challenging as new words and complex\ntext structure documents are available constantly.\n\n#### 9.3.1.2 Task Definition\n\nText summarization process generates text (document or document) summaries by\nrewriting and summarizing long text into short form (Mahalakshmi and Fatima\n2022). It refers to extract or refine text or text set key points through\ntechnologies to display original text or text set main contents or general\nidea. Text generation task is an information compression technique, whereas a\nsummarization process is considered as a function where input is a document or\ndocuments, and output is an input texts summary. Hence, input and output are\nquintessential types to classify summary tasks.\n\n#### 9.3.1.3 Basic Approach\n\nSummarization approaches are mainly divided into _extractive_ and\n_abstractive_ (Chen and Zhuge 2018).\n\nExtractive methods select important phrases from input text, combine them to\nform a summary like a copy and paste process. Many traditional text\nsummarization methods use _Extractive Text Summary_ _(ETS)_ because it is\nsimple to generate sentences without grammatical errors but cannot reflect\nexact sentences meanings. They are inflexible to use novel expressions, words,\nor connectors outside text descriptions.\n\n_Abstractive Text Summary_ _(ATS)_ methods use language generation methods to\nre-organize contents, generate new words, and conclude the implied information\nas compared with ETS. They paraphrase text meanings composed of new words with\noriginal words summary (Agrawal 2020) and mimic human understanding to develop\ncontents which may not contain in actual document text (Malki et al. 2020).\n\n#### 9.3.1.4 Task Goals\n\nSummarization task objectives are to assist users to understand raw text\nwithin a short period as shown in Fig. 9.18.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig18_HTML.png)\n\nFig. 9.18\n\nSummarization tasks objectives\n\n#### 9.3.1.5 Task Sub-processes\n\nSummarization tasks are divided into the following modules as shown in Fig.\n9.19.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig19_HTML.png)\n\nFig. 9.19\n\nSummarization tasks sub-processes\n\nInput document or documents are first combined and preprocessed from\ncontinuous text form to split sentences. The sentences will be encoded into\nvectors form data to fit into a matrix for similarity scores calculation to\nobtain sentence rankings, followed by a summary with the highest possibility\naccording to the ranking list.\n\n### 9.3.2 Text Summarization Datasets\n\nText summarization datasets commonly used include DUC (DUC 2022), New York\nTimes (NYT 2022), CNN/Daily Mail (CNN-DailyMail 2022), Gigaword (Gigaword\n2022), and LCSTS datasets (LCSTS 2022).\n\nDUC datasets (DUC 2022) are the most fundamental text summarization datasets\ndeveloped and used for testing purposes only. They consist of 500 news\narticles, each with four human-written summaries.\n\nNYT datasets (NYT 2022) contain articles published in New York Times between\n1996 and 2007 with abstracts compiled by experts. The abstract datasets are\nsometimes incomplete and sporadic short sentences with average of 40 words.\n\nCNN/Daily Mail datasets (CNN-DailyMail 2022) are widely used multi-sentence\nsummary datasets often trained by generative summary system. They have (a)\nanonymized version to include entity names and (b) non-anonymized version to\nreplace entities with specific indexes.\n\nGigaword datasets (Gigaword 2022) are abstracts comprising of the first\nsentence and article title with heuristic rules of approximately 4-million\narticles.\n\nLCSTS datasets (LCSTS 2022) are Chinese short texts abstract datasets\nconstructed by Weibo (2022).\n\n### 9.3.3 Types of Summarization Systems\n\nText summarization task for input documents can be divided into two types:\n\n  1. 1.\n\nSingle document summarization considers each input is one document.\n\n  2. 2.\n\nMultiple document summarization considers input has several documents.\n\nText summarization task viewpoint can be divided into three classes:\n\n  1. 1.\n\nQuery-focused summarization adds viewpoint to query.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3411dae-27c7-4320-b6e1-9e4139c1c9eb": {"__data__": {"id_": "b3411dae-27c7-4320-b6e1-9e4139c1c9eb", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5036856-90ce-4a0e-a95b-e49576288d61", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9199a75cdc66cfb6dfc59120d2529608b36dae951bf2e5bfc8049470eff9f5a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6c17a3f-c81a-47d8-b715-5e99fe7593fc", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "ddc5768b2625f10a7f0846459dc53356f776177740c56a5381318eb1550081fa", "class_name": "RelatedNodeInfo"}}, "hash": "6649493e5af683d41430c36b476763e8153df24002237595fdb47b01193f77dd", "text": "They have (a)\nanonymized version to include entity names and (b) non-anonymized version to\nreplace entities with specific indexes.\n\nGigaword datasets (Gigaword 2022) are abstracts comprising of the first\nsentence and article title with heuristic rules of approximately 4-million\narticles.\n\nLCSTS datasets (LCSTS 2022) are Chinese short texts abstract datasets\nconstructed by Weibo (2022).\n\n### 9.3.3 Types of Summarization Systems\n\nText summarization task for input documents can be divided into two types:\n\n  1. 1.\n\nSingle document summarization considers each input is one document.\n\n  2. 2.\n\nMultiple document summarization considers input has several documents.\n\nText summarization task viewpoint can be divided into three classes:\n\n  1. 1.\n\nQuery-focused summarization adds viewpoint to query.\n\n  2. 2.\n\nGeneric summarization is generic.\n\n  3. 3.\n\nUpdate summarization is a special type which sets _difference_ (update)\nviewpoint.\n\nSummarization systems based on contents can be divided into four types:\n\n  1. 1.\n\nIndicative summarization describes contexts without revealing details\nespecially the endings, it contains partial information only.\n\n  2. 2.\n\nInformative summarization contains all information in a document or documents.\n\n  3. 3.\n\nKeyword Summarization reveals output generation is sporadic text which\ncontains phrases or words of input documents.\n\n  4. 4.\n\nHeadline Summarization is usually single line summary\n\nThese summarization systems can be divided according to summary languages such\nas Arabic (Elsaid et al. 2022), Chinese (Yang et al. 2012), English and\nSpanish summarization systems, etc.\n\n### 9.3.4 Query-Focused Vs Generic Summarization Systems\n\nText summarization can be query-focused or generic. Summary associative with\nquery shows document contents is relative to initial search query. A query-\nrelated summary generation is a process of retrieving query-related\nsentences/paragraphs from a document that has a strong similarity to text\nretrieval process. Hence, abstracts relevant searches are often undertaken by\nextending traditional IR techniques with many text abstracts in the literature\nfall into this category. A general summary, however, provides an overall sense\nof the document\u2019s contents. A proper general summary should cover main topics\nand minimize redundancy. Since there are no queries or topics to feed into\nsummarization process, it is difficult to develop a high-quality general\nsummarization method for evaluation (Gong and Liu 2001).\n\n#### 9.3.4.1 Query-Focused Summarization Systems\n\n_Query-Focused Summarization (QFS)_ is primarily addressed using extractive\nmethods to produce text lacks coherence. QFS methods can overcome these\nlimitations and improve incoherent texts availability. A Relevance Sensitive\nAbstractive QFS (RSA-QFS) framework (Baumel et al. 2018) is shown in Fig.\n9.20.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig20_HTML.png)\n\nFig. 9.20\n\nRSA-QFS framework\n\nThis model assumes that a trained abstractive model includes reusable language\nknowledge to accomplish QFS tasks. Methods of enhancing this pre-trained\nsingle document abstraction model with explicit modelling of query\ndependencies are studied to improve multiple input documents operating ability\nand adjust generated abstractions lengths accordingly.\n\nFurther, a sequence-to-sequence (seq2seq) architecture is applied to obtain\nsum via an iterative extraction or abstraction pairs process: identify\nrelevant content batches from multiple documents and abstract into a coherent\ntext segments sequence.\n\nQFS task includes two stages as shown in Fig. 9.21:\n\n  1. 1.\n\na relevance model to determine passages relevance to input query from source\ndocuments and\n\n  2. 2.\n\na generic summarization method to combine relevant passages into a coherent\nsummary\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig21_HTML.png)\n\nFig. 9.21\n\nTwo stages of QFS\n\nQuery-related text summarization are practical for answering questions such as\nwhether a whole or partial document has relevance to a user\u2019s query. Query-\nrelated summaries do not provide an overall sense of the document\u2019s content,\nthey have query bias and unsuitable for content summaries to answer questions\nsuch as document category, key points, text summary, etc.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6c17a3f-c81a-47d8-b715-5e99fe7593fc": {"__data__": {"id_": "f6c17a3f-c81a-47d8-b715-5e99fe7593fc", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b3411dae-27c7-4320-b6e1-9e4139c1c9eb", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "6649493e5af683d41430c36b476763e8153df24002237595fdb47b01193f77dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "917ce3d2-de34-4533-82ae-de43e0475dee", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8a0bc86474354506f24a0c68312675ed8d2023faa5942efaab50876710b80f48", "class_name": "RelatedNodeInfo"}}, "hash": "ddc5768b2625f10a7f0846459dc53356f776177740c56a5381318eb1550081fa", "text": "QFS task includes two stages as shown in Fig. 9.21:\n\n  1. 1.\n\na relevance model to determine passages relevance to input query from source\ndocuments and\n\n  2. 2.\n\na generic summarization method to combine relevant passages into a coherent\nsummary\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig21_HTML.png)\n\nFig. 9.21\n\nTwo stages of QFS\n\nQuery-related text summarization are practical for answering questions such as\nwhether a whole or partial document has relevance to a user\u2019s query. Query-\nrelated summaries do not provide an overall sense of the document\u2019s content,\nthey have query bias and unsuitable for content summaries to answer questions\nsuch as document category, key points, text summary, etc.\n\n#### 9.3.4.2 Generic Summarization Systems\n\nA proper generic summarization should cover main topics as many as possible\nand minimize redundancy leading to fractious system generation and evaluation.\nIt often lacks consensus on summary output and performance judgments without\nquery provisions and topics to summary task.\n\nTypical generic summarization ranking models and selected sentences are based\non relevance similarity values and other semantic analysis (Gong and Liu\n2001).\n\n### 9.3.5 Single and Multiple Document Summarization\n\nSingle document extraction in journalism has developed to multi-document\nextraction since 1990. A variety of news articles, such as Google News (Google\n2022), Columbia News Blaster (Columbia 2022), and News Essence (NewsInEssence\n2022) are inspired by multi-document summaries. The reason is that individual\ndocuments always produce contradictory results through overlapping information\nfrom multiple documents (Alami et al. 2015) which may affect the performance\nof summarization results.\n\nSingle document summarization research method gradually faded in past decades\n(Svore et al. 2007) as mainstream research focused on multi-document\nsummarization which could reduce text size, gather ideas, compare documents,\nmaintain syntactic and semantic relationships (Pervin and Haque 2013).\n\n#### 9.3.5.1 Single Document Summarization\n\n_Single document summarization\u2019s_ challenge is to identify or generate\ninformative sentences significance of the document because it often has\ninconsistent and intermittent information.\n\nSalient features like sentence placement are early research (Baxendale 1958)\nwhere 200 paragraphs selected and identified paragraphs have topic sentences\nat the beginning and end of paragraphs with 85% and 7%, respectively.\n\nA corpus with around 400 technical documents research focusing on word\nfrequency and word position, and cue words and skeleton were proposed in 1969\n(Edmundson 1969). Results showed that extracted summary to actual summary\naccuracy rate was about 44%.\n\nFurther, lexical indicators (Rath et al. 1961), cohesion (Hasan 1984) and\nsemantic relationships (Halliday and Hasan 1976), algebraic method such as\nna\u00efve-bayes classifier processed features like uppercase words, lengths, words\nposition (Kupiec et al. 1995), symbolic word knowledge (Hovy and Lin 1999),\nhuman abstraction concept (Jing 2000) are research areas applied in this\nfield.\n\n#### 9.3.5.2 Multiple Document Summarization\n\n_Multiple document summarization_ similarity measures and extractive\ntechniques are comparable to single document summarization.\n\nIt used clustering to identify common themes (Erkan and Radev 2004), composite\nsentences from clusters (Barzilay et al. 1999), maximal marginal relevance\n(MMR) (Carbonell and Goldstein 1998), and concatenated to multilingual\nenvironment (Evans 2005).\n\nFurther, TFI X IDFI techniques (Salton 1989), TF/IDF (Fukumoto 2004), word\nhierarchical technique for frequent terms (You et al. 2009), graph-based\nmethods (Mani and Bloedorn 1997; Wan 2008), sentence co-relation method\n(Hariharan et al. 2013), logical closeness (Zhu and Zhao 2012), and query-\noriented approach (Agarwal et al. 2011) are well-developed.\n\n### 9.3.6 Contemporary Text Summarization Systems\n\n#### 9.3.6.1 Contemporary Extractive Text Summarization (ETS) System\n\nText summarization research methods aim to (Dong 2018):\n\n  1. 1.\n\nacquire important sentences.\n\n  2. 2.\n\npredict sentence option according to ranking sentences.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "917ce3d2-de34-4533-82ae-de43e0475dee": {"__data__": {"id_": "917ce3d2-de34-4533-82ae-de43e0475dee", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f6c17a3f-c81a-47d8-b715-5e99fe7593fc", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "ddc5768b2625f10a7f0846459dc53356f776177740c56a5381318eb1550081fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9b0b7a8-cf13-4756-8a80-5fa4be811099", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "99cf2d007498627f2188fccb83ccf8227f55a9a7e3a79a425ca30d9648b2b731", "class_name": "RelatedNodeInfo"}}, "hash": "8a0bc86474354506f24a0c68312675ed8d2023faa5942efaab50876710b80f48", "text": "Further, TFI X IDFI techniques (Salton 1989), TF/IDF (Fukumoto 2004), word\nhierarchical technique for frequent terms (You et al. 2009), graph-based\nmethods (Mani and Bloedorn 1997; Wan 2008), sentence co-relation method\n(Hariharan et al. 2013), logical closeness (Zhu and Zhao 2012), and query-\noriented approach (Agarwal et al. 2011) are well-developed.\n\n### 9.3.6 Contemporary Text Summarization Systems\n\n#### 9.3.6.1 Contemporary Extractive Text Summarization (ETS) System\n\nText summarization research methods aim to (Dong 2018):\n\n  1. 1.\n\nacquire important sentences.\n\n  2. 2.\n\npredict sentence option according to ranking sentences.\n\nThe extractive summarization for proper sentences selection from original\nsource text are required to:\n\n  1. 1.\n\ninclude logical and consistent summary information from original text.\n\n  2. 2.\n\nreduce similar and unimportant sentences information redundancy.\n\nLead 3 is a commonly used and effective method to extract the first three\nsentences as topic titles of an article. When dealing with important\nsentences, document equivalence to document topic and relevant sentences\nposition characteristics are considered. Topic modelling, frequency-based\nmodels LSA and Bayesian are methods applied (Farsi et al. 2021).\n\nExtractive summarization produces incoherent summaries compared with manual\nones, its shortcomings include unresolved anaphora, unreadable sentence order,\nlacks textual cohesion to extract salient information from long sentences.\nWhen the system focuses on a sentence, it extracts the entire sentence\n(Nallapati et al. 2017).\n\n#### 9.3.6.2 Graph-Based Method\n\nGraph-based ranking algorithms are successfully used in citation analysis,\nlink social networks\u2019 structure analysis and the World Wide Web.\n\nThey generate graphs from input document and summary by considering the\nrelationships between nodes (units of text) (Chi and Hu 2021). TextRank\n(Mihalcea and Tarau 2004) is a typical graph-based approach that has developed\nmany models. A summarization of TextRank system to extract keywords from a\nsample text and graph is shown in Figs. 9.22 and 9.23.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig22_HTML.png)\n\nFig. 9.22\n\nSample text\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig23_HTML.png)\n\nFig. 9.23\n\nSample graph for key phrase extraction in TextRank\n\nThis kind of system is based on PageRank algorithm (Langville and Meyer 2006)\napplied by Google\u2019s search engine, its algorithm principle is _linked pages\nare good, and even better if they come from multiple linked pages_. Links\nbetween pages are represented by matrices like circular tables. This matrix\ncan be converted to a transition probability matrix divided by the sum of\nlinks per page, and the page will be moved by page viewer following a feature\nmatrix in Fig. 9.24.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig24_HTML.png)\n\nFig. 9.24\n\nPageRank algorithm process\n\nTextRank processes words and sentences as pages in PageRank, its algorithm\ndefines _text units_ and adds them as nodes in a graph with _relations_ are\ndefined between text units and added as edges in the graph. Generally, the\nweights of edges are set by similarity or score values.\n\nThen, PageRank algorithm is used to solve the graph. There are other similar\nsystems such as LexRank (Erkan and Radev 2004) to consider sentences as nodes\nand similarity as relations or weights, i.e. IDF-modified cosine similarity to\ncalculate similarity.\n\n#### 9.3.6.3 Feature-Based Method\n\nFeature-based model extracts sentences features and evaluates their\nsignificances. There are many representative studies that include Luhn\u2019s\nAlgorithm (Luhn 1958), TextTeaser and SummaRuNNer (Nallapati et al. 2017).\n\nLuhn\u2019s Algorithm is used to evaluate input words significance calculated by\nfrequency. TextTeaser is an automatic feature-based summarization algorithm.\nSummaRuNNer is implemented by deep neural networks (DNN) structure as shown in\nFig. 9.25.\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9b0b7a8-cf13-4756-8a80-5fa4be811099": {"__data__": {"id_": "b9b0b7a8-cf13-4756-8a80-5fa4be811099", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "917ce3d2-de34-4533-82ae-de43e0475dee", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8a0bc86474354506f24a0c68312675ed8d2023faa5942efaab50876710b80f48", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba741414-e089-4eaa-af02-1bf9ebf40169", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "fc29b73c109dfe2329e09e8b84128d1f5819cc639e533d064cc21c15df188dd2", "class_name": "RelatedNodeInfo"}}, "hash": "99cf2d007498627f2188fccb83ccf8227f55a9a7e3a79a425ca30d9648b2b731", "text": "Then, PageRank algorithm is used to solve the graph. There are other similar\nsystems such as LexRank (Erkan and Radev 2004) to consider sentences as nodes\nand similarity as relations or weights, i.e. IDF-modified cosine similarity to\ncalculate similarity.\n\n#### 9.3.6.3 Feature-Based Method\n\nFeature-based model extracts sentences features and evaluates their\nsignificances. There are many representative studies that include Luhn\u2019s\nAlgorithm (Luhn 1958), TextTeaser and SummaRuNNer (Nallapati et al. 2017).\n\nLuhn\u2019s Algorithm is used to evaluate input words significance calculated by\nfrequency. TextTeaser is an automatic feature-based summarization algorithm.\nSummaRuNNer is implemented by deep neural networks (DNN) structure as shown in\nFig. 9.25.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig25_HTML.png)\n\nFig. 9.25\n\nNetwork structure of SummaRuNNer\n\nSummaRuNNer generates sentence feature (vector) by two layers bidirectional\nGate Recurrent Unit\u2014Recurrent Neural Network (GRU-RNN) from word embedding\nvectors. The lowest level classifies each sentence word level, while the\nhighest level classifies sentence level. Double arrows indicate two-way RNN.\nThe top layer numbered with 1s and 0s is a classification layer based on\nsigmoid activation to determine whether each sentence is a summary. Each\nsentence decision depends on substantial sentence contents, sentences to\ndocument relevance, sentences to cumulative summary representation\noriginality, and other positional characteristics.\n\n#### 9.3.6.4 Topic Based Method\n\nTopic-based model considers document\u2019s topic features and input sentences\nscores according to topic types contained as major topic would obtain a high\nrate when scoring sentences.\n\nLatent Semantic Analysis (LSA) is based on Singular Value Decomposition (SVD)\nto detect topics (Ozsoy et al. 2011). An LSA based sentence selection process\nis shown in Fig. 9.26 by topics represented by eigenvectors or principal axes\nwith corresponding scores.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig26_HTML.png)\n\nFig. 9.26\n\nLSA based sentence selection sample\n\n#### 9.3.6.5 Grammar-Based Method\n\nGrammar-based model parses text and constructs a syntax structure, selects, or\nreorders the substructure. A representation framework is shown in Fig. 9.27.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig27_HTML.png)\n\nFig. 9.27\n\nGrammar-based method sample network (Ozsoy et al. 2011)\n\nGrammar pattern can produce significant paraphrases based on grammatical\nstructures. The above example in Fig. 9.28 showed how paraphrase extraction\nand replacement can be achieved by using such method. Analyzing grammatical\nstructure feature is useful for semantic phrases reconstruction.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig28_HTML.png)\n\nFig. 9.28\n\nNetwork framework of point generator baseline model\n\n#### 9.3.6.6 Contemporary Abstractive Text Summarization (ATS) System\n\nAbstractive summarization often generates summary that maintains original\nintent completed by humans.\n\nThis process can generate words that are not in original input representations\nbut to facilitate summaries characteristics and fluency. However, it is\ncomplex to generate coherent phrases and connectors.\n\nAbstractive summarization systems applying deep learning methods,\nReinforcement Learning (RL), Transfer Learning (TL) and Pre-Trained Language\nModels (PTLMs) had developed rapidly (Alomari et al. 2022) in recent years.\nThese models use rules-based frameworks to consider significant events and\nsummaries. Tree methods are ontology-related methods for abstractions (Jain et\nal. 2020).\n\n#### 9.3.6.7 Aided Summarization Method\n\nThis method combines automatic computer model or algorithm to provide\nsignificant document information for human decision.\n\nMachine translation model to text summarization was proposed (Banko et al.\n2000) applying encoder\u2013decoder framework as neural network model mainstream\nand used in abstractive summarization systems (Chopra et al. 2016).\n\n#### 9.3.6.8 Contemporary Combined Text Summarization System\n\n_Pointer-Generator Networks_ (See et al.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba741414-e089-4eaa-af02-1bf9ebf40169": {"__data__": {"id_": "ba741414-e089-4eaa-af02-1bf9ebf40169", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9b0b7a8-cf13-4756-8a80-5fa4be811099", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "99cf2d007498627f2188fccb83ccf8227f55a9a7e3a79a425ca30d9648b2b731", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b566f661-0271-4a9e-a219-ec657392410b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "edb0a54e4afed7fc3aaa917d53ecf0628420a6205dcd25b6c800de25e6cc8deb", "class_name": "RelatedNodeInfo"}}, "hash": "fc29b73c109dfe2329e09e8b84128d1f5819cc639e533d064cc21c15df188dd2", "text": "2022) in recent years.\nThese models use rules-based frameworks to consider significant events and\nsummaries. Tree methods are ontology-related methods for abstractions (Jain et\nal. 2020).\n\n#### 9.3.6.7 Aided Summarization Method\n\nThis method combines automatic computer model or algorithm to provide\nsignificant document information for human decision.\n\nMachine translation model to text summarization was proposed (Banko et al.\n2000) applying encoder\u2013decoder framework as neural network model mainstream\nand used in abstractive summarization systems (Chopra et al. 2016).\n\n#### 9.3.6.8 Contemporary Combined Text Summarization System\n\n_Pointer-Generator Networks_ (See et al. 2017) is a frequently used baseline\nnetwork. It focuses on keywords and sentences with Attention technique\n(Vaswani et al. 2017), to lever generator and pointer network according to\ncalculated probability. Vocabulary and attention with different weights\ndistribution are then combined. A baseline pointer-generator network framework\nis depicted in Fig. 9.28.\n\nIt noted that article tokens are fed into an encoder layer, which is a single-\nlayer bidirectional LSTM with encoder hidden states provided. Decoder consists\nof a single-layer unidirectional LSTM, processes word embedding of previous\nwords on each step and output decoder state with attention distribution.\n\n## 9.4 Question-and-Answering Systems\n\n### 9.4.1 QA System and AI\n\nQA system is a remarkable way to mimic human-to-human interaction through\nstate-of-the-art technology development. Different from other classification\nor prediction problems, a QA system is a cross discipline in traditional\nlinguistic including computer science for computational linguistic with\nstatistics, pattern recognition, data mining, machine learning, deep learning\nmethods for a well-trained communication system. It has a critical role for\nautoresponder, personal assistant, sentiment chatbot nowadays.\n\nQA system is a popular research topic in NLP which contains one of the open-\ndomain common sense or special domain knowledge as a qualified conversation\npartner. Dialogue realization relies on automatic speech recognition (ASR),\nnatural language understanding (NLU), dialogue management (DM), natural\nlanguage generation (NLG), speech synthesis (SS). A QA system flowchart is\nshown in Fig. 9.29.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig29_HTML.png)\n\nFig. 9.29\n\nFlowchart of a typical QA system\n\nIt is an integral part of system acumen. Dialogue Management is the\ncommunication policy or dialogue strategy applied to large corpus for content\norganization. After transferring natural language to computer language in\nsequence-sequence data with character, word, or sentence level in Natural\nLanguage Understanding (NLU), machine intelligence selects suitable contents\nfor language generation. Backend technology with generated candidate answers\nis combined and reranked for optimization response in Natural Language\nGeneration (NLG). Apart from text aspect, ASR and TTS are procedures that\nresemble machine by human voice recognition and generation.\n\nQA system research is divided into two categories: (1) pattern matching with\nrule-based and (2) language generated-based on information retrieval and\nneural network. However, the backend equipped more than one method to generate\nmeaningful communication and provide meaningful feedbacks. A QA system in a\nchatbot includes an open-domain focus on (1) common sense/world knowledge and\n(2) task-oriented for special domain knowledge databases that resemble expert\nsystem involving in-depth knowledge base to support appropriate responses.\n\nFirst rule-based human\u2013computer interaction as in Fig. 9.30 pattern\nrecognition system challenged the Turing test in 1950s, reaching a milestone\nwhere humans could not recognize whether the opposite was a machine or human.\nAfter a long period of data collection, database used for dialogue pattern\nmatching is large enough to rank appropriate feedbacks and give the highest\nscoring answers, which is a process of selection from a database of human\nanswers regardless of the machine. After decades of development, search\nengines and data crawlers have supported sources for building knowledge bases,\nincluding information retrieval, enabling search engines to retrieve relevant\nand up-to-date data for structured processing to form answers from QA systems.\nThe advent of AI era enhanced QA systems mainstream can focus on cognitive\nscience than big data feeds of neural networks on systems generations.\nGradually, traditional QA system is replaced by AI machine communication as\nrule-based matching recurrent neural network training to realize large\nknowledge base to support the AI brain to imitate human reasoning called\nNatural Language Understanding (NLU).\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b566f661-0271-4a9e-a219-ec657392410b": {"__data__": {"id_": "b566f661-0271-4a9e-a219-ec657392410b", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba741414-e089-4eaa-af02-1bf9ebf40169", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "fc29b73c109dfe2329e09e8b84128d1f5819cc639e533d064cc21c15df188dd2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef3464ca-117d-46a4-bf59-338ef3972f22", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "b1f21629c2e2d68c47313cc8b12f2347fd1c8016ddc22a3b2c9a8f6e4fd05daf", "class_name": "RelatedNodeInfo"}}, "hash": "edb0a54e4afed7fc3aaa917d53ecf0628420a6205dcd25b6c800de25e6cc8deb", "text": "After a long period of data collection, database used for dialogue pattern\nmatching is large enough to rank appropriate feedbacks and give the highest\nscoring answers, which is a process of selection from a database of human\nanswers regardless of the machine. After decades of development, search\nengines and data crawlers have supported sources for building knowledge bases,\nincluding information retrieval, enabling search engines to retrieve relevant\nand up-to-date data for structured processing to form answers from QA systems.\nThe advent of AI era enhanced QA systems mainstream can focus on cognitive\nscience than big data feeds of neural networks on systems generations.\nGradually, traditional QA system is replaced by AI machine communication as\nrule-based matching recurrent neural network training to realize large\nknowledge base to support the AI brain to imitate human reasoning called\nNatural Language Understanding (NLU).\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig30_HTML.png)\n\nFig. 9.30\n\nHuman and machine interaction via QA system\n\nThe main source of knowledge base in a typical QA system comes from: (1)\nhuman\u2013human dialogue collection with handcraft is the answer from human\nlanguage in linguistic and meaning where database consist of pairs dialogues.\nWithout any imitation or learning ability, this first version rule-based QA\nsystem relies on pattern matching to measure the distance between proposed\nquestion and Question-Answer pattern stored pair in database. For example,\nArtificial Intelligence Markup Language (AIML) can answer most of daily or\neven professional dialogues based on large and classified handcraft database\nwithout intelligence; (2) building database focus on search engine for\nInformation Retrieval-based knowledge base. The feature of IR-based QA system\nis the combination of knowledge building from up-to-date knowledge bases. An\nIR-based QA system uses domain knowledge such as expert system to extract and\ngenerate knowledge. The procedure of unstructured data extraction and\nreorganization depends on Natural Language Understanding (NLU) for reasoning.\nNatural Language generation (NLG) includes knowledge engineering analysis for\nreasoning and rerank candidates\u2019 answers optimization.\n\nThe latest database used big data for data-driven model to realize machine\nintelligence. When neural network had provided with sufficient data, sequence-\nto-sequence model like Recurrent Neural Network and its related Long-Short-\nTerm Memory naturally model as in Fig. 9.31 skilled in sequential data\nprocessing (Cho et al. 2014). A neural network model is considered as the\nblack box producing learning ability with accuracy but cannot comprehend by\nhumans. Prior preprocessing data was fed to neural model, they required to\ntransform data format from natural word to vector for data training (Mikolov\net al. 2013). Tokenization has three levels: (1) character, (2) word, and (3)\nsentence. The input format decides output outcomes in encoder\u2013decoder\nframework. Recurrent Neural Network generated words may not be meaningful in\nEnglish dictionary because the character level training lacked enough corpus\nfor a well-trained model. Further, transfer learning with enormous data pre-\ntrained Transformer model required to select the intended decoder for training\ntarget. For example, Dialogue GPT from OpenAI focuses on formatted dialogue\ntraining to generate responses.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig31_HTML.png)\n\nFig. 9.31\n\nLSTM structure\n\nNeural Network system transformed natural language to word vectors for\nmathematical computation to acquire response in NLP. Neural Network can\ngenerate own natural language as compared with traditional techniques.\n\nTraditional Recurrent Neural Network (RNN) of seq2seq language model response\ngeneration performed lesser than big data-oriented transfer learning such as\nGoogle\u2019s BERT and Open AI\u2019s GPT.\n\nPre-trained unsupervised learning language model achieved satisfactory\nperformance in fine-tuning with small dataset than traditional ones, their\nperformances attributed to self-attention mechanism (Vaswani et al. 2017) and\nidentified relations in sequences with fluent and syntactic response for task\nexecution based on GPT with fine-tuned model (Wolf et al. 2018).\n\n#### 9.4.1.1 Rule-based QA Systems\n\n_Rule-based QA systems_ were proposed at the same time as Turing test in\n1950s. However, original QA systems only followed rules set by humans without\nself-improvement capabilities like machine learning, number of dialogue pairs\nis stored in database prior the system provided a concrete answer. The\nsimplest but most efficient way to measure similarity of two groups is the\ncosine distance of two vectors.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ef3464ca-117d-46a4-bf59-338ef3972f22": {"__data__": {"id_": "ef3464ca-117d-46a4-bf59-338ef3972f22", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b566f661-0271-4a9e-a219-ec657392410b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "edb0a54e4afed7fc3aaa917d53ecf0628420a6205dcd25b6c800de25e6cc8deb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25384309-fcc4-4142-ba43-67d35e18ace8", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "09fca7778fd4203ecfaaa253f093eb1df0df2bff927fcfb80e9d4f4328f333b7", "class_name": "RelatedNodeInfo"}}, "hash": "b1f21629c2e2d68c47313cc8b12f2347fd1c8016ddc22a3b2c9a8f6e4fd05daf", "text": "Pre-trained unsupervised learning language model achieved satisfactory\nperformance in fine-tuning with small dataset than traditional ones, their\nperformances attributed to self-attention mechanism (Vaswani et al. 2017) and\nidentified relations in sequences with fluent and syntactic response for task\nexecution based on GPT with fine-tuned model (Wolf et al. 2018).\n\n#### 9.4.1.1 Rule-based QA Systems\n\n_Rule-based QA systems_ were proposed at the same time as Turing test in\n1950s. However, original QA systems only followed rules set by humans without\nself-improvement capabilities like machine learning, number of dialogue pairs\nis stored in database prior the system provided a concrete answer. The\nsimplest but most efficient way to measure similarity of two groups is the\ncosine distance of two vectors. It is undeniable that rule-based systems have\ncollected huge dialogue corpora over decades, giving system confidence when\nrelying on new problems with high vector similarity. To date, mature rule-\nbased systems are quintessential for all commercial QA systems, as the\naccumulation of corpora can avoid meaningless responses that compensate for\ninsufficient domain knowledge with appropriate and specific human feedbacks.\n\n#### 9.4.1.2 Information Retrieval (IR)-based QA Systems\n\nThe knowledge base for IR is unstructured data source using data mining\nmethods obtained by websites, wordnet which are different from the paired\ndialogue. Question-Answer System (KBQA system) is a significant branch of IR-\nbased QA system knowledge base, its usage depends on knowledge base size of\nunstructured data for storage. That is related to knowledge base construction\nto extract purposeful knowledge from mass data. There are two methods (1)\nproperty and (2) relations to process natural language. Property refers to the\ndefinition or concept of one thing in English\u2013English dictionary to explain\nanother concept. Relations refer to the relationship between two entities,\nwhere a Name Entity Recognition (NER) and idea from Ontology with\nSubject\u2013Predicate\u2013Object (SPO) triple must be used to extract relation. KBQA\nextension is ontology or knowledge graph (KG) in research. When entities are\nlinked, the knowledge for one entity can be extracted according to questions\nduring Natural Language Understanding (NLU). A typical KBQA with domain\nknowledge about ontology is shown in Fig. 9.32, its fundamental question is\nabout who and what correspond to name and relations entities (Cui et al.\n2020).\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig32_HTML.png)\n\nFig. 9.32\n\nKBQA system demo\n\n#### 9.4.1.3 Neural Network-Based QA Systems\n\nNeural Network structure in a QA generated-based system is considered as\nmachine brain imitated by human. Encoder\u2013Decoder framework is a sequence-to-\nsequence model like RNN that has natural memory recalling priority and context\nwith attention mechanism. Dialogue system has identical requirements to\nrepresent dialogue history and avoid meaningless responses to improve users\u2019\nexperiences.\n\nDeep learning models such as TensorFlow and Pytorch, RNN is easy to implement\nfor text generation as language model. Google proposed masked language model\nto generate language representation called Bidirectional Encoder\nRepresentation from Transformer (BERT), focusing on encoder part trained by\nmagnitude unlabeled data in 2017. Neural network feeds data for training\naccording to network advantages due to different NLP tasks in long sentences.\nBERT can solve such problem because it deals with 11 common NLP tasks\ninitially. Language model pre-trained by magnitude data is used to understand\ncommon knowledge in NLP. Fine-tuned should be applied to training specific NLP\ntasks based on fundamental ability (Vaswani et al. 2017).\n\nOpen AI released another Transformer framework with unsupervised learning for\npre-trained model directing decoder scheme based on GPT, Open AI GPT-2, and\nGPT-3 (Brown et al. 2020). GPT with masked self-attention focuses on known\ntext so that the word preceding is predicated as different from BERT context\nself-attention. GPT-3 can do inference and synonym replacement in addition to\nnormal function for bilingual translation, text generation, and Question-\nAnswer. It seems that BERT can handle more NLP tasks than GPT, but GPT text\ngeneration prowess for pre-trained model is widely used in many commercial QA\nsystems and text summarization.\n\n### 9.4.2 Overview of Industrial QA Systems\n\nAn industrial QA system contains automatic dialogue system assembling chatbot\ninternal technologies.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25384309-fcc4-4142-ba43-67d35e18ace8": {"__data__": {"id_": "25384309-fcc4-4142-ba43-67d35e18ace8", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef3464ca-117d-46a4-bf59-338ef3972f22", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "b1f21629c2e2d68c47313cc8b12f2347fd1c8016ddc22a3b2c9a8f6e4fd05daf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "772737cd-f6f7-48b7-b804-c01878343b78", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8528b62801eec55d07c5ef9e9ab1862b3ab1e90695a515a3b686e66893c5f863", "class_name": "RelatedNodeInfo"}}, "hash": "09fca7778fd4203ecfaaa253f093eb1df0df2bff927fcfb80e9d4f4328f333b7", "text": "2017).\n\nOpen AI released another Transformer framework with unsupervised learning for\npre-trained model directing decoder scheme based on GPT, Open AI GPT-2, and\nGPT-3 (Brown et al. 2020). GPT with masked self-attention focuses on known\ntext so that the word preceding is predicated as different from BERT context\nself-attention. GPT-3 can do inference and synonym replacement in addition to\nnormal function for bilingual translation, text generation, and Question-\nAnswer. It seems that BERT can handle more NLP tasks than GPT, but GPT text\ngeneration prowess for pre-trained model is widely used in many commercial QA\nsystems and text summarization.\n\n### 9.4.2 Overview of Industrial QA Systems\n\nAn industrial QA system contains automatic dialogue system assembling chatbot\ninternal technologies. They have several backend composited control system\nresponses to equip with necessary knowledge. Meanwhile, QA system evaluation\nis proposed during the training period for language model performance (Chen et\nal. 2017) and on system design sufficient for both languages generations.\n\nSince Encoder\u2013Decoder framework proposed as an end-to-end system and a\nsequential language model, RNN is a popular generated-based model in\ncommercial and academics. However, its applications are mainly focused on\ncasual scenarios at open domain without proposed question details. Thus, the\nresponse from a generated-based QA system is appropriate in pairs but lack\ncontents due to the data-driven model considered basic linguistic and excluded\nfacts from knowledge base which are identical to traditional dialogue system\nwith meaningless answers. A knowledge grounded neural conversation model\n(Ghazvininejad et al. 2018) is proposed based on sequence-to-sequence RNN\nmodel and combined dialogue history with facts related to current contexts as\nshown in Fig. 9.33.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig33_HTML.png)\n\nFig. 9.33\n\nArchitecture of knowledge grounded model\n\nMicrosoft extended its industrial conversation system to achieve useful\nconversational applications on knowledge grounded with conversation history\nand external facts in 2018. It has significant progress in real situations\naccording to conversation history in Dialog-Encoder, word, and contextually\nrelevant facts in Facts Encoder to responses as compared with baseline seq2seq\nmodel.\n\nThe data-driven model of QA system divided source data into conversation data\nand non-conversational text which means the conversation pairs are used to\ntraining system in linguistic; however, non-conversational text is the\nknowledge base to be filled including real-world information related to system\ntarget usage.\n\nThe performance of versatility and scalability in open domain with external\ninformation knowledge combined with textual and structured data of QA system\nis shown in Fig. 9.34. Datasets like Wikipedia, IMDB, TripAdvisor are used to\ngenerate conversation with real-world information and included a\nrecommendation system function.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig34_HTML.png)\n\nFig. 9.34\n\nResponse from conversation model knowledge grounded (Ghazvininejad et al.\n2018)\n\nAfter fact-based encoder, the response from this system becomes more\nmeaningful with related information and logical content. Based on this model,\n23 million open domains Twitter conversations and 1.1 million Foursquare tips\nare used to achieve a significant improvement over the previous seq2seq model,\nand different from the traditional content filling which add the predefined\ncontent and fill the space in sentences.\n\nIt is well known that industrial QA systems are not limited to one model, many\nmodels have been assembled into a language model for end-to-end dialogue. In\nthis architecture, the dialogue encoder is independent of fact encoder in the\nsystem, but it is complementary to fact encoder when applied because facts\nrequire information from dialogue history, especially to match context-\ndependent information bands. There is intentional information as part of the\nresponse. From implementation perspective, multi-task learning is used to\nhandle factual, non-factual, and autoencoder tasks depending on intended work\nof the system. Multi-task learning can separate two encoders independently\nwhile training the model, and after training on dialogue dataset, the factual\nencoder part uses information retrieval (IR) to expand knowledge base for more\nmeaningful answers. In a way, a fact encoder is like a memory network, which\nuses a store of relevant facts relevant to a particular problem.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "772737cd-f6f7-48b7-b804-c01878343b78": {"__data__": {"id_": "772737cd-f6f7-48b7-b804-c01878343b78", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25384309-fcc4-4142-ba43-67d35e18ace8", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "09fca7778fd4203ecfaaa253f093eb1df0df2bff927fcfb80e9d4f4328f333b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d6c2b407-585b-40b9-af57-89741193073e", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "ae618d8b5bfdc9146dd9397ded2d0ab16235890ab6b36df0e83604230295b737", "class_name": "RelatedNodeInfo"}}, "hash": "8528b62801eec55d07c5ef9e9ab1862b3ab1e90695a515a3b686e66893c5f863", "text": "It is well known that industrial QA systems are not limited to one model, many\nmodels have been assembled into a language model for end-to-end dialogue. In\nthis architecture, the dialogue encoder is independent of fact encoder in the\nsystem, but it is complementary to fact encoder when applied because facts\nrequire information from dialogue history, especially to match context-\ndependent information bands. There is intentional information as part of the\nresponse. From implementation perspective, multi-task learning is used to\nhandle factual, non-factual, and autoencoder tasks depending on intended work\nof the system. Multi-task learning can separate two encoders independently\nwhile training the model, and after training on dialogue dataset, the factual\nencoder part uses information retrieval (IR) to expand knowledge base for more\nmeaningful answers. In a way, a fact encoder is like a memory network, which\nuses a store of relevant facts relevant to a particular problem. Once the\nquery contains a specific entity in the sentence, the sentence has been\nassigned a specific name entity, the name entity recognizes (NER) by matching\nkeywords or linked entities, or even named entities and calculates its weight\non input and dialogue history to generate a response. The original storage\nnetwork model uses a bag of words, but in this model the encoder directly\nconverts input set to a vector unlike storage network model.\n\nSince the system is a fully neural-based data-driven model, they created an\nend-to-end RNN system using a traditional seq2seq model, including long short-\nterm memory (LSTM) and Gate Recurrent Unit (GRU) model. For ensemble\nstructures such as two-class RNNs, constructing a simple GRU is usually faster\nthan LSTM model. The implementation of GRU means that the system does not have\nTransformer\u2019s attention mechanism or other invariants for neural network\ncomputation.\n\n#### 9.4.2.1 AliMe QA System\n\nAliMe is a module of Taobao app commercial QA product. The answer consists of\ninformation retrieval (IR) and sequence-to-sequence-based generation models\n(Qiu et al. 2017). The system reorders candidate\u2019s response and uses attention\nmechanism with context to select the best feedback to users. Using AliMe to\nreplace online human customer service for most known questions became a trend\nsince it released the first version. AliMe is a typical customer service QA\nsystem in e-commerce industry that answers millions of questions automatically\nper day. According to a survey of daily questions suggested by Taobao app\nusers on shopping problems, statistical data revealed that except most are\nbusiness questions, 5% of the remaining questions are chitchat. The 5%\nquestions on genuine demands motivate AliMe to add a common sense open domain\nchat function. It has satisfactory performance as both IR and generation-based\nsystem since the pre-trained seq2seq model is used twice for response\ngeneration and reranked with attention to a set of responses from information\nretrieval with knowledge originate based and seq2seq previously generated.\nFigure 9.35 shows the Seq2Seq model with attention learning.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig35_HTML.png)\n\nFig. 9.35\n\nSeq2Seq model with attention\n\nSince AliMe has two parts in generation that use different formats to obtain\ninformation as abovementioned. IR-based models use a natural language word\nmatching knowledge base, Seq2seq generative model, and a scoring model to re-\nscore output responses as they are generated is word embeddings with vectors.\nThe IR-based dataset consists of 9,164,834 QA pairs conversations by real\ncustomers from business domain. Researchers used an inverted index to match\nthese 9 million conversations with input sentences containing the same words\nand used BM25 to measure the similarity between input sentences and the\nselected questions to obtain answers to the most similar questions as answers\nto input questions. Traditional IR-based systems avoid problems where the\nsystem cannot answer common sense type questions.\n\nMicrosoft used GRU to reduce computational power and response time span as\nwell as AliMe selected RNN GRU to improve response efficiency. During\noptimization, beam search in decoder assisted to identify the highest\nconditional probability to obtain optimizer response sentence within\nparameters. The performance showed that _IR+generation+rerank_ approach by\nseq2seq model and mean probability scoring function evaluation obtained the\nhighest score as compared with other methods.\n\n#### 9.4.2.2 Xiao Ice QA System\n\nXiao Ice (Zhou et al.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6c2b407-585b-40b9-af57-89741193073e": {"__data__": {"id_": "d6c2b407-585b-40b9-af57-89741193073e", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "772737cd-f6f7-48b7-b804-c01878343b78", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8528b62801eec55d07c5ef9e9ab1862b3ab1e90695a515a3b686e66893c5f863", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "840bfb23-2e93-4441-ab43-3d430280a18f", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "00b59f1395e44b4903bee08c78a153b038e9d452a74b885ed82e278fa6fa3f2a", "class_name": "RelatedNodeInfo"}}, "hash": "ae618d8b5bfdc9146dd9397ded2d0ab16235890ab6b36df0e83604230295b737", "text": "Researchers used an inverted index to match\nthese 9 million conversations with input sentences containing the same words\nand used BM25 to measure the similarity between input sentences and the\nselected questions to obtain answers to the most similar questions as answers\nto input questions. Traditional IR-based systems avoid problems where the\nsystem cannot answer common sense type questions.\n\nMicrosoft used GRU to reduce computational power and response time span as\nwell as AliMe selected RNN GRU to improve response efficiency. During\noptimization, beam search in decoder assisted to identify the highest\nconditional probability to obtain optimizer response sentence within\nparameters. The performance showed that _IR+generation+rerank_ approach by\nseq2seq model and mean probability scoring function evaluation obtained the\nhighest score as compared with other methods.\n\n#### 9.4.2.2 Xiao Ice QA System\n\nXiao Ice (Zhou et al. 2020) is an AI companion sentient chatbot with more than\n660 million users worldwide which takes Intelligent Quotient (IQ) and\nEmotional Quotient (EQ) in system design as shown in Fig. 9.37. It focused on\nchitchat compared with other commonly used QA systems. According to\nConversation-turns Per Session (CPS) evaluation score, its grade is 23 higher\nthan most chatbots. Figure 9.36 shows a system architecture of Xiao Ice.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig36_HTML.png)\n\nFig. 9.36\n\nXiao Ice system architecture\n\nXiao Ice exists on 11 social media platforms including WeChat, Tencent QQ,\nWeibo, and Facebook as an industrial application. It has equipped with two-way\ntext-to-speech voice and can process text, images, voice, and video clips for\nmessage-based conversations. Also, its core chat function can distinguish\ncommon or specific domain topic chat types so that it can change topics easily\nand automatically provide users with deeper domain knowledge. A dialog manager\nis like an NLP general pipeline with dialog management to path conversation\nstates such as core chat contents for open or special domains to process data\nfrom different sources are tractable. The Global State Tracker is a vector of\nXiao Ice's responses to analyze text strings for entities and empathy. It is\nvacant and gradually filled with rounds of conversations. Dialogue strategies\nare primarily designed for long-term users, based on their feedbacks to\nenhance interactions engagement, optimize personality with two or three levels\nachievements. A trigger mechanism is to change topic when the chatbot repeats\nor answers information that are always valid, or when a user's feedback is\nmundane within three words. Once the user's input has a predefined format, a\nskill selection part is activated to process different input. For example,\nimages can be categorized into different task-oriented scenarios. If an image\nis food related, the user will be taken to a restaurant display, like a task\ncompletion by personal assistants in advising weather information or making\nreservations, etc.\n\nXiao Ice has a few knowledge graphs in the data layer as its original datasets\ncome from popular forums such as Instagram in English or Douban in Chinese.\nThese datasets are categorized as multiple topics with a small knowledge base\nas possible answers. It also follows the rules of updating knowledge base\nthrough machine learning when new topics emerge. It is noted that not all new\nentities or topics are collected unless the entity is contextually relevant,\nor a topic has higher popularity or freshness in the news for rankings. User's\npersonal interests can be adjusted individually.\n\nHowever, with so many features that can include the core part Empathetic\nComputing as an add-on, it is not a mandatory part of a full chatbot, but a\nfunctional and compelling feature to compete with the industry. The core of\nXiao Ice is a RNN language model that creates open and special domain\nknowledge. Figures 9.37 and 9.38 show an RNN-based neural response generator\nwith examples of inconsistent responses generated by seq2seq model in Xiao Ice\nQA system, respectively.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig37_HTML.png)\n\nFig. 9.37\n\nRNN-based neural response generator\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig38_HTML.png)\n\nFig.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "840bfb23-2e93-4441-ab43-3d430280a18f": {"__data__": {"id_": "840bfb23-2e93-4441-ab43-3d430280a18f", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6c2b407-585b-40b9-af57-89741193073e", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "ae618d8b5bfdc9146dd9397ded2d0ab16235890ab6b36df0e83604230295b737", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e73ae97e-5eb8-48f0-9863-d29a7b4bd638", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3966141ffdb6dbc771c15ae744b3349115535368ebf9e4b4e0a219a530f60fdb", "class_name": "RelatedNodeInfo"}}, "hash": "00b59f1395e44b4903bee08c78a153b038e9d452a74b885ed82e278fa6fa3f2a", "text": "User's\npersonal interests can be adjusted individually.\n\nHowever, with so many features that can include the core part Empathetic\nComputing as an add-on, it is not a mandatory part of a full chatbot, but a\nfunctional and compelling feature to compete with the industry. The core of\nXiao Ice is a RNN language model that creates open and special domain\nknowledge. Figures 9.37 and 9.38 show an RNN-based neural response generator\nwith examples of inconsistent responses generated by seq2seq model in Xiao Ice\nQA system, respectively.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig37_HTML.png)\n\nFig. 9.37\n\nRNN-based neural response generator\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig38_HTML.png)\n\nFig. 9.38\n\nExamples of inconsistent responses generated using a seq2seq model\n\nIn general, response generation in AliMe uses seq2seq model to generate\nnatural language and rerank the optimizer for user\u2019s answer, whereas Xiao Ice\nalso has a candidate generator and candidate ranking list. For the generator,\none is a sequential model trained by a pair of datasets learning the dialog\nformat, the other is querying the knowledge graph to obtain entities for\nrelated information stored in knowledge base. Candidate ranking includes\nsemantic computation and Xiao Ice personality for answer optimization with\ninformation retrieval, neural model, and knowledge graph selection.\n\n#### 9.4.2.3 TransferTransfo Conversational Agents\n\nA QA system consisted of traditional and current mainstream methods, the above\nsystems used Seq2Seq model responsible for both language model and candidate\nresponse optimizer. Since neural network is a data-driven model, its\nperformance relies on huge amount of big data. Transformer is a model\narchitecture forgone recurrence but entrusted in attention mechanism entirely\nto draw global dependencies between input and output based on attention\nmechanism.\n\nOpen AI GPT-2 transfer learning architecture has an outstanding feature to\ninclude decoder part layers advantages for response generation. The masked\nself-attention implemented on GPT-2 can generate the next word based on\nacquired information, understand the known text, predict, or use experience to\nfill up the blank for next word to match with the whole article meaning.\n\nGPT-2 fine-tune 40G pure text to learn natural language semantics, syntax with\ntarget usage and suitable dataset scalability for specific NLP tasks.\nTransferTransfo (Wolf et al. 2018) is a GBP-2 variant using persona-chat\ndataset to fine-tune the original model, its generated utterance changes from\nlong text to dialogue format. TransferTransfo prototype is a pre-trained model\non document-level continuous sequence and paragraphs with a wide range of\ninformation. After that, fine-tune strengthen input representation and use a\nmulti-task learning scheme for adjustments. Every input token included word\nand position embedding during input representation.\n\nFor Transfer Learning system dialogue example as in Fig. 9.39, personal-chat\ndatasets in real-world can define users\u2019 backgrounds and their interests as\ntopics during communications. The contexts contained are meaningful\nconversation that can reveal empirical improvements in discriminative language\nunderstanding tasks. Thus, Transformer is an evolutional system to imitate\nhuman behavior and promote neural network model.\n\n![](../images/533412_1_En_9_Chapter/533412_1_En_9_Fig39_HTML.png)\n\nFig. 9.39\n\nExample dialog from PERSONA-CHAT dataset\n\nExercises\n\n  1. 9.1.\n\nWhat is Information Retrieval (IR) in NLP? State and explain why IR is vital\nfor the implementation of NLP applications. Give two NLP applications to\nillustrate.\n\n  2. 9.2.\n\nIn terms of implementation technology of Information Retrieval (IR) systems,\nwhat are the major difference between traditional and latest IR systems. Give\none IR system implementation example to support your answer.\n\n  3. 9.3.\n\nWhat is Discourse Segmentation? State and explain why Discourse Segmentation\nis critical for the implementation of Information Retrieval (IR) systems.\n\n  4. 9.4.\n\nWhat is Text Summarization (TS) in NLP? State and explain the relationship and\ndifferences between TS system and IR (Information Retrieval) systems.\n\n  5. 9.5.\n\nWhat are two basic approaches of Text Summarization (TS)? Give live examples\nof TS systems to discuss how they work by using these two approaches.\n\n  6. 9.6.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e73ae97e-5eb8-48f0-9863-d29a7b4bd638": {"__data__": {"id_": "e73ae97e-5eb8-48f0-9863-d29a7b4bd638", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "840bfb23-2e93-4441-ab43-3d430280a18f", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "00b59f1395e44b4903bee08c78a153b038e9d452a74b885ed82e278fa6fa3f2a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "071796b2-c690-4e5e-8f66-52dc4a24a140", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "1c0f98e2e56505f4fc6b4b34a493e1a6712f84e1b7e12b44ee8c052307a80fb1", "class_name": "RelatedNodeInfo"}}, "hash": "3966141ffdb6dbc771c15ae744b3349115535368ebf9e4b4e0a219a530f60fdb", "text": "Give two NLP applications to\nillustrate.\n\n  2. 9.2.\n\nIn terms of implementation technology of Information Retrieval (IR) systems,\nwhat are the major difference between traditional and latest IR systems. Give\none IR system implementation example to support your answer.\n\n  3. 9.3.\n\nWhat is Discourse Segmentation? State and explain why Discourse Segmentation\nis critical for the implementation of Information Retrieval (IR) systems.\n\n  4. 9.4.\n\nWhat is Text Summarization (TS) in NLP? State and explain the relationship and\ndifferences between TS system and IR (Information Retrieval) systems.\n\n  5. 9.5.\n\nWhat are two basic approaches of Text Summarization (TS)? Give live examples\nof TS systems to discuss how they work by using these two approaches.\n\n  6. 9.6.\n\nWhat are the major differences between Single vs Multiple documentation\nsummarization systems? State and explain briefly the related technologies\nbeing used in these TS systems.\n\n  7. 9.7.\n\nWhat are the major characteristics of contemporary Text Summarization (TS)\nsystems as compared with traditional TS systems in the past century? Give live\nexample(s) to support your answer.\n\n  8. 9.8.\n\nWhat is a QA system in NLP? State and explain why QA system is critical to\nNLP. Give two live examples to support your answer.\n\n  9. 9.9.\n\nChoose any two industrial used QA systems and compare their pros and cons in\nterms of functionality and system performance.\n\n  10. 9.10.\n\nWhat is Transformer technology? State and explain how it can be used for the\nimplementation of QA system. Use a live example to support your answer.\n\nReferences\n\n  1. Agarwal, N., Kiran, G., Reddy, R. S. and Ros **\u00e9** , C. P. (2011) Towards Multi-Document Summarization of Scientific Articles: Making Interesting Comparisons with SciSumm. In Proc. of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, Portland, Oregon, pp. 8\u2013 15.\n\n  2. Agrawal, K. (2020) Legal case summarization: An application for text summarization. In Proc. Int. Conf. Comput. Commun. Informat. (ICCCI), pp. 1\u20136.\n\n  3. Aharon, M., Elad, M. and Bruckstein, A. (2006). K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation. IEEE Transactions on signal processing, 54(11): 4311-4322.\n\n  4. Aizawa A. (2003). An information-theoretic perspective of tf\u2013idf measures. Information Processing & Management, 39(1): 45-65.\n\n  5. Alami, N., Meknassi, M and Rais, N. (2015). Automatic texts summarization: Current state of the art. Journal of Asian Scientific Research, 5(1), 1-15.\n\n  6. Alomari, A., Idris, N., Sabri, A., and Alsmadi, I. (2022) Deep reinforcement and transfer learning for abstractive text summarization: A review. Comput. Speech Lang. 71: 101276.\n\n  7. Banko, M., Mittal, V. O. and Witbrock, M. J. (2000) Headline Generation Based on Statistical Translation. ACL 2000, pp. 318-325.\n\n  8. Baumel, T., Eyal, M. and Elhadad, M. (2018) Query Focused Abstractive Summarization: Incorporating Query Relevance, Multi-Document Coverage, and Summary Length Constraints into seq2seq Models. CoRR abs/1801.07704.\n\n  9. Barzilay, R., McKeown, K. and Elhadad, M. (1999) Information fusion in the context of multi-document summarization. In Proceedings of ACL '99, pp. 550\u2013557.\n\n  10. Baxendale, P. (1958) Machine-made index for technical literature - an experiment. IBM Journal of Research Development, 2(4):354-361.\n\n  11. Brown TB, Mann B, Ryder N, et al (2020) Language models are few-shot learners.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "071796b2-c690-4e5e-8f66-52dc4a24a140": {"__data__": {"id_": "071796b2-c690-4e5e-8f66-52dc4a24a140", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e73ae97e-5eb8-48f0-9863-d29a7b4bd638", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3966141ffdb6dbc771c15ae744b3349115535368ebf9e4b4e0a219a530f60fdb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c0fb3e8-b37d-4716-b3e7-e26f964ebf7b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "6abe0e5271af878b53736c1a6a1efa0bfe38d615bc31987b4cbf6659d997b464", "class_name": "RelatedNodeInfo"}}, "hash": "1c0f98e2e56505f4fc6b4b34a493e1a6712f84e1b7e12b44ee8c052307a80fb1", "text": "318-325.\n\n  8. Baumel, T., Eyal, M. and Elhadad, M. (2018) Query Focused Abstractive Summarization: Incorporating Query Relevance, Multi-Document Coverage, and Summary Length Constraints into seq2seq Models. CoRR abs/1801.07704.\n\n  9. Barzilay, R., McKeown, K. and Elhadad, M. (1999) Information fusion in the context of multi-document summarization. In Proceedings of ACL '99, pp. 550\u2013557.\n\n  10. Baxendale, P. (1958) Machine-made index for technical literature - an experiment. IBM Journal of Research Development, 2(4):354-361.\n\n  11. Brown TB, Mann B, Ryder N, et al (2020) Language models are few-shot learners. Adv Neural Inf Process Syst 2020-Decem:pp.3\u201363\n\n  12. Carbonell, J. and Goldstein, J. (1998) The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proceedings of SIGIR '98, pp. 335-336, NY, USA.\n\n  13. Chen, J. and Zhuge H. (2018) Abstractive text-image summarization using multi-modal attentional hierarchical RNN. In Proc. Conf. Empirical Methods Natural Lang. Process., Brussels, Belgium, pp. 4046\u20134056.\n\n  14. Chen H, Liu X, Yin D, Tang J (2017) A Survey on Dialogue Systems. ACM SIGKDD Explor Newsl 19:25\u201335. [https://\u200bdoi.\u200borg/\u200b10.\u200b1145/\u200b3166054.\u200b3166058](https://doi.org/10.1145/3166054.3166058)\n\n  15. Cheng, H. T. et al. (2016). Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems, pp. 7-10.\n\n  16. Chi, L. and Hu, L. (2021) ISKE: An unsupervised automatic keyphrase extraction approach using the iterated sentences based on graph method. Knowl. Based Syst. 223: 107014.\n\n  17. Chopra, S., Auli, M. and Rush, A. M. (2016) Abstractive Sentence Summarization with Attentive Recurrent Neural Networks. HLT-NAACL 2016, pp. 93-98.\n\n  18. Cho K, Van Merri\u00ebnboer B, Gulcehre C, et al (2014) Learning phrase representations using RNN encoder-decoder for statistical machine translation. EMNLP 2014 - 2014 Conf Empir Methods Nat Lang Process Proc Conf 1724\u20131734. [https://\u200bdoi.\u200borg/\u200b10.\u200b3115/\u200bv1/\u200bd14-1179](https://doi.org/10.3115/v1/d14-1179)\n\n  19. Church, K. W. (2017). Word2Vec. Natural Language Engineering, 23(1): 155-162.\n\n  20. CNN-DailyMail (2022) CNN/Daily-Mail Datasets. [https://\u200bwww.\u200bkaggle.\u200bcom/\u200bdatasets/\u200bgowrishankarp/\u200bnewspaper-text-summarization-cnn-dailymail](https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail). Accessed 9 Aug 2022.\n\n  21. Columbia (2022). Columbia Newsblaster. [http://\u200bnewsblaster.\u200bcs.\u200bcolumbia.\u200bedu](http://newsblaster.cs.columbia.edu). Accessed 14 June 2022.\n\n  22. Croft, W. B. & Harper, D. J. (1979). Using probabilistic models of document retrieval without relevance information. Journal of documentation, 35(4): 285-295.\n\n  23. Cui Y, Huang C, Lee R (2020) AI Tutor : A Computer Science Domain Knowledge Graph-Based QA System on JADE platform. Int J Ind Manuf Eng 14:603\u2013613\n\n  24.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c0fb3e8-b37d-4716-b3e7-e26f964ebf7b": {"__data__": {"id_": "9c0fb3e8-b37d-4716-b3e7-e26f964ebf7b", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "071796b2-c690-4e5e-8f66-52dc4a24a140", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "1c0f98e2e56505f4fc6b4b34a493e1a6712f84e1b7e12b44ee8c052307a80fb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c508c36d-07c2-433a-a0ad-8027f870e8bf", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "0f1446f78d07fe0607da143dd5e1b8760faf8081958e7f969fe5ede829c35b4c", "class_name": "RelatedNodeInfo"}}, "hash": "6abe0e5271af878b53736c1a6a1efa0bfe38d615bc31987b4cbf6659d997b464", "text": "Accessed 9 Aug 2022.\n\n  21. Columbia (2022). Columbia Newsblaster. [http://\u200bnewsblaster.\u200bcs.\u200bcolumbia.\u200bedu](http://newsblaster.cs.columbia.edu). Accessed 14 June 2022.\n\n  22. Croft, W. B. & Harper, D. J. (1979). Using probabilistic models of document retrieval without relevance information. Journal of documentation, 35(4): 285-295.\n\n  23. Cui Y, Huang C, Lee R (2020) AI Tutor : A Computer Science Domain Knowledge Graph-Based QA System on JADE platform. Int J Ind Manuf Eng 14:603\u2013613\n\n  24. Dong, Y. (2018) A Survey on Neural Network-Based Summarization Methods. CoRR abs/1804.04589.\n\n  25. DUC (2022) DUC Dataset. [https://\u200bpaperswithcode.\u200bcom/\u200bdataset/\u200bduc-2004](https://paperswithcode.com/dataset/duc-2004). Accessed 9 Aug 2022.\n\n  26. Edmundson, H. P. (1969) New Methods in Automatic Extracting. Journal of ACM 16(2): 264-285.\n\n  27. Elsaid, A., Mohammed, A., Ibrahim, L. F., Mohammed and Sakre, M. (2022) A Comprehensive Review of Arabic Text Summarization. IEEE Access 10: 38012-38030.\n\n  28. Erkan, G. and Radev. D. R. (2004) LexRank: Graph-based Lexical Centrality as Salience in Text Summarization. J. Artificial Intelligent Research 22: 457-479.\n\n  29. Evans, D. K. (2005) Similarity-based multilingual multidocument summarization. Technical Report CUCS-014- 05, Columbia University.\n\n  30. Farsi, M., Hosahalli, D., Manjunatha, B., Gad, I., Atlam, E., Ahmed, A., Elmarhomy, G., Elmarhoumy and Ghoneim, O. (2021) Parallel genetic algorithms for optimizing the SARIMA model for better forecasting of the NCDC weather data, Alexandria Eng. J., 60(1): 1299\u20131316.\n\n  31. Florian, R. (2002). Named entity recognition as a house of cards: Classifier stacking. In Proceedings of the 6th conference on Natural language learning (COLING-02). [https://\u200bdoi.\u200borg/\u200b10.\u200b3115/\u200b1118853.\u200b1118863](https://doi.org/10.3115/1118853.1118863).\n\n  32. Fraser, B. (1999). What are discourse markers? Journal of pragmatics, 31(7): 931-952.\n\n  33. Fukumoto, J. (2004) Multi-Document Summarization Using Document Set Type Classification. In Proc. of NTCIR- 4, Tokyo, pp. 412-416.\n\n  34. Ghazvininejad M, Brockett C, Chang MW, et al (2018) A knowledge-grounded neural conversation model. 32nd AAAI Conf Artif Intell AAAI 2018 5110\u20135117\n\n  35. Gigaword (2022) Gigaword Datasets. [https://\u200bhuggingface.\u200bco/\u200bdatasets/\u200bgigaword](https://huggingface.co/datasets/gigaword). Accessed 9 Aug 2022.\n\n  36. Gong, Y. and Liu, X. (2001) Generic Text Summarization Using Relevance Measure and Latent Semantic Analysis. SIGIR 2001, pp. 19-25.\n\n  37. Google (2022) Google News. [http://\u200bnews.\u200bgoogle.\u200bcom](http://news.google.com). Accessed 14 June 2022.\n\n  38. Jain, D., Borah, M. D. and Biswas, A. (2020) Fine-tuning textrank for legal document summarization: A Bayesian optimization-based approach. In Proc. Forum Inf. Retr. Eval., Hyderabad India, pp. 41\u201348.\n\n  39.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c508c36d-07c2-433a-a0ad-8027f870e8bf": {"__data__": {"id_": "c508c36d-07c2-433a-a0ad-8027f870e8bf", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c0fb3e8-b37d-4716-b3e7-e26f964ebf7b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "6abe0e5271af878b53736c1a6a1efa0bfe38d615bc31987b4cbf6659d997b464", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1887ceb-8bfb-49e5-a4a1-70c459e1746c", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "16383155f282d3903707cac013db05fcad61c0aca34f2dd46ad42837adbbce4e", "class_name": "RelatedNodeInfo"}}, "hash": "0f1446f78d07fe0607da143dd5e1b8760faf8081958e7f969fe5ede829c35b4c", "text": "Accessed 9 Aug 2022.\n\n  36. Gong, Y. and Liu, X. (2001) Generic Text Summarization Using Relevance Measure and Latent Semantic Analysis. SIGIR 2001, pp. 19-25.\n\n  37. Google (2022) Google News. [http://\u200bnews.\u200bgoogle.\u200bcom](http://news.google.com). Accessed 14 June 2022.\n\n  38. Jain, D., Borah, M. D. and Biswas, A. (2020) Fine-tuning textrank for legal document summarization: A Bayesian optimization-based approach. In Proc. Forum Inf. Retr. Eval., Hyderabad India, pp. 41\u201348.\n\n  39. Jing, H. (2000) Sentence Reduction for Automatic Text Summarization. In Proceedings of the 6th Applied Natural Language Processing Conference, Seattle, USA, pp. 310-315.\n\n  40. Halliday, M. A. K. and Hasan, R. (1976) Cohesion in English, Longman, London.\n\n  41. Hariharan, S., Ramkumar, T., Srinivasan, R. (2013) Enhanced Graph Based Approach for Multi Document Summarization,\u201d The International Arab Journal of Information Technology, 10 (4): 334-341.\n\n  42. Hasan, R. (1984) Coherence and Cohesive Harmony. In: Flood James (Ed.), Understanding Reading Comprehension: Cognition, Language and the Structure of Prose. Newark, Delaware: International Reading Association, pp. 181-219.\n\n  43. Hearst, M. A. (1997). Text tiling: Segmenting text into multi-paragraph subtopic passages. Computational linguistics, 23(1): 33-64.\n\n  44. Hovy, E., and Lin, C. Y. (1999) Automated Text Summarization in SUMMARIST, In: Inderjeet Mani and Mark T. Maybury (Eds.). Advances in Automatic Text Summarization, MIT Press, pp. 18-24.\n\n  45. Keneshloo, Y., Shi, T., Ramakrishnan, N. and Reddy, C. K. (2019). Deep reinforcement learning for sequence-to-sequence models. IEEE transactions on neural networks and learning systems, 31(7): 2469-2489.\n\n  46. Kupiec, J., Pedersen, J. and Chen, F. (1995) A Trainable Document Summarizer. In Proc. of the 18th annual international ACM SIGIR conference on Research and development in information retrieval, pp. 68-73.\n\n  47. Langville, A. N. and Meyer, C. D. (2006) Google's PageRank and beyond - the science of search engine rankings. Princeton University Press 2006, ISBN 978-0-691-12202-1, pp. I-VII, 1-224.\n\n  48. LCSTS (2022) LCSTS Dataset. [https://\u200bwww.\u200bkaggle.\u200bcom/\u200bxuguojin/\u200blcsts-dataset](https://www.kaggle.com/xuguojin/lcsts-dataset). Accessed 9 Aug 2022.\n\n  49. Luhn, H. P. (1958) The Automatic Creation of Literature Abstracts. IBM J. Res. Dev. 2(2): 159-165.\n\n  50. Mahalakshmi, P. and Fatima, N. S. (2022) Summarization of Text and Image Captioning in Information Retrieval Using Deep Learning Techniques. IEEE Access 10: 18289-18297.\n\n  51. Malki, Z., Atlam, E., Dagnew, G., Alzighaibi, A., Ghada, E. and Gad I. (2020) Bidirectional residual LSTM-based human activity recognition, Comput. Inf. Sci., 13(3):1\u201340.\n\n  52. Mani I. and Bloedorn, E. (1997) Multi-document summarization by graph search and matching. AAAI/IAAI, vol. cmplg/ 9712004, pp. 622-628, 1997.\n\n  53.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1887ceb-8bfb-49e5-a4a1-70c459e1746c": {"__data__": {"id_": "c1887ceb-8bfb-49e5-a4a1-70c459e1746c", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c508c36d-07c2-433a-a0ad-8027f870e8bf", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "0f1446f78d07fe0607da143dd5e1b8760faf8081958e7f969fe5ede829c35b4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "96a04c58-6f55-4692-8422-a08b421f9430", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "39c118467ebbeb4ea50c0faeb60be2d7d34a4bf21bd5c5b6394ebd13a284b6b0", "class_name": "RelatedNodeInfo"}}, "hash": "16383155f282d3903707cac013db05fcad61c0aca34f2dd46ad42837adbbce4e", "text": "Dev. 2(2): 159-165.\n\n  50. Mahalakshmi, P. and Fatima, N. S. (2022) Summarization of Text and Image Captioning in Information Retrieval Using Deep Learning Techniques. IEEE Access 10: 18289-18297.\n\n  51. Malki, Z., Atlam, E., Dagnew, G., Alzighaibi, A., Ghada, E. and Gad I. (2020) Bidirectional residual LSTM-based human activity recognition, Comput. Inf. Sci., 13(3):1\u201340.\n\n  52. Mani I. and Bloedorn, E. (1997) Multi-document summarization by graph search and matching. AAAI/IAAI, vol. cmplg/ 9712004, pp. 622-628, 1997.\n\n  53. Mihalcea, R. and Tarau, P. (2004) TextRank: Bringing Order into Text. EMNLP 2004: 404-411\n\n  54. Nallapati, R., Zhai, F. and Zhou. B. (2017) Summarunner: A recurrent neural network-based sequence model for extractive summarization of documents. AAAI 2017: 3075-3081. arXiv:1611.04230\n\n  55. NewsInEssence (2022). NewsInEssence News. [http://\u200bNewsInEssence.\u200bcom](http://newsinessence.com). Accessed 14 June 2022.\n\n  56. NYT (2022) NYT Dataset. [https://\u200bwww.\u200bkaggle.\u200bcom/\u200bdatasets/\u200bmanueldesiretair\u200ba/\u200bdataset-for-text-summarization](https://www.kaggle.com/datasets/manueldesiretaira/dataset-for-text-summarization). Accessed 9 Aug 2022.\n\n  57. Mikolov T, Sutskever I, Chen K, et al (2013) Distributed representations of words and phrases and their compositionality. Adv Neural Inf Process Syst 1\u20139\n\n  58. Ozsoy, M. G., Alpaslan, F. N. and Cicekli, I. (2011) Text summarization using Latent Semantic Analysis. J. Inf. Sci. 37(4): 405-417.\n\n  59. Pervin S. and Haque M. (2013) Literature Review of Automatic Multiple Documents Text Summarization, International Journal of Innovation and Applied Studies, 3(1) 121-129.\n\n  60. Qiu M, Li F-L, Wang S, et al (2017) AliMe Chat: A Sequence to Sequence and Rerank based Chatbot Engine. In: Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, Stroudsburg, PA, USA, pp 498\u2013503\n\n  61. Rath, G. J., Resnick A. and Savage, T. R. (1961) Comparisons of four types of lexical indicators of content. Journal of the American Society for Information Science and Technology, 12(2): 126-130.\n\n  62. Reimers, N., and Gurevych, I. (2019). Sentence-bert: Sentence embeddings using siamese bert-networks, arXiv preprint arXiv:1908.10084.\n\n  63. Robertson, A. M. & Willett, P. (1998). Applications of n-grams in textual information systems. Journal of Documentation, 54(1): 8-67.\n\n  64. Rothkopf, E. Z. (1971). Incidental memory for location of information in text. Journal of verbal learning and verbal behavior, 10(6), 608-613.\n\n  65. Sadler, L. & Spencer, A. (2001). Syntax as an exponent of morphological features. In Yearbook of morphology 2000, pp. 71-96. Springer.\n\n  66. Salton, G. (1989) Automatic Text Processing: the transformation, analysis, and retrieval of information by computer. Addison- Wesley Publishing Company, USA.\n\n  67. Salton G, Wong A and Yang C S.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96a04c58-6f55-4692-8422-a08b421f9430": {"__data__": {"id_": "96a04c58-6f55-4692-8422-a08b421f9430", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1887ceb-8bfb-49e5-a4a1-70c459e1746c", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "16383155f282d3903707cac013db05fcad61c0aca34f2dd46ad42837adbbce4e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6fd7a78e-6f18-42f2-9984-401de4bf5d57", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "924c2771c6825f55951b1988b323040f1f930b21fbc4e9d3f73843f0e4b0d101", "class_name": "RelatedNodeInfo"}}, "hash": "39c118467ebbeb4ea50c0faeb60be2d7d34a4bf21bd5c5b6394ebd13a284b6b0", "text": "Robertson, A. M. & Willett, P. (1998). Applications of n-grams in textual information systems. Journal of Documentation, 54(1): 8-67.\n\n  64. Rothkopf, E. Z. (1971). Incidental memory for location of information in text. Journal of verbal learning and verbal behavior, 10(6), 608-613.\n\n  65. Sadler, L. & Spencer, A. (2001). Syntax as an exponent of morphological features. In Yearbook of morphology 2000, pp. 71-96. Springer.\n\n  66. Salton, G. (1989) Automatic Text Processing: the transformation, analysis, and retrieval of information by computer. Addison- Wesley Publishing Company, USA.\n\n  67. Salton G, Wong A and Yang C S. (1975) A vector space model for automatic indexing. Communications of the ACM, 18(11): 613-620.\n\n  68. See, A., Liu, P. J. and Manning, C. D. (2017) Get To The Point: Summarization with Pointer-Generator Networks. ACL (1) 2017: 1073-1083.\n\n  69. Svore, K. M., Vanderwende L. and Burges, J.C. (2007) Enhancing Single document Summarization by Combining RankNet and Third-party Sources. In Proc. of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 448\u2013457.\n\n  70. Taboada, M. & Mann, W. C. (2006). Applications of rhetorical structure theory. Discourse studies, 8(4): 567-588.\n\n  71. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., Polosukhin, I. (2017) Attention is All you Need. NIPS 2017: 5998-6008. arXiv:1706.03762.\n\n  72. Wan, X. (2008) An Exploration of Document Impact on Graph-Based Multi-Document Summarization. Proc. of the Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, pp. 755\u2013762.\n\n  73. Weeds, J., Weir, D. and McCarthy, D. (2004). Characterising measures of lexical distributional similarity. In COLING 2004: Proceedings of the 20th international conference on Computational Linguistics, pp. 1015-1021.\n\n  74. Weibo (2022) Sina Weibo official site. [https://\u200bweibo.\u200bcom](https://weibo.com). Accessed 29 Sept 2022.\n\n  75. Whissell, J. S. & Clarke, C. L. (2011). Improving document clustering using Okapi BM25 feature weighting. Information retrieval, 14(5): 466-487.\n\n  76. Wolf T, Sanh V, Chaumond J, Delangue C (2018) TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents\n\n  77. Yang, R., Bu, Z. and Xia, Z. (2012) Automatic Summarization for Chinese Text Using Affinity Propagation Clustering and Latent Semantic Analysis. WISM 2012, pp. 543-550\n\n  78. Yang, J., Yi, X., Cheng, D. Z., Hong, L., Li, Y. and Wong, S. (2020). Mixed negative sampling for learning two-tower neural networks in recommendations. In Proceedings of the Web Conference 2020, pp. 441-447.\n\n  79. You O., Li W. and Lu, Q. (2009) An Integrated Multi-document Summarization Approach based on Word Hierarchical Representation. In Proc. of the ACL-IJCNLP Conference, Singapore, pp. 113\u2013116.\n\n  80. Zhang, Y., Jin, R. and Zhou, Z. H. (2010). Understanding bag-of-words model: a statistical framework. International Journal of Machine Learning and Cybernetics, 1(1): 43-52.\n\n  81.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6fd7a78e-6f18-42f2-9984-401de4bf5d57": {"__data__": {"id_": "6fd7a78e-6f18-42f2-9984-401de4bf5d57", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "96a04c58-6f55-4692-8422-a08b421f9430", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "39c118467ebbeb4ea50c0faeb60be2d7d34a4bf21bd5c5b6394ebd13a284b6b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f59c3cd3-9d2a-4bed-a8a8-a22915fca3f3", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "029932f8b29a1b6beeffa5502274b492b0d41f522952850a492f2825912e7ee8", "class_name": "RelatedNodeInfo"}}, "hash": "924c2771c6825f55951b1988b323040f1f930b21fbc4e9d3f73843f0e4b0d101", "text": "543-550\n\n  78. Yang, J., Yi, X., Cheng, D. Z., Hong, L., Li, Y. and Wong, S. (2020). Mixed negative sampling for learning two-tower neural networks in recommendations. In Proceedings of the Web Conference 2020, pp. 441-447.\n\n  79. You O., Li W. and Lu, Q. (2009) An Integrated Multi-document Summarization Approach based on Word Hierarchical Representation. In Proc. of the ACL-IJCNLP Conference, Singapore, pp. 113\u2013116.\n\n  80. Zhang, Y., Jin, R. and Zhou, Z. H. (2010). Understanding bag-of-words model: a statistical framework. International Journal of Machine Learning and Cybernetics, 1(1): 43-52.\n\n  81. Zhou L, Gao J, Li D, Shum H-Y (2020) The Design and Implementation of XiaoIce, an Empathetic Social Chatbot. Comput Linguist 46:53\u201393. [https://\u200bdoi.\u200borg/\u200b10.\u200b1162/\u200bcoli_\u200ba_\u200b00368](https://doi.org/10.1162/coli_a_00368)\n\n  82. Zhuang, S. & Zuccon, G. (2021). TILDE: Term independent likelihood moDEl for passage re-ranking. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 1483-1492.\n\n  83. Zhu T. and Zhao, X. (2012) An Improved Approach to Sentence Ordering For Multi-document Summarization. IACSIT Hong Kong Conferences, IACSIT Press, Singapore, vol. 25, pp. 29-33.\n\n\n#  Part II Natural Language Processing Workshops with Python Implementation in\n14 Hours\n\n\n\u00a9 The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.\n2024\n\nR. S. T. Lee Natural Language Processing\n<https://doi.org/10.1007/978-981-99-1999-4_10>\n\n# 10\\. Workshop#1 Basics of Natural Language Toolkit (Hour 1\u20132)\n\nRaymond S. T. Lee 1\n\n(1)\n\nUnited International College, Beijing Normal University-Hong Kong Baptist\nUniversity, Zhuhai, China\n\n## 10.1 Introduction\n\nPart 2 of this book will provide seven Python programming workshops on how\neach NLP core component operates and integrates with Python-based NLP tools\nincluding NLTK, spaCy, BERT, and Transformer Technology to construct a Q&A\nchatbot.\n\nWorkshop 1 will explore NLP basics including:\n\n  1. 1.\n\nConcepts and installation procedures\n\n  2. 2.\n\nText processing function with examples using NLTK\n\n  3. 3.\n\nText analysis lexical dispersion plot in Python\n\n  4. 4.\n\nTokenization in text analysis\n\n  5. 5.\n\nStatistical tools for text analysis\n\n## 10.2 What Is Natural Language Toolkit (NLTK)?\n\nNLTK (Natural Language Toolkit 2022) is one of the earliest Python-based NLP\ndevelopment tool invented by Prof. Steven Bird and Dr. Edward Loper in the\nDepartment of Computer and Information Science of University of Pennsylvania\nwith their classical book Natural Language Processing with Python published by\nO'Reilly Media Inc. in 2009 (Bird et al. 2009). There are over 30 universities\nin USA and 25 countries using NLTK for NLP related courses until present. This\nbook is considered as bible for anyone who wishes to learn and implement NLP\napplications using Python.\n\nNLTK offers user-oriented interfaces with over 50 corpora and lexical\nresources such as WordNet, a large lexical database of English. Nouns, verbs,\nadjectives, and adverbs are grouped into sets of cognitive synonyms (synsets);\neach expresses a distinct concept which is an important lexical database in\nNLP developed by Princeton University since 1980.\n\nOther lexical databases and corpora are Penn Treebank Corpus, Open\nMultilingual Wordnet, Problem Report Corpus, and Lin\u2019s Dependency Thesaurus.\n\nNLTK contains statistical-based text processing libraries of five fundamental\nNLP enabling technologies and basic semantic reasoning tools including\n(Albrecht et al. 2020; Antic 2021; Arumugam and Shanmugamani 2018; Hardeniya\net al.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f59c3cd3-9d2a-4bed-a8a8-a22915fca3f3": {"__data__": {"id_": "f59c3cd3-9d2a-4bed-a8a8-a22915fca3f3", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6fd7a78e-6f18-42f2-9984-401de4bf5d57", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "924c2771c6825f55951b1988b323040f1f930b21fbc4e9d3f73843f0e4b0d101", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb922417-c0b2-469f-bcc9-8efcdb34f34e", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "501b2234b84bf74870885309a48e48d141ad9c01441e875bafcd7fff17fd51b0", "class_name": "RelatedNodeInfo"}}, "hash": "029932f8b29a1b6beeffa5502274b492b0d41f522952850a492f2825912e7ee8", "text": "This\nbook is considered as bible for anyone who wishes to learn and implement NLP\napplications using Python.\n\nNLTK offers user-oriented interfaces with over 50 corpora and lexical\nresources such as WordNet, a large lexical database of English. Nouns, verbs,\nadjectives, and adverbs are grouped into sets of cognitive synonyms (synsets);\neach expresses a distinct concept which is an important lexical database in\nNLP developed by Princeton University since 1980.\n\nOther lexical databases and corpora are Penn Treebank Corpus, Open\nMultilingual Wordnet, Problem Report Corpus, and Lin\u2019s Dependency Thesaurus.\n\nNLTK contains statistical-based text processing libraries of five fundamental\nNLP enabling technologies and basic semantic reasoning tools including\n(Albrecht et al. 2020; Antic 2021; Arumugam and Shanmugamani 2018; Hardeniya\net al. 2016; Kedia and Rasu 2020; Perkins 2014):\n\n  * Word tokenization\n\n  * Stemming\n\n  * POS tagging\n\n  * Text classification\n\n  * Semantic analysis\n\n## 10.3 A Simple Text Tokenization Example Using NLTK\n\nLet us look at NLTK text tokenization using Jupyter Notebook (Jupyter 2022;\nWintjen and Vlahutin 2020) as below:\n\nIn[1]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figa_HTML.gif)\n\n|\n\n_# Import_ _NLTK_ _package_\n\n**import** nltk  \n  \n---|---|---  \n  \nIn[2]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figb_HTML.gif)\n\n|\n\n_# Create a sample utterance 1 (utt1)_\n\nutt1 = \"At every weekend, early in the morning. I drive my car to the car\ncenter for car washing. Like clock-work.\"  \n  \n---|---|---  \n  \nIn[3]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figc_HTML.gif)\n\n|\n\n_# Display utt1_\n\nutt1  \n  \n---|---|---  \n  \nOut[3]\n\n|\n\n'At every weekend, early in the morning. I drive my car to the car center for\ncar washing. Like clock-work.'  \n  \nIn[4]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figd_HTML.gif)\n\n|\n\n_# Create utterance tokens (utokens)_\n\nutokens = nltk.word_tokenize(utt1)  \n  \n---|---|---  \n  \nIn[5]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Fige_HTML.gif)\n\n|\n\n_# Display utokens_\n\nutokens  \n  \n---|---|---  \n  \nOut[5]\n\n|\n\n['At', 'every', 'weekend', ',', 'early', 'in', 'the', 'morning', '.', 'I',\n'drive', 'my', 'car', 'to', 'the', 'car', 'center', 'for', 'car', 'washing',\n'.', 'Like', 'clock-work', '']  \n  \n## 10.4 How to Install NLTK?\n\n**Step 1 Install Python 3.X**\n\n**Step 2 Install** **NLTK**\n\n2.1 Start CMD or other command line tool\n\n2.2 Type _pip install nltk_\n\nFigure 10.1 shows a screenshot of NLTK installation process.\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Fig1_HTML.png)\n\nFig. 10.1\n\nScreenshot of NLTK installation process\n\n**Step 3 Install** **NLTK** **Data**\n\nOnce NLTK is installed into Python, download NLTK data.\n\n3.1 Run Python\n\n3.2 Type the following to activate a NLTK downloader\n\nimport nltk\n\nnltk.download()\n\nNote: _nltk.downloader()_ will invoke NLTK downloader automatically, a\nseparate window-based downloading module for users to download four NLP\ndatabanks into their Python machines. They include (1) Collection libraries,\n(2) Corpora, (3) Modules, and (4) other NLP packages. Figures 10.2, 10.3, and\n10.4 show screenshots of NTLK downloader for Collection, Corpora, and NTLK\nmodels installations.\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb922417-c0b2-469f-bcc9-8efcdb34f34e": {"__data__": {"id_": "fb922417-c0b2-469f-bcc9-8efcdb34f34e", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f59c3cd3-9d2a-4bed-a8a8-a22915fca3f3", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "029932f8b29a1b6beeffa5502274b492b0d41f522952850a492f2825912e7ee8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "baa8a43a-847e-4883-bdfe-56562826cd5b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "a0b4a019a84536d3591dc50bbb47947c8d9b05622b189d421a3f038753f6dad4", "class_name": "RelatedNodeInfo"}}, "hash": "501b2234b84bf74870885309a48e48d141ad9c01441e875bafcd7fff17fd51b0", "text": "10.1\n\nScreenshot of NLTK installation process\n\n**Step 3 Install** **NLTK** **Data**\n\nOnce NLTK is installed into Python, download NLTK data.\n\n3.1 Run Python\n\n3.2 Type the following to activate a NLTK downloader\n\nimport nltk\n\nnltk.download()\n\nNote: _nltk.downloader()_ will invoke NLTK downloader automatically, a\nseparate window-based downloading module for users to download four NLP\ndatabanks into their Python machines. They include (1) Collection libraries,\n(2) Corpora, (3) Modules, and (4) other NLP packages. Figures 10.2, 10.3, and\n10.4 show screenshots of NTLK downloader for Collection, Corpora, and NTLK\nmodels installations.\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Fig2_HTML.png)\n\nFig. 10.2\n\nScreenshot of NTLK downloader of Collection library\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Fig3_HTML.png)\n\nFig. 10.3\n\nScreenshot of NTLK downloader of Corpora library\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Fig4_HTML.png)\n\nFig. 10.4\n\nScreenshot of NTLK downloader of NLTK models\n\n## 10.5 Why Using Python for NLP?\n\nPython toolkit and packages overtook C, C++, Java especially in data science,\nAI, and NLP software development since 2000 (Albrecht et al. 2020; Kedia and\nRasu 2020). There are several reasons to drive the changes because:\n\n  * it is a generic language without specific area unlike other language such as Java and JavaScript specifically designed for web applications and websites developments.\n\n  * it is easier to learn and user-friendly compared with C and C++ especially for non-computer science students and scientists.\n\n  * its lists and list-processing data types provide an ideal environment for NLP modelling and text analysis.\n\nA Python program performs tokenization task to process text as shown below:\n\nIn[6]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figf_HTML.gif)\n\n|\n\n_# Define utterance 2 (utt2)_\n\nutt2 = \"Hello world. How are you?\"  \n  \n---|---|---  \n  \nIn[7]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figg_HTML.gif)\n\n|\n\n_# Using split() method to split it into word tokens_\n\nutt2.split()  \n  \n---|---|---  \n  \nOut[7]\n\n|\n\n['Hello', 'world.', 'How', 'are', 'you?']  \n  \nIn[8]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figh_HTML.gif)\n\n|\n\n_# Check the no of word tokens_\n\nnwords = len(utt2.split())\n\nprint (\"'Hello world. How are you?' contains \",nwords,\" words.\")  \n  \n---|---|---  \n  \nOut[8]\n\n|\n\n'Hello world. How are you?' contains 5 words.  \n  \nPython codes perform word number counts from literature _Alice\u2019s Adventures in\nWonderland_ by Lewis Carroll (1832\u20131898) as below:\n\nIn[9]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figi_HTML.gif)\n\n|\n\n_# Define method to count the number of word tokens in text file (cwords)_\n\n**def** cwords(literature):\n\n**try** :\n\nwith open(literature, encoding='utf-8') as f_lit:\n\nc_lit = f_lit.read()\n\n**excep** t FileNotFoundError:\n\nerr = \"Sorry, the literature \" + literature + \" does not exist.\"\n\nprint(err)\n\n**else** :\n\nw_lit = c_lit.split()\n\nnwords = len(w_lit)\n\nprint(\"The literature \" + literature + \" contains \" + str(nwords) + \" words.\")\n\nliterature = 'alice.txt'\n\ncwords(literature)  \n  \n---|---|---  \n  \nOut[9]\n\n|\n\nThe literature alice.txt contains 29465 words  \n  \n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figj_HTML.gif)\n\n|\n\nThis workshop has extracted four famous literatures from Project Gutenberg\n(2022):\n\n1\\.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "baa8a43a-847e-4883-bdfe-56562826cd5b": {"__data__": {"id_": "baa8a43a-847e-4883-bdfe-56562826cd5b", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb922417-c0b2-469f-bcc9-8efcdb34f34e", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "501b2234b84bf74870885309a48e48d141ad9c01441e875bafcd7fff17fd51b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19d154ae-7244-4933-a2f3-9899d84f5c54", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "bac18e237b5d82a33a15e817d974313d16d3ad8013eb318a3f64b13b404b769f", "class_name": "RelatedNodeInfo"}}, "hash": "a0b4a019a84536d3591dc50bbb47947c8d9b05622b189d421a3f038753f6dad4", "text": "print(err)\n\n**else** :\n\nw_lit = c_lit.split()\n\nnwords = len(w_lit)\n\nprint(\"The literature \" + literature + \" contains \" + str(nwords) + \" words.\")\n\nliterature = 'alice.txt'\n\ncwords(literature)  \n  \n---|---|---  \n  \nOut[9]\n\n|\n\nThe literature alice.txt contains 29465 words  \n  \n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figj_HTML.gif)\n\n|\n\nThis workshop has extracted four famous literatures from Project Gutenberg\n(2022):\n\n1\\. _Alice\u2019s Adventures in Wonderland_ by Lewis Carroll (1832\u20131898)\n(alice.txt)\n\n2\\. _Little Women_ by Louisa May Alcott (1832\u20131888) (little_women.txt)\n\n3\\. _Moby Dick_ by Herman Melville (1819\u20131891) (moby_dick.txt)\n\n4\\. _The Adventures of_ _Sherlock Holmes_ by Sir Arthur Conan Doyle\n(1859\u20131930) (Adventures_Holmes.txt)  \n  \n---|---  \n  \nIn[10]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figk_HTML.gif)\n\n|\n\ncwords('Adventures_Holmes.txt')  \n  \n---|---|---  \n  \nOut[10]\n\n|\n\nThe literature Adventures_Holmes.txt contains 107411 words.  \n  \n## 10.6 NLTK with Basic Text Processing in NLP\n\nNLTK are Python tools and methods to learn and practice starting from basic\ntext processing in NLP. They include:\n\n  * Text processing as lists of words\n\n  * Statistics on text processing\n\n  * Simple text analysis\n\nNLTK provides 9 different types of text documents from classic literatures,\nBible texts, famous public speeches, news, and articles with personal corpus\nfor text processing. Let us start and load these text documents.\n\nIn[11]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figl_HTML.gif)\n\n|\n\n_# Let's load some sample books from the nltk databank_\n\n**import** nltk\n\n**from** nltk.book **import** *  \n  \n---|---|---  \n  \nOut[11]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figm_HTML.gif)  \n  \nIn[12]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Fign_HTML.gif)\n\n|\n\n_# Display the list of sample books_\n\ntexts()  \n  \n---|---|---  \n  \nOut[12]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figo_HTML.gif)  \n  \nIn[13]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figp_HTML.gif)\n\n|\n\n_# Check text1_\n\ntext1  \n  \n---|---|---  \n  \nOut[13]\n\n|\n\n<Text: Moby Dick by Herman Melville 1851>  \n  \nIn[14]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figq_HTML.gif)\n\n|\n\n_# To know more about text1, check this_\n\ntext1?  \n  \n---|---|---  \n  \nIn[15]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figr_HTML.gif)\n\n|\n\n_# Import word_tokenize as wtoken_\n\n**from** nltk.tokenize **import** word_tokenize\n\n_# Open Adventures_Holmes.txt and performs tokenization_\n\nfholmes = open(\"Adventures_Holmes.txt\",\"r\",encoding=\"utf-8\").read()\n\nwtokens = word_tokenize(fholmes)\n\ntholmes=nltk.text.Text(wtokens)  \n  \n## 10.7 Simple Text Analysis with NLTK\n\n_Text analysis_ is to study a particular word or phrase occurred in a text\ndocument such as literature or public speeches. NLTK has a _\"concordance()\"_\nfunction different from ordinary search function. It does not only indicate\noccurrence but also reveal neighboring words and phrases. Let us try texts\nexamples from _The Adventures of_ _Sherlock Holmes_ (Doyle 2019)\n\nIn[16]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19d154ae-7244-4933-a2f3-9899d84f5c54": {"__data__": {"id_": "19d154ae-7244-4933-a2f3-9899d84f5c54", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "baa8a43a-847e-4883-bdfe-56562826cd5b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "a0b4a019a84536d3591dc50bbb47947c8d9b05622b189d421a3f038753f6dad4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9417db7d-1e88-4534-a6bf-e395abd7b616", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "490bae60032f23fcdeb514d5c4ccdd716e77a2f00ffe07573de3b5d88d8d6eba", "class_name": "RelatedNodeInfo"}}, "hash": "bac18e237b5d82a33a15e817d974313d16d3ad8013eb318a3f64b13b404b769f", "text": "NLTK has a _\"concordance()\"_\nfunction different from ordinary search function. It does not only indicate\noccurrence but also reveal neighboring words and phrases. Let us try texts\nexamples from _The Adventures of_ _Sherlock Holmes_ (Doyle 2019)\n\nIn[16]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figs_HTML.gif)\n\n|\n\n_# Check concordance of word \"Sherlock \"_\n\ntholmes.concordance(\"Sherlock\")  \n  \n---|---|---  \n  \nOut[16]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figt_HTML.gif)  \n  \n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figu_HTML.gif)\n\n|\n\nThe above example shows all _Sherlock_ occurrences indicating that _Sherlock_\nis a special word linked with surname _Holmes_ in text document  \n  \nLet us look at word usage of _extreme_ from the same literature:\n\nIn[17]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figv_HTML.gif)\n\n|\n\n_# Check concordance of word \"extreme\"_\n\ntholmes.concordance(\"extreme\")  \n  \n---|---|---  \n  \nOut[17]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figw_HTML.gif)  \n  \n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figx_HTML.gif)\n\n|\n\n_Concordance_ techniques are means to learn grammars, words, or phrases called\nUse of English, also called Learn by Examples. In this example, we learnt how\nto use the word _extreme_ in various situations and scenarios.  \n  \n---|---  \n  \nIn[18]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figy_HTML.gif)\n\n|\n\ntholmes.similar(\"extreme\")  \n  \n---|---|---  \n  \nOut[18]\n\n|\n\ndense gathering  \n  \nIn[19]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figz_HTML.gif)\n\n|\n\n_# Check concordance of word \"extreme\" in text2_\n\ntext2.concordance(\"extreme\")  \n  \n---|---|---  \n  \nOut[19]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figaa_HTML.gif)  \n  \nIn[20]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figab_HTML.gif)\n\n|\n\n_# Check similar word \"extreme\" in text2_\n\n_text2.similar(\"extreme\")_  \n  \n---|---|---  \n  \nOut[20]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figac_HTML.gif)  \n  \nIn[21]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figad_HTML.gif)\n\n|\n\n_# Check concordance word \"extreme\" in text4_\n\ntext4.concordance (\"extreme\")  \n  \n---|---|---  \n  \nOut[21]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figae_HTML.gif)  \n  \nIn[22]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figaf_HTML.gif)\n\n|\n\n_# Check similar word \"extreme\" in text4_\n\ntext4.similar(\"extreme\")  \n  \nOut[22]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figag_HTML.gif)  \n  \n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figah_HTML.gif)\n\n|\n\nIt showed that word usage of _extreme_ varies by authors and text types, e.g.\nit has different styles in _The Adventures of_ _Sherlock Holmes_ as compared\nwith usage in _Sense and Sensibility_ by Jane Austin (1775\u20131817) which is more\nvivid but has standard and fixed usage in _Inaugural Address_ _Corpus_.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9417db7d-1e88-4534-a6bf-e395abd7b616": {"__data__": {"id_": "9417db7d-1e88-4534-a6bf-e395abd7b616", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19d154ae-7244-4933-a2f3-9899d84f5c54", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "bac18e237b5d82a33a15e817d974313d16d3ad8013eb318a3f64b13b404b769f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60b11cc4-41a6-4d86-99a4-afa86401c63e", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "37a4b3e1cde0439aff355dbb958ad3e89a71588ee7263e2e9e5dd773b96d3fd1", "class_name": "RelatedNodeInfo"}}, "hash": "490bae60032f23fcdeb514d5c4ccdd716e77a2f00ffe07573de3b5d88d8d6eba", "text": "[](../images/533412_1_En_10_Chapter/533412_1_En_10_Figaf_HTML.gif)\n\n|\n\n_# Check similar word \"extreme\" in text4_\n\ntext4.similar(\"extreme\")  \n  \nOut[22]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figag_HTML.gif)  \n  \n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figah_HTML.gif)\n\n|\n\nIt showed that word usage of _extreme_ varies by authors and text types, e.g.\nit has different styles in _The Adventures of_ _Sherlock Holmes_ as compared\nwith usage in _Sense and Sensibility_ by Jane Austin (1775\u20131817) which is more\nvivid but has standard and fixed usage in _Inaugural Address_ _Corpus_.  \n  \n---|---  \n  \nThe _common_contexts()_ method is to examine contexts shared by two or more\nwords. _The Adventures of_ _Sherlock Holmes_ is used with common contexts of\ntwo words _extreme_ and _huge_.\n\nFirst, call common contexts() function from object _tholmes_.\n\nIn[23]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figai_HTML.gif)\n\n|\n\n_# Check common contexts on tholmes_\n\ntholmes.common_contexts([\"extreme\",\"huge\"])  \n  \n---|---|---  \n  \nOut[23]\n\n|\n\nNo common contexts were found  \n  \nwhich means after analyzing _extreme_ and _huge_ in _The Adventures of_\n_Sherlock Holmes_ , no common context meaning can be found.\n\nCall _concordance()_ function of these two words and check against the\nextracted patterns as shown below:\n\nIn[24]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figaj_HTML.gif)\n\n|\n\n_# Check concordance word \"extreme\" in tholmes_\n\ntholmes.concordance(\"extreme\")  \n  \n---|---|---  \n  \nOut[24]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figak_HTML.gif)  \n  \nIn[25]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figal_HTML.gif)\n\n|\n\n_# Check concordance word \"huge\" in tholmes_\n\ntholmes.concordance(\"huge\")  \n  \n---|---|---  \n  \nOut[25]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figam_HTML.gif)  \n  \n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figan_HTML.gif)\n\n|\n\nCan you see how important it is in\n\n1\\. NLP?\n\n2\\. Use of English and technical writing?  \n  \n---|---  \n  \n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figao_HTML.gif)\n\n|\n\n**Workshop 1.1 Simple** **Text Processing** **using** **NLTK**\n\n1\\. Try to use concordance(), similar(), and common_contexts() functions to\nlook for two more frequently used words usage.\n\n2\\. Compare their usages from four sources: _Moby Dick, Sense and Sensibility,\nInaugural Address_ _Corpus_ _, and Wall Street Journal_.\n\n3\\. Are there any pattern(s)?\n\n4\\. What are their differences in Use of English?  \n  \n---|---  \n  \n## 10.8 Text Analysis Using Lexical Dispersion Plot\n\nText analysis was learnt to study word patterns and common contexts in\nprevious workshop.\n\n_Dispersion_ _Plot_ in Python NLTK is to identify occurrence frequencies of\nkeywords from the whole document.\n\n### 10.8.1 What Is a Lexical Dispersion Plot?\n\n_Dispersion_ is quantification of each point deviation from the mean value in\nbasic statistics.\n\nNLTK _Dispersion_ _Plot_ produces a plot showing words distribution throughout\nthe text. _Lexical dispersion_ is used to indicate homogeneity of words (word\ntokens) occurred in the corpus (text document) achieved by the\ndispersion_plot() in NLTK.\n\nTo start, let us use NLTK book object to call function _dispersion_plot()_.\n\nNote: requires pylab installation prior this function.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60b11cc4-41a6-4d86-99a4-afa86401c63e": {"__data__": {"id_": "60b11cc4-41a6-4d86-99a4-afa86401c63e", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9417db7d-1e88-4534-a6bf-e395abd7b616", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "490bae60032f23fcdeb514d5c4ccdd716e77a2f00ffe07573de3b5d88d8d6eba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "076e0f0d-2a58-4075-8205-64d34161176d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3d8c7c212c7165c4c8599dacabede64adca5c6c961531c8afb3211c7697a9276", "class_name": "RelatedNodeInfo"}}, "hash": "37a4b3e1cde0439aff355dbb958ad3e89a71588ee7263e2e9e5dd773b96d3fd1", "text": "4\\. What are their differences in Use of English?  \n  \n---|---  \n  \n## 10.8 Text Analysis Using Lexical Dispersion Plot\n\nText analysis was learnt to study word patterns and common contexts in\nprevious workshop.\n\n_Dispersion_ _Plot_ in Python NLTK is to identify occurrence frequencies of\nkeywords from the whole document.\n\n### 10.8.1 What Is a Lexical Dispersion Plot?\n\n_Dispersion_ is quantification of each point deviation from the mean value in\nbasic statistics.\n\nNLTK _Dispersion_ _Plot_ produces a plot showing words distribution throughout\nthe text. _Lexical dispersion_ is used to indicate homogeneity of words (word\ntokens) occurred in the corpus (text document) achieved by the\ndispersion_plot() in NLTK.\n\nTo start, let us use NLTK book object to call function _dispersion_plot()_.\n\nNote: requires pylab installation prior this function.\n\nThe following example uses text1 to verify basic information about\n_dispersion_plot()_.\n\nIn[26]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figap_HTML.gif)\n\n|\n\ntext1.dispersion_plot **?**  \n  \n---|---|---  \n  \n### 10.8.2 Lexical Dispersion Plot Over Context Using Sense and Sensibility\n\nAre there any lexical patterns for positive words such as _good, happy_ , and\n_strong_ versus negative words such as _bad, sad,_ or _weak_ in literature?\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figaq_HTML.gif)\n\n|\n\n**Workshop 1.2 Lexical** **Dispersion** **Plot** **over Context using Sense\nand Sensibility**\n\nUse dispersion_plot to plot _Lexical Dispersion Plot_ keywords: _good, happy,\nstrong, bad, sad,_ and _weak_ from _Sense and Sensibility_.\n\n1\\. Study any lexical pattern between positive and negative keywords.\n\n2\\. Check these patterns against _Moby Dick_ to see if this pattern occurs and\nexplain.\n\n3\\. Choose two other sentiment keywords to see if this pattern remains valid.  \n  \n---|---  \n  \nIn[27]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figar_HTML.gif)\n\n|\n\ntext2.dispersion_plot([\"good\", \"happy\", \"strong\", \"bad\", \"sad\", \"weak\"])  \n  \n---|---|---  \n  \nOut[27]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figas_HTML.gif)  \n  \n### 10.8.3 Lexical Dispersion Plot Over Time Using Inaugural Address Corpus\n\nLexical usage is to analyze word pattern changes in written English over time.\nThe Inaugural Address Corpus addressed by US presidents of past 220 years is a\ntext document in NLTK book library to study lexical dispersion plot patterns\nchanges on keywords _war, peace, freedom,_ and _united_ for this workshop.\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figat_HTML.gif)\n\n|\n\n**Workshop 1.3 Lexical** **Dispersion** **Plot** **over Time using Inaugural\nAddress** **Corpus**\n\n1\\. Use dispersion_plot to invoke Lexical Dispersion Plot for _Inaugural\nAddress_ _Corpus_.\n\n2\\. Study and explain lexical pattern changes for keywords _America_ ,\n_citizens_ , _democracy_ , _freedom_ , _war_ , _peace_ , _equal_ , _united_.\n\n3\\. Choose any two meaningful keywords and check for lexical pattern changes.  \n  \n---|---  \n  \nIn[28]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figau_HTML.gif)\n\n|\n\ntext4.dispersion_plot([\"America\" ,\"citizens\" ,\"democracy\", \"freedom\", \"war\",\n\"peace\", \"equal\", \"united\"])  \n  \n---|---|---  \n  \nOut[28]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figav_HTML.gif)  \n  \n## 10.9 Tokenization in NLP with NLTK\n\n### 10.9.1 What Is Tokenization in NLP?\n\nA _token_ can be words, part of a word, characters, numbers, punctuations, or\nsymbols.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "076e0f0d-2a58-4075-8205-64d34161176d": {"__data__": {"id_": "076e0f0d-2a58-4075-8205-64d34161176d", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60b11cc4-41a6-4d86-99a4-afa86401c63e", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "37a4b3e1cde0439aff355dbb958ad3e89a71588ee7263e2e9e5dd773b96d3fd1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e49791e-be80-48bc-9abe-94fb78056f86", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "037f62b945938a3d32eacd5bc0fe0a27b21d49ce4672181ddf6de5eb84c76670", "class_name": "RelatedNodeInfo"}}, "hash": "3d8c7c212c7165c4c8599dacabede64adca5c6c961531c8afb3211c7697a9276", "text": "3\\. Choose any two meaningful keywords and check for lexical pattern changes.  \n  \n---|---  \n  \nIn[28]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figau_HTML.gif)\n\n|\n\ntext4.dispersion_plot([\"America\" ,\"citizens\" ,\"democracy\", \"freedom\", \"war\",\n\"peace\", \"equal\", \"united\"])  \n  \n---|---|---  \n  \nOut[28]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figav_HTML.gif)  \n  \n## 10.9 Tokenization in NLP with NLTK\n\n### 10.9.1 What Is Tokenization in NLP?\n\nA _token_ can be words, part of a word, characters, numbers, punctuations, or\nsymbols. It is a principal constituent and complex NLP task due to every\nlanguage has own grammatical constructions to generate grammatic and syntactic\nrules.\n\nTokenization is an NLP process of dividing sentences/utterances from a text,\ndocument, or speeches into chunks called _tokens_. By using tokenization, a\nvocabulary from a document or corpus can be formed. Tokenization for\nsentence/utterances _Jane lent $100 to Peter early this morning_ is shown in\nFig. 10.5.\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Fig5_HTML.png)\n\nFig. 10.5\n\nTokenization example of a sample utterance \u201c _Jane lent $100 to Peter early\nthis morning_ \u201d\n\nNLTK provides flexibility to tokenize any string of text using _tokenize()_\nfunction as shown below:\n\nIn[29]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figaw_HTML.gif)\n\n|\n\n_# Create utterance 3 (utt3) and performs tokenization_\n\nutt3 = 'Jane lent $100 to Peter early this morning.'\n\nwtokens = nltk.word_tokenize(utt3)\n\nwtokens  \n  \n---|---|---  \n  \nOut[29]\n\n|\n\n['Jane', 'lent', '$', '100', 'to', 'Peter', 'early', 'this', 'morning', '.']  \n  \n### 10.9.2 Different Between Tokenize() vs Split()\n\nPython provides _split()_ function to split a sentence of text into words as\nrecalled in Sect. 10.1 Let us see how it works with Tokenize() function.\n\nIn[30]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figax_HTML.gif)\n\n|\n\n_# Use split() to perform word tokenization_\n\nwords = utt3.split()\n\nwords  \n  \n---|---|---  \n  \nOut[30]\n\n|\n\n['Jane', 'lent', '$100', 'to', 'Peter', 'early', 'this', 'morning.']  \n  \n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figay_HTML.gif)\n\n|\n\nWhy are they different?\n\nHow is it important in\n\n1\\. NLP?\n\n2\\. Meanings?  \n  \n---|---  \n  \n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figaz_HTML.gif)\n\n|\n\n**Workshop 1.4** **Tokenization** **on The Adventures of** **Sherlock Holmes**\n**with** **NLTK**\n\n1\\. Read Adventures_Holmes.txt text file.\n\n2\\. Save contents into a string object \"holmes_doc\".\n\n3\\. Use split() to cut it into list object \"holmes\".\n\n4\\. Count total number of words in the document.\n\n5\\. Tokenize document using NLTK tokenize() function.\n\n6\\. Count total number of tokens.\n\n7\\. Compare the two figures.\n\n(The file open part is provided to start with.)  \n  \nIn[31]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figba_HTML.gif)\n\n|\n\n_# Workshop 10.4 Solution_\n\n**with** open('Adventures_Holmes.txt', encoding='utf-8') **as** f_lit:\n\ndholmes = f_lit.read()\n\n# Count number of words in the literature  \n  \n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbb_HTML.gif)\n\n|\n\nNLTK provides a simple way to count total number of tokens in a Text Document\nusing len() in NLTK package.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e49791e-be80-48bc-9abe-94fb78056f86": {"__data__": {"id_": "2e49791e-be80-48bc-9abe-94fb78056f86", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "076e0f0d-2a58-4075-8205-64d34161176d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3d8c7c212c7165c4c8599dacabede64adca5c6c961531c8afb3211c7697a9276", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d22bbedb-a982-43e1-b839-29ea388c95c4", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "5fe404ee5462edc28dde6592bf4c5dad555d6230f1af12560682b669f80ee5c1", "class_name": "RelatedNodeInfo"}}, "hash": "037f62b945938a3d32eacd5bc0fe0a27b21d49ce4672181ddf6de5eb84c76670", "text": "4\\. Count total number of words in the document.\n\n5\\. Tokenize document using NLTK tokenize() function.\n\n6\\. Count total number of tokens.\n\n7\\. Compare the two figures.\n\n(The file open part is provided to start with.)  \n  \nIn[31]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figba_HTML.gif)\n\n|\n\n_# Workshop 10.4 Solution_\n\n**with** open('Adventures_Holmes.txt', encoding='utf-8') **as** f_lit:\n\ndholmes = f_lit.read()\n\n# Count number of words in the literature  \n  \n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbb_HTML.gif)\n\n|\n\nNLTK provides a simple way to count total number of tokens in a Text Document\nusing len() in NLTK package.\n\nTry len(tholmes) will notice:  \n  \nIn[32]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbc_HTML.gif)\n\n|\n\nlen(tholmes)  \n  \n---|---|---  \n  \nOut[32]\n\n|\n\n128366  \n  \n### 10.9.3 Count Distinct Tokens\n\nText analysis is to study distinct words, or vocabulary occurred in a text\ndocument.\n\nWhen text document is tokenized as token objects, Python can group them easily\ninto a set of distinct object using Set() method.\n\nSet() in Python is to extract distinct objects of any types from a list of\nobjects with repeated instances.\n\nTry the following using _The Adventures of_ _Sherlock Holmes_ will notice:\n\nIn[33]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbd_HTML.gif)\n\n|\n\ntholmes **?**  \n  \n---|---|---  \n  \nIn[34]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbe_HTML.gif)\n\n|\n\nset(tholmes)  \n  \n---|---|---  \n  \nOut[34]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbf_HTML.gif)  \n  \nIn[35]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbg_HTML.gif)\n\n|\n\nlen(set(tholmes))  \n  \n---|---|---  \n  \nOut[35]\n\n|\n\n10048  \n  \n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbh_HTML.gif)\n\n|\n\nThis example showed that _The Adventures of_ _Sherlock Holmes_ contains\n128,366 tokens, i.e. words and punctuations, and 10,048 distinct tokens, or\ntypes. Try other literatures and see vocabulary can be learnt from these great\nliteratures.  \n  \n---|---  \n  \nThe following example shows how to sort distinct tokens using sorted()\nfunction.\n\nIn[36]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbi_HTML.gif)\n\n|\n\nsorted(set(tholmes)  \n  \n---|---|---  \n  \nOut[36]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbj_HTML.gif)  \n  \nSince books are tokenized in NLTK as a list book object, contents can be\naccessed by using list indexing method as below:\n\nIn[37]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbk_HTML.gif)\n\n|\n\n# Access the First 20 tokens\n\ntholmes[1:20]  \n  \n---|---|---  \n  \nOut[37]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbl_HTML.gif)  \n  \nIn[38]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbm_HTML.gif)\n\n|\n\n# Access the MIDDLE content\n\ntholmes[100:150]  \n  \n---|---|---  \n  \nOut[38]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbn_HTML.gif)  \n  \nIn[39]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d22bbedb-a982-43e1-b839-29ea388c95c4": {"__data__": {"id_": "d22bbedb-a982-43e1-b839-29ea388c95c4", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e49791e-be80-48bc-9abe-94fb78056f86", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "037f62b945938a3d32eacd5bc0fe0a27b21d49ce4672181ddf6de5eb84c76670", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "840a3c47-d0d1-4be7-9aa3-3601eb0ba40a", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "83ece32235e37cb19a3d8e516e5f297e85b264a1fda80e816d88d25c56508520", "class_name": "RelatedNodeInfo"}}, "hash": "5fe404ee5462edc28dde6592bf4c5dad555d6230f1af12560682b669f80ee5c1", "text": "[](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbk_HTML.gif)\n\n|\n\n# Access the First 20 tokens\n\ntholmes[1:20]  \n  \n---|---|---  \n  \nOut[37]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbl_HTML.gif)  \n  \nIn[38]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbm_HTML.gif)\n\n|\n\n# Access the MIDDLE content\n\ntholmes[100:150]  \n  \n---|---|---  \n  \nOut[38]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbn_HTML.gif)  \n  \nIn[39]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbo_HTML.gif)\n\n|\n\n# Access from the END\n\ntholmes[-20:]  \n  \n---|---|---  \n  \nOut[39]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbp_HTML.gif)  \n  \n### 10.9.4 Lexical Diversity\n\n#### 10.9.4.1 Token Usage Frequency (Lexical Diversity)\n\nToken usage frequency, also called _Lexical Diversity_ is to divide the total\nnumber of tokens by total number of token types as shown:\n\nIn[40]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbq_HTML.gif)\n\n|\n\nlen(text1)/len(set(text1))  \n  \n---|---|---  \n  \nOut[40]\n\n|\n\n13.502044830977896  \n  \nIn[41]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbr_HTML.gif)\n\n|\n\nlen(text2)/len(set(text2))  \n  \n---|---|---  \n  \nOut[41]\n\n|\n\n20.719449729255086  \n  \nIn[42]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbs_HTML.gif)\n\n|\n\nlen(text3)/len(set(text3))  \n  \n---|---|---  \n  \nOut[42]\n\n|\n\n16.050197203298673  \n  \nIn[43]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbt_HTML.gif)\n\n|\n\nlen(text4)/len(set(text4))  \n  \n---|---|---  \n  \nOut[43]\n\n|\n\n15.251970074812968  \n  \n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbu_HTML.gif)\n\n|\n\nPython codes above analyze token usage frequency of four literatures: _Moby\nDick, Sense and Sensibility, Book of Genesis, and Inaugural Address_ _Corpus_\n_._ It has usage frequency range from 13.5 to 20.7. What are the implications?  \n  \n---|---  \n  \n#### 10.9.4.2 Word Usage Frequency\n\nThere are many commonly used words in English. The following example shows the\npattern of word usage frequency for _the_ from above literatures.\n\nIn[44]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbv_HTML.gif)\n\n|\n\ntext1.count('the')  \n  \n---|---|---  \n  \nOut[44]\n\n|\n\n13721  \n  \nIn[45]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbw_HTML.gif)\n\n|\n\ntext1.count('the')/len(text1)*100  \n  \n---|---|---  \n  \nOut[45]\n\n|\n\n5.260736372733581  \n  \nIn[46]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbx_HTML.gif)\n\n|\n\ntext2.count('the')/len(text2)*100  \n  \n---|---|---  \n  \nOut[46]\n\n|\n\n2.7271571452788606  \n  \nIn[47]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figby_HTML.gif)\n\n|\n\ntext3.count('the')/len(text3)*100  \n  \n---|---|---  \n  \nOut[47]\n\n|\n\n5.386024483960325  \n  \nIn[48]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "840a3c47-d0d1-4be7-9aa3-3601eb0ba40a": {"__data__": {"id_": "840a3c47-d0d1-4be7-9aa3-3601eb0ba40a", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d22bbedb-a982-43e1-b839-29ea388c95c4", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "5fe404ee5462edc28dde6592bf4c5dad555d6230f1af12560682b669f80ee5c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b76ef6ff-ef7e-4a70-9a93-53418a22ce7d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "0830d80a7a6b9819feb30b8b8a21b5493246ceb2d4888a8333786adef13d8231", "class_name": "RelatedNodeInfo"}}, "hash": "83ece32235e37cb19a3d8e516e5f297e85b264a1fda80e816d88d25c56508520", "text": "[](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbx_HTML.gif)\n\n|\n\ntext2.count('the')/len(text2)*100  \n  \n---|---|---  \n  \nOut[46]\n\n|\n\n2.7271571452788606  \n  \nIn[47]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figby_HTML.gif)\n\n|\n\ntext3.count('the')/len(text3)*100  \n  \n---|---|---  \n  \nOut[47]\n\n|\n\n5.386024483960325  \n  \nIn[48]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figbz_HTML.gif)\n\n|\n\ntext4.count('the')/len(text4)*100  \n  \n---|---|---  \n  \nOut[48]\n\n|\n\n6.2491416014283745  \n  \n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figca_HTML.gif)\n\n|\n\n1\\. Are there any patterns found from these literatures?\n\n2\\. Use other words _of_ , _a_ , _I_ to study if there exists other\npattern(s).  \n  \n---|---  \n  \n## 10.10 Basic Statistical Tools in NLTK\n\n### 10.10.1 Frequency Distribution: FreqDist()\n\nText analysis is a NTLK tool that can tokenize a string or a book of text\ndocument.\n\nFrequency Distribution\u2014 _FreqDist()_ is an initial built-in method in NLTK to\nanalyze frequency distribution of every token type in a text document.\n\n_Inaugural Address_ _Corpus_ is used as an example to show how it works.\n\nIn[49]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figcb_HTML.gif)\n\n|\n\ntext4  \n  \n---|---|---  \n  \nOut[49]\n\n|\n\n<Text: Inaugural Address Corpus>  \n  \nIn[50]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figcc_HTML.gif)\n\n|\n\nFreqDist?  \n  \n---|---|---  \n  \nIn[51]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figcd_HTML.gif)\n\n|\n\nfd4 = FreqDist(text4)  \n  \n---|---|---  \n  \nIn[52]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figce_HTML.gif)\n\n|\n\nfd4  \n  \n---|---|---  \n  \nOut[52]\n\n|\n\nFreqDist({'the': 9555, ',': 7275, 'of': 7169, 'and': 5226, '.': 5011, 'to':\n4477, 'in': 2604, 'a': 2229, 'our': 2062, 'that': 1769, ...})  \n  \n#### 10.10.1.1 FreqDist() as Dictionary Object\n\nIt is noted that _FreqDist()_ will return _key-value_ pairs from _Dictionary_\nobject to reflect the _Key_ that store _Token Type_ name and the _Value_ which\nare corresponding frequency of occurrence in a text. Since _FreqDist()_\nreturns a _Dictionary_ object, _keys()_ can be used to return the list of all\n_Token Types_ as shown below.\n\nIn[53]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figcf_HTML.gif)\n\n|\n\ntoken4 = fd4.keys()\n\ntoken4  \n  \n---|---|---  \n  \nOut[53]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figcg_HTML.gif)  \n  \n#### 10.10.1.2 Access FreqDist of Any Token Type\n\nUse list item access method to obtain frequency distribution of any token\ntypes. FD value of token type for _the_ is shown below.\n\nIn[54]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figch_HTML.gif)\n\n|\n\nfd4['the']  \n  \n---|---|---  \n  \nOut[54]\n\n|\n\n9555  \n  \n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figci_HTML.gif)\n\n|\n\n1\\. What are five common word types (token types without punctuations) in any\ntext document?\n\n2\\.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b76ef6ff-ef7e-4a70-9a93-53418a22ce7d": {"__data__": {"id_": "b76ef6ff-ef7e-4a70-9a93-53418a22ce7d", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "840a3c47-d0d1-4be7-9aa3-3601eb0ba40a", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "83ece32235e37cb19a3d8e516e5f297e85b264a1fda80e816d88d25c56508520", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b091f2f0-8541-4b80-80bf-f5fceb61406c", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "aceb20332e183ea9a1cccc7aa45386abe7d7b7e4427719bc5bdb48605fdd9787", "class_name": "RelatedNodeInfo"}}, "hash": "0830d80a7a6b9819feb30b8b8a21b5493246ceb2d4888a8333786adef13d8231", "text": "[](../images/533412_1_En_10_Chapter/533412_1_En_10_Figcg_HTML.gif)  \n  \n#### 10.10.1.2 Access FreqDist of Any Token Type\n\nUse list item access method to obtain frequency distribution of any token\ntypes. FD value of token type for _the_ is shown below.\n\nIn[54]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figch_HTML.gif)\n\n|\n\nfd4['the']  \n  \n---|---|---  \n  \nOut[54]\n\n|\n\n9555  \n  \n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figci_HTML.gif)\n\n|\n\n1\\. What are five common word types (token types without punctuations) in any\ntext document?\n\n2\\. Use FreqDist() to verify.  \n  \n---|---  \n  \n#### 10.10.1.3 Frequency Distribution Plot from NLTK\n\nNLTK is a useful tool to study the top frequency distribution token types for\nany document using plot() function with FreqDist() method. FreqDist.plot() can\nalso plot the top XX frequently used token types in a text document.\n\n  1. 1.\n\nUse fd3 to study _FreqDist.plot()_ documentation using _fd3.plot()._\n\n  2. 2.\n\nPlot top 30 frequently used token types from the _Book of Genesis_ (Non-\nCumulative mode).\n\n  3. 3.\n\nDo the same plot with _Cumulative_ mode.\n\nIn[55]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figcj_HTML.gif)\n\n|\n\nfd4.plot **?**  \n  \n---|---|---  \n  \nIn[56]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figck_HTML.gif)\n\n|\n\nfd4.plot (30,cumulative=False)  \n  \n---|---|---  \n  \nOut[56]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figcl_HTML.gif)  \n  \nIn[57]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figcm_HTML.gif)\n\n|\n\nfd4.plot (30,cumulative=True)  \n  \n---|---|---  \n  \nOut[57]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figcn_HTML.gif)  \n  \n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figco_HTML.gif)\n\n|\n\n**Workshop 1.5 Frequency Distribution Analysis on Classics Literatures**\n\n1\\. What are top 5 frequently used word types in the _Book of Genesis_ (ignore\npunctuations)?\n\n2\\. Will it be the same with other great literatures?\n\n3\\. Verify against ( _1) Moby Dick, (2) Sense and Sensibility, and (3)\nInaugural Address_ _Corpus_ to see if they have the same patterns. Why or why\nnot?\n\n4\\. Why the study of common word types is also important in cryptography?  \n  \n---|---  \n  \n### 10.10.2 Rare Words: Hapax\n\nHapaxes are words that occur only once in a body of work whether it is a\npublication or an entire language.\n\nAncient texts are full of hapaxes. For instance, in Shakespeare's _Love's\nLabour's Lost_ contains hapax _honorificabilitudinitatibus_ which means able\nto achieve honors.\n\nNLTK provides method hapaxes() under FreqDist object to list out all word\ntypes that occurred once in text document.\n\nTry FreqDist() with _The Adventures of_ _Sherlock Holmes_ and see how useful\nit is.\n\nIn[58]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figcp_HTML.gif)\n\n|\n\ntholmes  \n  \n---|---|---  \n  \nOut[58]\n\n|\n\n<Text: The Adventures of Sherlock Holmes by Arthur Conan...>  \n  \nIn[59]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figcq_HTML.gif)\n\n|\n\nfd = FreqDist(tholmes)  \n  \n---|---|---  \n  \nIn[60]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b091f2f0-8541-4b80-80bf-f5fceb61406c": {"__data__": {"id_": "b091f2f0-8541-4b80-80bf-f5fceb61406c", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b76ef6ff-ef7e-4a70-9a93-53418a22ce7d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "0830d80a7a6b9819feb30b8b8a21b5493246ceb2d4888a8333786adef13d8231", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aaa04e9b-1b9f-43aa-a323-56f1acbf036c", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "373d5f2f09104a59ebfac933723f46ffa49317c33ddbd25a0b80ad74e5f9b4c3", "class_name": "RelatedNodeInfo"}}, "hash": "aceb20332e183ea9a1cccc7aa45386abe7d7b7e4427719bc5bdb48605fdd9787", "text": "NLTK provides method hapaxes() under FreqDist object to list out all word\ntypes that occurred once in text document.\n\nTry FreqDist() with _The Adventures of_ _Sherlock Holmes_ and see how useful\nit is.\n\nIn[58]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figcp_HTML.gif)\n\n|\n\ntholmes  \n  \n---|---|---  \n  \nOut[58]\n\n|\n\n<Text: The Adventures of Sherlock Holmes by Arthur Conan...>  \n  \nIn[59]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figcq_HTML.gif)\n\n|\n\nfd = FreqDist(tholmes)  \n  \n---|---|---  \n  \nIn[60]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figcr_HTML.gif)\n\n|\n\nhap = fd.hapaxes()\n\nhap[1:50]  \n  \n---|---|---  \n  \nOut[60]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figcs_HTML.gif)  \n  \n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figct_HTML.gif)\n\n|\n\n**Workshop 1.6 Learn Vocabulary using** **Hapaxes**\n\nHapaxes are useful for us to learn vocabulary contain more than 12 characters.\nThe following example uses hapaxes() with Python in-line function to implement\n[w for w in hap1 if len(w) > 12]:\n\n1\\. Create the Python script and extract vocabulary contain more than 12\ncharacters from _Moby Dick_.\n\n2\\. Select five meaningful vocabularies with their meanings.\n\n3\\. Check with _The Adventures of_ _Sherlock Holmes_ to learn another five\nvocabularies.\n\n(Python script to generate vocabulary with over 12 characters are given.)  \n  \n---|---  \n  \nIn[61]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figcu_HTML.gif)\n\n|\n\n_# Workshop 10.6 Solutions_\n\nvoc12 = [w **for** w **in** hap **if** len(w) > 12]\n\nvoc12  \n  \n---|---|---  \n  \nOut[61]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figcv_HTML.gif)  \n  \n### 10.10.3 Collocations\n\n#### 10.10.3.1 What Are Collocations?\n\nA _collocation_ is a work grouping for a set of words usually appeared\ntogether to convey semantic meanings. The word _collocation_ is originated\nfrom Latin word meaning place together and was first introduced by Prof. John\nR Firth (1890\u20131960) with his famous quote \u201cYou shall know a word by the\ncompany it keeps.\u201d\n\nThere are many cases in English where strong collocations are word pairings\nalways appear together such as _make_ and _do,_ e.g. You make a cup of coffee,\nbut you do your work.\n\nCollocations are frequently used in business settings when nouns are combined\nwith verbs or adjectives, e.g. set up an appointment, conduct a meeting, set\nthe price etc.\n\n#### 10.10.3.2 Collocations in NLTK\n\nNLTK also provides a build-in method to handle collocations using NLTK method\u2014\n_collocations()._\n\nThe following example is to generate collocations lists from _Moby Dick, Sense\nand Sensibility_ , _Book of Genesis,_ and _Inaugural Address_ _Corpus_.\n\nLet us look at some extracted collocation terms:\n\nIn[62]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figcw_HTML.gif)\n\n|\n\ntext1.collocations()  \n  \n---|---|---  \n  \nOut[62]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figcx_HTML.gif)  \n  \nIn[63]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figcy_HTML.gif)\n\n|\n\ntext2.collocations()  \n  \n---|---|---  \n  \nOut[63]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figcz_HTML.gif)  \n  \nIn[64]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aaa04e9b-1b9f-43aa-a323-56f1acbf036c": {"__data__": {"id_": "aaa04e9b-1b9f-43aa-a323-56f1acbf036c", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b091f2f0-8541-4b80-80bf-f5fceb61406c", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "aceb20332e183ea9a1cccc7aa45386abe7d7b7e4427719bc5bdb48605fdd9787", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4d84669-bde2-45ff-9867-3fd174a84d14", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "e5509005458b95f56bb45d05e441849627eff7816134599cd3b414318ec1d878", "class_name": "RelatedNodeInfo"}}, "hash": "373d5f2f09104a59ebfac933723f46ffa49317c33ddbd25a0b80ad74e5f9b4c3", "text": "Let us look at some extracted collocation terms:\n\nIn[62]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figcw_HTML.gif)\n\n|\n\ntext1.collocations()  \n  \n---|---|---  \n  \nOut[62]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figcx_HTML.gif)  \n  \nIn[63]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figcy_HTML.gif)\n\n|\n\ntext2.collocations()  \n  \n---|---|---  \n  \nOut[63]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figcz_HTML.gif)  \n  \nIn[64]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figda_HTML.gif)\n\n|\n\ntext3.collocations()  \n  \n---|---|---  \n  \nOut[64]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figdb_HTML.gif)  \n  \nIn[65]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figdc_HTML.gif)\n\n|\n\ntext4.collocations()  \n  \n---|---|---  \n  \nOut[65]\n\n|\n\n![](../images/533412_1_En_10_Chapter/533412_1_En_10_Figdd_HTML.gif)  \n  \nReferences\n\n  1. Albrecht, J., Ramachandran, S. and Winkler, C. (2020) Blueprints for Text Analytics Using Python: Machine Learning-Based Solutions for Common Real World (NLP) Applications. O\u2019Reilly Media.\n\n  2. Antic, Z. (2021) Python Natural Language Processing Cookbook: Over 50 recipes to understand, analyze, and generate text for implementing language processing tasks. Packt Publishing.\n\n  3. Arumugam, R. and Shanmugamani, R. (2018) Hands-On Natural Language Processing with Python: A practical guide to applying deep learning architectures to your NLP applications. Packt Publishing.\n\n  4. Bird, S., Klein, E., and Loper, E. (2009). Natural language processing with python. O'Reilly.\n\n  5. Doyle, A. C. (2019) The Adventures of Sherlock Holmes (AmazonClassics Edition). AmazonClassics.\n\n  6. Gutenberg (2022) Project Gutenberg official site. [https://\u200bwww.\u200bgutenberg.\u200borg/\u200b](https://www.gutenberg.org/) Accessed 16 June 2022.\n\n  7. Hardeniya, N., Perkins, J. and Chopra, D. (2016) Natural Language Processing: Python and NLTK. Packt Publishing.\n\n  8. Jupyter (2022) Jupyter official site. [https://\u200bjupyter.\u200borg/\u200b](https://jupyter.org/). Accessed 16 June 2022.\n\n  9. Kedia, A. and Rasu, M. (2020) Hands-On Python Natural Language Processing: Explore tools and techniques to analyze and process text with a view to building real-world NLP applications. Packt Publishing.\n\n  10. NLTK (2022) NLTK official site. [https://\u200bwww.\u200bnltk.\u200borg/\u200b](https://www.nltk.org/). Accessed 16 June 2022.\n\n  11. Perkins, J. (2014). Python 3 text processing with NLTK 3 cookbook. Packt Publishing Ltd.\n\n  12. Wintjen, M. and Vlahutin, A. (2020) Practical Data Analysis Using Jupyter Notebook: Learn how to speak the language of data by extracting useful and actionable insights using Python. Packt Publishing.\n\n  13. WordNet (2022) WordNet official site. [https://\u200bwordnet.\u200bprinceton.\u200bedu/\u200b](https://wordnet.princeton.edu/). Accessed 16 June 2022.\n\n\n\u00a9 The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.\n2024\n\nR. S. T. Lee Natural Language Processing\n<https://doi.org/10.1007/978-981-99-1999-4_11>\n\n# 11\\.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4d84669-bde2-45ff-9867-3fd174a84d14": {"__data__": {"id_": "b4d84669-bde2-45ff-9867-3fd174a84d14", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aaa04e9b-1b9f-43aa-a323-56f1acbf036c", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "373d5f2f09104a59ebfac933723f46ffa49317c33ddbd25a0b80ad74e5f9b4c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b4150c2-c8a7-402a-a2c7-a8e9957c668e", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "e60b82fd4a298ca6a502f3af93049950fcfd7048a2ea3d5086ba50bba13150fb", "class_name": "RelatedNodeInfo"}}, "hash": "e5509005458b95f56bb45d05e441849627eff7816134599cd3b414318ec1d878", "text": "11. Perkins, J. (2014). Python 3 text processing with NLTK 3 cookbook. Packt Publishing Ltd.\n\n  12. Wintjen, M. and Vlahutin, A. (2020) Practical Data Analysis Using Jupyter Notebook: Learn how to speak the language of data by extracting useful and actionable insights using Python. Packt Publishing.\n\n  13. WordNet (2022) WordNet official site. [https://\u200bwordnet.\u200bprinceton.\u200bedu/\u200b](https://wordnet.princeton.edu/). Accessed 16 June 2022.\n\n\n\u00a9 The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.\n2024\n\nR. S. T. Lee Natural Language Processing\n<https://doi.org/10.1007/978-981-99-1999-4_11>\n\n# 11\\. Workshop#2 N-grams in NLTK and Tokenization in SpaCy (Hour 3\u20134)\n\nRaymond S. T. Lee 1\n\n(1)\n\nUnited International College, Beijing Normal University-Hong Kong Baptist\nUniversity, Zhuhai, China\n\n## 11.1 Introduction\n\nWorkshop 2 consists of two parts:\n\nPart 1 will introduce N-gram language model using NLTK in Python and N-grams\nclass to generate N-gram statistics on any sentence, text objects, whole\ndocument, literature to provide a foundation technique for text analysis,\nparsing and semantic analysis in subsequent workshops.\n\nPart 2 will introduce spaCy, the second important NLP Python implementation\ntools not only for teaching and learning (like NLTK) but also widely used for\nNLP applications including text summarization, information extraction, and Q&A\nchatbot. It is a critical mass to integrate with Transformer Technology in\nsubsequent workshops.\n\n## 11.2 What Is N-Gram?\n\nN-gram is an algorithm based on statistical language model (Bird et al. 2009;\nPerkins 2014; Arumugam and Shanmugamani 2018), its basic idea is that contents\nsuch as phonemes, syllables, letters, words, or base pairs in texts are\noperated by a sliding window of size N to form a byte fragments sequence of\nlength N (Sidorov 2019).\n\nN can be 1, 2, or other positive integer, although usually large N is not\nconsidered because they rarely occur.\n\nEach byte fragment is called a gram, and the frequency of all grams is counted\nand filtered according to a pre-set threshold to form a list of key grams,\nwhich is the text\u2019s vector feature space, and each kind of gram in the list is\na feature vector dimension.\n\n## 11.3 Applications of N-Grams in NLP\n\nN-gram models are widely used (Albrecht et al. 2020; Arumugam and Shanmugamani\n2018; Hardeniya et al. 2016; Kedia and Rasu 2020) in:\n\n  * Speech recognition where phonemes and sequences of phonemes are modeled using a N-gram distribution.\n\n  * Parsing on words are modeled so that each N-gram is composed of N words. For language identification, sequences of characters/graphemes (e.g. letters of the alphabet) are modeled for different languages.\n\n  * Auto sentences completion\n\n  * Auto spell-check\n\n  * Semantic analysis\n\n## 11.4 Generation of N-Grams in NLTK\n\nNLTK (NLTK 2022; Bird et al. 2009; Perkins 2014) offers useful tools in NLP\nprocessing.\n\n_Ngrams()_ function in NLTK facilitates N-gram operation.\n\nPython code uses N-grams in NLTK to generate N-grams for any text string. Try\nit and study how it works.\n\nThe following example is the first sentence of _A Scandal in Bohemia_ from\n_The Adventures of_ _Sherlock Holmes_ (Doyle 2019): _To Sherlock Holmes she is\nalways \u201cThe Woman.\u201d I have seldom heard him mention her under any other name,_\ndemonstrating how N-gram generator works in NLTK.\n\nIn[1]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b4150c2-c8a7-402a-a2c7-a8e9957c668e": {"__data__": {"id_": "6b4150c2-c8a7-402a-a2c7-a8e9957c668e", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b4d84669-bde2-45ff-9867-3fd174a84d14", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "e5509005458b95f56bb45d05e441849627eff7816134599cd3b414318ec1d878", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b0f522d-72d1-4a39-9d3d-9440cf5aa039", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "264da25bbc40aef82e29ade14cef6d5ed7a8198006e47fae8e98723290a38b06", "class_name": "RelatedNodeInfo"}}, "hash": "e60b82fd4a298ca6a502f3af93049950fcfd7048a2ea3d5086ba50bba13150fb", "text": "letters of the alphabet) are modeled for different languages.\n\n  * Auto sentences completion\n\n  * Auto spell-check\n\n  * Semantic analysis\n\n## 11.4 Generation of N-Grams in NLTK\n\nNLTK (NLTK 2022; Bird et al. 2009; Perkins 2014) offers useful tools in NLP\nprocessing.\n\n_Ngrams()_ function in NLTK facilitates N-gram operation.\n\nPython code uses N-grams in NLTK to generate N-grams for any text string. Try\nit and study how it works.\n\nThe following example is the first sentence of _A Scandal in Bohemia_ from\n_The Adventures of_ _Sherlock Holmes_ (Doyle 2019): _To Sherlock Holmes she is\nalways \u201cThe Woman.\u201d I have seldom heard him mention her under any other name,_\ndemonstrating how N-gram generator works in NLTK.\n\nIn[1]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figa_HTML.gif)\n\n|\n\n**import** nltk\n\n**from** nltk **import** ngrams\n\nsentence = input( \"Enter the sentence: \" )\n\nn = int(input( \"Enter the value of n: \" ))\n\nn_grams = ngrams(sentence.split(), n)\n\n**for** grams **in** n_grams:\n\nprint(grams)  \n  \n---|---|---  \n  \nOut[1]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figb_HTML.gif)  \n  \nHere are the Bigrams. Let us try Trigrams N=3.\n\nIn[2]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figc_HTML.gif)\n\n|\n\n**import** nltk\n\n**from** nltk **import** ngrams\n\nsentence = input( \"Enter the sentence: \" )\n\nn = int(input( \"Enter the value of n: \" ))\n\nn_grams = ngrams(sentence.split(), n)\n\n**for** grams **in** n_grams:\n\nprint(grams)  \n  \n---|---|---  \n  \nOut[2]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figd_HTML.gif)  \n  \nHow about Quadrigram N=4? Let us use the same sentence.\n\nIn[3]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Fige_HTML.gif)\n\n|\n\n**import** nltk\n\n**from** nltk **import** ngrams\n\nsentence = input( \"Enter the sentence: \" )\n\nn = int(input( \"Enter the value of n: \" ))\n\nn_grams = ngrams(sentence.split(), n)\n\n**for** grams **in** n_grams:\n\nprint(grams)  \n  \n---|---|---  \n  \nOut[3]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figf_HTML.gif)  \n  \n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figg_HTML.gif)\n\n|\n\nNLTK offers an easy solution to generate N-gram of any N-number which are\nuseful in N-gram probability calculations and text analysis  \n  \n---|---  \n  \n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figh_HTML.gif)\n\n|\n\n**Workshop 2.1 N-Grams on The Adventures of** **Sherlock Holmes**\n\n1\\. Read Adventures_Holmes.txt text file.\n\n2\\. Save contents into a string object \"holmes_doc\".\n\n3\\. Extract favorite paragraph from \"holmes_doc\" into \"holmes_para\".\n\n4\\. Use above Python code to generate N-grams for N=3, N=4, and N=5.  \n  \n## 11.5 Generation of N-Grams Statistics\n\nOnce N-grams are generated, the next step is to calculate term frequency (TF)\nof each N-grams from a document to list out top items.\n\nNLTK-based Python codes extend previous example to create N-grams statistics\nto list out top 10 N-grams.\n\nLet us try first two sentences of _A Scandal in Bohemia_ from _The Adventures\nof_ _Sherlock Holmes_.\n\nIn[4]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figi_HTML.gif)\n\n|\n\nsentence  \n  \n---|---|---  \n  \nOut[4]\n\n|\n\n'To Sherlock Holmes she is always \"The Woman\".", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b0f522d-72d1-4a39-9d3d-9440cf5aa039": {"__data__": {"id_": "0b0f522d-72d1-4a39-9d3d-9440cf5aa039", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b4150c2-c8a7-402a-a2c7-a8e9957c668e", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "e60b82fd4a298ca6a502f3af93049950fcfd7048a2ea3d5086ba50bba13150fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a7ad520-810c-4d3b-8402-6e2ac6363ea9", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "ed1bd4da91a76fd84b26465b3e0f6dae5dd5796d952696999b13ccd44b582e29", "class_name": "RelatedNodeInfo"}}, "hash": "264da25bbc40aef82e29ade14cef6d5ed7a8198006e47fae8e98723290a38b06", "text": "Extract favorite paragraph from \"holmes_doc\" into \"holmes_para\".\n\n4\\. Use above Python code to generate N-grams for N=3, N=4, and N=5.  \n  \n## 11.5 Generation of N-Grams Statistics\n\nOnce N-grams are generated, the next step is to calculate term frequency (TF)\nof each N-grams from a document to list out top items.\n\nNLTK-based Python codes extend previous example to create N-grams statistics\nto list out top 10 N-grams.\n\nLet us try first two sentences of _A Scandal in Bohemia_ from _The Adventures\nof_ _Sherlock Holmes_.\n\nIn[4]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figi_HTML.gif)\n\n|\n\nsentence  \n  \n---|---|---  \n  \nOut[4]\n\n|\n\n'To Sherlock Holmes she is always \"The Woman\". I have seldom heard him mention\nher under any other name.'  \n  \nImport RE package to do some simple text preprocessing:\n\nIn[5]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figj_HTML.gif)\n\n|\n\n**import** re, string\n\n_# get rid of all the XML markup_\n\nsentence = re.sub ('<.*>' , ' ', sentence)\n\n_# get rid of punctuation (except periods!)_\n\npunctuationNoPeriod = \"[\" + re.sub(\"\\\\.\",\"\",string.punctuation) + \"]\"\n\nsentence = re.sub(punctuationNoPeriod, \"\", sentence)\n\n_# first get individual words_\n\ntokenized = sentence.split()\n\n_# and get a list of all the bi-grams_\n\nBigrams = ngrams(tokenized, 2)  \n  \n---|---|---  \n  \nReview N-grams to see how they work:\n\nIn[6]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figk_HTML.gif)\n\n|\n\nngrams **?**  \n  \n---|---|---  \n  \nTo generate N-gram statistics, first import \"collections\" class and invoke\nCounter() method over Bigrams to perform N-gram statistics analysis.\n\nIn[7]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figl_HTML.gif)\n\n|\n\n**import** collections\n\n_# get the frequency of each bigram in our corpus_\n\nBigramFreq = collections.Counter(Bigrams)\n\n_# what are the ten most popular ngrams in this corpus?_\n\nBigramFreq.most_common(10)  \n  \n---|---|---  \n  \nOut[7]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figm_HTML.gif)  \n  \n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Fign_HTML.gif)\n\n|\n\nIt is noted that top 10 bigram frequency are all with count 1.\n\nThis is because the sample sentence is short and does not contain any\nbigram(s) with a frequent bigram statistic. To sort out this problem, let us\ntry a longer text. The following example uses the whole first paragraph of _A\nScandal in Bohemia_ from _The Adventures of_ _Sherlock Holmes_ and see whether\nit has a preferable result.  \n  \n---|---  \n  \nThe first paragraph looks like this:\n\nIn[8]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figo_HTML.gif)\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figp_HTML.gif)  \n  \n---|---|---  \n  \nLet us review this first paragraph:\n\nIn[9]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figq_HTML.gif)\n\n|\n\nfirst_para  \n  \n---|---|---  \n  \nUse Python script to remove punctuation marks and tokenize the first_para\nobject:\n\nIn[10]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figr_HTML.gif)\n\n|\n\n**import** re, string\n\n_# get rid of all the XML markup_\n\nfirst_para = re.sub ('<.*>' , ' ', first_para)\n\n_# get rid of punctuation (except periods!)_\n\npunctuationNoPeriod = \"[\" + re.sub(\"\\\\.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a7ad520-810c-4d3b-8402-6e2ac6363ea9": {"__data__": {"id_": "0a7ad520-810c-4d3b-8402-6e2ac6363ea9", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b0f522d-72d1-4a39-9d3d-9440cf5aa039", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "264da25bbc40aef82e29ade14cef6d5ed7a8198006e47fae8e98723290a38b06", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ae0b6fe-524f-48a1-82bc-dc3bfe0bf78e", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "b34eb9eb5e6730faf6eb8d1dbab8ed37ad155924c47476a4645a699e6eb49b6d", "class_name": "RelatedNodeInfo"}}, "hash": "ed1bd4da91a76fd84b26465b3e0f6dae5dd5796d952696999b13ccd44b582e29", "text": "[](../images/533412_1_En_11_Chapter/533412_1_En_11_Figp_HTML.gif)  \n  \n---|---|---  \n  \nLet us review this first paragraph:\n\nIn[9]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figq_HTML.gif)\n\n|\n\nfirst_para  \n  \n---|---|---  \n  \nUse Python script to remove punctuation marks and tokenize the first_para\nobject:\n\nIn[10]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figr_HTML.gif)\n\n|\n\n**import** re, string\n\n_# get rid of all the XML markup_\n\nfirst_para = re.sub ('<.*>' , ' ', first_para)\n\n_# get rid of punctuation (except periods!)_\n\npunctuationNoPeriod = \"[\" + re.sub(\"\\\\.\",\"\",string.punctuation) + \"]\"\n\nfirst_para = re.sub(punctuationNoPeriod, \"\", first_para)\n\n_# first get individual words_\n\ntokenized = first_para.split()\n\n_# and get a list of all the bi-grams_\n\nBigrams = ngrams(tokenized, 2)  \n  \n---|---|---  \n  \nUse Counter() method of Collections class to calculate Bigram statistics of\nfirst_para:\n\nIn[11]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figs_HTML.gif)\n\n|\n\n**import** collections\n\n_# get the frequency of each bigram in our corpus_\n\nBigramFreq = collections.Counter(Bigrams)\n\n_# what are the ten most popular ngrams in this corpus?_\n\nBigramFreq.most_common(10)  \n  \n---|---|---  \n  \nOut[11]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figt_HTML.gif)  \n  \n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figu_HTML.gif)\n\n|\n\nThe results are satisfactory. It is noted that bigram _in a_ has the most\noccurrence frequency, i.e. three times while four other bigrams: _Irene Adler_\n, _and that_ , _for the_ , _his own_ have occurred twice each within the\nparagraph. Bigram _in a_ , _and that_ and _for the_ are frequently used\nEnglish phrases which occurred in almost every text document. How about _To\nSherlock_ and _Irene Adler_? There are two N-gram types frequently used in\nN-gram Language Model studied in Chap. [2](533412_1_En_2_Chapter.xhtml). One\nis the frequently used N-gram phrase in English like _in a_ , _and that_ and\n_for that_ in our case. These bigrams are common phrases in other documents\nand literature writings. Another is domain-specific N-grams. These types are\nonly frequently used in specific domain, documents, and genre of literatures.\nHence, _To Sherlock_ and _Irene Adler_ are frequently used related to this\nstory only and not in other situations.  \n  \n---|---  \n  \n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figv_HTML.gif)\n\n|\n\n**Workshop 2.2 N-grams Statistics on The Adventures of** **Sherlock Holmes**\n\n1\\. Read Adventures_Holmes.txt text file.\n\n2\\. Save contents into a string object \"holmes_doc\".\n\n3\\. Generate a more representative N-gram statistic using the whole\nholmes_doc.\n\n4\\. Generate top 10 N-grams summary for N=3, N=4, and N=5.\n\n5\\. Review results and comments on pattern(s) found.  \n  \n---|---  \n  \nBigram analysis is required to examine which bigrams are commonly used not\nonly on single paragraph but also for the whole document or literature.\nRemember in Workshop 1 NLTK has a built-in list of tokenized sample\nliteratures in nltk.book. Let us refer them first by using the nltk.book\nimport statement.\n\nIn[12]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figw_HTML.gif)\n\n|\n\n_# Let's load some sample books from the nltk databank_\n\n**import** nltk\n\n**from** nltk.book **import ***  \n  \n---|---|---  \n  \nOut[12]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ae0b6fe-524f-48a1-82bc-dc3bfe0bf78e": {"__data__": {"id_": "7ae0b6fe-524f-48a1-82bc-dc3bfe0bf78e", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a7ad520-810c-4d3b-8402-6e2ac6363ea9", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "ed1bd4da91a76fd84b26465b3e0f6dae5dd5796d952696999b13ccd44b582e29", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9bd7c0a-1d77-4f66-9b3c-a6ae4d37a1a7", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "45a72627da8ebee98504a23a8ecc2b0cc3bf0dba2b0566d9e551382728915ddf", "class_name": "RelatedNodeInfo"}}, "hash": "b34eb9eb5e6730faf6eb8d1dbab8ed37ad155924c47476a4645a699e6eb49b6d", "text": "4\\. Generate top 10 N-grams summary for N=3, N=4, and N=5.\n\n5\\. Review results and comments on pattern(s) found.  \n  \n---|---  \n  \nBigram analysis is required to examine which bigrams are commonly used not\nonly on single paragraph but also for the whole document or literature.\nRemember in Workshop 1 NLTK has a built-in list of tokenized sample\nliteratures in nltk.book. Let us refer them first by using the nltk.book\nimport statement.\n\nIn[12]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figw_HTML.gif)\n\n|\n\n_# Let's load some sample books from the nltk databank_\n\n**import** nltk\n\n**from** nltk.book **import ***  \n  \n---|---|---  \n  \nOut[12]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figx_HTML.gif)  \n  \nCheck with text1 to see what they are:\n\nIn[13]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figy_HTML.gif)\n\n|\n\ntext1  \n  \n---|---|---  \n  \nOut[13]\n\n|\n\n<Text: Moby Dick by Herman Melville 1851>  \n  \nor download using nltk.corpus.gutenberg.words() from Project Gutenberg of\ncopyright clearance classic literature (Gutenberg 2022). Let us use this\nmethod to download _Moby Dick_ (Melville 2006).\n\nIn[14]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figz_HTML.gif)\n\n|\n\n**import** nltk.corpus\n\n**from** nltk.text **import** Text\n\nmoby = Text(nltk.corpus.gutenberg.words( 'melville-moby_dick.txt' ))  \n  \n---|---|---  \n  \nIn[15]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figaa_HTML.gif)\n\n|\n\nmoby  \n  \n---|---|---  \n  \nOut[15]\n\n|\n\n<Text: Moby Dick by Herman Melville 1851>  \n  \nReview the first 50 elements of _Moby Dick_ text object to see whether they\nare tokenized.\n\nIn[16]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figab_HTML.gif)\n\n|\n\nmoby [1:50]  \n  \n---|---|---  \n  \nOut[16]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figac_HTML.gif)  \n  \nUse _Collections_ class and _ngrams()_ method for Bigram statistics to\nidentify top 20 most frequently bigrams occurred for the entire _Moby Dick_\nliterature.\n\nIn[17]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figad_HTML.gif)\n\n|\n\n**import** collections\n\n_# and get a list of all the bi-grams_\n\nBigrams = ngrams(moby, 2)\n\n_# get the frequency of each bigram in our corpus_\n\nBigramFreq = collections.Counter(Bigrams)\n\n_# what are the 20 most popular ngrams in this corpus?_\n\nBigramFreq.most_common(20)  \n  \n---|---|---  \n  \nOut[17]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figae_HTML.gif)  \n  \n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figaf_HTML.gif)\n\n|\n\n**Workshop 2.3 N-grams Statistics with removal of unnecessary punctuations**\n\nThe results are average and unsatisfactory. It is noted that _and_ , _of the_\n, _s_ and _in the_ are top 4 bigrams occurred in the entire Moby Dick\nliterature. It is average since these bigrams are common English usage but\noriginal bigram statistics in simple sentences required to remove all\npunctuations by:\n\n1\\. List out all punctuations required to remove.\n\n2\\. Revise above Python script to remove these punctuation symbol from the\ntoken list.\n\n3\\. Generate a top 20 Bigram summary for _Moby Dick_ without punctuations.\n\n4\\.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f9bd7c0a-1d77-4f66-9b3c-a6ae4d37a1a7": {"__data__": {"id_": "f9bd7c0a-1d77-4f66-9b3c-a6ae4d37a1a7", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ae0b6fe-524f-48a1-82bc-dc3bfe0bf78e", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "b34eb9eb5e6730faf6eb8d1dbab8ed37ad155924c47476a4645a699e6eb49b6d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7fde8df-6e12-4ab4-a76e-d8c7542e9e55", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "b84ceab524a71ecc85c5b6b45c1bd4cf77841d5dde6298d81f498609c3b48d22", "class_name": "RelatedNodeInfo"}}, "hash": "45a72627da8ebee98504a23a8ecc2b0cc3bf0dba2b0566d9e551382728915ddf", "text": "[](../images/533412_1_En_11_Chapter/533412_1_En_11_Figaf_HTML.gif)\n\n|\n\n**Workshop 2.3 N-grams Statistics with removal of unnecessary punctuations**\n\nThe results are average and unsatisfactory. It is noted that _and_ , _of the_\n, _s_ and _in the_ are top 4 bigrams occurred in the entire Moby Dick\nliterature. It is average since these bigrams are common English usage but\noriginal bigram statistics in simple sentences required to remove all\npunctuations by:\n\n1\\. List out all punctuations required to remove.\n\n2\\. Revise above Python script to remove these punctuation symbol from the\ntoken list.\n\n3\\. Generate a top 20 Bigram summary for _Moby Dick_ without punctuations.\n\n4\\. Use sample method to generate (cleaned) Bigram statistics from _Moby Dick,\nAdventures of_ _Sherlock Holmes_ _, Sense and Sensibility, Book of Genesis,\nInaugural Address Corpus, and Wall Street Journal_.\n\n5\\. Verify results and comments of any pattern(s) found.\n\n6\\. Try the same analysis for Trigram (N=3) and Quadrigram (N=4) to find any\npattern(s).  \n  \n---|---  \n  \n## 11.6 spaCy in NLP\n\n### 11.6.1 What Is spaCy?\n\nSpaCy (2022) is a free, open-source library for advanced NLP written in Python\nand Cython programming languages.\n\nThe library is published under MIT license developed by Dr. Matthew Honnibal\nand Dr. Ines Montani, founders of software company Explosion.\n\nSpaCy is designed specifically for production use and build NLP applications\nto process large volumes of text (Altinok 2021; Srinivasa-Desikan 2018;\nVasiliev 2020) different from NLTK focused on teaching and learning\nperspective.\n\nIt also provides workflow pipelines for machine learning and deep learning\ntools that can integrate with common platforms such as PyTorch, MXNet, and\nTensorFlow with its machine learning library called _Thinc_. spaCy provides\nrecurrent neural models such as convolution neural networks (CNN) by adopting\nThinc for NLP implementation such as Dependency Parsing (DP), NER (Named\nEntity Recognition), POS Tagging, and Text Classification and other advanced\nNLP applications such as Natural Language Understanding (NLU) systems,\nInformation Retrieval (IR), Information Extraction (IE) systems, and Question-\nand-Answer Chatbot systems.\n\nA spaCy system architecture is shown in Fig. 11.1, its major features support:\n\n  * NLP-based statistical models for over 19 commonly used languages,\n\n  * tokenization tools implementation for over 60 international languages,\n\n  * NLP pipeline components include NER, POS Tagging, DP, Text Classification, and Chatbot implementation,\n\n  * integration with common Python platforms such as TensorFlow, PyTorch and other high-level frameworks,\n\n  * integration with the latest Transformer and BERT technologies,\n\n  * user-friendly modular system packaging, evaluation, and deployment tools.\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Fig1_HTML.png)\n\nFig. 11.1\n\nSystem architecture of spaCy\n\n## 11.7 How to Install spaCy?\n\nSpaCy can be installed in MacOS/OSX, MS Windows, and Linux platforms (SpaCy\n2022) as per other Python-based development tools like NLTK.\n\nspaCy.io provides a one-stop-process for users to select own spaCy (1)\nlanguage(s) as trained pipelines, (2) optimal target in system efficiency vs\naccuracy for NLP applications development based on a large dataset and lexical\ndatabase, and (3) download appropriate APIs and modules to maximize efficiency\nunder CPU and GPU hardware configuration. Figure 11.2 shows a Windows-based\nPIP download environment using CUDA 11.3 GPU in English as trained pipelines\nand target for speed efficiency over accuracy.\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Fig2_HTML.png)\n\nFig. 11.2\n\nScreenshot of spaCy configuration selection\n\n## 11.8 Tokenization using spaCy\n\nTokenization is an operation in NLP. spaCy provides an easy-to-use scheme to\ntokenize any text document into sentences like NLTK and further tokenize\nsentences into words.\n\nThis section uses _Adventures_Holmes.txt_ as example to demonstrate\ntokenization in spaCy.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b7fde8df-6e12-4ab4-a76e-d8c7542e9e55": {"__data__": {"id_": "b7fde8df-6e12-4ab4-a76e-d8c7542e9e55", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9bd7c0a-1d77-4f66-9b3c-a6ae4d37a1a7", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "45a72627da8ebee98504a23a8ecc2b0cc3bf0dba2b0566d9e551382728915ddf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a65a646e-257f-4ca4-b433-2edb6a9c0e25", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "b01ed333aa52729d327fbd7d5e7ab7dd0717c1b9f88e0cea4be4044afbdc6602", "class_name": "RelatedNodeInfo"}}, "hash": "b84ceab524a71ecc85c5b6b45c1bd4cf77841d5dde6298d81f498609c3b48d22", "text": "Figure 11.2 shows a Windows-based\nPIP download environment using CUDA 11.3 GPU in English as trained pipelines\nand target for speed efficiency over accuracy.\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Fig2_HTML.png)\n\nFig. 11.2\n\nScreenshot of spaCy configuration selection\n\n## 11.8 Tokenization using spaCy\n\nTokenization is an operation in NLP. spaCy provides an easy-to-use scheme to\ntokenize any text document into sentences like NLTK and further tokenize\nsentences into words.\n\nThis section uses _Adventures_Holmes.txt_ as example to demonstrate\ntokenization in spaCy.\n\n### 11.8.1 Step 1: Import spaCy Module\n\nIn[18]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figag_HTML.gif)\n\n|\n\n**import** spacy  \n  \n---|---|---  \n  \n### 11.8.2 Step 2: Load spaCy Module \"en_core_web_sm\".\n\nUse en_core_web_md-3.2.0 package for English pipeline optimized for CPU in the\ncurrent platform with components including: tok2vec, tagger, parser, senter,\nner, attribute_ruler, lemmatizer.\n\nIn[19]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figah_HTML.gif)\n\n|\n\nnlp = spacy.load( \"en_core_web_sm\" )  \n  \n---|---|---  \n  \n### 11.8.3 Step 3: Open and Read Text File \"Adventures_Holmes.txt\" Into\nfile_handler \"fholmes\"\n\nNote: Since text file already exists, skip try-except module to save\nprogramming steps.\n\nIn[20]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figai_HTML.gif)\n\n|\n\nfholmes = open( \"Adventures_Holmes.txt\", \"r\", encoding=\"utf-8\")  \n  \n---|---|---  \n  \n### 11.8.4 Step 4: Read Adventures of Sherlock Holmes\n\nUse read() method to read whole text document as a complex string object\n\"holmes\".\n\nIn[21]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figaj_HTML.gif)\n\n|\n\nholmes = fholmes.read()\n\nholmes  \n  \n---|---|---  \n  \nOut[21]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figak_HTML.gif)  \n  \n### 11.8.5 Step 5: Replace All Newline Symbols\n\nReplace all newline characters \"\\n\" into space characters.\n\nIn[22]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figal_HTML.gif)\n\n|\n\nholmes = holmes.replace( \"\\n\", \" \" )  \n  \n---|---|---  \n  \n### 11.8.6 Step 6: Simple Counting\n\nReview total number of characters in _The Adventures of_ _Sherlock Holmes_ and\nexamine the result document.\n\nIn[23]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figam_HTML.gif)\n\n|\n\nlen (holmes)  \n  \n---|---|---  \n  \nOut[23]\n\n|\n\n580632  \n  \nIn[24]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figan_HTML.gif)\n\n|\n\nholmes  \n  \n---|---|---  \n  \nOut[24]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figao_HTML.gif)  \n  \n### 11.8.7 Step 7: Invoke nlp() Method in spaCy\n\nSpaCy nlp() method is an important Text Processing Pipeline to initialize nlp\nobject (English in our case) for NLP processing such as tokenization. It will\nconvert any text string object into a nlp object.\n\nStudy nlp() docstring to see how it works.\n\nIn[25]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figap_HTML.gif)\n\n|\n\nnlp **?**  \n  \n---|---|---  \n  \nIn[26]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a65a646e-257f-4ca4-b433-2edb6a9c0e25": {"__data__": {"id_": "a65a646e-257f-4ca4-b433-2edb6a9c0e25", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7fde8df-6e12-4ab4-a76e-d8c7542e9e55", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "b84ceab524a71ecc85c5b6b45c1bd4cf77841d5dde6298d81f498609c3b48d22", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8474ecaf-d19d-418f-bbb1-6c70fdf3e2ab", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "14d832e9298c7b2e68646301cc73ff4c185185e6214279ef479533e1cd7bb2d4", "class_name": "RelatedNodeInfo"}}, "hash": "b01ed333aa52729d327fbd7d5e7ab7dd0717c1b9f88e0cea4be4044afbdc6602", "text": "[](../images/533412_1_En_11_Chapter/533412_1_En_11_Figao_HTML.gif)  \n  \n### 11.8.7 Step 7: Invoke nlp() Method in spaCy\n\nSpaCy nlp() method is an important Text Processing Pipeline to initialize nlp\nobject (English in our case) for NLP processing such as tokenization. It will\nconvert any text string object into a nlp object.\n\nStudy nlp() docstring to see how it works.\n\nIn[25]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figap_HTML.gif)\n\n|\n\nnlp **?**  \n  \n---|---|---  \n  \nIn[26]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figaq_HTML.gif)\n\n|\n\nholmes_doc = nlp(holmes)  \n  \n---|---|---  \n  \nIn[27]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figar_HTML.gif)\n\n|\n\nholmes_doc  \n  \n---|---|---  \n  \nOut[27]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figas_HTML.gif)  \n  \n### 11.8.8 Step 8: Convert Text Document Into Sentence Object\n\nSpaCy is practical for text document tokenization to convert text document\nobject into (1) sentence objects and (2) tokens.\n\nThis example uses _for-in_ statement to convert the whole Sherlock Holmes\ndocument into holmes_sentences.\n\nIn[28]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figat_HTML.gif)\n\n|\n\nholmes_sentences = [sentence.text **for** sentence **in** holmes_doc.sents]\n\nholmes_sentences  \n  \n---|---|---  \n  \nOut[28]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figau_HTML.gif)  \n  \nExamine the structure of spaCy sentences and see what can be found.\n\nIn[29]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figav_HTML.gif)\n\n|\n\nholmes_sentences?  \n  \n---|---|---  \n  \nStudy the numbers of sentences contained in _The Adventures of_ _Sherlock\nHolmes_.\n\nIn[30]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figaw_HTML.gif)\n\n|\n\nlen (holmes_sentences)  \n  \n---|---|---  \n  \nOut[30]\n\n|\n\n6830  \n  \nList out sentence numbers 50th to 59th to review.\n\nIn[31]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figax_HTML.gif)\n\n|\n\nholmes_sentences[50:60]  \n  \n---|---|---  \n  \nOut[31]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figay_HTML.gif)  \n  \n### 11.8.9 Step 9: Directly Tokenize Text Document\n\nTokenize text document into word tokens by using \u201ctoken\u201d object in spaCy\ninstead of text document object extraction into sentence list object. Study\nhow it operates.\n\nIn[32]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figaz_HTML.gif)\n\n|\n\nholmes_words = [token.text **for** token **in** holmes_doc]\n\nholmes_words [130:180]  \n  \n---|---|---  \n  \nOut[32]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figba_HTML.gif)  \n  \nIn[33]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figbb_HTML.gif)\n\n|\n\nholmes_words **?**  \n  \n---|---|---  \n  \nIn[34]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figbc_HTML.gif)\n\n|\n\nlen (holmes_words)  \n  \n---|---|---  \n  \nOut[34]\n\n|\n\n133749  \n  \nIn[35]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8474ecaf-d19d-418f-bbb1-6c70fdf3e2ab": {"__data__": {"id_": "8474ecaf-d19d-418f-bbb1-6c70fdf3e2ab", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a65a646e-257f-4ca4-b433-2edb6a9c0e25", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "b01ed333aa52729d327fbd7d5e7ab7dd0717c1b9f88e0cea4be4044afbdc6602", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5cae8a8e-2461-41ed-9f4d-84ecb323a469", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "b172f252fa92771b9ae6e01ed31b36733575dd260abf7e4a8089864b4167f7d9", "class_name": "RelatedNodeInfo"}}, "hash": "14d832e9298c7b2e68646301cc73ff4c185185e6214279ef479533e1cd7bb2d4", "text": "[](../images/533412_1_En_11_Chapter/533412_1_En_11_Figba_HTML.gif)  \n  \nIn[33]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figbb_HTML.gif)\n\n|\n\nholmes_words **?**  \n  \n---|---|---  \n  \nIn[34]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figbc_HTML.gif)\n\n|\n\nlen (holmes_words)  \n  \n---|---|---  \n  \nOut[34]\n\n|\n\n133749  \n  \nIn[35]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figbd_HTML.gif)\n\n|\n\nnltk_homles_tokens = nltk.word_tokenize(holmes)  \n  \n---|---|---  \n  \nIn[36]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figbe_HTML.gif)\n\n|\n\nnltk_homles_tokens [104:153]  \n  \n---|---|---  \n  \nOut[36]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figbf_HTML.gif)  \n  \n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figbg_HTML.gif)\n\n|\n\nAccording to extracted tokens, they seem to be identical.\n\n1\\. Are they 100% identical?\n\n2\\. What is/are the difference(s)?\n\n3\\. Which one is better?  \n  \n---|---  \n  \n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figbh_HTML.gif)\n\n|\n\n**Workshop 2.4 SpaCy or** **NLTK** **: Which one is Faster?**\n\nIn many applications, especially in AI and NLP application, _speed_ (i.e.\nefficiency) is one of the most important consideration because:\n\n1\\. Many AI and NLP applications involve a huge data/database/databank for\nsystem training with huge population size, e.g. Lexical database of English\nand Chinese. So, whether an NLP engine/application is fast enough in every NLP\noperation such as tokenization, tagging and POS tagging, and parsing is an\nimportant factor.\n\n2\\. In many AI-based related NLP application such as Deep Learning for real-\ntime Information Extraction, it involves tedious network training and learning\nprocess, how efficient of every NLP operation is a critical process to decide\nwhether NLP application can be used in real-world scenario.\n\nThis workshop studies how efficient NLTK vs spaCy in terms of text document\nTokenization.\n\nTo achieve this, integrate Python codes of NTLK/spaCy document tokenization\nwith Timer object - time.\n\n1\\. Implement tokenization codes in NTLK and spaCy to time tokenization time\nby using time object, the following codes can be used as reference.\n\n2\\. Examine time taken for Tokenization process of \"Adventures_Holmes.txt\"\nusing NTLK vs spaCy methods.\n\n3\\. Which one is faster? or are they similar? Why?\n\n4\\. How about Document->Text efficiency? Compare NTLK vs spaCy on Doc->Text\nefficiency.\n\nHint: Like spaCy, NLTK can also implement Document->Text by two simple codes:\n\nnltk_tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n\nnltk_sentences = tokenizer.tokenize(holmes) # holmes is the text document\nstring object  \n  \n---|---  \n  \nIn[37]\n\n|\n\n![](../images/533412_1_En_11_Chapter/533412_1_En_11_Figbi_HTML.gif)\n\n|\n\n_# Sample code for Efficiency Performance of the NLP Engine_\n\n**import** nltk # or spacy\n\n**import** time\n\nstart = time.time()\n\n_#_\n\n_# YOUR NTLK or_ _spaCy_ _Tokenization_ _codes_\n\n_#_\n\nprint( \"Time taken: %s s\" % (time.time() - start))  \n  \n---|---|---  \n  \nOut[37]\n\n|\n\n0.0s  \n  \nReferences\n\n  1. Albrecht, J., Ramachandran, S. and Winkler, C. (2020) Blueprints for Text Analytics Using Python: Machine Learning-Based Solutions for Common Real World (NLP) Applications. O\u2019Reilly Media.\n\n  2. Altinok, D.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5cae8a8e-2461-41ed-9f4d-84ecb323a469": {"__data__": {"id_": "5cae8a8e-2461-41ed-9f4d-84ecb323a469", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8474ecaf-d19d-418f-bbb1-6c70fdf3e2ab", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "14d832e9298c7b2e68646301cc73ff4c185185e6214279ef479533e1cd7bb2d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "126abbbc-585d-4278-a6f3-61a1593773fb", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "601a1fb0e430d3c06123bc94f0eb4232aeca787e73b16eabef23442ab6913aae", "class_name": "RelatedNodeInfo"}}, "hash": "b172f252fa92771b9ae6e01ed31b36733575dd260abf7e4a8089864b4167f7d9", "text": "[](../images/533412_1_En_11_Chapter/533412_1_En_11_Figbi_HTML.gif)\n\n|\n\n_# Sample code for Efficiency Performance of the NLP Engine_\n\n**import** nltk # or spacy\n\n**import** time\n\nstart = time.time()\n\n_#_\n\n_# YOUR NTLK or_ _spaCy_ _Tokenization_ _codes_\n\n_#_\n\nprint( \"Time taken: %s s\" % (time.time() - start))  \n  \n---|---|---  \n  \nOut[37]\n\n|\n\n0.0s  \n  \nReferences\n\n  1. Albrecht, J., Ramachandran, S. and Winkler, C. (2020) Blueprints for Text Analytics Using Python: Machine Learning-Based Solutions for Common Real World (NLP) Applications. O\u2019Reilly Media.\n\n  2. Altinok, D. (2021) Mastering spaCy: An end-to-end practical guide to implementing NLP applications using the Python ecosystem. Packt Publishing.\n\n  3. Arumugam, R., & Shanmugamani, R. (2018). Hands-on natural language processing with python. Packt Publishing.\n\n  4. Bird, S., Klein, E., and Loper, E. (2009). Natural language processing with python. O'Reilly.\n\n  5. Doyle, A. C. (2019) The Adventures of Sherlock Holmes (AmazonClassics Edition). AmazonClassics.\n\n  6. Gutenberg (2022) Project Gutenberg official site. [https://\u200bwww.\u200bgutenberg.\u200borg/\u200b](https://www.gutenberg.org/) Accessed 16 June 2022.\n\n  7. Hardeniya, N., Perkins, J. and Chopra, D. (2016) Natural Language Processing: Python and NLTK. Packt Publishing.\n\n  8. Melville, H. (2006) Moby Dick. Hard Press.\n\n  9. Kedia, A. and Rasu, M. (2020) Hands-On Python Natural Language Processing: Explore tools and techniques to analyze and process text with a view to building real-world NLP applications. Packt Publishing.\n\n  10. NLTK (2022) NLTK official site. [https://\u200bwww.\u200bnltk.\u200borg/\u200b](https://www.nltk.org/). Accessed 16 June 2022.\n\n  11. Perkins, J. (2014). Python 3 text processing with NLTK 3 cookbook. Packt Publishing Ltd.\n\n  12. SpaCy (2022) spaCy official site. [https://\u200bspacy.\u200bio/\u200b](https://spacy.io/). Accessed 16 June 2022.\n\n  13. Sidorov, G. (2019) Syntactic n-grams in Computational Linguistics (SpringerBriefs in Computer Science). Springer.\n\n  14. Srinivasa-Desikan, B. (2018). Natural language processing and computational linguistics: A practical guide to text analysis with python, gensim, SpaCy, and keras. Packt Publishing, Limited.\n\n  15. Vasiliev, Y. (2020) Natural Language Processing with Python and spaCy: A Practical Introduction. No Starch Press.\n\n\n\u00a9 The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.\n2024\n\nR. S. T. Lee Natural Language Processing\n<https://doi.org/10.1007/978-981-99-1999-4_12>\n\n# 12\\. Workshop#3 POS Tagging Using NLTK (Hour 5\u20136)\n\nRaymond S. T. Lee 1\n\n(1)\n\nUnited International College, Beijing Normal University-Hong Kong Baptist\nUniversity, Zhuhai, China\n\n## 12.1 Introduction\n\nIn Chap. [3](533412_1_En_3_Chapter.xhtml), we studied basic concepts and\ntheories related to Part-of-Speech (POS) and various POS tagging techniques.\nThis workshop will explore how to implement POS tagging by using NLTK starting\nfrom a simple recap on tokenization techniques and two fundamental processes\nin word-level progressing: stemming and stop-word removal, which will\nintroduce two types of stemming techniques: Porter Stemmer and Snowball\nStemmer that can be integrated with WordCloud commonly used in data\nvisualization followed by the main theme of this workshop with the\nintroduction of PENN Treebank Tagset and to create your own POS tagger.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "126abbbc-585d-4278-a6f3-61a1593773fb": {"__data__": {"id_": "126abbbc-585d-4278-a6f3-61a1593773fb", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5cae8a8e-2461-41ed-9f4d-84ecb323a469", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "b172f252fa92771b9ae6e01ed31b36733575dd260abf7e4a8089864b4167f7d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d22509a-fcdd-4cab-b86d-a30672628857", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "23f5924cf4508b7478cd99a442e99f74cff352dd022131a4f5709fffb5a569f8", "class_name": "RelatedNodeInfo"}}, "hash": "601a1fb0e430d3c06123bc94f0eb4232aeca787e73b16eabef23442ab6913aae", "text": "[3](533412_1_En_3_Chapter.xhtml), we studied basic concepts and\ntheories related to Part-of-Speech (POS) and various POS tagging techniques.\nThis workshop will explore how to implement POS tagging by using NLTK starting\nfrom a simple recap on tokenization techniques and two fundamental processes\nin word-level progressing: stemming and stop-word removal, which will\nintroduce two types of stemming techniques: Porter Stemmer and Snowball\nStemmer that can be integrated with WordCloud commonly used in data\nvisualization followed by the main theme of this workshop with the\nintroduction of PENN Treebank Tagset and to create your own POS tagger.\n\n## 12.2 A Revisit on Tokenization with NLTK\n\nText sentences are divided into subunits first and map into vectors in most\nNLP tasks. These vectors are fed into a model to encode where output is sent\nto a downstream task for results. NLTK (NLTK 2022) provides methods to divide\ntext into subunits as tokenizers. Twitter sample corpus is extracted from NLTK\nto perform tokenization (Hardeniya et al. 2016; Kedia and Rasu 2020; Perkins\n2014) in procedures below (Albrecht et al. 2020; Antic 2021; Bird et al.\n2009):\n\n  1. 1.\n\nImport NLTK package.\n\n  2. 2.\n\nImport Twitter sample data.\n\n  3. 3.\n\nList out fields.\n\n  4. 4.\n\nGet Twitter string list.\n\n  5. 5.\n\nList out first 15 Twitters.\n\n  6. 6.\n\nTokenize twitter.\n\nLet us start with the import of NLTK package and download Twitter samples\nprovided by NLTK platform.\n\nIn[1]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figa_HTML.gif)\n\n|\n\n_# Import_ _NLTK_\n\n**import** nltk\n\n_# Download twitter_samples_\n\n_# nltk.download('twitter_samples')_  \n  \n---|---|---  \n  \nImport twitter samples dataset as _twtr_ and check file id using _fileids()_\nmethod:\n\nIn[2]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figb_HTML.gif)\n\n|\n\n_# Import twitter samples from NTLK corpus (twtr)_\n\n**from** nltk.corpus **import** twitter_samples **as** twtr  \n  \n---|---|---  \n  \nIn[3]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figc_HTML.gif)\n\n|\n\n_# Display Field IDs_\n\ntwtr.fileids()  \n  \n---|---|---  \n  \nOut[3]\n\n|\n\n['negative_tweets.json','positive_tweets.json', 'tweets.20150430-23406.json']  \n  \nReview first 5 Twitter messages:\n\nIn[4]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figd_HTML.gif)\n\n|\n\n_# Assign sample twitters (stwtr)_\n\nstwtr = twtr.strings('tweets.20150430-223406.json')  \n  \n---|---|---  \n  \nIn[5]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Fige_HTML.gif)\n\n|\n\n_# Display the first 5 sample twitters_\n\nstwtr[:5]  \n  \n---|---|---  \n  \nOut[5]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figf_HTML.gif)  \n  \nImport word_tokenize method from NLTK, name as _w_tok_ to perform tokenization\non fifth Twitter message:\n\nIn[6]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figg_HTML.gif)\n\n|\n\n_# Import_ _NLTK_ _word tokenizer_\n\nfrom nltk.tokenize import word_tokenize as w_tok  \n  \n---|---|---  \n  \nIn[7]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figh_HTML.gif)\n\n|\n\n_# tokenize stwtr[4]_\n\nw_tok(stwtr[4])  \n  \n---|---|---  \n  \nOut[7]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figi_HTML.gif)  \n  \n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d22509a-fcdd-4cab-b86d-a30672628857": {"__data__": {"id_": "9d22509a-fcdd-4cab-b86d-a30672628857", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "126abbbc-585d-4278-a6f3-61a1593773fb", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "601a1fb0e430d3c06123bc94f0eb4232aeca787e73b16eabef23442ab6913aae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a18b872d-4a23-4fa1-8218-7953940e6511", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9f856258187186526efc583f83b8360c516fdc17a3865b3264bcd8f4af310422", "class_name": "RelatedNodeInfo"}}, "hash": "23f5924cf4508b7478cd99a442e99f74cff352dd022131a4f5709fffb5a569f8", "text": "[](../images/533412_1_En_12_Chapter/533412_1_En_12_Figg_HTML.gif)\n\n|\n\n_# Import_ _NLTK_ _word tokenizer_\n\nfrom nltk.tokenize import word_tokenize as w_tok  \n  \n---|---|---  \n  \nIn[7]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figh_HTML.gif)\n\n|\n\n_# tokenize stwtr[4]_\n\nw_tok(stwtr[4])  \n  \n---|---|---  \n  \nOut[7]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figi_HTML.gif)  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figj_HTML.gif)\n\n|\n\nNLTK offers tokenization for punctuation and spaces _wordpunct_tokenize_. Let\nus use the fifth Twitter message to see how it works  \n  \n---|---  \n  \nIn[8]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figk_HTML.gif)\n\n|\n\n**from** nltk.tokenize **import** wordpunct_tokenize **as** wp_tok\n\nwp_tok(stwtr[4])  \n  \n---|---|---  \n  \nOut[8]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figl_HTML.gif)  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figm_HTML.gif)\n\n|\n\nIt can also tokenize words between hyphens and other punctuations. Further,\nNLTK\u2019s regular expression (RegEx) tokenizer can build custom tokenizers:  \n  \n---|---  \n  \nIn[9]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Fign_HTML.gif)\n\n|\n\n_# Import the RegEx tokenizer_\n\nfrom nltk import regexp_tokenize as rx_tok\n\nrx_pattern1 = '\\w+'\n\nrx_tok(stwtr[4],rx_pattern1)  \n  \n---|---|---  \n  \nOut[9]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figo_HTML.gif)  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figp_HTML.gif)\n\n|\n\nA simple regular expression filtered out words with alphanumeric characters\nonly, but not punctuations in previous code. Another regular expression can\ndetect and filter out both words containing alphanumeric characters and\npunctuation marks in the following code:  \n  \n---|---  \n  \nIn[10]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figq_HTML.gif)\n\n|\n\n_# Create Rx pattern2 and perform the RX tokenize again_\n\nrx_pattern2 = '\\w+|[!,\\\\-,]'\n\nrx_tok(stwtr[4],rx_pattern2)  \n  \n---|---|---  \n  \nOut[10]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figr_HTML.gif)  \n  \n## 12.3 Stemming Using NLTK\n\nAfter tokenization has sentences divided into words, stemming is a procedure\nto unify words and extract the root, base form of each word, e.g. stemming of\nword _compute_ is shown in Fig. 12.1.\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Fig1_HTML.png)\n\nFig. 12.1\n\nStemming of compute\n\n### 12.3.1 What Is Stemming?\n\nStemming usually removes prefixes or suffixes such as _-er_ , - _ion_ , -\n_ization_ from words to extract the base or root form of a word, e.g.\ncomputers, computation, and computerization. Although these words spell\ndifferently but shared identical concept related to compute, so compute is the\nstem of these words.\n\n### 12.3.2 Why Stemming?\n\nThere is needless to extract every single word in a document but only concept\nor notion they represent such as information extraction and topic\nsummarization in NLP applications. It can save computational capacity and\npreserve overall meaning of the passage. Stemming technique is to extract the\noverall meaning or words\u2019 base form instead of distinct words.\n\nLet us look at how to perform stemming on text data.\n\n### 12.3.3 How to Perform Stemming?", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a18b872d-4a23-4fa1-8218-7953940e6511": {"__data__": {"id_": "a18b872d-4a23-4fa1-8218-7953940e6511", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d22509a-fcdd-4cab-b86d-a30672628857", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "23f5924cf4508b7478cd99a442e99f74cff352dd022131a4f5709fffb5a569f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e691f74a-8eff-4238-91f5-2bffa27b16b4", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f002ceece771f446eb8bc1c515d08c59f4b17e7a6b44fcd5d03657025cdbb122", "class_name": "RelatedNodeInfo"}}, "hash": "9f856258187186526efc583f83b8360c516fdc17a3865b3264bcd8f4af310422", "text": "Stemming usually removes prefixes or suffixes such as _-er_ , - _ion_ , -\n_ization_ from words to extract the base or root form of a word, e.g.\ncomputers, computation, and computerization. Although these words spell\ndifferently but shared identical concept related to compute, so compute is the\nstem of these words.\n\n### 12.3.2 Why Stemming?\n\nThere is needless to extract every single word in a document but only concept\nor notion they represent such as information extraction and topic\nsummarization in NLP applications. It can save computational capacity and\npreserve overall meaning of the passage. Stemming technique is to extract the\noverall meaning or words\u2019 base form instead of distinct words.\n\nLet us look at how to perform stemming on text data.\n\n### 12.3.3 How to Perform Stemming?\n\nNLTK provides practical solution to implement stemming without sophisticated\nprogramming. Let us try two commonly used methods: (1) Porter Stemmer and (2)\nSnowball Stemmer in NLP.\n\n### 12.3.4 Porter Stemmer\n\n_Porter Stemmer_ is the earliest stemming technique used in 1980s. Its key\nprocedure is to remove words common endings and parse into generic forms. This\nmethod is simple and used in many NLP applications effectively.\n\nImport Porter Stemmer from NLTK library:\n\nIn[11]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figs_HTML.gif)\n\n|\n\n_# Import PorterStemmer as p_stem_\n\n**from** nltk.stem.porter **import** PorterStemmer **as** p_stem  \n  \n---|---|---  \n  \nTry to stem words like _computer_.\n\nIn[12]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figt_HTML.gif)\n\n|\n\np_stem().stem(\"computer\")  \n  \n---|---|---  \n  \nOut[12]\n\n|\n\n'comput'  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figu_HTML.gif)\n\n|\n\n_PorterStemmer_ simply removes suffix _-er_ when processing _computer_ to\nacquire _compute_ which is incorrect. Hence this stemmer is basic  \n  \n---|---  \n  \nNext, try to stem _dogs_ to see what happens.\n\nIn[13]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figv_HTML.gif)\n\n|\n\np_stem().stem(\"dogs\")  \n  \n---|---|---  \n  \nOut[13]\n\n|\n\n'dog'  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figw_HTML.gif)\n\n|\n\nFor the above code, _dogs_ are converted from plural to singular, remove\nsuffix _-s_ and convert to _dog_  \n  \n---|---  \n  \nLet us try more, say _traditional_.\n\nIn[14]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figx_HTML.gif)\n\n|\n\np_stem().stem(\"traditional\")  \n  \n---|---|---  \n  \nOut[14]\n\n|\n\n'tradit'  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figy_HTML.gif)\n\n|\n\nStemmer may output an invalid word when dealing with special words, e.g.\ntradit is acquired if suffix _-ional_ is removed. _tradit_ is not a word in\nEnglish, it is a root form.  \n  \n---|---  \n  \nLet us work on words in plural form. There are 26 words extracted from a to z\nin plural form to perform PorterStemming:\n\nIn[15]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figz_HTML.gif)\n\n|\n\n_# Define some plural words_\n\nw_plu =\n['apes','bags','computers','dogs','egos','frescoes','generous','hats','igloos','jungles',\n'kites','learners','mice','natives','openings','photos','queries','rats','scenes',\n'trees','utensils','veins','wells','xylophones','yoyos','zens']  \n  \n---|---|---  \n  \nIn[16]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e691f74a-8eff-4238-91f5-2bffa27b16b4": {"__data__": {"id_": "e691f74a-8eff-4238-91f5-2bffa27b16b4", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a18b872d-4a23-4fa1-8218-7953940e6511", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9f856258187186526efc583f83b8360c516fdc17a3865b3264bcd8f4af310422", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ecc61760-3573-46d9-a2c9-34ddd81a791b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "60e1bb369a02472f784b0278083f1b557142a08a6af60c9fc9a7dd3130d00bd5", "class_name": "RelatedNodeInfo"}}, "hash": "f002ceece771f446eb8bc1c515d08c59f4b17e7a6b44fcd5d03657025cdbb122", "text": "_tradit_ is not a word in\nEnglish, it is a root form.  \n  \n---|---  \n  \nLet us work on words in plural form. There are 26 words extracted from a to z\nin plural form to perform PorterStemming:\n\nIn[15]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figz_HTML.gif)\n\n|\n\n_# Define some plural words_\n\nw_plu =\n['apes','bags','computers','dogs','egos','frescoes','generous','hats','igloos','jungles',\n'kites','learners','mice','natives','openings','photos','queries','rats','scenes',\n'trees','utensils','veins','wells','xylophones','yoyos','zens']  \n  \n---|---|---  \n  \nIn[16]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figaa_HTML.gif)\n\n|\n\n**from** nltk.stem.porter **import** PorterStemmer **as** p_stem\n\nw_sgl = [p_stem().stem(wplu) **for** wplu **in** w_plu]\n\nprint(' '.join(w_sgl))  \n  \n---|---|---  \n  \nOut[16]\n\n|\n\nape bag comput dog ego fresco gener hat igloo jungl kite learner mice nativ\nopen photo queri rat scene tree utensil vein well xylophon yoyo zen  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figab_HTML.gif)\n\n|\n\nPorter Stemming will remove suffixes _-s_ or _-es_ to extract root form, that\nmay result in single form such as _apes_ , _bags_ , _dogs_ , etc. But in some\ncases, it will generate non-English words such as _gener_ , _jungl,_ and\n_queri_  \n  \n---|---  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figac_HTML.gif)\n\n|\n\n**Workshop 3.1 Try to stem a paragraph from The Adventures of Sherlock\nHolmes**\n\n1\\. Read Adventures_Holmes.txt _text file from The Adventures of Sherlock\nHolmes_ (Doyle 2019; Gutenberg 2022)\n\n2\\. Save contents into a string object \"holmes_doc\"\n\n3\\. Extract a paragraph and tokenize it\n\n4\\. Use Porter Stemming and output a list of stemmed words.  \n  \n---|---  \n  \n### 12.3.5 Snowball Stemmer\n\n_Snowball Stemmer_ provides improvement in stemming results as compared with\nPorter Stemmer and provides multi-language stemming solution. One can check\nlanguages using languages() method. Import from NLTK package to invoke\nSnowball Stemmer:\n\nIn[17]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figad_HTML.gif)\n\n|\n\n_# Import_ _Snowball Stemmer_ _as s_stem_\n\n**from** nltk.stem.snowball **import** SnowballStemmer **as** s_stem  \n  \n---|---|---  \n  \nReview what languages Snowball stemmer can support:\n\nIn[18]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figae_HTML.gif)\n\n|\n\n_# Display the s_stem language set_\n\nprint(s_stem.languages)  \n  \n---|---|---  \n  \nOut[18]\n\n|\n\n('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german',\n'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian',\n'russian', 'spanish', 'swedish')  \n  \nSnowball Stemmer provides a variety of solutions in commonly used languages\nfrom Arabic to Swedish.\n\nIn[19]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figaf_HTML.gif)\n\n|\n\n_# Import_ _Snowball Stemmer_ _as s_stem and assign to English language_\n\n**from** nltk.stem.snowball **import** SnowballStemmer **as** s_stem\n\ns_stem_ENG = s_stem(language=\"english\")  \n  \n---|---|---  \n  \nUse same list of plural words (w_plu) to check how it works in Snowball\nStemmer for comparison:\n\nIn[20]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ecc61760-3573-46d9-a2c9-34ddd81a791b": {"__data__": {"id_": "ecc61760-3573-46d9-a2c9-34ddd81a791b", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e691f74a-8eff-4238-91f5-2bffa27b16b4", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f002ceece771f446eb8bc1c515d08c59f4b17e7a6b44fcd5d03657025cdbb122", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea07a436-ffb3-4aad-be18-c36ad5aa6358", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "13be5150e310e9ca1fd770fe68189805e0bfb41e4f4c2b60c07fe65f37429032", "class_name": "RelatedNodeInfo"}}, "hash": "60e1bb369a02472f784b0278083f1b557142a08a6af60c9fc9a7dd3130d00bd5", "text": "In[19]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figaf_HTML.gif)\n\n|\n\n_# Import_ _Snowball Stemmer_ _as s_stem and assign to English language_\n\n**from** nltk.stem.snowball **import** SnowballStemmer **as** s_stem\n\ns_stem_ENG = s_stem(language=\"english\")  \n  \n---|---|---  \n  \nUse same list of plural words (w_plu) to check how it works in Snowball\nStemmer for comparison:\n\nIn[20]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figag_HTML.gif)\n\n|\n\n_# Display the list of plural words_\n\nw_plu  \n  \n---|---|---  \n  \nOut[20]\n\n|\n\n['apes', 'bags', 'computers', 'dogs', 'egos', 'frescoes', 'generous', 'hats',\n\n'igloos', 'jungles', 'kites', 'learners', 'mice', 'natives', 'openings',\n'photos',\n\n'queries', 'rats', 'scenes', 'trees', 'utensils', 'veins', 'wells',\n'xylophones',\n\n'yoyos', 'zens']  \n  \nIn[21]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figah_HTML.gif)\n\n|\n\n_# Apply_ _Snowball Stemmer_ _onto the plural words_\n\nsgls = [s_stem_ENG.stem(wplu) for wplu in w_plu]\n\nprint(' '.join(sgls))  \n  \n---|---|---  \n  \nOut[21]\n\n|\n\nape bag comput dog ego fresco generous hat igloo jungl kite learner mice nativ\nopen photo queri rat scene tree utensil vein well xylophon yoyo zen  \n  \nTry to compare with previous stemmer. What are the differences?\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figai_HTML.gif)\n\n|\n\n1\\. Snowball Stemmer achieved similar results as Porter Stemmer in most cases\nexcept in _generously_ where Snowball Stemmer came up with a meaningful root\nform _generous_ instead of _gener_ in Porter Stemmer\n\n2\\. Try some plural words to compare performance between Porter Stemmer vs\nSnowball Stemmer  \n  \n---|---  \n  \n## 12.4 Stop-Words Removal with NLTK\n\n### 12.4.1 What Are Stop-Words?\n\nThere are input words and utterances to filter out impractical stop-words in\nNLP preprocessing such as: _a_ , _is_ , _the_ , _of_ , etc.\n\nNLTK already provides a built-in stop-words package for this function. Let us\nsee how it works.\n\n### 12.4.2 NLTK Stop-Words List\n\nImport stopwords module and call stopwords.words() method to list out all\nstop-words in English.\n\nIn[22]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figaj_HTML.gif)\n\n|\n\n_# Import_ _NLTK_ _stop-words as wstops_\n\nfrom nltk.corpus import stopwords as wstops\n\nprint(wstops.words('english'))  \n  \n---|---|---  \n  \nOut[22]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figak_HTML.gif)  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figal_HTML.gif)\n\n|\n\n1\\. Stop-words corpus size is not large\n\n2\\. All stop-words are commonly used in many documents. They effect storage\nand system efficiency in NLP applications if they are not removed\n\n3\\. This stop-word corpus is incomplete and subjective. There may be words\nconsidered as stop-words not included in this databank  \n  \n---|---  \n  \nUse _stopwords.fileids()_ function to review how many languages library of\nstop-words NLTK contains.\n\nIn[23]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figam_HTML.gif)\n\n|\n\n_# Import_ _NLTK_ _stop-words as wstops and display the FILE_IDs_\n\n**from** nltk.corpus **import** stopwords **as** wstops\n\nprint(wstops.fileids())  \n  \n---|---|---  \n  \nOut[23]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea07a436-ffb3-4aad-be18-c36ad5aa6358": {"__data__": {"id_": "ea07a436-ffb3-4aad-be18-c36ad5aa6358", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ecc61760-3573-46d9-a2c9-34ddd81a791b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "60e1bb369a02472f784b0278083f1b557142a08a6af60c9fc9a7dd3130d00bd5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d26df7cd-905e-4ad8-8cf5-44c2b04b7aa3", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "4dd7356a4dcfbb7674d7454240604eed31cd4c402ad7ce246a3d3e1d4ead5664", "class_name": "RelatedNodeInfo"}}, "hash": "13be5150e310e9ca1fd770fe68189805e0bfb41e4f4c2b60c07fe65f37429032", "text": "Stop-words corpus size is not large\n\n2\\. All stop-words are commonly used in many documents. They effect storage\nand system efficiency in NLP applications if they are not removed\n\n3\\. This stop-word corpus is incomplete and subjective. There may be words\nconsidered as stop-words not included in this databank  \n  \n---|---  \n  \nUse _stopwords.fileids()_ function to review how many languages library of\nstop-words NLTK contains.\n\nIn[23]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figam_HTML.gif)\n\n|\n\n_# Import_ _NLTK_ _stop-words as wstops and display the FILE_IDs_\n\n**from** nltk.corpus **import** stopwords **as** wstops\n\nprint(wstops.fileids())  \n  \n---|---|---  \n  \nOut[23]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figan_HTML.gif)  \n  \n### 12.4.3 Try Some Texts\n\nThe above list shows all stop-words. Let us use a simple utterance:\n\nIn[24]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figao_HTML.gif)\n\n|\n\n_# Import_ _NLTK_ _stop-words as wstops_\n\n**from** nltk.corpus **import** stopwords **as** wstops\n\nwstops_ENG = wstops.words('english')\n\nutterance = \"Try to test for the stop word remove function to see how it\nworks.\"\n\nutterance_clean =[w for w in utterance.split()\n\nif w not in wstops_ENG]  \n  \n---|---|---  \n  \nReview results:\n\nIn[25]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figap_HTML.gif)\n\n|\n\n# Display the cleaned utterance\n\nutterance_clean  \n  \n---|---|---  \n  \nOut[25]\n\n|\n\n['We', 'look', 'words', 'removed', 'text', 'following', 'code']  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figaq_HTML.gif)\n\n|\n\n1\\. All commonly used stop-words such as _to_ , _for_ , _the_ , _it,_ are\nremoved as shown in the example\n\n2\\. It has little effect to overall meaning of the utterance\n\n3\\. It requires same computational time and effort  \n  \n---|---  \n  \nThe following example uses _Hamlet_ from _The Complete Works of Shakespeare_\nto demonstrate how stop-words are removed from text processing in NLP.\n\nIn[26]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figar_HTML.gif)\n\n|\n\n_# Import the Gutenberg library from_ _NLTK_\n\n**from** nltk.corpus **import** gutenberg **as** gub\n\nhamlet = gub.words('shakespeare-hamlet.txt')\n\nhamlet_clean = [w **for** w **in** hamlet **if** w **not in** wstops_ENG]  \n  \n---|---|---  \n  \nIn[27]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figas_HTML.gif)\n\n|\n\nlen(hamlet_clean)*100.0/len(hamlet)  \n  \n---|---|---  \n  \nOut[27]\n\n|\n\n69.26124197002142  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figat_HTML.gif)\n\n|\n\nThis classic literature contains deactivated words. Nevertheless, these stop-\nwords are unmeaningful in many NLP tasks that may affect results, so most of\nthem are removed during preprocessing  \n  \n---|---  \n  \n### 12.4.4 Create Your Own Stop-Words\n\nStop-word corpus can extract a list of string that can add any stop-words with\nsimple _append()_ function, but it is advisable to create a new stop-word\nlibrary object name to begin.\n\n#### 12.4.4.1 Step 1: Create Own Stop-Word Library List\n\nIn[28]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d26df7cd-905e-4ad8-8cf5-44c2b04b7aa3": {"__data__": {"id_": "d26df7cd-905e-4ad8-8cf5-44c2b04b7aa3", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea07a436-ffb3-4aad-be18-c36ad5aa6358", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "13be5150e310e9ca1fd770fe68189805e0bfb41e4f4c2b60c07fe65f37429032", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47cd5ba4-2c63-4b7f-8ae2-4ba862ec3bba", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f6e060f4277960d7eb2df012f5030282805448ff71a73afc3ec090e4fed99988", "class_name": "RelatedNodeInfo"}}, "hash": "4dd7356a4dcfbb7674d7454240604eed31cd4c402ad7ce246a3d3e1d4ead5664", "text": "[](../images/533412_1_En_12_Chapter/533412_1_En_12_Figat_HTML.gif)\n\n|\n\nThis classic literature contains deactivated words. Nevertheless, these stop-\nwords are unmeaningful in many NLP tasks that may affect results, so most of\nthem are removed during preprocessing  \n  \n---|---  \n  \n### 12.4.4 Create Your Own Stop-Words\n\nStop-word corpus can extract a list of string that can add any stop-words with\nsimple _append()_ function, but it is advisable to create a new stop-word\nlibrary object name to begin.\n\n#### 12.4.4.1 Step 1: Create Own Stop-Word Library List\n\nIn[28]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figau_HTML.gif)\n\n|\n\nMy_sws = wstops.words('english')  \n  \n---|---|---  \n  \n#### 12.4.4.2 Step 2: Check Object Type and Will See It Has a Simple List\n\nIn[29]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figav_HTML.gif)\n\n|\n\nMy_sws **?**  \n  \n---|---|---  \n  \n#### 12.4.4.3 Step 3: Study Stop-Word List\n\nIn[30]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figaw_HTML.gif)\n\n|\n\nMy_sws  \n  \n---|---|---  \n  \nOut[30]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figax_HTML.gif)  \n  \n#### 12.4.4.4 Step 4: Add New Stop-Word \"sampleSW\" Using Append()\n\nIn[31]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figay_HTML.gif)\n\n|\n\nMy_sws.append(\u2018sampleSW\u2019)\n\nMy_sws[160:]  \n  \n---|---|---  \n  \nOut[31]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figaz_HTML.gif)  \n  \nTry this to see how it works.\n\nIn[32]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figba_HTML.gif)\n\n|\n\n_# Import word_tokenize as w_tok_\n\n**from** nltk.tokenize **import** word_tokenize **as** w_tok\n\n_# Create the sample utterance_\n\nutterance = \"This is a sample utterance which consits of eg as stop word\nsampleSW.\"\n\n_# Tokenize the utterance_\n\nutt_toks = w_tok(utterance)\n\n_# Stop word removal_\n\nutt_nosw = [w **for** w **in** utt_toks **if no** t w **in** My_sws]\n\n_# Display utterance without My stopwords_\n\nprint(utt_nosw)  \n  \n---|---|---  \n  \nOut[32]\n\n|\n\n['This', 'sample', 'utterance', 'consits', 'eg', 'stop', 'word', '.']  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbb_HTML.gif)\n\n|\n\n**Workshop 3.2 Stop-word Filtering on The Adventures of Sherlock Holmes**\n\nUse stop-word filtering technique for _The Adventures of Sherlock Holmes_ :\n\n1\\. Read Adventures_Holmes.txt text file\n\n2\\. Save contents into a string object \"holmes_doc\"\n\n3\\. Use stop-word technique just learnt to tokenize holmes_doc\n\n4\\. Generate a list of word tokens with stop-words removed\n\n5\\. Check any 3 possible stop-words to add into own stop-word list\n\n6\\. Regenerate a new token list with additional stop-word removed  \n  \n---|---  \n  \n## 12.5 Text Analysis with NLTK\n\nWhen text data has been processed and tokenized, basic analysis are required\nto calculate words or tokens, their distribution and usage frequency in NLP\ntasks. This allows understanding of main contents and topics accuracy in the\ndocument. Import a sample webtext (Firefox.txt) from NLTK library.\n\nIn[33]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47cd5ba4-2c63-4b7f-8ae2-4ba862ec3bba": {"__data__": {"id_": "47cd5ba4-2c63-4b7f-8ae2-4ba862ec3bba", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d26df7cd-905e-4ad8-8cf5-44c2b04b7aa3", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "4dd7356a4dcfbb7674d7454240604eed31cd4c402ad7ce246a3d3e1d4ead5664", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52724a04-e86c-42bf-89f2-6a0328353ef8", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "257ea0d621efecd9070b68effb6b901b74f835a20e42ca39826fc5c8f26ccc08", "class_name": "RelatedNodeInfo"}}, "hash": "f6e060f4277960d7eb2df012f5030282805448ff71a73afc3ec090e4fed99988", "text": "Read Adventures_Holmes.txt text file\n\n2\\. Save contents into a string object \"holmes_doc\"\n\n3\\. Use stop-word technique just learnt to tokenize holmes_doc\n\n4\\. Generate a list of word tokens with stop-words removed\n\n5\\. Check any 3 possible stop-words to add into own stop-word list\n\n6\\. Regenerate a new token list with additional stop-word removed  \n  \n---|---  \n  \n## 12.5 Text Analysis with NLTK\n\nWhen text data has been processed and tokenized, basic analysis are required\nto calculate words or tokens, their distribution and usage frequency in NLP\ntasks. This allows understanding of main contents and topics accuracy in the\ndocument. Import a sample webtext (Firefox.txt) from NLTK library.\n\nIn[33]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbc_HTML.gif)\n\n|\n\n_# Import webtext as wbtxt_\n\n**from** nltk.corpus **import** webtext **as** wbtxt\n\n_# Create sample webtext_\n\nwbtxt_s = wbtxt.sents('firefox.txt')\n\nwbtxt_w = wbtxt.words('firefox.txt')\n\n_# Display total nos of webtext sentences in firefox.txt_\n\nlen(wbtxt_s)  \n  \n---|---|---  \n  \nOut[33]\n\n|\n\n1138  \n  \nReview the number of words as well.\n\nIn[34]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbd_HTML.gif)\n\n|\n\n_# Display total nos of webtext words in firefox.txt_\n\nlen(wbtxt_w)  \n  \n---|---|---  \n  \nOut[34]\n\n|\n\n102,457  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbe_HTML.gif)\n\n|\n\nFireFox.txt contains sample texts extracted from Firefox discussion forum to\nserve as useful dataset for basic text-level analysis in NLP  \n  \n---|---  \n  \nIt can also obtain vocabulary size by passing through a set as shown in the\nfollowing code:\n\nIn[35]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbf_HTML.gif)\n\n|\n\n_# Define vocabulary object (vocab)_\n\nvocab = set(wbtxt_w)\n\n_# Display the size of Vocab_\n\nlen(vocab)  \n  \n---|---|---  \n  \nOut[35]\n\n|\n\n8296  \n  \n_nltk.FreqDist()_ function is used to generate words frequency distribution\noccurred in the whole text as shown:\n\nIn[36]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbg_HTML.gif)\n\n|\n\n_# Define Frequency Distribution object_\n\nfdist = nltk.FreqDist(wbtxt_w)  \n  \n---|---|---  \n  \nIn[37]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbh_HTML.gif)\n\n|\n\nsorted(fdist, key=fdist.__getitem__,reverse= **True** )[0:30]  \n  \n---|---|---  \n  \nOut[37]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbi_HTML.gif)  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbj_HTML.gif)\n\n|\n\nThe above code generates top 30 frequently used words and punctuations in the\nwhole text. _in_ , _to_ and _the_ are top 3 on the list like other literatures\nas Firefox.txt text is the collection of users\u2019 discussion messages and\ncontents about Firefox browser like conversations  \n  \n---|---  \n  \nTo exclude stop-words such as _the,_ and _not_ , use the following code to see\nf words frequency distribution longer than 3.\n\nIn[38]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbk_HTML.gif)\n\n|\n\n_# Import Matplotlib pyplot object_\n\n**import** matplotlib.pyplot **as** pyplt\n\npyplt.figure(figsize=(20, 8))\n\nlwords = dict([(k,v) **for** k,v **in** fdist.items() **if** len(k)>3])\n\nfdist = nltk.FreqDist(lwords)\n\nfdist.plot(50,cumulative= **False** )  \n  \n---|---|---  \n  \nOut[38]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52724a04-e86c-42bf-89f2-6a0328353ef8": {"__data__": {"id_": "52724a04-e86c-42bf-89f2-6a0328353ef8", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47cd5ba4-2c63-4b7f-8ae2-4ba862ec3bba", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f6e060f4277960d7eb2df012f5030282805448ff71a73afc3ec090e4fed99988", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6f406bf-c6b3-474a-b5c6-4771d39de426", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "bc5657bda93f21f9c92b9720741fcfeb77940b3701edb727d1e239c9882554bf", "class_name": "RelatedNodeInfo"}}, "hash": "257ea0d621efecd9070b68effb6b901b74f835a20e42ca39826fc5c8f26ccc08", "text": "In[38]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbk_HTML.gif)\n\n|\n\n_# Import Matplotlib pyplot object_\n\n**import** matplotlib.pyplot **as** pyplt\n\npyplt.figure(figsize=(20, 8))\n\nlwords = dict([(k,v) **for** k,v **in** fdist.items() **if** len(k)>3])\n\nfdist = nltk.FreqDist(lwords)\n\nfdist.plot(50,cumulative= **False** )  \n  \n---|---|---  \n  \nOut[38]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbl_HTML.gif)  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbm_HTML.gif)\n\n|\n\nExclude stop-words such as _the_ , _and_ , _is and_ create a tuple dictionary\nto record words frequency. Visualize and transform them into a NLTK frequency\ndistribution graph based on this dictionary as shown above  \n  \n---|---  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbn_HTML.gif)\n\n|\n\n**Workshop 3.3** **Text Analysis** **on The Adventures of Sherlock Holmes**\n\n1\\. Read Adventures_Holmes.txt text file\n\n2\\. Save contents into a string object \"holmes_doc\"\n\n3\\. Use stop-word technique from tokenize holmes_doc\n\n4\\. Generate a word tokens list with stop-words removed\n\n5\\. Use the technique learnt to plot first 30 frequently occurred words from\nthis literature\n\n6\\. Identify any special pattern related to word distribution. If no, try\nfirst 50 ranking words  \n  \n---|---  \n  \n## 12.6 Integration with WordCloud\n\nSee Fig. 12.2.\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Fig2_HTML.png)\n\nFig. 12.2\n\nA sample WordCloud\n\n### 12.6.1 What Is WordCloud?\n\nWordcloud, also known as tag cloud, is a data visualization method commonly\nused in many web statistics and data analysis scenarios. It is a graphical\nrepresentation of all words and keywords in sizes and colors. A word has the\nlargest and bold in word cloud means it occurs frequently in the text\n(dataset).\n\nTo generate frequency distribution of all words occurred in a text document,\nthe most natural way is to generate statistics in a WordCloud.\n\nPython provides a built-in WordCloud package \"WordCloud\".\n\nIt can obtain an intuitive visualization of words used in the text from the\nfrequency distribution.\n\nInstall wordcloud package first using the _pip install_ command:\n\npip install wordcloud.\n\nOnce Wordcloud package is installed, import wordcloud package using import\ncommand and invoke the frequency generator with _generate_from frequencies()\nmethod_ :\n\nIn[39]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbo_HTML.gif)\n\n|\n\n_# Import_ _WordCloud_ _as wCloud_\n\n**from** wordcloud **import** WordCloud **as** wCloud  \n  \n---|---|---  \n  \nIn[40]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbp_HTML.gif)\n\n|\n\nwcld = wCloud().generate_from_frequencies(fdist)  \n  \n---|---|---  \n  \nIn[41]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbq_HTML.gif)\n\n|\n\n**Import** matplotlib.pyplot **as** pyplt\n\npyplt.figure(figsize=(20, 8))\n\npyplt.imshow(wcld, interpolation=\"bilinear\")\n\npyplt.axis(\"off\")\n\npyplt.show()  \n  \n---|---|---  \n  \nOut[41]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbr_HTML.gif)  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbs_HTML.gif)\n\n|\n\n**Workshop 3.4** **WordCloud** **for The Adventures of Sherlock Holmes**\n\n1\\. Read Adventures_Holmes.txt text file\n\n2\\. Save contents into a string object \"holmes_doc\"\n\n3\\. Use stop-word technique from tokenize holmes_doc\n\n4\\. Generate word tokens list with stop-words removed\n\n5\\.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6f406bf-c6b3-474a-b5c6-4771d39de426": {"__data__": {"id_": "b6f406bf-c6b3-474a-b5c6-4771d39de426", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52724a04-e86c-42bf-89f2-6a0328353ef8", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "257ea0d621efecd9070b68effb6b901b74f835a20e42ca39826fc5c8f26ccc08", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ecde0cd6-14a7-45d4-ae68-2f425caae914", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "067e5cf10147e2979597b66cc8f084b61674daaab39c9cb517f4836321369255", "class_name": "RelatedNodeInfo"}}, "hash": "bc5657bda93f21f9c92b9720741fcfeb77940b3701edb727d1e239c9882554bf", "text": "[](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbr_HTML.gif)  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbs_HTML.gif)\n\n|\n\n**Workshop 3.4** **WordCloud** **for The Adventures of Sherlock Holmes**\n\n1\\. Read Adventures_Holmes.txt text file\n\n2\\. Save contents into a string object \"holmes_doc\"\n\n3\\. Use stop-word technique from tokenize holmes_doc\n\n4\\. Generate word tokens list with stop-words removed\n\n5\\. Extract top 100 frequently words occurred from this literature\n\n6\\. Generate Wordcloud for this literature  \n  \n---|---  \n  \n## 12.7 POS Tagging with NLTK\n\nThe earlier part of this workshop had studied several NLP preprocessing tasks:\ntokenization, stemming, stop-word removal, word distribution in text corpus\nand data visualization using WordCloud. This section will explore POS tagging\nin NLTK.\n\n### 12.7.1 What Is POS Tagging?\n\nPart-of-Speech (POS) refers to words categorization process in a\nsentence/utterance into specific syntactic or grammatical functions.\n\nThere are 9 major POS in English: Nouns, Pronouns, Adjectives, Verbs,\nPrepositions, Adverbs, Determiners, interjection, and Conjunctions. POS\ntagging is to assign POS tags into each word token in the sentence/utterance.\n\nNTLK supports commonly used tagset such as PENN Treebank (Treebank 2022) and\nBrown corpus. It allows to create own tags used for specific NLP applications.\n\n### 12.7.2 Universal POS Tagset\n\nA tagset consists of 12 universal POS categories which is constructed to\nfacilitate future requirements for unsupervised induction of syntactic\nstructure. When is combined with original treebank data, this universal tagset\nand mapping produce a dataset consisting of common POS in 22 languages\n(Albrecht et al. 2020; Antic 2021; Bird et al. 2009).\n\nFigure 12.3 shows a table of universal POS tagset in English.\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Fig3_HTML.png)\n\nFig. 12.3\n\nTable of Universal POS Tagset in English\n\n### 12.7.3 PENN Treebank Tagset (English and Chinese)\n\nEnglish Penn Treebank Tagset is used with English corpora developed by Prof.\nHelmut Schmid in TC project at the Institute for Computational Linguistics of\nthe University of Stuttgart (TreeBank 2022). Figure 12.4 shows an original 45\nused Penn Treebank Tagset.\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Fig4_HTML.png)\n\nFig. 12.4\n\nOriginal 45 used Penn Treebank Tagset\n\nA recent version of this English POS Tagset can be found at Sketchengine.eu\n(Sketchengine 2022a) and Chinese POS Tagset (Sketchengine 2022b).\n\nNLTK provides direct mapping from tagged corpus such as Brown Corpus (NLTK\n2022) to universal tags for implementation, e.g. tags VBD (for past tense\nverb) and VB (for base form verb) map to VERB only in universal tagset.\n\nIn[42]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbt_HTML.gif)\n\n|\n\n_# Import Brown Corpus as bwn_\n\n**from** nltk.corpus **import** brown **as** bwn  \n  \n---|---|---  \n  \nIn[43]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbu_HTML.gif)\n\n|\n\nbwn.tagged_words()[0:40]  \n  \n---|---|---  \n  \nOut[43]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbv_HTML.gif)  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbw_HTML.gif)\n\n|\n\n_Fulton_ is tagged as NP-TL in example code above, a _proper noun (NP)_\nappears in a title (TL) context in Brown corpus that mapped to _noun_ in\nuniversal tagset.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ecde0cd6-14a7-45d4-ae68-2f425caae914": {"__data__": {"id_": "ecde0cd6-14a7-45d4-ae68-2f425caae914", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6f406bf-c6b3-474a-b5c6-4771d39de426", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "bc5657bda93f21f9c92b9720741fcfeb77940b3701edb727d1e239c9882554bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07a57000-bdad-4b8f-a3bb-6ad10a2ed640", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "5467e28f073fbce223e7d434a021d5b8116a7601b56d17d27a3fb59319478ec6", "class_name": "RelatedNodeInfo"}}, "hash": "067e5cf10147e2979597b66cc8f084b61674daaab39c9cb517f4836321369255", "text": "[](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbu_HTML.gif)\n\n|\n\nbwn.tagged_words()[0:40]  \n  \n---|---|---  \n  \nOut[43]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbv_HTML.gif)  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbw_HTML.gif)\n\n|\n\n_Fulton_ is tagged as NP-TL in example code above, a _proper noun (NP)_\nappears in a title (TL) context in Brown corpus that mapped to _noun_ in\nuniversal tagset. These subcategories are to be considered instead of\ngeneralized universal tags in NLP application  \n  \n---|---  \n  \n### 12.7.4 Applications of POS Tagging\n\nPOS tagging is commonly used in many NLP applications ranging from Information\nExtraction (IE), Named Entity Recognition (NER) to Sentiment Analysis and\nQuestion-&-Answering systems.\n\nTry the following and see how it works:\n\nIn[44]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbx_HTML.gif)\n\n|\n\n_## Import word_tokenize and pos_tag as w_tok and p_tag_\n\n**from** nltk.tokenize **import** word_tokenize **as** w_tok\n\n**from** nltk **import** pos_tag **as** p_tag\n\n_# Create and tokenizer two sample utterances utt1 and utt2_\n\nutt1 = w_tok(\"Give me a call\")\n\nutt2 = w_tok(\"Call me later\")  \n  \n---|---|---  \n  \nReview these two utterances\u2019 POS tags:\n\nIn[45]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figby_HTML.gif)\n\n|\n\np_tag(utt1, tagset=\"universal\" )  \n  \n---|---|---  \n  \nOut[45]\n\n|\n\n[('Give', 'VERB'), ('me', 'PRON'), ('a', 'DET'), ('call', 'NOUN')]  \n  \nIn[46]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figbz_HTML.gif)\n\n|\n\np_tag(utt2, tagset=\"universal\" )  \n  \n---|---|---  \n  \nOut[46]\n\n|\n\n[('Call', 'VERB'), ('me', 'PRON'), ('later', 'ADV')]  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figca_HTML.gif)\n\n|\n\n1\\. The word _call_ is a noun in text 1 and a verb in text 2\n\n2\\. POS tagging is used to identify a person, a place, or a location, based on\nthe tags in NER\n\n3\\. NLTK also provides a classifier to identify such entities in text as shown\nin the following code:  \n  \n---|---  \n  \nIn[47]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figcb_HTML.gif)\n\n|\n\nutt_untag = w_tok(\"My dad was born in South America\")\n\nutt_untag  \n  \n---|---|---  \n  \nOut[47]\n\n|\n\n['My', 'dad', 'was', 'born', 'in', 'South', 'America']  \n  \nIn[48]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figcc_HTML.gif)\n\n|\n\nutt_tagged = p_tag(utt_untag)\n\nutt_tagged  \n  \n---|---|---  \n  \nOut[48]\n\n|\n\n[('My', 'PRP$'), ('dad', 'NN'), ('was', 'VBD'), ('born', 'VBN'),\n\n('in', 'IN'), ('South', 'NNP'), ('America', 'NNP')]  \n  \nIn[49]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figcd_HTML.gif)\n\n|\n\n_# Import svgling package_\n\n**import** svgling\n\n_# Import_ _NLTK_ _.ne_chunk as chunk_\n\n**from** nltk **import** ne_chunk **as** chunk\n\n_# Display POS Tags chunk_\n\nchunk(utt_tagged)  \n  \n---|---|---  \n  \nOut[49]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figce_HTML.gif)  \n  \n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "07a57000-bdad-4b8f-a3bb-6ad10a2ed640": {"__data__": {"id_": "07a57000-bdad-4b8f-a3bb-6ad10a2ed640", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ecde0cd6-14a7-45d4-ae68-2f425caae914", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "067e5cf10147e2979597b66cc8f084b61674daaab39c9cb517f4836321369255", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6de95ad1-f70c-4504-a3cb-daab98c3d4fb", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f8bda542ea7fdef7dfea7afbf53d009cf6d579770c4a11149523e98c4237be90", "class_name": "RelatedNodeInfo"}}, "hash": "5467e28f073fbce223e7d434a021d5b8116a7601b56d17d27a3fb59319478ec6", "text": "[](../images/533412_1_En_12_Chapter/533412_1_En_12_Figcd_HTML.gif)\n\n|\n\n_# Import svgling package_\n\n**import** svgling\n\n_# Import_ _NLTK_ _.ne_chunk as chunk_\n\n**from** nltk **import** ne_chunk **as** chunk\n\n_# Display POS Tags chunk_\n\nchunk(utt_tagged)  \n  \n---|---|---  \n  \nOut[49]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figce_HTML.gif)  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figcf_HTML.gif)\n\n|\n\nNLTK chunk() function is applied to NER to identify the chunker South America\nas a geopolitical entity (GPE) in this example. So far, there are examples\nusing NLTK\u2019s built-in taggers. Next section will look at how to develop own\nPOS tagger  \n  \n---|---  \n  \nIn[50]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figcg_HTML.gif)\n\n|\n\n_# Try another example_\n\nutt_tok = w_tok(\"Can you please buy me Haagen-Dazs Icecream? It's $30.8.\")\n\nprint(\"Tokens are: \", utt_tok)  \n  \n---|---|---  \n  \nOut[50]\n\n|\n\nTokens are: ['Can', 'you', 'please', 'buy', 'me', 'Haagen-Dazs', 'Icecream',\n'?', 'It', \"'s\", '$', '30.8', '.']  \n  \nIn[51]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figch_HTML.gif)\n\n|\n\nutt_tagged = p_tag(utt_tok)\n\nchunk(utt_tagged)  \n  \n---|---|---  \n  \nOut[51]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figci_HTML.gif)  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figcj_HTML.gif)\n\n|\n\n1\\. The system treats '$', '30.8', and '.' as separate tokens in this example.\nIt is crucial because contractions have their own semantic meanings and own\nPOS leading to the ensuing part of NLTK library POS tagger\n\n2\\. POS tagger in NLTK library outputs specific tags for certain words\n\n3\\. However, it makes a mistake in this example. Where is it?\n\n4\\. Compare POS Tagging for the following sentence to identify problem.\nExplain  \n  \n---|---  \n  \nIn[52]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figck_HTML.gif)\n\n|\n\n_# Try one more example_\n\nutt_tok = w_tok(\"Can you please buy me New-Zealand Icecream? It's $30.8.\")\n\nprint(\"Tokens are: \", utt_tok)\n\nutt_tagged = nltk.pos_tag(utt_tok)\n\nchunk(utt_tagged)  \n  \n---|---|---  \n  \nOut[52]\n\n|\n\nTokens are: ['Can', 'you', 'please', 'buy', 'me', 'New-Zealand', 'Icecream',\n'?', 'It', \"'s\", '$', '30.8', '.']\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figcl_HTML.gif)  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figcm_HTML.gif)\n\n|\n\n**Workshop 3.5** **POS Tagging** **on** _**The Adventures of Sherlock\nHolmes**_\n\n1\\. Read Adventures_Holmes.txt text file\n\n2\\. Save contents into a string object \"holmes_doc\"\n\n3\\. Extract three typical sentences from three stories of this literature\n\n4\\. Use POS Tagging to these sentences\n\n5\\. Use ne_chunk function to display POS tagging tree for these three\nsentences\n\n6\\. Compare POS Tags among these example sentences and examine on how they\nwork  \n  \n---|---  \n  \n## 12.8 Create Own POS Tagger with NLTK\n\nThis section will create own POS tagger using NLTK\u2019s tagged set corpora and\nsklearn Random Forest machine learning model.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6de95ad1-f70c-4504-a3cb-daab98c3d4fb": {"__data__": {"id_": "6de95ad1-f70c-4504-a3cb-daab98c3d4fb", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07a57000-bdad-4b8f-a3bb-6ad10a2ed640", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "5467e28f073fbce223e7d434a021d5b8116a7601b56d17d27a3fb59319478ec6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "414fc246-cf1f-4387-b9e9-50a59269df94", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "d45becda003c4bf9f61c8a10f1033425ad106b00930eee65f1f5b7d45cc32c3f", "class_name": "RelatedNodeInfo"}}, "hash": "f8bda542ea7fdef7dfea7afbf53d009cf6d579770c4a11149523e98c4237be90", "text": "[](../images/533412_1_En_12_Chapter/533412_1_En_12_Figcm_HTML.gif)\n\n|\n\n**Workshop 3.5** **POS Tagging** **on** _**The Adventures of Sherlock\nHolmes**_\n\n1\\. Read Adventures_Holmes.txt text file\n\n2\\. Save contents into a string object \"holmes_doc\"\n\n3\\. Extract three typical sentences from three stories of this literature\n\n4\\. Use POS Tagging to these sentences\n\n5\\. Use ne_chunk function to display POS tagging tree for these three\nsentences\n\n6\\. Compare POS Tags among these example sentences and examine on how they\nwork  \n  \n---|---  \n  \n## 12.8 Create Own POS Tagger with NLTK\n\nThis section will create own POS tagger using NLTK\u2019s tagged set corpora and\nsklearn Random Forest machine learning model.\n\nThe following example demonstrates a classification task to predict POS tag\nfor a word in a sentence using NLTK treebank dataset for POS tagging, and\nextract word prefixes, suffixes, previous and neighboring words as features\nfor system training.\n\nImport all necessary Python packages as below:\n\nIn[53]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figcn_HTML.gif)\n\n|\n\n_# Import all necessary Python packages_\n\n**import** nltk\n\n**import** numpy **as** np\n\n**from** nltk **import** word_tokenize **as** w_tok\n\n**import** matplotlib.pyplot **as** pyplt\n\n%matplotlib inline\n\n**from** sklearn.feature_extraction **import** DictVectorizer **as** DVect\n\n**from** sklearn.model_selection **import** train_test_split **as** tt_split\n\n**from** sklearn.ensemble **import** RandomForestClassifier **as**\nRFClassifier\n\n**from** sklearn.metrics **import** accuracy_score **as** a_score\n\n**from** sklearn.metrics **import** confusion_matrix **as** c_matrix  \n  \n---|---|---  \n  \nIn[54]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figco_HTML.gif)\n\n|\n\n_# Define the ufeatures() class_\n\n**def** ufeatures(utt, idx):\n\nftdist = {}\n\nftdist['word'] = utt[idx]\n\nftdist['dist_from_first'] = idx - 0\n\nftdist['dist_from_last'] = len(utt) - idx\n\nftdist['capitalized'] = utt[idx][0].upper() == utt[idx][0]\n\nftdist['prefix1'] = utt[idx][0]\n\nftdist['prefix2'] = utt[idx][:2]\n\nftdist['prefix3'] = utt[idx][:3]\n\nftdist['suffix1'] = utt[idx][-1]\n\nftdist['suffix2'] = utt[idx][-2:]\n\nftdist['suffix3'] = utt[idx][-3:]\n\nftdist['prev_word'] = '' **if** idx==0 **else** utt[idx-1]\n\nftdist['next_word'] = '' **if** idx==(len(utt)-1) **else** utt[idx+1]\n\nftdist['numeric'] = utt[idx].isdigit()\n\n**return** ftdist  \n  \n---|---|---  \n  \nIn[55]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figcp_HTML.gif)\n\n|\n\n_# Define the Retreive Untagged Utterance (RUutterance) class_\n\n**def** RUutterance(utt_tagged):\n\n[utt,t] = zip(*utt_tagged)\n\n**return** list(utt)  \n  \n---|---|---  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figcq_HTML.gif)\n\n|\n\nFunction _ufeatures()_ converts input text into a dict object of features,\nwhereas each utterance is passed with corresponding index of current token\nword which features are extracted. Let us use treebank tagged utterances with\nuniversal tags to label and train data:  \n  \n---|---  \n  \nIn[56]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figcr_HTML.gif)\n\n|\n\nutt_tagged = nltk.corpus.treebank.tagged_sents(tagset='universal')  \n  \n---|---|---  \n  \nIn[57]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figcs_HTML.gif)\n\n|\n\nutt_tagged  \n  \n---|---|---  \n  \nOut[57]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "414fc246-cf1f-4387-b9e9-50a59269df94": {"__data__": {"id_": "414fc246-cf1f-4387-b9e9-50a59269df94", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6de95ad1-f70c-4504-a3cb-daab98c3d4fb", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f8bda542ea7fdef7dfea7afbf53d009cf6d579770c4a11149523e98c4237be90", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "332f11eb-a03f-49f0-b0c2-57e31cb8579f", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "2ee26b86ecd09414c2fc348d66b400a43afc6870490635d791c677a08f0e9b39", "class_name": "RelatedNodeInfo"}}, "hash": "d45becda003c4bf9f61c8a10f1033425ad106b00930eee65f1f5b7d45cc32c3f", "text": "Let us use treebank tagged utterances with\nuniversal tags to label and train data:  \n  \n---|---  \n  \nIn[56]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figcr_HTML.gif)\n\n|\n\nutt_tagged = nltk.corpus.treebank.tagged_sents(tagset='universal')  \n  \n---|---|---  \n  \nIn[57]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figcs_HTML.gif)\n\n|\n\nutt_tagged  \n  \n---|---|---  \n  \nOut[57]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figct_HTML.gif)  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figcu_HTML.gif)\n\n|\n\n1\\. In this example, universal tags are used for simplicity\n\n2\\. Of course, one can also use fine-gained treebank POS Tags for\nimplementation\n\n3\\. Once do so, can now extract the features for each tagged utterance in\ncorpus with training labels\n\nUse following code to extract the features:  \n  \n---|---  \n  \nIn[58]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figcv_HTML.gif)\n\n|\n\n_# Define Extract Feature class (exfeatures)_\n\n**def** exfeatures(utt_tag):\n\nutt, tag = [], []\n\n**for** ut **in** utt_tag:\n\n**for** idx **in** range(len(ut)):\n\nutt.append(ufeatures(RUutterance(ut), idx))\n\ntag.append(ut[idx][1])\n\n**return** utt, tag  \n  \n---|---|---  \n  \nIn[59]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figcw_HTML.gif)\n\n|\n\nX,y = exfeatures(utt_tagged)  \n  \n---|---|---  \n  \nThis example uses DVect to convert feature-value dictionary into training\nvectors.\n\nIf the number of possible values for suffix3 feature is 40, there will be 40\nfeatures in output. Use following code to DVect:\n\nIn[60]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figcx_HTML.gif)\n\n|\n\n_# Define sample size_\n\nnsize = 10000\n\n_# Invoke Dict Vectorizer_\n\ndvect = DVect(sparse= **False** )\n\nXtran = dvect.fit_transform(X[0:nsize])\n\nysap = y[0:nsize]  \n  \n---|---|---  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figcy_HTML.gif)\n\n|\n\nThis example has a sample size of 10,000 utterances in which 80% of the\ndataset is used for training and other 20% is used for testing. RF (Random\nForecast) Classifier is used as POS tagger model as shown:  \n  \n---|---  \n  \nIn[61]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figcz_HTML.gif)\n\n|\n\nXtrain,Xtest,ytrain,ytest = tt_split(Xtran, ysap, test_size=0.2,\nrandom_state=123)  \n  \n---|---|---  \n  \nIn[62]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figda_HTML.gif)\n\n|\n\nrfclassifier = RFClassifier(n_jobs=4)\n\nrfclassifier.fit(Xtrain,ytrain)  \n  \n---|---|---  \n  \nOut[62]\n\n|\n\nRandomForestClassifier(n_jobs=4)  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figdb_HTML.gif)\n\n|\n\nAfter system training, can perform POS tagger validation by using some sample\nutterances. But before passing to _ptag_predict()_ method, extract features\nare required by _ufeatures()_ method as shown:  \n  \n---|---  \n  \nIn[63]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "332f11eb-a03f-49f0-b0c2-57e31cb8579f": {"__data__": {"id_": "332f11eb-a03f-49f0-b0c2-57e31cb8579f", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "414fc246-cf1f-4387-b9e9-50a59269df94", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "d45becda003c4bf9f61c8a10f1033425ad106b00930eee65f1f5b7d45cc32c3f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52270854-9553-4699-a114-1c433b7bb5da", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3c0fd5a5ec858160309104854651709378add0410f88ac3e992368bb06f07a6b", "class_name": "RelatedNodeInfo"}}, "hash": "2ee26b86ecd09414c2fc348d66b400a43afc6870490635d791c677a08f0e9b39", "text": "[](../images/533412_1_En_12_Chapter/533412_1_En_12_Figda_HTML.gif)\n\n|\n\nrfclassifier = RFClassifier(n_jobs=4)\n\nrfclassifier.fit(Xtrain,ytrain)  \n  \n---|---|---  \n  \nOut[62]\n\n|\n\nRandomForestClassifier(n_jobs=4)  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figdb_HTML.gif)\n\n|\n\nAfter system training, can perform POS tagger validation by using some sample\nutterances. But before passing to _ptag_predict()_ method, extract features\nare required by _ufeatures()_ method as shown:  \n  \n---|---  \n  \nIn[63]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figdc_HTML.gif)\n\n|\n\n_# Define the POS Tags Predictor class (ptag_predict)_\n\n**def** ptag_predict(utt):\n\nutt_tagged = []\n\nfts = [ufeatures(utt, idx) **for** idx **in** range(len(utt))]\n\nfts = dvect.transform(fts)\n\ntgs = rfclassifier.predict(fts)\n\n**return** zip(utt, tgs)  \n  \n---|---|---  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figdd_HTML.gif)\n\n|\n\nConvert utterance into corresponding features with _ufeatures()_ method. The\nfeatures dictionary extracted from this method is vectorized using previously\ntrained _dvect_ :  \n  \n---|---  \n  \nIn[64]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figde_HTML.gif)\n\n|\n\n_# Test with a sample utterance (utt3)_\n\nutt3 = \"It is an example for POS tagger\"\n\n**for** utt_tagged **in** ptag_predict(utt3.split()):\n\nprint(utt_tagged)  \n  \n---|---|---  \n  \nOut[64]\n\n|\n\n('It', 'PRON') ('is', 'VERB') ('an', 'DET') ('example', 'NOUN')\n\n('for', 'ADP') ('POS', 'NOUN') ('tagger', 'NOUN')  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figdf_HTML.gif)\n\n|\n\nUse a sample utterance \u201cutt3\u201d and invoke ptag_predict() method to output tags\nfor each word token inside utt3 and review for accuracy afterwards  \n  \n---|---  \n  \nIn[65]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figdg_HTML.gif)\n\n|\n\npredict = rfclassifier.predict(Xtest)  \n  \n---|---|---  \n  \nIn[66]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figdh_HTML.gif)\n\n|\n\na_score(ytest,predict)  \n  \n---|---|---  \n  \nOut[66]\n\n|\n\n0.9355  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figdi_HTML.gif)\n\n|\n\nThe overall a_score has approximately 93.6% accuracy rate and satisfactory.\nNext, let us look at confusion matrix (c-mat) to check how well can POS tagger\nperform  \n  \n---|---  \n  \nIn[67]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figdj_HTML.gif)\n\n|\n\nc_mat = c_matrix(ytest,predict)  \n  \n---|---|---  \n  \nIn[68]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figdk_HTML.gif)\n\n|\n\npyplt.figure(figsize=(10,10))\n\npyplt.xticks(np.arange(len(rfclassifier.classes_)),rfclassifier.classes_)\n\npyplt.yticks(np.arange(len(rfclassifier.classes_)),rfclassifier.classes_)\n\npyplt.imshow(c_mat, cmap=pyplt.cm.Blues)\n\npyplt.colorbar()  \n  \n---|---|---  \n  \nOut[68]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figdl_HTML.gif)  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figdm_HTML.gif)\n\n|\n\nUse classes from RF classifier as x and y labels to create a c-mat (confusion\nmatrix).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52270854-9553-4699-a114-1c433b7bb5da": {"__data__": {"id_": "52270854-9553-4699-a114-1c433b7bb5da", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "332f11eb-a03f-49f0-b0c2-57e31cb8579f", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "2ee26b86ecd09414c2fc348d66b400a43afc6870490635d791c677a08f0e9b39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1dc3c49a-c099-48bc-b29e-7c863951889d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "d93361b4ae90646b498560bf5cedc8aad191cce763eab2cfeb2d92955f2aec05", "class_name": "RelatedNodeInfo"}}, "hash": "3c0fd5a5ec858160309104854651709378add0410f88ac3e992368bb06f07a6b", "text": "[](../images/533412_1_En_12_Chapter/533412_1_En_12_Figdk_HTML.gif)\n\n|\n\npyplt.figure(figsize=(10,10))\n\npyplt.xticks(np.arange(len(rfclassifier.classes_)),rfclassifier.classes_)\n\npyplt.yticks(np.arange(len(rfclassifier.classes_)),rfclassifier.classes_)\n\npyplt.imshow(c_mat, cmap=pyplt.cm.Blues)\n\npyplt.colorbar()  \n  \n---|---|---  \n  \nOut[68]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figdl_HTML.gif)  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figdm_HTML.gif)\n\n|\n\nUse classes from RF classifier as x and y labels to create a c-mat (confusion\nmatrix). These labels are POS tags used for system training. The plot that\nfollows shows a pictorial representation of confusion matrix  \n  \n---|---  \n  \nUse classes from random forest classifier as x and y labels in the code for\nplotting confusion matrix.\n\nIt looks like the tagger performs relatively well for nouns, verbs, and\ndeterminers in sentences reflected in dark regions of the plot. Let us look at\nsome top features of the model from following code:\n\nIn[69]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figdn_HTML.gif)\n\n|\n\nflist = zip(dvect.get_feature_names_out(),rfclassifier.feature_importances_)\n\nsfeatures = sorted(flist,key=lambda x: x[1], reverse= **True** )\n\nprint(sfeatures[0:20])  \n  \n---|---|---  \n  \nOut[69]\n\n|\n\n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figdo_HTML.gif)  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figdp_HTML.gif)\n\n|\n\n1\\. The RF feature importance is stored in Python feature_importances list.\nSome of the suffix features have higher importance scores than others\n\n2\\. For instances, words ending with _-ed_ are usually verbs in past tense\nwhich make sense in many situations, and punctuations like commas may affect\nPOS tagging performance in some situations  \n  \n---|---  \n  \n![](../images/533412_1_En_12_Chapter/533412_1_En_12_Figdq_HTML.gif)\n\n|\n\n**Workshop 3.6 Revisit** **POS Tagging** **on The Adventures of Sherlock\nHolmes with Additional Tagger**\n\n1\\. Read Adventures_Holmes.txt text file\n\n2\\. Save contents into a string object \"holmes_doc\"\n\n3\\. Extract three typical sentences from three stories of this literature\n\n4\\. Use method learnt to create own POS taggers. What are new POS tags to add\nor use?\n\n5\\. Try new POS taggers for these three typical sentences and compare results\nwith previous workshop  \n  \n---|---  \n  \nReferences\n\n  1. Albrecht, J., Ramachandran, S. and Winkler, C. (2020) Blueprints for Text Analytics Using Python: Machine Learning-Based Solutions for Common Real World (NLP) Applications. O\u2019Reilly Media.\n\n  2. Antic, Z. (2021) Python Natural Language Processing Cookbook: Over 50 recipes to understand, analyze, and generate text for implementing language processing tasks. Packt Publishing.\n\n  3. Bird, S., Klein, E., and Loper, E. (2009). Natural language processing with python. O'Reilly.[zbMATH](http://www.emis.de/MATH-item?1187.68630)\n\n  4. Doyle, A. C. (2019) The Adventures of Sherlock Holmes (AmazonClassics Edition). AmazonClassics.\n\n  5. Gutenberg (2022) Project Gutenberg official site. [https://\u200bwww.\u200bgutenberg.\u200borg/\u200b](https://www.gutenberg.org/) Accessed 16 June 2022.\n\n  6. Hardeniya, N., Perkins, J. and Chopra, D. (2016) Natural Language Processing: Python and NLTK. Packt Publishing.\n\n  7. Kedia, A. and Rasu, M. (2020) Hands-On Python Natural Language Processing: Explore tools and techniques to analyze and process text with a view to building real-world NLP applications. Packt Publishing.\n\n  8.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1dc3c49a-c099-48bc-b29e-7c863951889d": {"__data__": {"id_": "1dc3c49a-c099-48bc-b29e-7c863951889d", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52270854-9553-4699-a114-1c433b7bb5da", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3c0fd5a5ec858160309104854651709378add0410f88ac3e992368bb06f07a6b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc4d2b0b-f3fe-4d60-be8f-d310998632cb", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9bb5c22d52fba151ca2940492787cb7be7ee1aa3a21332e13fff5c55f66606bb", "class_name": "RelatedNodeInfo"}}, "hash": "d93361b4ae90646b498560bf5cedc8aad191cce763eab2cfeb2d92955f2aec05", "text": "O'Reilly.[zbMATH](http://www.emis.de/MATH-item?1187.68630)\n\n  4. Doyle, A. C. (2019) The Adventures of Sherlock Holmes (AmazonClassics Edition). AmazonClassics.\n\n  5. Gutenberg (2022) Project Gutenberg official site. [https://\u200bwww.\u200bgutenberg.\u200borg/\u200b](https://www.gutenberg.org/) Accessed 16 June 2022.\n\n  6. Hardeniya, N., Perkins, J. and Chopra, D. (2016) Natural Language Processing: Python and NLTK. Packt Publishing.\n\n  7. Kedia, A. and Rasu, M. (2020) Hands-On Python Natural Language Processing: Explore tools and techniques to analyze and process text with a view to building real-world NLP applications. Packt Publishing.\n\n  8. NLTK (2022) NLTK official site. [https://\u200bwww.\u200bnltk.\u200borg/\u200b](https://www.nltk.org/). Accessed 16 June 2022.\n\n  9. Perkins, J. (2014). Python 3 text processing with NLTK 3 cookbook. Packt Publishing Ltd.\n\n  10. Sketchengine (2022a) Recent version of English POS Tagset by Sketchengine. [https://\u200bwww.\u200bsketchengine.\u200beu/\u200benglish-treetagger-pipeline-2/\u200b](https://www.sketchengine.eu/english-treetagger-pipeline-2/). Accessed 21 June 2022.\n\n  11. Sketchengine (2022b) Recent version of Chinese POS Tagset by Sketchengine. [https://\u200bwww.\u200bsketchengine.\u200beu/\u200bchinese-penn-treebank-part-of-speech-tagset/\u200b](https://www.sketchengine.eu/chinese-penn-treebank-part-of-speech-tagset/). Accessed 21 June 2022.\n\n  12. Treebank (2022) Penn TreeBank Release 2 official site. [https://\u200bcatalog.\u200bldc.\u200bupenn.\u200bedu/\u200bdocs/\u200bLDC95T7/\u200btreebank2.\u200bindex.\u200bhtml](https://catalog.ldc.upenn.edu/docs/LDC95T7/treebank2.index.html). Accessed 21 June 2022.\n\n\n\u00a9 The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.\n2024\n\nR. S. T. Lee Natural Language Processing\n<https://doi.org/10.1007/978-981-99-1999-4_13>\n\n# 13\\. Workshop#4 Semantic Analysis and Word Vectors Using spaCy (Hour 7\u20138)\n\nRaymond S. T. Lee 1\n\n(1)\n\nUnited International College, Beijing Normal University-Hong Kong Baptist\nUniversity, Zhuhai, China\n\n## 13.1 Introduction\n\nIn Chaps. [5](533412_1_En_5_Chapter.xhtml) and\n[6](533412_1_En_6_Chapter.xhtml), we studied the basic concepts and theories\nrelated to meaning representation and semantic analysis. This workshop will\nexplore how to use spaCy technology to perform semantic analysis starting from\na revisit on word vectors concept, implement and pre-train them followed by\nthe study of similarity method and other advanced semantic analysis.\n\n## 13.2 What Are Word Vectors?\n\n_Word vectors_ (Albrecht et al. 2020; Bird et al. 2009; Hardeniya et al. 2016;\nKedia and Rasu 2020; NLTK 2022) are practical tools in NLP.\n\nA word vector is a dense representation of a word. Word vectors are important\nfor semantic similarity applications like similarity calculations between\nwords, phrases, sentences, and documents, e.g. they provide information about\nsynonymity, semantic analogies at word level.\n\nWord vectors are produced by algorithms to reflect similar words appear in\nsimilar contexts. This paradigm captures target word meaning by collecting\ninformation from surrounding words which is called distributional semantics.\n\nThey are accompanied by associative semantic similarity methods including word\nvector computations such as distance, analogy calculations, and visualization\nto solve NLP problems.\n\nIn this workshop, we are going to cover the following main topics (Altinok\n2021; Arumugam and Shanmugamani 2018; Perkins 2014; spaCy 2022; Srinivasa-\nDesikan 2018; Vasiliev 2020):\n\n  * Understanding word vectors.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc4d2b0b-f3fe-4d60-be8f-d310998632cb": {"__data__": {"id_": "fc4d2b0b-f3fe-4d60-be8f-d310998632cb", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1dc3c49a-c099-48bc-b29e-7c863951889d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "d93361b4ae90646b498560bf5cedc8aad191cce763eab2cfeb2d92955f2aec05", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9da16b3f-2b5c-4828-b597-86854a258b89", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "6ff039217c9dce92eaa249f14dcda18c581eaec4251fd2448cf913edd677b00c", "class_name": "RelatedNodeInfo"}}, "hash": "9bb5c22d52fba151ca2940492787cb7be7ee1aa3a21332e13fff5c55f66606bb", "text": "A word vector is a dense representation of a word. Word vectors are important\nfor semantic similarity applications like similarity calculations between\nwords, phrases, sentences, and documents, e.g. they provide information about\nsynonymity, semantic analogies at word level.\n\nWord vectors are produced by algorithms to reflect similar words appear in\nsimilar contexts. This paradigm captures target word meaning by collecting\ninformation from surrounding words which is called distributional semantics.\n\nThey are accompanied by associative semantic similarity methods including word\nvector computations such as distance, analogy calculations, and visualization\nto solve NLP problems.\n\nIn this workshop, we are going to cover the following main topics (Altinok\n2021; Arumugam and Shanmugamani 2018; Perkins 2014; spaCy 2022; Srinivasa-\nDesikan 2018; Vasiliev 2020):\n\n  * Understanding word vectors.\n\n  * Using spaCy\u2019s pre-trained vectors.\n\n  * Advanced semantic similarity methods.\n\n## 13.3 Understanding Word Vectors\n\nWord vectors, or word2vec are important quantity units in statistical methods\nto represent text in statistical NLP algorithms. There are several ways of\ntext vectorization to provide words semantic representation.\n\n### 13.3.1 Example: A Simple Word Vector\n\nLet us look at a basic way to assign words vectors:\n\n  * Assign an index value to each word in vocabulary and encode this value into a sparse vector.\n\n  * Consider _tennis_ as vocabulary and assign an index to each word according to vocabulary order as in Fig. 13.1:\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Fig1_HTML.png)\n\nFig. 13.1\n\nA basic word vector example consists of 9 words\n\nVocabulary word vector will be 0, except for word corresponding index value\nposition as in Fig. 13.2:\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Fig2_HTML.png)\n\nFig. 13.2\n\nWord vectors corresponding index value consists of 9 words\n\nSince each row corresponds to one word, a sentence represents a matrix, e.g.\n_I play tennis today_ is represented by matrix as in Fig. 13.3:\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Fig3_HTML.png)\n\nFig. 13.3\n\nWord vector matrix for _I play tennis today_\n\nVectors length is equal to word numbers in vocabulary as shown above. Each\ndimension is apportioned to one word explicitly. When applying this encoding\nvectorization to text, each word is replaced by its vector and the sentence is\ntransformed into a ( _N_ , _V_ ) matrix, where _N_ is words number in sentence\nand _V_ is vocabulary size.\n\nThis text representation is easy to compute, debug, and interpret. It looks\ngood so far but there are potential problems:\n\n  * Vectors are sparse. Each vector contains many 0 s but has one 1. If words have similar meanings, they can group to share dimensions, this vector will deplete space. Also, numerical algorithms do not accept high dimension and sparse vectors in general.\n\n  * A sizeable vocabulary is comparable to high dimensions vectors that are impractical for memory storage and computation.\n\n  * Similar words do not assign with similar vectors resulting in unmeaningful vectors, e.g. _cheese, topping, salami,_ and _pizza_ have related meanings but have unrelated vectors. These vectors depend on corresponding word\u2019s index and assign randomly in vocabulary, indicating that one-hot encoded vectors are incapable to capture semantic relationships and against word vectors\u2019 purpose to answer preceding list concerns.\n\n## 13.4 A Taste of Word Vectors\n\nA word vector is a fixed-size, dense, and real-valued vector. It is a learnt\nrepresentation of text where semantic similar words correspond to similar\nvectors and a solution to preceding problems.\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figa_HTML.png)\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figb_HTML.gif)\n\n|\n\nThis is a 50-dimensional vector for word _the,_ these dimensions have floating\npoints:\n\n1\\. What do dimensions represent?\n\n2\\. These individual dimensions do not have inherent meanings typically but\ninstead they represent vector space locations, and the distance between these\nvectors indicates the similarity of corresponding words\u2019 meanings\n\n3\\. Hence, a word\u2019s meaning is distributed across dimensions\n\n4\\.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9da16b3f-2b5c-4828-b597-86854a258b89": {"__data__": {"id_": "9da16b3f-2b5c-4828-b597-86854a258b89", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc4d2b0b-f3fe-4d60-be8f-d310998632cb", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9bb5c22d52fba151ca2940492787cb7be7ee1aa3a21332e13fff5c55f66606bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22cdf90a-2a95-47c0-b3c8-f12f7396ab15", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "181585f26a9054358b31b9ed914ae5ac4d38dcf17c03cac7bfb45501dc04875a", "class_name": "RelatedNodeInfo"}}, "hash": "6ff039217c9dce92eaa249f14dcda18c581eaec4251fd2448cf913edd677b00c", "text": "## 13.4 A Taste of Word Vectors\n\nA word vector is a fixed-size, dense, and real-valued vector. It is a learnt\nrepresentation of text where semantic similar words correspond to similar\nvectors and a solution to preceding problems.\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figa_HTML.png)\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figb_HTML.gif)\n\n|\n\nThis is a 50-dimensional vector for word _the,_ these dimensions have floating\npoints:\n\n1\\. What do dimensions represent?\n\n2\\. These individual dimensions do not have inherent meanings typically but\ninstead they represent vector space locations, and the distance between these\nvectors indicates the similarity of corresponding words\u2019 meanings\n\n3\\. Hence, a word\u2019s meaning is distributed across dimensions\n\n4\\. This type of word\u2019s meaning representation is called distributional\nsemantics  \n  \n---|---  \n  \nUse word vector visualizer for TensorFlow from (TensorFlow 2022) Google which\noffers word vectors for 10,000 words. Each vector is 200-dimensional and\nprojected into three dimensions for visualization. Let us look at the\nrepresentation of _tennis_ as in Fig. 13.4:\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figc_HTML.gif)\n\n|\n\n_tennis_ is semantically grouped with other sports, i.e. hockey, basketball,\nchess, etc. Words in proximity are calculated by their cosine distances as\nshown in Fig. 13.5  \n  \n---|---  \n  \n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Fig4_HTML.png)\n\nFig. 13.4\n\nVector representation of _tennis_ and semantic similar words\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Fig5_HTML.png)\n\nFig. 13.5\n\n_tennis_ proximity words in three-dimensional space\n\nWord vectors are trained on a large corpus such as Wikipedia which included to\nlearn proper nouns representations, e.g. _Alice_ is a proper noun represented\nby vector as in Fig. 13.6:\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Fig6_HTML.png)\n\nFig. 13.6\n\nVector representation of _alice_\n\nIt showed that all vocabulary input words are in lower cases to avoid multiple\nrepresentations of the same word. _Alice_ and _Bob_ are person names to be\nlisted. In addition, _lewis_ and _carroll_ have relevance to _Alice_ because\n_of the famous literature Alice\u2019s Adventures in Wonderland_ written by _Lewis\nCarroll_. Further, it also showed syntactic category of all neighboring words\nare nouns but not verbs.\n\nWord vectors can capture synonyms, antonyms, and semantic categories such as\nanimals, places, plants, names, and abstract concepts.\n\n## 13.5 Analogies and Vector Operations\n\nWord vectors capture semantics, support vector addition, subtraction, and\nanalogies. A word analogy is a semantic relationship between a pair of words.\nThere are many relationship types such as synonymity, anonymity, and whole-\npart relation. Some example pairs are ( _King\u2014man_ , _Queen\u2014woman_ ), (\n_airplane\u2014air_ , _ship\u2014sea_ ), ( _fish\u2014sea_ , _bird\u2014air_ ), ( _branch\u2014tree_ ,\n_arm\u2014human_ ), ( _forward\u2014backward_ , _absent\u2014present_ ) etc.\n\nFor example, gender mapping represents _Queen_ and _King_ as _Queen\u2014Woman +\nMan = King_. If _woman_ is subtracted by _Queen_ and add _Man_ instead to\nobtain _King_. Then, this analogy interprets queen is attributed to king as\nwoman is attributed to man. Embeddings can generate analogies such as gender,\ntense, and capital city as shown in Fig. 13.7:\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Fig7_HTML.jpg)\n\nFig. 13.7\n\nAnalogies created by word vectors\n\n## 13.6 How to Create Word Vectors?\n\nThere are many ways to produce and pre-trained word vectors:\n\n  1. 1.\n\nword2vec is a name of statistical algorithm created by Google to produce word\nvectors.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22cdf90a-2a95-47c0-b3c8-f12f7396ab15": {"__data__": {"id_": "22cdf90a-2a95-47c0-b3c8-f12f7396ab15", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9da16b3f-2b5c-4828-b597-86854a258b89", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "6ff039217c9dce92eaa249f14dcda18c581eaec4251fd2448cf913edd677b00c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b278d73c-a50e-446c-8179-937d59970111", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "926ce17b02ce2b0c378d0e5125fe8567dd6dc15bf6e1ee4909ee4564e3ac3bf3", "class_name": "RelatedNodeInfo"}}, "hash": "181585f26a9054358b31b9ed914ae5ac4d38dcf17c03cac7bfb45501dc04875a", "text": "For example, gender mapping represents _Queen_ and _King_ as _Queen\u2014Woman +\nMan = King_. If _woman_ is subtracted by _Queen_ and add _Man_ instead to\nobtain _King_. Then, this analogy interprets queen is attributed to king as\nwoman is attributed to man. Embeddings can generate analogies such as gender,\ntense, and capital city as shown in Fig. 13.7:\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Fig7_HTML.jpg)\n\nFig. 13.7\n\nAnalogies created by word vectors\n\n## 13.6 How to Create Word Vectors?\n\nThere are many ways to produce and pre-trained word vectors:\n\n  1. 1.\n\nword2vec is a name of statistical algorithm created by Google to produce word\nvectors. Word vectors are trained with a neural network architecture to\nprocess windows of words and predicts each word vector depending on\nsurrounding words. These pre-trained word vectors can be downloaded from\nSynthetic (2022).\n\n  2. 2.\n\nGlove vectors are invented by Stanford NLP group. This method depends on\nsingular value decomposition used in word co-occurrences matrix. The pre-\ntrained vectors are available at nlp.stanford.edu (Stanford 2022).\n\n  3. 3.\n\nfastText (FastText 2022) was created by Facebook Research like word2vec.\nword2vec predicts words based on their surrounding context, while fastText\npredicts subwords i.e. character N-grams. For example, the word _chair_\ngenerates the following subwords:\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figd_HTML.png)\n\n## 13.7 spaCy Pre-trained Word Vectors\n\nWord vectors are part of many spaCy language models. For instance,\nen_core_web_md model ships with 300-dimensional vectors for 20,000 words,\nwhile en_core_web_lg model ships with 300-dimensional vectors with a 685,000\nwords vocabulary.\n\nTypically, small models (names end with sm) do not include any word vectors\nbut context-sensitive tensors. Semantic similarity calculations can perform\nbut results will not be accurate as word vector computations.\n\nA word\u2019s vector is via token.vector method. Let us look at this method using\ncode query word vector for _banana_ :\n\nIn[1]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Fige_HTML.gif)\n\n|\n\n_# Import_ _spaCy_ _and load the en_core_web_md model_\n\n**import** spacy\n\nnlp = spacy.load(\"en_core_web_md\")\n\n_# Create a sample utterance (utt1)_\n\nutt1 = nlp(\"I ate a banana.\")  \n  \n---|---|---  \n  \nIn[2]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figf_HTML.gif)\n\n|\n\n**import** en_core_web_md\n\nnlp = en_core_web_md.load()  \n  \n---|---|---  \n  \nUse the following script to show Word Vector for _banana_ :\n\nIn[3]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figg_HTML.gif)\n\n|\n\nutt1[3].vector  \n  \n---|---|---  \n  \nOut[3]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figh_HTML.gif)  \n  \n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figi_HTML.gif)\n\n|\n\nIn this example, _token.vector_ returns a NumPy ndarray.\n\nUse the following command to call NumPy methods for result.  \n  \n---|---  \n  \nIn[4]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figj_HTML.gif)\n\n|\n\ntype(utt1[3].vector)  \n  \n---|---|---  \n  \nOut[4]\n\n|\n\nnumpy.ndarray  \n  \nIn[5]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figk_HTML.gif)\n\n|\n\nutt1[3].vector.shape  \n  \n---|---|---  \n  \nOut[5]\n\n|\n\n(300,)  \n  \n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b278d73c-a50e-446c-8179-937d59970111": {"__data__": {"id_": "b278d73c-a50e-446c-8179-937d59970111", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22cdf90a-2a95-47c0-b3c8-f12f7396ab15", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "181585f26a9054358b31b9ed914ae5ac4d38dcf17c03cac7bfb45501dc04875a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de74b1b6-6916-48f3-8b77-0b5c23d44a26", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "48d7c477ef6400a48f7fa098ba00a157b67de53e6d13a4c6542ac77a1cc17e0e", "class_name": "RelatedNodeInfo"}}, "hash": "926ce17b02ce2b0c378d0e5125fe8567dd6dc15bf6e1ee4909ee4564e3ac3bf3", "text": "[](../images/533412_1_En_13_Chapter/533412_1_En_13_Figi_HTML.gif)\n\n|\n\nIn this example, _token.vector_ returns a NumPy ndarray.\n\nUse the following command to call NumPy methods for result.  \n  \n---|---  \n  \nIn[4]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figj_HTML.gif)\n\n|\n\ntype(utt1[3].vector)  \n  \n---|---|---  \n  \nOut[4]\n\n|\n\nnumpy.ndarray  \n  \nIn[5]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figk_HTML.gif)\n\n|\n\nutt1[3].vector.shape  \n  \n---|---|---  \n  \nOut[5]\n\n|\n\n(300,)  \n  \n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figl_HTML.gif)\n\n|\n\nQuery Python type of word vector in this code segment. Then, invoke shape()\nmethod of NumPy array on the vector  \n  \n---|---  \n  \nDoc and Span objects also have vectors. A sentence vector or a span is the\naverage of words\u2019 vectors. Run following code and view results:\n\nIn[6]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figm_HTML.gif)\n\n|\n\n_# Create second utterance (utt2)_\n\nutt2 = nlp(\"I like a banana,\")\n\nutt2.vector\n\nutt2[1:3].vector  \n  \n---|---|---  \n  \nOut[6]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Fign_HTML.gif)  \n  \n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figo_HTML.gif)\n\n|\n\nOnly words in model\u2019s vocabulary have vectors, words not in vocabulary are\ncalled out-of-vocabulary (OOV) words. token.is_oov and token.has_vector are\ntwo methods to query whether a token is in the model\u2019s vocabulary and has a\nword vector  \n  \n---|---  \n  \nIn[7]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figp_HTML.gif)\n\n|\n\n_# Create the utterance 3_\n\nutt3 = nlp(\"You went there afskfsd.\")  \n  \n---|---|---  \n  \nIn[8]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figq_HTML.gif)\n\n|\n\n**for** token **in** utt3:\n\nprint( \"Token is: \",token, \"OOV: \", token.is_oov, \"Token has vector:\" ,\ntoken.has_vector)  \n  \n---|---|---  \n  \nOut[8]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figr_HTML.gif)  \n  \n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figs_HTML.gif)\n\n|\n\nThis is basically how to use spaCy\u2019s pre-trained word vectors. Next, discover\nhow to invoke spaCy\u2019s semantic similarity method on Doc, Span, and Token\nobjects  \n  \n---|---  \n  \n## 13.8 Similarity Method in Semantic Analysis\n\nEvery container type object has a similarity method that allows to calculate\nsemantic similarity of other container objects by comparing word vectors in\nspaCy. Semantic similarity between two container objects is different\ncontainer types. For instance, a Token object to a Doc object and a Doc object\nto a Span object.\n\nThe following example computes two Span objects similarity:\n\nIn[9]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figt_HTML.gif)\n\n|\n\n_# Create utt4 and utt5 and measure the similarity_\n\nutt4 = nlp(\"I visited England.\")\n\nutt5 = nlp(\"I went to London.\")\n\nutt4[1:3].similarity(utt5[1:4])  \n  \n---|---|---  \n  \nOut[9]\n\n|\n\n0.6539691090583801  \n  \nCompare two Token objects, London and England:\n\nIn[10]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figu_HTML.gif)\n\n|\n\nutt4[2]  \n  \n---|---|---  \n  \nOut[10]\n\n|\n\nEngland  \n  \nIn[11]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de74b1b6-6916-48f3-8b77-0b5c23d44a26": {"__data__": {"id_": "de74b1b6-6916-48f3-8b77-0b5c23d44a26", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b278d73c-a50e-446c-8179-937d59970111", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "926ce17b02ce2b0c378d0e5125fe8567dd6dc15bf6e1ee4909ee4564e3ac3bf3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1433fcb-417c-42b0-9d33-480713fb5bdc", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "2a7eb2e96d099dd4299e0e652f007c5b2c6d2eeaabc384b745a40a94ecb71cdf", "class_name": "RelatedNodeInfo"}}, "hash": "48d7c477ef6400a48f7fa098ba00a157b67de53e6d13a4c6542ac77a1cc17e0e", "text": "The following example computes two Span objects similarity:\n\nIn[9]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figt_HTML.gif)\n\n|\n\n_# Create utt4 and utt5 and measure the similarity_\n\nutt4 = nlp(\"I visited England.\")\n\nutt5 = nlp(\"I went to London.\")\n\nutt4[1:3].similarity(utt5[1:4])  \n  \n---|---|---  \n  \nOut[9]\n\n|\n\n0.6539691090583801  \n  \nCompare two Token objects, London and England:\n\nIn[10]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figu_HTML.gif)\n\n|\n\nutt4[2]  \n  \n---|---|---  \n  \nOut[10]\n\n|\n\nEngland  \n  \nIn[11]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figv_HTML.gif)\n\n|\n\nutt4[2].similarity(utt5[3])  \n  \n---|---|---  \n  \nOut[11]\n\n|\n\n0.7389127612113953  \n  \nThe sentence\u2019s similarity is computed by calling similarity() on Doc objects:\n\nIn[12]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figw_HTML.gif)\n\n|\n\nutt4.similarity(utt5)  \n  \n---|---|---  \n  \nOut[12]\n\n|\n\n0.8771558796234277  \n  \n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figx_HTML.gif)\n\n|\n\n1\\. The preceding code segment calculates semantic similarity between two\nsentences _I visited England_ and _I went to London_\n\n2\\. Similarity score is high enough to consider both sentences are similar\n(similarity degree ranges from 0 to 1, 0 represents unrelated and 1 represents\nidentical)  \n  \n---|---  \n  \n_similarity()_ method returns 1 compare an object to itself unsurprisingly:\n\nIn[13]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figy_HTML.gif)\n\n|\n\nutt4.similarity(utt4)  \n  \n---|---|---  \n  \nOut[13]\n\n|\n\n1.0  \n  \nJudge the distance with numbers is complex but review vectors on paper can\nunderstand how vocabulary word groups are formed.\n\nCode snippet below visualizes a vocabulary of two graphical semantic classes.\nThe first word class is for animals and the second class is for food.\n\nIn[14]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figz_HTML.gif)\n\n|\n\n**import** matplotlib.pyplot **as** plt\n\n**from** sklearn.decomposition **import** PCA\n\n**import** numpy **as** np\n\n**import** spacy\n\nnlp = spacy.load( \"en_core_web_md\" )\n\nvocab = nlp( \"cat dog tiger elephant bird monkey lion cheetah burger pizza\nfood cheese wine salad noodles macaroni fruit vegetable\" )\n\nwords = [word.text for word in vocab]  \n  \n---|---|---  \n  \nCreate Word Vector vecs:\n\nIn[15]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figaa_HTML.gif)\n\n|\n\nvecs = np.vstack([word.vector **for** word **in** vocab **if**\nword.has_vector])  \n  \n---|---|---  \n  \nUse PCA (principal component analysis) similarity analysis and plot similarity\nresults with plt class.\n\nIn[16]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figab_HTML.gif)\n\n|\n\npca = PCA(n_components=2)\n\nvecs_transformed = pca.fit_transform(vecs)\n\nplt.figure(figsize=(20,15))\n\nplt.scatter(vecs_transformed[:,0], vecs_transformed[:,1])\n\n**for** word, coord **in** zip(words, vecs_transformed):\n\nx,y = coord\n\nplt.text(x,y,word, size=15)\n\nplt.show()  \n  \n---|---|---  \n  \nOut[16]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figac_HTML.gif)  \n  \n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figad_HTML.gif)\n\n|\n\n1\\. Import matplotlib library to create a graph.\n\n2\\. Next two imports are for vectors calculation.\n\n3\\. Import spaCy and create a nlp object.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1433fcb-417c-42b0-9d33-480713fb5bdc": {"__data__": {"id_": "e1433fcb-417c-42b0-9d33-480713fb5bdc", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de74b1b6-6916-48f3-8b77-0b5c23d44a26", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "48d7c477ef6400a48f7fa098ba00a157b67de53e6d13a4c6542ac77a1cc17e0e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba0b9c0b-76ae-4e1b-b98b-2c5956f89abe", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "6a5632919de6f62ac0a26863dfa3de070e2d59dcbbf1085ff47627e418e44238", "class_name": "RelatedNodeInfo"}}, "hash": "2a7eb2e96d099dd4299e0e652f007c5b2c6d2eeaabc384b745a40a94ecb71cdf", "text": "[](../images/533412_1_En_13_Chapter/533412_1_En_13_Figac_HTML.gif)  \n  \n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figad_HTML.gif)\n\n|\n\n1\\. Import matplotlib library to create a graph.\n\n2\\. Next two imports are for vectors calculation.\n\n3\\. Import spaCy and create a nlp object.\n\n4\\. Create a Doc object from vocabulary.\n\n5\\. Stack word vectors vertically by calling np.vstack.\n\n6\\. Project vectors into a two-dimensional space for visualization since they\nare 300-dimensional. Extract two principal components via principal component\nanalysis (PCA) for projection.\n\n7\\. Create a scatter plot for rest of the code to deal with matplotlib\nfunction calls.  \n  \n---|---  \n  \nIt showed that spaCy word vectors can visualize two semantic classes that are\ngrouped. The distance between animals is reduced and uniformly distributed,\nwhile food class formed groups within the group.\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figae_HTML.gif)\n\n|\n\n**Workshop 4.1 Word Vector Analysis on The Adventures of Sherlock Holmes**\n\nIn this workshop, we have just learnt how to use spaCy to produce word vector\nto compare the similarity of two text objects/document. Try to use _The\nAdventures of Sherlock Holmes_ (Doyle 2019; Gutenberg 2022) to select two\n\"presentative\" texts from this detective story:\n\n1\\. Read Adventures_Holmes.txt text file.\n\n2\\. Save contents into a string object \"holmes_doc\".\n\n3\\. Plot Semantic Graphs for these two texts.\n\n4\\. Perform Similarity text for these two documents. See what are found.  \n  \n---|---  \n  \n## 13.9 Advanced Semantic Similarity Methods with spaCy\n\nIt has learnt that spaCy\u2019s similarity method can calculate semantic similarity\nto obtain scores but there are advanced semantic similarity methods to\ncalculate words, phrases, and sentences similarity.\n\n### 13.9.1 Understanding Semantic Similarity\n\nIt is necessary to identify example characteristics when collecting data or\ntext data (any sort of data), i.e. calculate two text similarity scores.\nSemantic similarity is a metric to define the distance between texts based on\nsemantics texts.\n\nA mathematics is basically a distance function. Every metric induces a\ntopology on vector space. Word vectors are vectors that can be used to\ncalculate the distance between them as a similarity score.\n\nThere are two commonly used distance functions: (1) Euclidian distance and (2)\ncosine distance.\n\n### 13.9.2 Euclidian Distance\n\nEuclidian distance counts on vector magnitude and disregards orientation. If a\nvector is drawn from an origin, let us call it a _dog_ vector to another\npoint, call a _cat_ vector and subtract one vector from and other, the\ndistance represents the magnitude of vectors is shown in Fig. 13.8.\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Fig8_HTML.png)\n\nFig. 13.8\n\nEuclidian distance between two vectors: _dog_ and _cat_\n\nIf two or more semantically similar words ( _canine, terrier_ ) to _dog_ and\nmake it a text of three words, i.e. _dog canine terrier_. Obviously, the _dog_\nvector will now grow in magnitude, possibly in the same direction. This time,\nthe distance will be much bigger due to geometry, although the semantics of\nfirst piece of text (now _dog canine terrier_ ) remain the same.\n\nThis is the main drawback of using Euclidian distance for semantic similarity\nas the orientation of two vectors in the space is not considered. Figure 13.9\nillustrates the distance between _dog_ and _cat_ , and the distance between\n_dog canine terrier_ and _cat_.\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Fig9_HTML.png)\n\nFig. 13.9\n\nDistance between _dog_ and _cat_ , as well as the distance between _dog canine\nterrier_ and _cat_\n\nHow can we fix this problem? There is another way of calculating similarity\ncalled cosine similarity to address this problem.\n\n### 13.9.3 Cosine Distance and Cosine Similarity\n\nContrary to Euclidian distance, cosine distance is more concerned with the\norientation of two vectors in the space.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba0b9c0b-76ae-4e1b-b98b-2c5956f89abe": {"__data__": {"id_": "ba0b9c0b-76ae-4e1b-b98b-2c5956f89abe", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1433fcb-417c-42b0-9d33-480713fb5bdc", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "2a7eb2e96d099dd4299e0e652f007c5b2c6d2eeaabc384b745a40a94ecb71cdf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d976fc42-f398-47cd-a442-46abf33022a6", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "4c5a47f432639fa01f9fbbefa1fa3ef97dc725eac0324434455e9206576e6542", "class_name": "RelatedNodeInfo"}}, "hash": "6a5632919de6f62ac0a26863dfa3de070e2d59dcbbf1085ff47627e418e44238", "text": "This is the main drawback of using Euclidian distance for semantic similarity\nas the orientation of two vectors in the space is not considered. Figure 13.9\nillustrates the distance between _dog_ and _cat_ , and the distance between\n_dog canine terrier_ and _cat_.\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Fig9_HTML.png)\n\nFig. 13.9\n\nDistance between _dog_ and _cat_ , as well as the distance between _dog canine\nterrier_ and _cat_\n\nHow can we fix this problem? There is another way of calculating similarity\ncalled cosine similarity to address this problem.\n\n### 13.9.3 Cosine Distance and Cosine Similarity\n\nContrary to Euclidian distance, cosine distance is more concerned with the\norientation of two vectors in the space. The cosine similarity of two vectors\nis basically the cosine of angle created by these two vectors. Figure 13.10\nshows the angle between _dog_ and _cat_ vectors:\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Fig10_HTML.png)\n\nFig. 13.10\n\nThe angle between _dog_ and _cat_ vectors. Here, the semantic similarity is\ncalculated by cos(\u03b8)\n\nThe maximum similarity score that is allowed by cosine similarity is 1. This\nis obtained when the angle between two vectors is 0 degree (hence, the vectors\ncoincide). The similarity between two vectors is 0 when the angle between them\nis 90 degrees.\n\nCosine similarity provides scalability when vectors grow in magnitude. If one\nof the input vectors is expanded as in Fig. 13.10, the angle between them\nremains the same and so as the cosine similarity score.\n\nNote that here is to calculate semantic similarity score and not distance. The\nhighest possible value is 1 when vectors coincide, while the lowest score is 0\nwhen two vectors are perpendicular. The cosine distance is 1 \u2013 cos( _\u03b8_ )\nwhich is a distance function.\n\nspaCy uses cosine similarity to calculate semantic similarity. Hence, calling\nthe similarity method helps to perform cosine similarity calculations.\n\nSo far, we have learnt to calculate similarity scores, but still have not\ndiscovered words meaning. Obviously, not all words in a sentence have the same\nimpact on the semantics of sentence. The similarity method will only calculate\nthe semantic similarity score, but the right keywords are required for\ncalculation results comparison.\n\nConsider the following text snippet:\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figaf_HTML.png)\n\nIf is interested in finding the biggest mammals on the planet, the phrases\n_biggest mammals_ and _in the world_ will be keywords. By comparing these\nphrases with the search phrases _largest mammals_ and _on the planet_ should\ngive a high similarity score. But if is interested in finding out about places\nin the world, _California_ will be keyword. _California_ is semantically like\nword _geography_ and more suitably, the entity type is a geographical noun.\n\nSince we have learnt how to calculate similarity score, the next section will\nlearn about where to look for the meaning. It will cover a case study on text\ncategorization before improving task results via key phrase extraction with\nsimilarity score calculations.\n\n### 13.9.4 Categorizing Text with Semantic Similarity\n\nDetermining two sentences\u2019 semantic similarity can categorize texts into\npredefined categories or spot only the relevant texts. This case study will\nfilter users\u2019 comments in an e-commerce website related to the word _perfume_.\nSuppose to evaluate the following comments:\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figag_HTML.png)\n\nHere, it is noted that only the second sentence is related. This is because it\ncontains the word _fragrance_ and adjectives describing scents. To understand\nwhich sentences are related, can try several comparison strategies.\n\nTo start, compare _perfume_ to each sentence. Recall that spaCy generates a\nword vector for a sentence by averaging the word vector of its tokens. The\nfollowing code snippet compares preceding sentences to _perfume_ search key:\n\nIn[17]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d976fc42-f398-47cd-a442-46abf33022a6": {"__data__": {"id_": "d976fc42-f398-47cd-a442-46abf33022a6", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba0b9c0b-76ae-4e1b-b98b-2c5956f89abe", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "6a5632919de6f62ac0a26863dfa3de070e2d59dcbbf1085ff47627e418e44238", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "753d27f6-feb2-4496-9ae7-0b1726d4670c", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "66d54288316b92341d124daee4964c42e6e22caffbb1ded4a001e6650a4c2d2f", "class_name": "RelatedNodeInfo"}}, "hash": "4c5a47f432639fa01f9fbbefa1fa3ef97dc725eac0324434455e9206576e6542", "text": "This case study will\nfilter users\u2019 comments in an e-commerce website related to the word _perfume_.\nSuppose to evaluate the following comments:\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figag_HTML.png)\n\nHere, it is noted that only the second sentence is related. This is because it\ncontains the word _fragrance_ and adjectives describing scents. To understand\nwhich sentences are related, can try several comparison strategies.\n\nTo start, compare _perfume_ to each sentence. Recall that spaCy generates a\nword vector for a sentence by averaging the word vector of its tokens. The\nfollowing code snippet compares preceding sentences to _perfume_ search key:\n\nIn[17]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figah_HTML.gif)\n\n|\n\nutt6 = nlp( \"I purchased a science fiction book last week. I loved everything\nrelated to this fragrance: light, floral and feminine... I purchased a bottle\nof wine. \" )\n\nkey = nlp( \"perfume\" )\n\n**for** utt **in** utt6.sents:\n\nprint(utt.similarity(key))  \n  \n---|---|---  \n  \nOut[17]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figai_HTML.gif)  \n  \nThe following steps are performed:\n\nCreate a Doc object with three preceding sentences. For each sentence,\ncalculate similarity score with _perfume_ and print the score by invoking\nsimilarity() method on the sentence. The degree of similarity between\n_perfume_ and the first sentence is minute, indicating that this sentence is\nirrelevant to the search key. The second sentence looks relevant which means\nthat semantic similarity is correctly identified.\n\nHow about the third sentence? The script identified that the third sentence is\nrelevant somehow, most probably because it includes the word _bottle_ and\nperfumes are sold in bottles. The word _bottle_ appears in similar contexts\nwith the word _perfume_. For this reason, the similarity score of this\nsentence and search key is not small enough; also, the scores of second and\nthird sentences are not distant enough to make the second sentence\nsignificant.\n\nIn practice, long texts such as web documents can be dealt with but averaging\nover them diminish the importance of keywords.\n\nLet us look at how to identify key phrases in a sentence to improve\nperformance.\n\n### 13.9.5 Extracting Key Phrases\n\nSemantic categorization is more effectively to extract important word phrases\nand compare them to the search key. Instead of comparing the key to different\nparts of speech, we can compare the key to noun phrases. Noun phrases are\nsubjects, direct objects, and indirect objects of sentences that convey high\npercentages of sentences semantics.\n\nFor example, in sentence _Blue whales live in California_ , focuses will\nlikely be on _blue whales, whales, California,_ or _whales in California_.\n\nSimilarly, in the preceding sentence about _perfume_ , the focused is to pick\nout _fragrance_ the noun. Different semantic tasks may need other context\nwords such as verbs to decide what the sentence is about, but for semantic\nsimilarity, noun phrases convey significant weights.\n\nWhat is a noun phrase? A noun phrase (NP) is a group of words that consist of\na noun and its modifiers. Modifiers are usually pronouns, adjectives, and\ndeterminers. The following phrases are noun phrases:\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figaj_HTML.png)\n\nspaCy extracts noun phases by parsing the output of dependency parser. It can\nidentify noun phrases of a sentence by using doc.noun_chunks method:\n\nIn[18]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figak_HTML.gif)\n\n|\n\nutt7 = nlp( \"My beautiful and cute dog jumped over the fence\" )  \n  \n---|---|---  \n  \nIn[19]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figal_HTML.gif)\n\n|\n\nutt7.noun_chunks  \n  \n---|---|---  \n  \nOut[19]\n\n|\n\n<generator at 0x1932f2de900>  \n  \nIn[20]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "753d27f6-feb2-4496-9ae7-0b1726d4670c": {"__data__": {"id_": "753d27f6-feb2-4496-9ae7-0b1726d4670c", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d976fc42-f398-47cd-a442-46abf33022a6", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "4c5a47f432639fa01f9fbbefa1fa3ef97dc725eac0324434455e9206576e6542", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a763ca33-232b-4b6c-9631-3c007080beb4", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "1926b16ad949e8776cf5344cb2e2b81a53c8189295df0a6b0d52004f4b849f7d", "class_name": "RelatedNodeInfo"}}, "hash": "66d54288316b92341d124daee4964c42e6e22caffbb1ded4a001e6650a4c2d2f", "text": "[](../images/533412_1_En_13_Chapter/533412_1_En_13_Figaj_HTML.png)\n\nspaCy extracts noun phases by parsing the output of dependency parser. It can\nidentify noun phrases of a sentence by using doc.noun_chunks method:\n\nIn[18]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figak_HTML.gif)\n\n|\n\nutt7 = nlp( \"My beautiful and cute dog jumped over the fence\" )  \n  \n---|---|---  \n  \nIn[19]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figal_HTML.gif)\n\n|\n\nutt7.noun_chunks  \n  \n---|---|---  \n  \nOut[19]\n\n|\n\n<generator at 0x1932f2de900>  \n  \nIn[20]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figam_HTML.gif)\n\n|\n\nlist(utt7.noun_chunks)  \n  \n---|---|---  \n  \nOut[20]\n\n|\n\n[My beautiful and cute dog, the fence]  \n  \nLet us modify the preceding code snippet. Instead of comparing the search key\nperfume to the entire sentence, this time will only compare it with sentence\u2019s\nnoun chunks:\n\nIn[21]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figan_HTML.gif)\n\n|\n\n**for** utt **in** utt7.sents:\n\nnchunks = [nchunk.text **for** nchunk **in** utt.noun_chunks]\n\nnchunk_utt = nlp(\" \".join(nchunks))\n\nprint(nchunk_utt.similarity(key))  \n  \n---|---|---  \n  \nOut[21]\n\n|\n\n0.27409999728254997  \n  \n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figao_HTML.gif)\n\n|\n\nThe following is performed for the preceding code:\n\n1\\. Iterate over sentences\n\n2\\. Extract noun chunks and store them in a Python list for each sentence\n\n3\\. Join noun chunks in the list into a Python string and convert it into a\nDoc object\n\n4\\. Compare this Doc object of noun chunks to search key _perfume_ to\ndetermine semantic similarity scores  \n  \n---|---  \n  \nIf these scores are compared with previous scores, it is noted that that the\nfirst sentence remains irrelevant, so its score decreased marginally but the\nsecond sentence\u2019s score increased significantly. Also, the second and third\nsentences scores are distant from each other to reflect that second sentence\nis the most related sentence.\n\n### 13.9.6 Extracting and Comparing Named Entities\n\nIn some cases, it can focus to extract proper nouns instead of every noun.\nHence, it is required to extract named entities. Let us compare the following\nparagraphs:\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figap_HTML.png)\n\nThe codes should be able to recognize that first two paragraphs are about\nlarge technology companies and their products, whereas the third paragraph is\nabout a geographic location.\n\nComparing all noun phrases in these sentences may not be helpful because many\nof them such as volume are irrelevant to categorization. The topics of these\nparagraphs are determined by phrases within them, that is, _Google Search,\nGoogle, Microsoft Bing, Microsoft, Windows, Dead Sea, Jordan Valley,_ and\n_Israel_. spaCy can identify these entities:\n\nIn[22]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figaq_HTML.gif)\n\n|\n\nutt8 = nlp( \"Google Search, often referred as Google, is the most popular\nsearch engine nowadays. It answers a huge volume of queries every day.\" )\n\nutt9 = nlp( \"Microsoft Bing is another popular search engine. Microsoft is\nknown by its star product Microsoft Windows, a popular operating system sold\nover the world.\" )\n\nutt10 = nlp( \"The Dead Sea is the lowest lake in the world, located in the\nJordan Valley of Israel. It is also the saltiest lake in the world.\" )  \n  \n---|---|---  \n  \nIn[23]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figar_HTML.gif)\n\n|\n\nutt8.ents  \n  \n---|---|---  \n  \nOut[23]\n\n|\n\n(Google Search, Google, every day)  \n  \nIn[24]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a763ca33-232b-4b6c-9631-3c007080beb4": {"__data__": {"id_": "a763ca33-232b-4b6c-9631-3c007080beb4", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "753d27f6-feb2-4496-9ae7-0b1726d4670c", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "66d54288316b92341d124daee4964c42e6e22caffbb1ded4a001e6650a4c2d2f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "550dc4b8-d885-43d0-bef9-696742bf0089", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9ad7be411c5292c8a7b141ae5a922450c25963a7c752e05ede4ba7ae6c8bd335", "class_name": "RelatedNodeInfo"}}, "hash": "1926b16ad949e8776cf5344cb2e2b81a53c8189295df0a6b0d52004f4b849f7d", "text": "It answers a huge volume of queries every day.\" )\n\nutt9 = nlp( \"Microsoft Bing is another popular search engine. Microsoft is\nknown by its star product Microsoft Windows, a popular operating system sold\nover the world.\" )\n\nutt10 = nlp( \"The Dead Sea is the lowest lake in the world, located in the\nJordan Valley of Israel. It is also the saltiest lake in the world.\" )  \n  \n---|---|---  \n  \nIn[23]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figar_HTML.gif)\n\n|\n\nutt8.ents  \n  \n---|---|---  \n  \nOut[23]\n\n|\n\n(Google Search, Google, every day)  \n  \nIn[24]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figas_HTML.gif)\n\n|\n\nutt9.ents  \n  \n---|---|---  \n  \nOut[24]\n\n|\n\n(Microsoft Bing, Microsoft, Microsoft Windows)  \n  \nIn[25]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figat_HTML.gif)\n\n|\n\nutt10.ents  \n  \n---|---|---  \n  \nOut[25]\n\n|\n\n(the Jordan Valley, Israel)  \n  \nSince words are extracted for comparison, let\u2019s calculate similarity scores:\n\nIn[26]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figau_HTML.gif)\n\n|\n\nents1 = [ent.text **for** ent **in** utt8.ents]\n\nents2 = [ent.text **for** ent **in** utt9.ents]\n\nents3 = [ent.text **for** ent **in** utt10.ents]\n\nents1 = nlp(\" \".join(ents1))\n\nents2 = nlp(\" \".join(ents2))\n\nents3 = nlp(\" \".join(ents3))  \n  \n---|---|---  \n  \nIn[27]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figav_HTML.gif)\n\n|\n\nents1.similarity(ents2)  \n  \n---|---|---  \n  \nOut[27]\n\n|\n\n0.5394545341415748  \n  \nIn[28]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figaw_HTML.gif)\n\n|\n\nents1.similarity(ents3)  \n  \n---|---|---  \n  \nOut[28]\n\n|\n\n0.48605042335384385  \n  \nIn[29]\n\n|\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figax_HTML.gif)\n\n|\n\nents2.similarity(ents3)  \n  \nOut[29]\n\n|\n\n0.39674953175052086  \n  \n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figay_HTML.gif)\n\n|\n\nThese figures revealed that the highest level of similarity exists between\nfirst and second paragraphs, which are both about large tech companies. The\nthird paragraph is unlike other paragraphs. How can this calculation be\nobtained by using word vectors only? It is probably because words _Google_ and\n_Microsoft_ often appear together in news and other social media text corpora,\nhence producing similar word vectors  \n  \n---|---  \n  \nThis is the conclusion of advanced semantic similarity methods section with\ndifferent ways to combine word vectors with linguistic features such as key\nphrases and named entities.\n\n![](../images/533412_1_En_13_Chapter/533412_1_En_13_Figaz_HTML.gif)\n\n|\n\n**Workshop 4.2 Further** **Semantic Analysis** **on The Adventures of Sherlock\nHolmes**\n\nIt has learnt to further improve semantic Analysis results on document\nsimilarity comparison by extracting (1) key phrases; (2) and comparing names\nentities. Try to use these techniques on _The Adventures of Sherlock Holmes_ :\n\n1\\. Extract three \"representative texts\" from this novel\n\n2\\. Perform key phrases extraction to improve the similarity rate as compared\nwith Workshop 4.1 results\n\n3\\. Extract and compare name entities to identify significant name entities\nfrom this literature to further improve semantic analysis performance\n\n4\\. Remember to plot semantic diagram to show how these entities and keywords\nare related\n\n5\\. Discuss and explain what are found  \n  \n---|---  \n  \nReferences\n\n  1. Albrecht, J., Ramachandran, S. and Winkler, C.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "550dc4b8-d885-43d0-bef9-696742bf0089": {"__data__": {"id_": "550dc4b8-d885-43d0-bef9-696742bf0089", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a763ca33-232b-4b6c-9631-3c007080beb4", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "1926b16ad949e8776cf5344cb2e2b81a53c8189295df0a6b0d52004f4b849f7d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "08a73a94-cbec-4e2f-892b-6e676479116d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f570a93a84f4942ca09e0d840c1753f1ea882c7630ee982fa16295bc2a4c718d", "class_name": "RelatedNodeInfo"}}, "hash": "9ad7be411c5292c8a7b141ae5a922450c25963a7c752e05ede4ba7ae6c8bd335", "text": "Try to use these techniques on _The Adventures of Sherlock Holmes_ :\n\n1\\. Extract three \"representative texts\" from this novel\n\n2\\. Perform key phrases extraction to improve the similarity rate as compared\nwith Workshop 4.1 results\n\n3\\. Extract and compare name entities to identify significant name entities\nfrom this literature to further improve semantic analysis performance\n\n4\\. Remember to plot semantic diagram to show how these entities and keywords\nare related\n\n5\\. Discuss and explain what are found  \n  \n---|---  \n  \nReferences\n\n  1. Albrecht, J., Ramachandran, S. and Winkler, C. (2020) Blueprints for Text Analytics Using Python: Machine Learning-Based Solutions for Common Real World (NLP) Applications. O\u2019Reilly Media.\n\n  2. Altinok, D. (2021) Mastering spaCy: An end-to-end practical guide to implementing NLP applications using the Python ecosystem. Packt Publishing.\n\n  3. Arumugam, R., & Shanmugamani, R. (2018). Hands-on natural language processing with python. Packt Publishing.\n\n  4. Bird, S., Klein, E., and Loper, E. (2009). Natural language processing with python. O'Reilly.[zbMATH](http://www.emis.de/MATH-item?1187.68630)\n\n  5. Doyle, A. C. (2019) The Adventures of Sherlock Holmes (AmazonClassics Edition). AmazonClassics.\n\n  6. FastText (2022) FastText official site. [https://\u200bfasttext.\u200bcc/\u200b](https://fasttext.cc/). Accessed 22 June 2022.\n\n  7. Gutenberg (2022) Project Gutenberg official site. [https://\u200bwww.\u200bgutenberg.\u200borg/\u200b](https://www.gutenberg.org/) Accessed 16 June 2022.\n\n  8. Hardeniya, N., Perkins, J. and Chopra, D. (2016) Natural Language Processing: Python and NLTK. Packt Publishing.\n\n  9. Kedia, A. and Rasu, M. (2020) Hands-On Python Natural Language Processing: Explore tools and techniques to analyze and process text with a view to building real-world NLP applications. Packt Publishing.\n\n  10. NLTK (2022) NLTK official site. [https://\u200bwww.\u200bnltk.\u200borg/\u200b](https://www.nltk.org/). Accessed 16 June 2022.\n\n  11. Perkins, J. (2014). Python 3 text processing with NLTK 3 cookbook. Packt Publishing Ltd.\n\n  12. SpaCy (2022) spaCy official site. [https://\u200bspacy.\u200bio/\u200b](https://spacy.io/). Accessed 16 June 2022.\n\n  13. Srinivasa-Desikan, B. (2018). Natural language processing and computational linguistics: A practical guide to text analysis with python, gensim, SpaCy, and keras. Packt Publishing Limited.\n\n  14. Stanford (2022) NLP.stanford.edu Glove official site. [https://\u200bnlp.\u200bstanford.\u200bedu/\u200bprojects/\u200bglove/\u200b](https://nlp.stanford.edu/projects/glove/). Accessed 22 June 2022.\n\n  15. Synthetic (2022) Synthetic Intelligent Network site on Word2Vec Model. [https://\u200bdeveloper.\u200bsyn.\u200bco.\u200bin/\u200btutorial/\u200bbot/\u200boscova/\u200bpretrained-vectors.\u200bhtml#word2vec-and-glove-models](https://developer.syn.co.in/tutorial/bot/oscova/pretrained-vectors.html#word2vec-and-glove-models). Accessed 22 June 2022.\n\n  16. TensorFlow (2022) TensorFlow official site. [https://\u200bprojector.\u200btensorflow.\u200borg/\u200b](https://projector.tensorflow.org/). Accessed 22 June 2022.\n\n  17. Vasiliev, Y. (2020) Natural Language Processing with Python and spaCy: A Practical Introduction. No Starch Press.\n\n\n\u00a9 The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "08a73a94-cbec-4e2f-892b-6e676479116d": {"__data__": {"id_": "08a73a94-cbec-4e2f-892b-6e676479116d", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "550dc4b8-d885-43d0-bef9-696742bf0089", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9ad7be411c5292c8a7b141ae5a922450c25963a7c752e05ede4ba7ae6c8bd335", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2df5ffbc-5c79-4aa6-bd66-f7cc6a2e5967", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8e185f018d03938ce111ac8cf7eb1093b2c430c88c999593fc1efbc2b57c7271", "class_name": "RelatedNodeInfo"}}, "hash": "f570a93a84f4942ca09e0d840c1753f1ea882c7630ee982fa16295bc2a4c718d", "text": "15. Synthetic (2022) Synthetic Intelligent Network site on Word2Vec Model. [https://\u200bdeveloper.\u200bsyn.\u200bco.\u200bin/\u200btutorial/\u200bbot/\u200boscova/\u200bpretrained-vectors.\u200bhtml#word2vec-and-glove-models](https://developer.syn.co.in/tutorial/bot/oscova/pretrained-vectors.html#word2vec-and-glove-models). Accessed 22 June 2022.\n\n  16. TensorFlow (2022) TensorFlow official site. [https://\u200bprojector.\u200btensorflow.\u200borg/\u200b](https://projector.tensorflow.org/). Accessed 22 June 2022.\n\n  17. Vasiliev, Y. (2020) Natural Language Processing with Python and spaCy: A Practical Introduction. No Starch Press.\n\n\n\u00a9 The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.\n2024\n\nR. S. T. Lee Natural Language Processing\n<https://doi.org/10.1007/978-981-99-1999-4_14>\n\n# 14\\. Workshop#5 Sentiment Analysis and Text Classification with LSTM Using\nspaCy (Hour 9\u201310)\n\nRaymond S. T. Lee 1\n\n(1)\n\nUnited International College, Beijing Normal University-Hong Kong Baptist\nUniversity, Zhuhai, China\n\n## 14.1 Introduction\n\nNLTK and spaCy are two major NLP Python implementation tools for basic text\nprocessing, N-gram modeling, POS tagging, and semantic analysis introduced in\nlast four workshops. Workshop 5 will explore how to position these NLP\nimplementation techniques into two important NLP applications: text\nclassification and sentiment analysis. TensorFlow and Kera are two vital\ncomponents to implement Long-Short Term Memory networks (LSTM networks), a\ncommonly used Recurrent Neural Networks (RNN) on machine learning especially\nin NLP applications.\n\nThis workshop will:\n\n  1. 1.\n\nstudy text classification concepts in NLP and how spaCy NLP pipeline works on\ntext classifier training.\n\n  2. 2.\n\nuse movie reviews as a problem domain to demonstrate how to implement\nsentiment analysis with spaCy.\n\n  3. 3.\n\nintroduce Artificial Neural Networks (ANN) concepts, TensorFlow, and Kera\ntechnologies.\n\n  4. 4.\n\nintroduce sequential modeling scheme with LSTM technology using movie reviews\ndomain as example to integrate these technologies for text classification and\nmovie sentiment analysis.\n\n## 14.2 Text Classification with spaCy and LSTM Technology\n\nText classification is a vital component in sentiment analysis application.\n\n_TextCategorizer_ is a spaCy\u2019s text classifier component applied in dataset\nfor sentiment analysis to perform text classification with two vital Python\nframeworks: (1) TensorFlow Keras API and (2) spaCy technology.\n\nSequential data modelling with LSTM technology is used to process text for\nmachine learning tasks with Keras\u2019s text preprocessing module and implement a\nneural network with tf.keras.\n\nThis workshop will cover the following key topics:\n\n  * Basic concept and knowledge of text classification.\n\n  * Model training of spaCy text classifier.\n\n  * Sentiment Analysis with spaCy.\n\n  * Sequential modeling with LSTM Technology.\n\n## 14.3 Technical Requirements\n\nCodes for training spaCy text classifier and sentiment analysis are spaCy v3.0\ncompatible. Text classification with spaCy and Keras requires Python libraries\nas follows:\n\n  * TensorFlow (version 2.3 or above)\n\n  * NumPy\n\n  * pandas\n\n  * Matplotlib\n\nIf these packages are not installed into PC/notebook, use _pip install xxx_\ncommand.\n\n## 14.4 Text Classification in a Nutshell\n\n### 14.4.1 What Is Text Classification?\n\nText Classification (Albrecht et al. 2020; Bird et al. 2009; George 2022;\nSarkar 2019; Siahaan and Sianipar 2022; Srinivasa-Desikan 2018) is the task of\nassigning a set of predefined labels to text.\n\nThey are classified by manual tagging, but machine learning techniques are\napplied progressively to train classification system with known examples, or\ntrain samples to classify unseen cases. It is a fundamental task of NLP\n(Perkins 2014; Sarkar 2019) using various machine learning method such as LSTM\ntechnology (Arumugam and Shanmugamani 2018; G\u00e9ron 2019; Kedia and Rasu 2020).\n\nText classification types are (Agarwal 2020; George 2022; Pozzi et al.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2df5ffbc-5c79-4aa6-bd66-f7cc6a2e5967": {"__data__": {"id_": "2df5ffbc-5c79-4aa6-bd66-f7cc6a2e5967", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "08a73a94-cbec-4e2f-892b-6e676479116d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f570a93a84f4942ca09e0d840c1753f1ea882c7630ee982fa16295bc2a4c718d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6479543-c9e2-4ed0-94ec-050bffee8255", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "a689446c291c6db65de9b63ebdb19ae58166226431636a14cde150561fd0013b", "class_name": "RelatedNodeInfo"}}, "hash": "8e185f018d03938ce111ac8cf7eb1093b2c430c88c999593fc1efbc2b57c7271", "text": "Text Classification (Albrecht et al. 2020; Bird et al. 2009; George 2022;\nSarkar 2019; Siahaan and Sianipar 2022; Srinivasa-Desikan 2018) is the task of\nassigning a set of predefined labels to text.\n\nThey are classified by manual tagging, but machine learning techniques are\napplied progressively to train classification system with known examples, or\ntrain samples to classify unseen cases. It is a fundamental task of NLP\n(Perkins 2014; Sarkar 2019) using various machine learning method such as LSTM\ntechnology (Arumugam and Shanmugamani 2018; G\u00e9ron 2019; Kedia and Rasu 2020).\n\nText classification types are (Agarwal 2020; George 2022; Pozzi et al. 2016):\n\n  * Language detection is the first step of many NLP systems, i.e. machine translation.\n\n  * Topic generation and detection are the process of summarization, or classification of a batch of sentences, paragraphs, or texts into certain Topic of Interest (TOI) or topic titles, e.g. customers\u2019 email request refund or complaints about products or services.\n\n  * Sentiment analysis to classify or analyze users\u2019 responses, comments, and messages on a particular topic attribute to positive, neutral, or negative sentiments. It is an essential task in e-commerce and social media platforms.\n\nText classifiers can emphasize on overall text sentiments, text language\ndetection, and words levels, i.e. verbs. A text classifier of a customer\nservice automation system is shown in Fig. 14.1.\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Fig1_HTML.png)\n\nFig. 14.1\n\nExample of top detection for customer complaint in CSAS (Customer Service\nAutomation System)\n\n### 14.4.2 Text Classification as AI Applications\n\nText classification is considered as Supervised Learning (SL) task in AI which\nmeans that the classifier can predict class label of a text based on sample\ninput text-class label pairs. It must require sufficient input (text)-output\n(classified labels) pairs databank for network training, testing, and\nvalidation. Hence, a labeled dataset is a list of text-label pairs required to\ntrain a text classifier. An example dataset of five training sentences with\nsentiment labels is shown in Fig. 14.2.\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Fig2_HTML.png)\n\nFig. 14.2\n\nSample input texts and their corresponding output class labels\n\nWhen a classifier encounters a new text which is not in the training text, it\npredicts a class label of this unseen text based on examples during training\nphase to induce a text classifier output is always a class label.\n\nText classification can also be divided into (1) binary, (2) multi-class, and\n(3) multi-label categories:\n\n  1. 1.\n\nBinary text classification refers to categorize text with two classes.\n\n  2. 2.\n\nMulti-class text classification refers to categorize texts with more than two\nclasses. Each class is mutually exclusive where one text is associated with\nsingle class, e.g. rating customer reviews are represented by 1\u20135 stars\ncategory single class label.\n\n  3. 3.\n\nMulti-label text classification system is to generalize its multi-class\ncounterpart assigned to each example text e.g. _toxic, severe toxic, insult,\nthreat, obscenity_ levels of negative sentiment. What are Labels in Text\nClassification?\n\nLabels are class names for output. A class label can be categorical (string)\nor numerical (a number).\n\nText classification has the following class labels:\n\n  * Sentiment analysis has positive and negative class labels abbreviated by pos and neg where 0 represents negative sentiment and 1 represents positive sentiment. Binary class labels are popular as well.\n\n  * The identical numeric representation applies to binary classification problems, i.e. use 0\u20131 for class labels.\n\n  * Class labeled with a meaningful name for multi-class and multi-label problems, e.g. movie genre classifier has labels _action, scifi, weekend, Sunday movie,_ etc. Numbers are labels for a five-class classification problem, i.e. 1\u20135.\n\n## 14.5 Text Classifier with spaCy NLP Pipeline\n\n_TextCategorizer_ ( _tCategorizer_ ) is spaCy\u2019s text classifier component\n(Altinok 2021; SpaCy 2022; Vasiliev 2020).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6479543-c9e2-4ed0-94ec-050bffee8255": {"__data__": {"id_": "b6479543-c9e2-4ed0-94ec-050bffee8255", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2df5ffbc-5c79-4aa6-bd66-f7cc6a2e5967", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8e185f018d03938ce111ac8cf7eb1093b2c430c88c999593fc1efbc2b57c7271", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "afca20e9-5413-48ad-b52c-684eb8684299", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9a923a59e71a7747869bba9ce6ecd03e77156ac2a107a8f9ed39e8dbf2adb796", "class_name": "RelatedNodeInfo"}}, "hash": "a689446c291c6db65de9b63ebdb19ae58166226431636a14cde150561fd0013b", "text": "A class label can be categorical (string)\nor numerical (a number).\n\nText classification has the following class labels:\n\n  * Sentiment analysis has positive and negative class labels abbreviated by pos and neg where 0 represents negative sentiment and 1 represents positive sentiment. Binary class labels are popular as well.\n\n  * The identical numeric representation applies to binary classification problems, i.e. use 0\u20131 for class labels.\n\n  * Class labeled with a meaningful name for multi-class and multi-label problems, e.g. movie genre classifier has labels _action, scifi, weekend, Sunday movie,_ etc. Numbers are labels for a five-class classification problem, i.e. 1\u20135.\n\n## 14.5 Text Classifier with spaCy NLP Pipeline\n\n_TextCategorizer_ ( _tCategorizer_ ) is spaCy\u2019s text classifier component\n(Altinok 2021; SpaCy 2022; Vasiliev 2020). It required class labels and\nexamples in NLP pipeline to perform training procedure as shown in Fig. 14.3.\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Fig3_HTML.png)\n\nFig. 14.3\n\n_TextCategorizer_ in the spaCy NLP pipeline\n\n_TextCategorizer_ provides user-friendly and end-to-end approaches to train\nclassifier so that it does not need to deal with neural network architecture\ndirectly.\n\n### 14.5.1 TextCategorizer Class\n\nImport spaCy and load nlp component from \"en_core_web_md\":\n\nIn[1]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figa_HTML.gif)\n\n|\n\n_# Load and import spacy package_\n\n**import** spacy\n\n_# Load the en_core_web_md module_\n\nnlp = spacy.load( \"en_core_web_md\" )  \n  \n---|---|---  \n  \nImport _TextCategorizer_ from spaCy pipeline components:\n\nIn[2]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figb_HTML.gif)\n\n|\n\n# Import the Single Text Categorizer Model\n\n**from** spacy.pipeline.textcat **import** DEFAULT_SINGLE_TEXTCAT_MODEL  \n  \n---|---|---  \n  \n_TextCategorizer_ consists of (1) single-label and (2) multi-label\nclassifiers.\n\nA multi-label classifier can predict more than single class. A single-label\nclassifier predicts a single class for each example and classes are mutually\nexclusive.\n\nThe preceding import line imports single-label classifier, and the following\ncode imports multi-label classifier:\n\nIn[3]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figc_HTML.gif)\n\n|\n\n_# Import the Multiple Text Categorizer Model_\n\n**from** spacy.pipeline.textcat_multilabel **import**\nDEFAULT_MULTI_TEXTCAT_MODEL  \n  \n---|---|---  \n  \nThere are two parameters (1) a threshold value and (2) a model name (either\nSingle or Multi depends on classification task) required for a\n_TextCategorizer_ component configuration.\n\n_TextCategorizer_ generates a probability for each class and a class is\nassigned to text if the probability of this class is higher than the threshold\nvalue.\n\nA traditional threshold value for text classification is 0.5; however, if\nprediction is required for a higher confidence, it can adjust threshold to\n0.6\u20130.8.\n\nA single-label _TextCategorizer_ ( _tCategorizer_ ) component is added to nlp\npipeline as follows:\n\nIn[4]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figd_HTML.gif)\n\n|\n\n_# Import the Single Text Categorizer Model_\n\n_# Define the model parameters: threshold and model_\n\n**from** spacy.pipeline.textcat **import** DEFAULT_SINGLE_TEXTCAT_MODEL\n\nconfig = {\n\n\"threshold\": 0.5,\n\n\"model\": DEFAULT_SINGLE_TEXTCAT_MODEL\n\n}  \n  \n---|---|---  \n  \nIn[5]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Fige_HTML.gif)\n\n|\n\n_# Define the Text Categorizer object (tCategorizer)_\n\ntCategorizer = nlp.add_pipe(\"textcat\", config=config)  \n  \n---|---|---  \n  \nLet us look at Textcategorizer object (tCategorizer):\n\nIn[6]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "afca20e9-5413-48ad-b52c-684eb8684299": {"__data__": {"id_": "afca20e9-5413-48ad-b52c-684eb8684299", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6479543-c9e2-4ed0-94ec-050bffee8255", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "a689446c291c6db65de9b63ebdb19ae58166226431636a14cde150561fd0013b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2a94554-90db-4b7e-9cd4-504d5e21f9e5", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "39772be9c1c53a265645d3b9286edb1ec634db39c9502a31a158ffda722fbd05", "class_name": "RelatedNodeInfo"}}, "hash": "9a923a59e71a7747869bba9ce6ecd03e77156ac2a107a8f9ed39e8dbf2adb796", "text": "[](../images/533412_1_En_14_Chapter/533412_1_En_14_Figd_HTML.gif)\n\n|\n\n_# Import the Single Text Categorizer Model_\n\n_# Define the model parameters: threshold and model_\n\n**from** spacy.pipeline.textcat **import** DEFAULT_SINGLE_TEXTCAT_MODEL\n\nconfig = {\n\n\"threshold\": 0.5,\n\n\"model\": DEFAULT_SINGLE_TEXTCAT_MODEL\n\n}  \n  \n---|---|---  \n  \nIn[5]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Fige_HTML.gif)\n\n|\n\n_# Define the Text Categorizer object (tCategorizer)_\n\ntCategorizer = nlp.add_pipe(\"textcat\", config=config)  \n  \n---|---|---  \n  \nLet us look at Textcategorizer object (tCategorizer):\n\nIn[6]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figf_HTML.gif)\n\n|\n\ntCategorizer  \n  \n---|---|---  \n  \nOut[6]\n\n|\n\n<spacy.pipeline.textcat. _TextCategorizer_ at 0x1bf406cedc0>  \n  \nAdd a multi-label component to nlp pipeline:\n\nIn[7]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figg_HTML.gif)\n\n|\n\n_# Import the Multiple Text Categorizer Model_\n\n_# Define the model parameters: threshold and model_\n\n**from** spacy.pipeline.textcat_multilabel **import**\nDEFAULT_MULTI_TEXTCAT_MODEL\n\nconfig = {\n\n\"threshold\": 0.5,\n\n\"model\": DEFAULT_MULTI_TEXTCAT_MODEL\n\n}  \n  \n---|---|---  \n  \nIn[8]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figh_HTML.gif)\n\n|\n\ntCategorizer = nlp.add_pipe( \"textcat_multilabel\" , config=config)  \n  \n---|---|---  \n  \nIn[9]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figi_HTML.gif)\n\n|\n\ntCategorizer  \n  \n---|---|---  \n  \nOut[9]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figj_HTML.gif)  \n  \n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figk_HTML.gif)\n\n|\n\nAdd a _TextCategorizer_ pipeline component to nlp pipeline object at the last\nline of each preceding code blocks. The newly created _TextCategorizer_\ncomponent is captured by textcat variable and set for training  \n  \n---|---  \n  \n### 14.5.2 Formatting Training Data for the TextCategorizer\n\nLet us prepare a customer sentiment dataset for binary text classification.\n\nThe label (category) will be called sentiment to obtain two possible values, 0\nand 1 corresponding to negative and positive sentiments.\n\nThere are six examples from IMDB with three each of positive and negative as\nbelow:\n\nIn[10]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figl_HTML.gif)\n\n|\n\nmovie_comment1 = [\n\n(\"This movie is perfect and worth watching. \",{\"cats\": {\"Positive Sentiment\":\n1}}),\n\n(\"This movie is great, the performance of Al Pacino is brilliant.\", {\"cats\":\n{\"Positive Sentiment\": 1}}),\n\n(\"A very good and funny movie. It should be the best this year!\", {\"cats\":\n{\"Positive Sentiment\": 1}}),\n\n(\"This movie is so bad that I really want to leave after the first hour\nwatching.\", {\"cats\": {\"Positive Sentiment\": 0}}),\n\n(\"Even free I won't see this movie again. Totally failure!\", {\"cats\":\n{\"Positive Sentiment\": 0}}),\n\n(\"I think it is the worst movie I saw so far this year.\", {\"cats\": {\"Positive\nSentiment\": 0}})\n\n]]  \n  \n---|---|---  \n  \nCheck on any movie comment1 element:\n\nIn[11]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figm_HTML.gif)\n\n|\n\nmovie_comment1 [1]  \n  \n---|---|---  \n  \nOut[11]\n\n|\n\n('This movie is great, the performance of Al Pacino is brilliant.',\n\n{'cats': {'Positive Sentiment': 1}})  \n  \n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e2a94554-90db-4b7e-9cd4-504d5e21f9e5": {"__data__": {"id_": "e2a94554-90db-4b7e-9cd4-504d5e21f9e5", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "afca20e9-5413-48ad-b52c-684eb8684299", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9a923a59e71a7747869bba9ce6ecd03e77156ac2a107a8f9ed39e8dbf2adb796", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e49cfda0-7847-472d-9224-298a38f7a643", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "accc73ce607f619f0a464a65fff6df0f01d12b77efc4b93e946fe6438e5c1534", "class_name": "RelatedNodeInfo"}}, "hash": "39772be9c1c53a265645d3b9286edb1ec634db39c9502a31a158ffda722fbd05", "text": "\", {\"cats\": {\"Positive Sentiment\": 0}}),\n\n(\"Even free I won't see this movie again. Totally failure!\", {\"cats\":\n{\"Positive Sentiment\": 0}}),\n\n(\"I think it is the worst movie I saw so far this year.\", {\"cats\": {\"Positive\nSentiment\": 0}})\n\n]]  \n  \n---|---|---  \n  \nCheck on any movie comment1 element:\n\nIn[11]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figm_HTML.gif)\n\n|\n\nmovie_comment1 [1]  \n  \n---|---|---  \n  \nOut[11]\n\n|\n\n('This movie is great, the performance of Al Pacino is brilliant.',\n\n{'cats': {'Positive Sentiment': 1}})  \n  \n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Fign_HTML.gif)\n\n|\n\n\u2022 Each training example (movie_coment1) is a tuple object consists of a text\nand a nested dictionary\n\n\u2022 The dictionary contains a class category in a format recognized by spaCy\n\n\u2022 The _cats_ field means categories\n\n\u2022 Include class category sentiment and its value. The value should always be a\nfloating-point number  \n  \n---|---  \n  \nThe code will introduce a class category selected for _TextCategorizer_\ncomponent.\n\nIn[12]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figo_HTML.gif)\n\n|\n\n**import** random\n\n**import** spacy\n\n**from** spacy.training **import** Example\n\n**from** spacy.pipeline.textcat **import** DEFAULT_SINGLE_TEXTCAT_MODEL  \n  \n---|---|---  \n  \n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figp_HTML.gif)\n\n|\n\n\u2022 Import a built-in library random to shuffle dataset\n\n\u2022 Import spaCy as usual, then import _Example_ to prepare training samples in\nspaCy format\n\n\u2022 Import _TextCategorizer_ model in final statement  \n  \n---|---  \n  \nInitialize pipeline and _TextCategorizer_ component.\n\nWhen a new _TextCategorizer_ component _tCategorizer_ is created, use calling\n_add_label_ method to introduce category sentiment to _TextCategorizer_\ncomponent with examples.\n\nThe following code adds label to _TextCategorizer_ component and initialize\n_TextCategorizer_ model\u2019s weights with training samples:\n\nIn[13]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figq_HTML.gif)\n\n|\n\n**import** random\n\n**import** spacy\n\n**from** spacy.training **import** Example\n\n**from** spacy.pipeline.textcat **import** DEFAULT_SINGLE_TEXTCAT_MODEL\n\n_# Load the_ _spaCy_ _NLP model_\n\nnlp = spacy.load('en_core_web_md')\n\n_# Set the threshold and model_\n\nconfig = {\n\n\"threshold\": 0.5,\n\n\"model\": DEFAULT_SINGLE_TEXTCAT_MODEL\n\n}\n\n# Define TextCategorizer object (tCategorizer)\n\ntCategorizer = nlp.add_pipe(\"textcat\", config=config)  \n  \n---|---|---  \n  \nLet us look at pipe_names:\n\nIn[14]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figr_HTML.gif)\n\n|\n\nnlp.pipe_names  \n  \n---|---|---  \n  \nOut[14]\n\n|\n\n['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer',\n\n'ner', 'textcat']  \n  \nWhen a new _TextCategorizer_ component textcat is created, use calling\n_add_label_ method to introduce label sentiment to the _TextCategorizer_\ncomponent and initialize this component with examples.\n\nThe following code adds label to _TextCategorizer_ component and initializes\n_TextCategorizer_ model\u2019s weights with training samples ( _movie_comment_exp_\n):\n\nIn[15]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figs_HTML.gif)\n\n|\n\n_# Create the two sentiment categories_\n\ntCategorizer.add_label(\"Positive Sentiment\")\n\ntCategorizer.add_label(\"Negative Sentiment\")\n\n_# Create the movie comment samples_\n\nmovie_comment_exp = [Example.from_dict(nlp.make_doc(comments), category)\n**for** comments,category **in** movie_comment1]\n\ntCategorizer.initialize( **lambda** : movie_comment_exp, nlp=nlp)  \n  \n---|---|---  \n  \nLet us look at movie_comment_exp:\n\nIn[16]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e49cfda0-7847-472d-9224-298a38f7a643": {"__data__": {"id_": "e49cfda0-7847-472d-9224-298a38f7a643", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2a94554-90db-4b7e-9cd4-504d5e21f9e5", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "39772be9c1c53a265645d3b9286edb1ec634db39c9502a31a158ffda722fbd05", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8354677e-2e80-4a7c-a595-25d11f5012ef", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "106892f1dd833b64cd4ce5822bf702199c56571f44c8045422e317a582d986f0", "class_name": "RelatedNodeInfo"}}, "hash": "accc73ce607f619f0a464a65fff6df0f01d12b77efc4b93e946fe6438e5c1534", "text": "The following code adds label to _TextCategorizer_ component and initializes\n_TextCategorizer_ model\u2019s weights with training samples ( _movie_comment_exp_\n):\n\nIn[15]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figs_HTML.gif)\n\n|\n\n_# Create the two sentiment categories_\n\ntCategorizer.add_label(\"Positive Sentiment\")\n\ntCategorizer.add_label(\"Negative Sentiment\")\n\n_# Create the movie comment samples_\n\nmovie_comment_exp = [Example.from_dict(nlp.make_doc(comments), category)\n**for** comments,category **in** movie_comment1]\n\ntCategorizer.initialize( **lambda** : movie_comment_exp, nlp=nlp)  \n  \n---|---|---  \n  \nLet us look at movie_comment_exp:\n\nIn[16]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figt_HTML.gif)\n\n|\n\nmovie_comment_exp  \n  \n---|---|---  \n  \nOut[16]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figu_HTML.gif)  \n  \n### 14.5.3 System Training\n\nTraining loop is all set to be defined.\n\nFirst, disable other pipe components to allow only textcat can be trained.\n\nSecond, create an optimizer object by calling _resume_training_ to keep the\nweights of existing statistical models.\n\nExamine each epoch training example one by one and update the weights of\ntextcat. Examine data for 20 epochs.\n\nTry the whole program with training loop:\n\nIn[17]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figv_HTML.gif)\n\n|\n\nmovie_comment1  \n  \n---|---|---  \n  \nOut[17]\n\n|\n\n[('This movie is perfect and worth watching. ',\n\n{'cats': {'Positive Sentiment': 1}}),\n\n('This movie is great, the performance of Al Pacino is brilliant.',\n\n{'cats': {'Positive Sentiment': 1}}),\n\n('A very good and funny movie. It should be the best this year!',\n\n{'cats': {'Positive Sentiment': 1}}),\n\n('This movie is so bad that I really want to leave after the first hour\nwatching.',\n\n{'cats': {'Positive Sentiment': 0}}),\n\n(\"Even free I won't see this movie again. Totally failure!\",\n\n{'cats': {'Positive Sentiment': 0}}),\n\n('I think it is the worst movie I saw so far this year.',\n\n{'cats': {'Positive Sentiment': 0}})]  \n  \nIn[18]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8354677e-2e80-4a7c-a595-25d11f5012ef": {"__data__": {"id_": "8354677e-2e80-4a7c-a595-25d11f5012ef", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e49cfda0-7847-472d-9224-298a38f7a643", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "accc73ce607f619f0a464a65fff6df0f01d12b77efc4b93e946fe6438e5c1534", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a24a74b-898f-459c-9918-0d3c9355dc17", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "93ab980c696e8c9deea7daa7b8c85a034751e855c3f0f646d8f763a360344b2a", "class_name": "RelatedNodeInfo"}}, "hash": "106892f1dd833b64cd4ce5822bf702199c56571f44c8045422e317a582d986f0", "text": "',\n\n{'cats': {'Positive Sentiment': 1}}),\n\n('This movie is great, the performance of Al Pacino is brilliant.',\n\n{'cats': {'Positive Sentiment': 1}}),\n\n('A very good and funny movie. It should be the best this year!',\n\n{'cats': {'Positive Sentiment': 1}}),\n\n('This movie is so bad that I really want to leave after the first hour\nwatching.',\n\n{'cats': {'Positive Sentiment': 0}}),\n\n(\"Even free I won't see this movie again. Totally failure!\",\n\n{'cats': {'Positive Sentiment': 0}}),\n\n('I think it is the worst movie I saw so far this year.',\n\n{'cats': {'Positive Sentiment': 0}})]  \n  \nIn[18]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figw_HTML.gif)\n\n|\n\n_# Full implementation of the Movie Sentiment Analysis System_\n\n**import** random\n\n**import** spacy\n\nfrom spacy.training **import** Example\n\n**from** spacy.pipeline.textcat **import** DEFAULT_SINGLE_TEXTCAT_MODEL\n\n_# Load the_ _spaCy_ _NLP model_\n\nnlp = spacy.load('en_core_web_md')\n\n_# Set the threshold and model_\n\nconfig = {\n\n\"threshold\": 0.5,\n\n\"model\": DEFAULT_SINGLE_TEXTCAT_MODEL\n\n}\n\n_# Create the_ _TextCategorizer_ _object (tCategorizer)_\n\ntCategorizer = nlp.add_pipe(\"textcat\", config=config)\n\n_# Add the two movie sentiment categories_\n\ntCategorizer.add_label(\"Positive Sentiment\")\n\ntCategorizer.add_label(\"Negative Sentiment\")\n\n_# Create the movie sample comments_\n\nmovie_comment_exp = [Example.from_dict(nlp.make_doc(comments), category) for\ncomments,category in movie_comment1]\n\ntCategorizer.initialize(lambda: movie_comment_exp, nlp=nlp)\n\n_# Set the training epochs and loss values_\n\nepochs=20\n\nlosses = {}\n\n_# Main program loop_\n\n**with** nlp.select_pipes(enable=\"textcat\"):\n\noptimizer = nlp.resume_training()\n\n**for** i **in** range(epochs):\n\nrandom.shuffle(movie_comment1)\n\n**for** comments, category **in** movie_comment1:\n\nmdoc = nlp.make_doc(comments)\n\nexp = Example.from_dict(mdoc, category)\n\nnlp.update([exp], sgd=optimizer, losses=losses)\n\nprint(\"Epoch #\",i, \"Losses: \",losses)  \n  \n---|---|---  \n  \nOut[18]\n\n|\n\nEpoch # 0 Losses: {'textcat': 1.5183179080486298}\n\nEpoch # 1 Losses: {'textcat': 2.7817106544971466}\n\nEpoch # 2 Losses: {'textcat': 3.5506987050175667}\n\nEpoch # 3 Losses: {'textcat': 3.8420143127441406}\n\nEpoch # 4 Losses: {'textcat': 3.9003750435076654}\n\nEpoch # 5 Losses: {'textcat': 3.9074860664550215}\n\nEpoch # 6 Losses: {'textcat': 3.908426207563025}\n\nEpoch # 7 Losses: {'textcat': 3.908603171435061}\n\nEpoch # 8 Losses: {'textcat': 3.9086502377413126}\n\nEpoch # 9 Losses: {'textcat': 3.908672676368724}\n\nEpoch # 10 Losses: {'textcat': 3.9086846519847427}\n\nEpoch # 11 Losses: {'textcat': 3.908692961549093}\n\nEpoch # 12 Losses: {'textcat': 3.9086987949742706}\n\nEpoch # 13 Losses: {'textcat': 3.9087034759107553}\n\nEpoch # 14 Losses: {'textcat': 3.908707513363254}\n\nEpoch # 15 Losses: {'textcat': 3.90871090364098}\n\nEpoch # 16 Losses: {'textcat': 3.9087138364217537}\n\nEpoch # 17 Losses: {'textcat': 3.9087164432144874}\n\nEpoch # 18 Losses: {'textcat': 3.9087187692100116}\n\nEpoch # 19 Losses: {'textcat': 3.908720835553922}  \n  \n### 14.5.4 System Testing\n\nLet us test a new textcategorizer component, doc.cats property holds the class\nlabels:\n\nIn[19]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a24a74b-898f-459c-9918-0d3c9355dc17": {"__data__": {"id_": "7a24a74b-898f-459c-9918-0d3c9355dc17", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8354677e-2e80-4a7c-a595-25d11f5012ef", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "106892f1dd833b64cd4ce5822bf702199c56571f44c8045422e317a582d986f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c57e169-5f39-4e06-ac8e-ff56b0d1c1fd", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "0c9f26cc866f51249946f1cbb17b80c768c5e04b2ef3608b3f351b2d6e1bcd93", "class_name": "RelatedNodeInfo"}}, "hash": "93ab980c696e8c9deea7daa7b8c85a034751e855c3f0f646d8f763a360344b2a", "text": "[](../images/533412_1_En_14_Chapter/533412_1_En_14_Figx_HTML.gif)\n\n|\n\n_# Test 1: This movie sucks_\n\ntest1 = nlp(\"This movie sucks and the worst I ever saw.\")\n\ntest1.cats  \n  \n---|---|---  \n  \nOut[19]\n\n|\n\n{'Positive Sentiment': 0.05381184443831444,\n\n'Negative Sentiment': 0.9461881518363953}  \n  \nIn[20]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figy_HTML.gif)\n\n|\n\n_# Test 2: I'll watch it again, how amazing._\n\ntest2 = nlp(\"This movie really very great!\")\n\ntest2.cats  \n  \n---|---|---  \n  \nOut[20]\n\n|\n\n{'Positive Sentiment': 0.8159973621368408,\n\n'Negative Sentiment': 0.1840025931596756}  \n  \n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figz_HTML.gif)\n\n|\n\nThe small dataset trained spaCy text classifier successfully for a binary text\nclassification problem to perform correct sentiment analysis. Now, let us\nperform multi-label classification  \n  \n---|---  \n  \n### 14.5.5 Training TextCategorizer for Multi-Label Classification\n\nMulti-label classification means the classifier can predict more than single\nlabel for an example text. Naturally, the classes are not mutually exclusive.\n\nProvide training samples with at least more than 2 categories to train a\nmultiple labelled classifier.\n\nConstruct a small training set to train spaCy\u2018s _TextCategorizer_ for multi-\nlabel classification. This time will form a set of movie reviews, where the\nmulti-category is:\n\n  * ACTION.\n\n  * SCIFI.\n\n  * WEEKEND.\n\nHere is a small sample dataset ( _movie_comment2_ ):\n\nIn[21]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figaa_HTML.gif)\n\n|\n\nmovie_comment2 = [\n\n(\"This movie is great for weekend watching.\", {\"cats\": {\"WEEKEND\": **True**\n}}),\n\n(\"This a 100% action movie, I enjoy it.\", {\"cats\": {\"ACTION\": **True** }}),\n\n(\"Avatar is the best Scifi movie I ever seen!\" , {\"cats\": {\"SCIFI\": **Tru**\ne}}),\n\n(\"Such a good Scifi movie to watch during the weekend!\", {\"cats\": {\"WEEKEND\":\n**True** , \"SCIFI\": **True** }}),\n\n(\"Matrix a great Scifi movie with a lot of action. Pure action, great!\",\n{\"cats\": {\"SCIFI\": **True** , \"ACTION\": **True** }})\n\n]  \n  \n---|---|---  \n  \nCheck dataset first:\n\nIn[22]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figab_HTML.gif)\n\n|\n\nmovie_comment2  \n  \n---|---|---  \n  \nOut[22]\n\n|\n\n[('This movie is great for weekend watching.', {'cats': {'WEEKEND': True}}),\n\n('This a 100% action movie, I enjoy it.', {'cats': {'ACTION': True}}),\n\n('Avatar is the best Scifi movie I ever seen!', {'cats': {'SCIFI': True}}),\n\n('Such a good Scifi movie to watch during the weekend!',\n\n{'cats': {'WEEKEND': True, 'SCIFI': True}}),\n\n('Matrix a great Scifi movie with a lot of action. Pure action, great!',\n\n{'cats': {'SCIFI': True, 'ACTION': True}})]  \n  \nIn[23]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figac_HTML.gif)\n\n|\n\nmovie_comment2[1]  \n  \n---|---|---  \n  \nOut[23]\n\n|\n\n('This a 100% action movie, I enjoy it.', {'cats': {'ACTION': True}})  \n  \nProvide examples with a single label, such as first example (the first\nsentence of _movie_comment2_ , the second line of preceding code block), and\nexamples with more than single label, such as fourth example of\n_movie_comment2_.\n\nImport after the training set is formed.\n\nIn[24]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c57e169-5f39-4e06-ac8e-ff56b0d1c1fd": {"__data__": {"id_": "6c57e169-5f39-4e06-ac8e-ff56b0d1c1fd", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a24a74b-898f-459c-9918-0d3c9355dc17", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "93ab980c696e8c9deea7daa7b8c85a034751e855c3f0f646d8f763a360344b2a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62655d5a-6d9b-4984-acab-29528a6aec7f", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "83fb1863d90ede39d7c24043ac89cb17a450ddd9647ee8699af0e4723a141e27", "class_name": "RelatedNodeInfo"}}, "hash": "0c9f26cc866f51249946f1cbb17b80c768c5e04b2ef3608b3f351b2d6e1bcd93", "text": "',\n\n{'cats': {'WEEKEND': True, 'SCIFI': True}}),\n\n('Matrix a great Scifi movie with a lot of action. Pure action, great!',\n\n{'cats': {'SCIFI': True, 'ACTION': True}})]  \n  \nIn[23]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figac_HTML.gif)\n\n|\n\nmovie_comment2[1]  \n  \n---|---|---  \n  \nOut[23]\n\n|\n\n('This a 100% action movie, I enjoy it.', {'cats': {'ACTION': True}})  \n  \nProvide examples with a single label, such as first example (the first\nsentence of _movie_comment2_ , the second line of preceding code block), and\nexamples with more than single label, such as fourth example of\n_movie_comment2_.\n\nImport after the training set is formed.\n\nIn[24]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figad_HTML.gif)\n\n|\n\n**import** random\n\n**import** spacy\n\n**from** spacy.training **import** Example\n\n**from** spacy.pipeline.textcat_multilabel **import**\nDEFAULT_MULTI_TEXTCAT_MODEL\n\n_# Load_ _spaCy_ _NLP model_\n\nnlp = spacy.load( 'en_core_web_md' )  \n  \n---|---|---  \n  \nNote that the last line has different code than previous section. Import\nmulti-label model instead of single-label model.\n\nNext, add multi-label classifier component to nlp pipeline.\n\nAlso note that pipeline component name is textcat_multilabel as compared with\nprevious section\u2019s textcat:\n\nIn[25]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figae_HTML.gif)\n\n|\n\n_# Set the threshold and model_\n\nconfig = {\n\n\"threshold\": 0.5,\n\n\"model\": DEFAULT_MULTI_TEXTCAT_MODEL\n\n}\n\n_# Create the_ _TextCategorizer_ _object (tCategorizer)_\n\ntCategorizer = nlp.add_pipe( \"textcat_multilabel\" , config=config)  \n  \n---|---|---  \n  \nAdd categories to _TextCategorizer_ component and initialize model like\nprevious text classifier section.\n\nAdd three labels instead of one:\n\nIn[26]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figaf_HTML.gif)\n\n|\n\n_# Create the categorizer object with 3 categories_\n\ncategories = [\"SCIFI\", \"ACTION\", \"WEEKEND\"]\n\n_# Using For Loop to add the 3 categories_\n\n**for** category **in** categories:\n\ntCategorizer.add_label(category)\n\n_# Create the movie comment sample for training_\n\nmovie_comment_exp = [Example.from_dict(nlp.make_doc(comments), category)\n**for** comments,category **in** movie_comment2]\n\n_# Initializer the tCategorizer_\n\ntCategorizer.initialize( **lambda** : movie_comment_exp, nlp=nlp)  \n  \n---|---|---  \n  \nTraining loop is all set to be defined.\n\nCode functions are like previous section\u2019s code, the only difference is\ncomponent name _textcat_multilabel_ in the first line:\n\nIn[27]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62655d5a-6d9b-4984-acab-29528a6aec7f": {"__data__": {"id_": "62655d5a-6d9b-4984-acab-29528a6aec7f", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c57e169-5f39-4e06-ac8e-ff56b0d1c1fd", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "0c9f26cc866f51249946f1cbb17b80c768c5e04b2ef3608b3f351b2d6e1bcd93", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38160117-0ac5-400c-a8f4-7ddd30499712", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8d715b4db11803e4c4135952c39390dfd54723a00cce2a97f002da6bc852746b", "class_name": "RelatedNodeInfo"}}, "hash": "83fb1863d90ede39d7c24043ac89cb17a450ddd9647ee8699af0e4723a141e27", "text": "Code functions are like previous section\u2019s code, the only difference is\ncomponent name _textcat_multilabel_ in the first line:\n\nIn[27]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figag_HTML.gif)\n\n|\n\n_# Set the training epochs and loss values_\n\nepochs=20\n\nlosses = {}\n\n_# Main Loop of the program_\n\n**with** nlp.select_pipes(enable=\"textcat_multilabel\"):\n\noptimizer = nlp.resume_training()\n\n**for** i **in** range(epochs):\n\nrandom.shuffle(movie_comment2)\n\n**for** comments, category in movie_comment2:\n\nmdoc = nlp.make_doc(comments)\n\nexp = Example.from_dict(mdoc, category)\n\nnlp.update([exp], sgd=optimizer, losses=losses)\n\nprint(losses)  \n  \n---|---|---  \n  \nOut[27]\n\n|\n\n{'textcat_multilabel': 6.85278207868123e-06}\n\n{'textcat_multilabel': 1.3591816482971808e-05}\n\n{'textcat_multilabel': 2.003331736943892e-05}\n\n{'textcat_multilabel': 2.6209833507095937e-05}\n\n{'textcat_multilabel': 3.211208475306648e-05}\n\n{'textcat_multilabel': 3.780755992011109e-05}\n\n{'textcat_multilabel': 4.3277938615915446e-05}\n\n{'textcat_multilabel': 4.857392603696553e-05}\n\n{'textcat_multilabel': 5.372697206951216e-05}\n\n{'textcat_multilabel': 5.867910277856936e-05}\n\n{'textcat_multilabel': 6.350277087108225e-05}\n\n{'textcat_multilabel': 6.81268817857017e-05}\n\n{'textcat_multilabel': 7.271952523524305e-05}\n\n{'textcat_multilabel': 7.709734516936351e-05}\n\n{'textcat_multilabel': 8.136703193883932e-05}\n\n{'textcat_multilabel': 8.552625510560574e-05}\n\n{'textcat_multilabel': 8.953699784797209e-05}\n\n{'textcat_multilabel': 9.34374557175488e-05}\n\n{'textcat_multilabel': 9.724928190735227e-05}\n\n{'textcat_multilabel': 0.00010092528287941605}  \n  \nThe output should look like output of previous section but use multiple\ncategory for system training. Let\u2019s test the new multi-label classifier:\n\nIn[28]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figah_HTML.gif)\n\n|\n\ntest3 = nlp(\"Definitely in my weekend scifi movie night list\")\n\ntest3.cats  \n  \n---|---|---  \n  \nOut[28]\n\n|\n\n{'SCIFI': 0.9995421171188354,\n\n'ACTION': 0.5897207260131836,\n\n'WEEKEND': 0.9992324113845825}  \n  \nIn[29]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figai_HTML.gif)\n\n|\n\ntest4 = nlp(\"Go to watch action scifi movie this weekend.\")\n\ntest4.cats  \n  \n---|---|---  \n  \nOut[29]\n\n|\n\n{'SCIFI': 0.994023859500885,\n\n'ACTION': 0.9914324283599854,\n\n'WEEKEND': 0.9998989105224609}  \n  \n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figaj_HTML.gif)\n\n|\n\nAlthough sample size is small, but the multiple textcategorizer can classify\ntwo IMDB user comments correctly into three categories: SCIFI, ACTION, and\nWEEKEND. Note that over thousands of IMDB user comments are required to\nperform a satisfactory sentiment analysis in real situations  \n  \n---|---  \n  \nThis section has learnt how to train a spaCy\u2018s _TextCategorizer_ component for\nbinary and multilabel text classifications.\n\nNow, _TextCategorizer_ will be trained on a real-world dataset for a sentiment\nanalysis using IMDB user comments dataset.\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "38160117-0ac5-400c-a8f4-7ddd30499712": {"__data__": {"id_": "38160117-0ac5-400c-a8f4-7ddd30499712", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62655d5a-6d9b-4984-acab-29528a6aec7f", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "83fb1863d90ede39d7c24043ac89cb17a450ddd9647ee8699af0e4723a141e27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a0e182a-f626-4801-87d7-5ad200c2439b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "46600d5eb3b37be49d10cebc2642406648d1d5be318f9581e871f7144b5427ae", "class_name": "RelatedNodeInfo"}}, "hash": "8d715b4db11803e4c4135952c39390dfd54723a00cce2a97f002da6bc852746b", "text": "[](../images/533412_1_En_14_Chapter/533412_1_En_14_Figaj_HTML.gif)\n\n|\n\nAlthough sample size is small, but the multiple textcategorizer can classify\ntwo IMDB user comments correctly into three categories: SCIFI, ACTION, and\nWEEKEND. Note that over thousands of IMDB user comments are required to\nperform a satisfactory sentiment analysis in real situations  \n  \n---|---  \n  \nThis section has learnt how to train a spaCy\u2018s _TextCategorizer_ component for\nbinary and multilabel text classifications.\n\nNow, _TextCategorizer_ will be trained on a real-world dataset for a sentiment\nanalysis using IMDB user comments dataset.\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figak_HTML.gif)\n\n|\n\n**Workshop 5.1** **Movie comments** **from** **IMDB** **.com**\n\nMovie comments is a significant classification in social media. This workshop\nconstructs a simple movie comment classification with millions of user\ncomments from IMDB.com, the world biggest movie social media platform.\n\n1\\. Try to collect 900 comments with 300 _Good_ , 300 _Average_ and 300 _Bad_\ncomments to train system. Make sure they make sense or system won\u2019t function\n\n2\\. Construct a Multi-label Classification System to create three movie\ncomments: _Good, Average_ or _Bad_\n\n3\\. Train system with at least 100 epochs\n\n4\\. Use 10 examples to test and see whether it works  \n  \n---|---  \n  \n## 14.6 Sentiment Analysis with spaCy\n\n### 14.6.1 IMDB Large Movie Review Dataset\n\nThis section will work on a real-world dataset using IMDB Large Movie Reviews\nDataset from Kaggle (2022).\n\nThe original _imdb_sup.csv_ dataset has 50,000 rows. They need to down-size\nand select first 5000 records into datafile imdb_5000.csv to speed up\ntraining. This movie reviews dataset consists of movie reviews, reviews sizes,\nIMDB Ratings (1\u201310), and Sentiment Ratings (0 or 1).\n\nThe dataset can be downloaded from workshop directory namely: imdb_sup.csv\n(complete dataset) or imdb_5000.csv (5000 records).\n\n### 14.6.2 Explore the Dataset\n\nLet us have some understanding from dataset prior sentiment analysis.\n\n  1. 1.\n\nFirst, import to read and visualize dataset:\n\nIn[30]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figal_HTML.gif)\n\n|\n\n**import** pandas **as** pd\n\n**import** matplotlib.pyplot **as** plt\n\n**%** matplotlib inline  \n  \n---|---|---  \n  \n  1. 2.\n\nRead imdb_5000.csv datafile into a pandas DataFrame (mcommentDF) and output\nthe shape of DataFrame:\n\nIn[31]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figam_HTML.gif)\n\n|\n\nmcommentDF=pd.read_csv( 'imdb_5000.csv' )  \n  \n---|---|---  \n  \nIn[32]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figan_HTML.gif)\n\n|\n\nmcommentDF.shape  \n  \n---|---|---  \n  \nOut[32]\n\n|\n\n(5000, 3)  \n  \nNote: This IMDB movie reviews dataset contains 5000 records, each record has 3\nfields attributes: Review, Rating, and Sentiment\n\n  1. 3.\n\nExamine rows and columns of dataset by printing the first few rows using\nhead() method:\n\nIn[33]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figao_HTML.gif)\n\n|\n\nmcommentDF.head()  \n  \n---|---|---  \n  \nOut[33]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figap_HTML.gif)  \n  \n  1. 4.\n\nUse Review and Sentiment columns only in this workshop. Hence, drop other\ncolumns that won't use, and call dropna() method to drop the rows with missing\nvalues:\n\nIn[34]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a0e182a-f626-4801-87d7-5ad200c2439b": {"__data__": {"id_": "5a0e182a-f626-4801-87d7-5ad200c2439b", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38160117-0ac5-400c-a8f4-7ddd30499712", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8d715b4db11803e4c4135952c39390dfd54723a00cce2a97f002da6bc852746b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e019c0b-8b74-4e5e-b92c-05959c245376", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "803fd8e707007b9294b13fff4795f147aef2bf7022d260d7c620a8941f40797c", "class_name": "RelatedNodeInfo"}}, "hash": "46600d5eb3b37be49d10cebc2642406648d1d5be318f9581e871f7144b5427ae", "text": "3.\n\nExamine rows and columns of dataset by printing the first few rows using\nhead() method:\n\nIn[33]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figao_HTML.gif)\n\n|\n\nmcommentDF.head()  \n  \n---|---|---  \n  \nOut[33]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figap_HTML.gif)  \n  \n  1. 4.\n\nUse Review and Sentiment columns only in this workshop. Hence, drop other\ncolumns that won't use, and call dropna() method to drop the rows with missing\nvalues:\n\nIn[34]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figaq_HTML.gif)\n\n|\n\nmcommentDF_clean = mcommentDF[[ 'Review', 'Sentiment' ]].dropna()  \n  \n---|---|---  \n  \nIn[35]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figar_HTML.gif)\n\n|\n\nmcommentDF_clean.head()  \n  \n---|---|---  \n  \nOut[35]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figas_HTML.gif)  \n  \n  1. 5.\n\nLet us look at how review scores are distributed:\n\nIn[36]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figat_HTML.gif)\n\n|\n\naxplot=mcommentDF.Rating.value_counts().plot(kind='bar', colormap=\"Paired\")\n\nplt.show()  \n  \n---|---|---  \n  \nOut[36]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figau_HTML.gif)  \n  \n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figav_HTML.gif)\n\n|\n\n1\\. Users prefer to give high rating, i.e. 8 or above, and 10 is the highest\nas shown\n\n2\\. It is better to select sample set with even distribution to balance sample\ndata rating\n\n3\\. Check system performance first. If it is not as good as predicted, can use\nfine-tune sampling method to improve system performance  \n  \n---|---  \n  \nHere use the sentiments already labeled.\n\n  1. 6.\n\nPlot ratings distribution:\n\nIn[37]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figaw_HTML.gif)\n\n|\n\naxplot=mcommentDF.Sentiment.value_counts().plot (kind= 'bar',\ncolormap=\"Paired\")\n\nplt.show()  \n  \n---|---|---  \n  \nOut[37]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figax_HTML.gif)  \n  \n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figay_HTML.gif)\n\n|\n\nNote that rating distribution has better results than the previous one, it has\nhigher number of positive reviews, but negative reviews are also significant\nas shown  \n  \n---|---  \n  \nAfter the dataset is processed, it can be reduced to a two-column dataset with\nnegative and positive ratings. So, call _mcommentDF.head()_ again and the\nfollowing result is obtained:\n\nIn[38]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figaz_HTML.gif)\n\n|\n\nmcommentDF.head()  \n  \n---|---|---  \n  \nOut[38]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figba_HTML.gif)  \n  \n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbb_HTML.gif)\n\n|\n\nComplete dataset exploration and display review scores with class categories\ndistribution. The dataset is ready to be processed. Drop unused columns and\nconvert review scores to binary class labels. Let us begin with the training\nprocedure  \n  \n---|---  \n  \n### 14.6.3 Training the TextClassfier\n\nUse multi-label classifier to train a binary text classifier this time.\n\n  1. 1.\n\nImport spaCy classes as follows:\n\nIn[39]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e019c0b-8b74-4e5e-b92c-05959c245376": {"__data__": {"id_": "4e019c0b-8b74-4e5e-b92c-05959c245376", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a0e182a-f626-4801-87d7-5ad200c2439b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "46600d5eb3b37be49d10cebc2642406648d1d5be318f9581e871f7144b5427ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5384145-7a6a-47c4-a092-31ab4d25dcea", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "12e7df57a630672c130efac1258763042f1a8f28eef73bc768bb442b4c699ea0", "class_name": "RelatedNodeInfo"}}, "hash": "803fd8e707007b9294b13fff4795f147aef2bf7022d260d7c620a8941f40797c", "text": "[](../images/533412_1_En_14_Chapter/533412_1_En_14_Figba_HTML.gif)  \n  \n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbb_HTML.gif)\n\n|\n\nComplete dataset exploration and display review scores with class categories\ndistribution. The dataset is ready to be processed. Drop unused columns and\nconvert review scores to binary class labels. Let us begin with the training\nprocedure  \n  \n---|---  \n  \n### 14.6.3 Training the TextClassfier\n\nUse multi-label classifier to train a binary text classifier this time.\n\n  1. 1.\n\nImport spaCy classes as follows:\n\nIn[39]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbc_HTML.gif)\n\n|\n\n**import** spacy\n\n**import** random\n\n**from** spacy.training **import** Example\n\n**from** spacy.pipeline.textcat_multilabel **import**\nDEFAULT_MULTI_TEXTCAT_MODEL  \n  \n---|---|---  \n  \n  1. 2.\n\nCreate a pipeline object nlp, define classifier configuration, and add\nTextCategorizer component to nlp with the following configuration:\n\nIn[40]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbd_HTML.gif)\n\n|\n\n_# Load the_ _spaCy_ _NLP model_\n\nnlp = spacy.load( \"en_core_web_md\" )\n\n_# Set the threshold and model_\n\nconfig = {\n\n\"threshold\": 0.5,\n\n\"model\": DEFAULT_MULTI_TEXTCAT_MODEL\n\n}\n\n_# Create the_ _TextCategorizer_ _object (tCategorizer)_\n\ntCategorizer = nlp.add_pipe(\"textcat_multilabel\", config=config)  \n  \n---|---|---  \n  \n  1. 3.\n\nWhen TextCategorizer object is available, create movie comment sample object\nas a list and load all user comments and categories into it.\n\nIn[41]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbe_HTML.gif)\n\n|\n\n_# Create the_ _IMDB_ _movie comment sample object_\n\nmovie_comment_exp = []\n\n_# Load all the_ _IMDB_ _user comments and categories_\n\n**for** idx, rw **in** mcommentDF.iterrows():\n\ncomments = rw[\"Review\"]\n\nrating = rw[\"Sentiment\"]\n\ncategory = {\"POS\": **True** , \"NEG\": **False** } **if** rating == 1 **else**\n{\"NEG\": **True** , \"POS\": **False** }\n\nmovie_comment_exp.append(Example.from_dict(nlp.make_doc(comments), {\"cats\":\ncategory}))  \n  \n---|---|---  \n  \n  1. 4.\n\nLet us check _movie_comment_exp_ :\n\nIn[42]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbf_HTML.gif)\n\n|\n\nmovie_comment_exp[0]  \n  \n---|---|---  \n  \nOut[42]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbg_HTML.gif)  \n  \n  1. 5.\n\nUse POS and NEG labels for positive and negative sentiment respectively.\nIntroduce these labels to the new component and initialize it with examples.\n\nIn[43]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbh_HTML.gif)\n\n|\n\n_# Add the two sentiment categories into tCategorizer_\n\ntCategorizer.add_label(\"POS\")\n\ntCategorizer.add_label(\"NEG\")\n\ntCategorizer.initialize( **lambda** : movie_comment_exp, nlp=nlp)  \n  \n---|---|---  \n  \nIn[44]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbi_HTML.gif)\n\n|\n\ntCategorizer  \n  \n---|---|---  \n  \nOut[44]\n\n|\n\n<spacy.pipeline.textcat_multilabel.MultiLabel_ _TextCategorizer_ at\n0x2626f9c4ee0>  \n  \n  1. 6.\n\nDefine training loop by examining the training set for two epochs but can\nexamine further if necessary. The following code snippet will train the new\ntext categorizer component:\n\nIn[45]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5384145-7a6a-47c4-a092-31ab4d25dcea": {"__data__": {"id_": "d5384145-7a6a-47c4-a092-31ab4d25dcea", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e019c0b-8b74-4e5e-b92c-05959c245376", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "803fd8e707007b9294b13fff4795f147aef2bf7022d260d7c620a8941f40797c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4b071c0-100f-442a-a76f-8e521b021b15", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "d57c2cea6358fe89245ab14de35478b06f29fef7d60559c6d4097ba868b8463d", "class_name": "RelatedNodeInfo"}}, "hash": "12e7df57a630672c130efac1258763042f1a8f28eef73bc768bb442b4c699ea0", "text": "[](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbi_HTML.gif)\n\n|\n\ntCategorizer  \n  \n---|---|---  \n  \nOut[44]\n\n|\n\n<spacy.pipeline.textcat_multilabel.MultiLabel_ _TextCategorizer_ at\n0x2626f9c4ee0>  \n  \n  1. 6.\n\nDefine training loop by examining the training set for two epochs but can\nexamine further if necessary. The following code snippet will train the new\ntext categorizer component:\n\nIn[45]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbj_HTML.gif)\n\n|\n\n_# Set the training epochs to 2 to save time_\n\nepochs = 2\n\n_# Main program loop_\n\n**with** nlp.select_pipes(enable=\"textcat_multilabel\"):\n\noptimizer = nlp.resume_training()\n\n**for** i **in** range(epochs):\n\nrandom.shuffle(movie_comment_exp)\n\n**for** exp **in** movie_comment_exp:\n\nnlp.update([exp], sgd=optimizer)  \n  \n---|---|---  \n  \n  1. 7.\n\nTest how text classifier component works for two example sentences:\n\nIn[46]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbk_HTML.gif)\n\n|\n\ntest5 = nlp(\"This is the best movie that I have ever watched\")  \n  \n---|---|---  \n  \nIn[47]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbl_HTML.gif)\n\n|\n\ntest5.cats  \n  \n---|---|---  \n  \nOut[47]\n\n|\n\n{'POS': 0.9857660531997681, 'NEG': 0.018266398459672928}  \n  \nIn[48]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbm_HTML.gif)\n\n|\n\ntest6 = nlp(\"This movie is so bad\")  \n  \n---|---|---  \n  \nIn[49]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbn_HTML.gif)\n\n|\n\ntest6.cats  \n  \n---|---|---  \n  \nOut[49]\n\n|\n\n{'POS': 0.1364014744758606, 'NEG': 0.8908849358558655}  \n  \n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbo_HTML.gif)\n\n|\n\nNote both NEG and POS labels appeared in prediction results because it used\nmulti-label classifier. The results are satisfactory, but it can improve if\nthe numbers for training epochs are increased. The first sentence has a high\npositive probability output, and the second sentence has predicted as negative\nwith a high probability  \n  \n---|---  \n  \nSpaCy\u2019s text classifier component training is completed.\n\nThe next section will explore Kera, a popular deep leaning library and how to\nwrite Keras code for text classification with another machine learning\nlibrary\u2014TensorFlow\u2018s Keras API.\n\n## 14.7 Artificial Neural Network in a Nutshell\n\nThis workshop section will learn how to incorporate spaCy technology with ANN\n(Artificial Neural Networks) technology using TensorFlow and its Keras package\n(G\u00e9ron 2019; Kedia and Rasu 2020; TensorFlow 2022).\n\nA typical ANN has:\n\n  1. 1.\n\nInput layer consists of input neurons, or nodes.\n\n  2. 2.\n\nHidden layer consists of hidden neurons, or nodes.\n\n  3. 3.\n\nOutput layer consists of output neurons, or nodes\n\nANN will learn knowledge by its network weights update through network\ntraining with sufficient sample inputs and target outputs pairs. The network\ncan predict or match unseen inputs to corresponding output result after it had\nsufficient training to a predefined accuracy. A typical ANN architecture is\nshown in Fig. 14.4.\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Fig4_HTML.png)\n\nFig. 14.4\n\nSystem architecture of ANN\n\n## 14.8 An Overview of TensorFlow and Keras\n\nTensorFlow (G\u00e9ron 2019; TensorFlow 2022) is a popular Python tool widely used\nfor machine learning. It has huge community support and great documentation\navailable at TensorFlow official site (TensorFlow 2022), while Keras is a\nPython based deep learning tool that can be integrated with Python platforms\nsuch as TensorFlow, Theano, and CNTK.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4b071c0-100f-442a-a76f-8e521b021b15": {"__data__": {"id_": "a4b071c0-100f-442a-a76f-8e521b021b15", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5384145-7a6a-47c4-a092-31ab4d25dcea", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "12e7df57a630672c130efac1258763042f1a8f28eef73bc768bb442b4c699ea0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e2a0177-3005-4abd-b7a8-f56792e2dfa5", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "d259069692866531ee8d3b89092bb5f6c7f71e66485521d49d62347bb6afcfb4", "class_name": "RelatedNodeInfo"}}, "hash": "d57c2cea6358fe89245ab14de35478b06f29fef7d60559c6d4097ba868b8463d", "text": "The network\ncan predict or match unseen inputs to corresponding output result after it had\nsufficient training to a predefined accuracy. A typical ANN architecture is\nshown in Fig. 14.4.\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Fig4_HTML.png)\n\nFig. 14.4\n\nSystem architecture of ANN\n\n## 14.8 An Overview of TensorFlow and Keras\n\nTensorFlow (G\u00e9ron 2019; TensorFlow 2022) is a popular Python tool widely used\nfor machine learning. It has huge community support and great documentation\navailable at TensorFlow official site (TensorFlow 2022), while Keras is a\nPython based deep learning tool that can be integrated with Python platforms\nsuch as TensorFlow, Theano, and CNTK.\n\nTensorFlow 1 was disagreeable to symbolic graph computations and other low-\nlevel computations, but TensorFlow 2 initiated great changes in machine\nlearning methods allowing developers to use Keras with TensorFlow\u2019s low-level\nmethods. Keras is popular in R&D because it supports rapid prototyping and\nuser-friendly API to neural network architectures (Kedia and Rasu 2020;\nSrinivasa-Desikan 2018).\n\nNeural networks are commonly used for computer vision and NLP tasks including\nobject detection, image classification, scene understanding, text\nclassification, POS tagging, text summarization, and natural language\ngeneration.\n\nTensorFlow 2 will be used to study the details of a neural network\narchitecture for text classification with tf.keras implementation throughout\nthis section.\n\n## 14.9 Sequential Modeling with LSTM Technology\n\nLong-Short Term Memory network (LSTM network) is one of the significant\nrecurrent networks used in various machine learning applications such as NLP\napplications nowadays (Ekman 2021; Korstanje 2021).\n\nRNNs are special neural networks that can process sequential data in steps.\n\nAll inputs and outputs are independent but not for text data in neural\nnetworks. Every word\u2019s presence depends on neighboring words, e.g. a word is\npredicted by considering all preceding predicted words and stored the past\nsequence token of words within a LTSM cell in a machine translation task. A\nLSTM is showed in Fig. 14.5.\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Fig5_HTML.png)\n\nFig. 14.5\n\nRNN with LSTM technology\n\nAn LSTM cell is moderately complex than an RNN cell, but computation logic is\nidentical. A diagram of a LSTM cell is shown in Fig. 14.6. Note that input and\noutput steps are identical to RNN counterparts:\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Fig6_HTML.png)\n\nFig. 14.6\n\nArchitecture of LSTM cell\n\nKeras has extensive support for RNN variations GRU, LSTM, and simple API for\ntraining RNNs. RNN variations are crucial for NLP tasks as language data\u2019s\nnature is sequential, i.e. text is a sequence of words, speech is a sequence\nof sounds, and so on.\n\nSince the type of statistical model has identified in the design, it can\ntransform a sequence of words into a word IDs sequence and build vocabulary\nwith Keras preprocessing module simultaneously.\n\n## 14.10 Keras Tokenizer in NLP\n\nText is a sequence of words or characters data. A sentence can be fed by a\ntokens sequence. Hence, tokens are to be vectorized first by the following\nsteps:\n\n  1. 1.\n\nTokenize each utterance and turn these utterances into a sequence of tokens.\n\n  2. 2.\n\nBuild a vocabulary from set of tokens presented in step 1. These are tokens to\nbe recognized by neural network design.\n\n  3. 3.\n\nCreate a vocabulary and assign ID to each token.\n\n  4. 4.\n\nMap token vectors with corresponding token IDs.\n\nLet us look at a short example of a corpus for three sentences:\n\nIn[50]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbp_HTML.gif)\n\n|\n\ntestD = [ \"I am going to buy a gift for Christmas tomorrow morning.\",\n\n\"Yesterday my mom cooked a wonderful meal.\",\n\n\"Jack promised he would remember to turn off the lights.\" ]  \n  \n---|---|---  \n  \nIn[51]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7e2a0177-3005-4abd-b7a8-f56792e2dfa5": {"__data__": {"id_": "7e2a0177-3005-4abd-b7a8-f56792e2dfa5", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4b071c0-100f-442a-a76f-8e521b021b15", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "d57c2cea6358fe89245ab14de35478b06f29fef7d60559c6d4097ba868b8463d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71c42d16-407f-4a27-b78c-6d9300942be4", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "71464ff2cd456ffe61427e64a841bd76c0ae7b090cb0d83bc72d109732fdfb7f", "class_name": "RelatedNodeInfo"}}, "hash": "d259069692866531ee8d3b89092bb5f6c7f71e66485521d49d62347bb6afcfb4", "text": "1.\n\nTokenize each utterance and turn these utterances into a sequence of tokens.\n\n  2. 2.\n\nBuild a vocabulary from set of tokens presented in step 1. These are tokens to\nbe recognized by neural network design.\n\n  3. 3.\n\nCreate a vocabulary and assign ID to each token.\n\n  4. 4.\n\nMap token vectors with corresponding token IDs.\n\nLet us look at a short example of a corpus for three sentences:\n\nIn[50]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbp_HTML.gif)\n\n|\n\ntestD = [ \"I am going to buy a gift for Christmas tomorrow morning.\",\n\n\"Yesterday my mom cooked a wonderful meal.\",\n\n\"Jack promised he would remember to turn off the lights.\" ]  \n  \n---|---|---  \n  \nIn[51]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbq_HTML.gif)\n\n|\n\ntestD  \n  \n---|---|---  \n  \nOut[51]\n\n|\n\n['I am going to buy a gift for Christmas tomorrow morning.',\n\n'Yesterday my mom cooked a wonderful meal.',\n\n'Jack promised he would remember to turn off the lights.']  \n  \nLet us tokenize words into utterances:\n\nIn[52]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbr_HTML.gif)\n\n|\n\n**import** spacy\n\n_# Load the NLP model_\n\nnlp = spacy.load(\"en_core_web_md\")\n\n_# Create the utterances object_\n\nutterances = [[token.text **for** token **in** nlp(utterance)] **for**\nutterance **in** testD]\n\n**for** utterance **in** utterances:\n\nutterance  \n  \n---|---|---  \n  \n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbs_HTML.gif)\n\n|\n\nAll tokens of doc object generated by calling nlp(sentence) are iterated in\nthe preceding code. Note that punctuation marks have not filtered as this\nfiltering depends on the task, e.g. punctuation marks such as \u2018!\u2019, Correlate\nto the result in sentiment analysis, they are preserved in this example  \n  \n---|---  \n  \nBuild vocabulary and token sequences into token-ID sequences using _Tokenizer_\nas shown:\n\nIn[53]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbt_HTML.gif)\n\n|\n\n_**# Import Tokenizer**_\n\n**from** tensorflow.keras.preprocessing.text **import** Tokenizer\n\n_# Create tokenizer object (ktoken)_\n\nktoken = Tokenizer(lower=True)  \n  \n---|---|---  \n  \nIn[54]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbu_HTML.gif)\n\n|\n\nktoken.fit_on_texts(testD)\n\nktoken  \n  \n---|---|---  \n  \nOut[54]\n\n|\n\n<keras_preprocessing.text.Tokenizer at 0x2322f3ae970>  \n  \nIn[55]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbv_HTML.gif)\n\n|\n\nktoken.word_index  \n  \n---|---|---  \n  \nOut[55]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbw_HTML.gif)  \n  \n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbx_HTML.gif)\n\n|\n\nThe following are performed in the above codes:\n\n1\\. Import Tokenizer from Keras text preprocessing module\n\n2\\. Create a tokenizer object ( _ktoken_ ) with parameter lower=True, which\nmeans tokenizer should lower all words for vocabulary formation\n\n3\\. Call _ktoken.fit_on_texts_ on data to form vocabulary. _fit_on_text_ work\non a tokens sequence; input should always be a list of tokens\n\n4\\. Examine vocabulary by printing _ktoken.word_index_. _Word_index_ is a\ndictionary where keys are vocabulary tokens and values are token-IDs  \n  \n---|---  \n  \nCall _ktoken.texts_to_sequences()_ method to retrieve a token ID.\n\nNotice that the input to this method should always be a list, even if single\ntoken is fed.\n\nFeed one-word input as a list (notice list brackets) in the following code\nsegment:\n\nIn[56]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71c42d16-407f-4a27-b78c-6d9300942be4": {"__data__": {"id_": "71c42d16-407f-4a27-b78c-6d9300942be4", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e2a0177-3005-4abd-b7a8-f56792e2dfa5", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "d259069692866531ee8d3b89092bb5f6c7f71e66485521d49d62347bb6afcfb4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1a7e625-d391-4b87-9d1a-f1e1efb00005", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "5280721825cff12bb276326b3715a305b32b24f7bd4dfb738e1f0c6aa983a5d8", "class_name": "RelatedNodeInfo"}}, "hash": "71464ff2cd456ffe61427e64a841bd76c0ae7b090cb0d83bc72d109732fdfb7f", "text": "Import Tokenizer from Keras text preprocessing module\n\n2\\. Create a tokenizer object ( _ktoken_ ) with parameter lower=True, which\nmeans tokenizer should lower all words for vocabulary formation\n\n3\\. Call _ktoken.fit_on_texts_ on data to form vocabulary. _fit_on_text_ work\non a tokens sequence; input should always be a list of tokens\n\n4\\. Examine vocabulary by printing _ktoken.word_index_. _Word_index_ is a\ndictionary where keys are vocabulary tokens and values are token-IDs  \n  \n---|---  \n  \nCall _ktoken.texts_to_sequences()_ method to retrieve a token ID.\n\nNotice that the input to this method should always be a list, even if single\ntoken is fed.\n\nFeed one-word input as a list (notice list brackets) in the following code\nsegment:\n\nIn[56]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figby_HTML.gif)\n\n|\n\nktoken.texts_to_sequences([\"Christmas\"])  \n  \n---|---|---  \n  \nOut[56]\n\n|\n\n[[9]]  \n  \nIn[57]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figbz_HTML.gif)\n\n|\n\nktoken.texts_to_sequences([\"cooked\", \"meal\"])  \n  \n---|---|---  \n  \nOut[57]\n\n|\n\n[[15], [18]]  \n  \n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figca_HTML.gif)\n\n|\n\n1\\. Note token-IDs start from 1 and not 0. 0 is a reserved value, which means\na padding value with specific meaning\n\n2\\. Keras cannot process utterances of different lengths, hence need to pad\nall utterances\n\n3\\. Pad each sentence of dataset to a maximum length by adding padding\nutterances either at the start or end of utterances\n\n4\\. Keras inserts 0 for the padding which means it is a padding value without\na token  \n  \n---|---  \n  \nLet us understand how padding works with a simple example.\n\nIn[58]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figcb_HTML.gif)\n\n|\n\n# Import the pad_sequences package\n\n**from** tensorflow.keras.preprocessing.sequence **import** pad_sequences\n\n_# Create the utterance sequences_\n\nseq_utterance = [[7], [8,1], [9,11,12,14]]\n\n_# Define Maximum Length (MLEN)_\n\nMLEN=4\n\n_# Pad the utterance sequences._\n\npad_sequences(seq_utterance, MLEN, padding=\"post\")  \n  \n---|---|---  \n  \nOut[58]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figcc_HTML.gif)  \n  \nIn[59]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figcd_HTML.gif)\n\n|\n\npad_sequences(seq_utterance, MLEN, padding=\"pre\")  \n  \n---|---|---  \n  \nOut[59]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figce_HTML.gif)  \n  \nCall pad_sequences on this sequences list and every sequence is padded with\nzeros so that its length reaches MAX_LEN = 4 which is the length of the\nlongest sequence. Then pad sequences from the right or left with _post_ and\n_pre_ options. Sentences with post option were padded in the preceding code,\nhence the sentences were padded from the right.\n\nWhen these sequences are organized, the complete text preprocessing steps are\nas follows:\n\nIn[60]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figcf_HTML.gif)\n\n|\n\n_# Import the Tokenizer and pad sequences package_\n\n**from** tensorflow.keras.preprocessing.text **import** Tokenizer\n\n**from** tensorflow.keras.preprocessing.sequence **import** pad_sequences\n\n_# Create the token object_\n\nktoken = Tokenizer(lower= **True** )\n\nktoken.fit_on_texts(testD)\n\n_# Create the sequence utterance object_\n\nsutterance = ktoken.texts_to_sequences(testD)\n\nMLEN=7\n\n_# Pad the utterance sequences_\n\npseq_utterance = pad_sequences(sutterance, MLEN, padding=\"post\")\n\npseq_utterance  \n  \n---|---|---  \n  \nOut[60]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1a7e625-d391-4b87-9d1a-f1e1efb00005": {"__data__": {"id_": "f1a7e625-d391-4b87-9d1a-f1e1efb00005", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71c42d16-407f-4a27-b78c-6d9300942be4", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "71464ff2cd456ffe61427e64a841bd76c0ae7b090cb0d83bc72d109732fdfb7f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd8660e1-811b-4090-bf77-608d17849b66", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "e0e63889c99e1ebd8bd569b04cea8c11310ec296e6c1fd232aae2df0111414c3", "class_name": "RelatedNodeInfo"}}, "hash": "5280721825cff12bb276326b3715a305b32b24f7bd4dfb738e1f0c6aa983a5d8", "text": "When these sequences are organized, the complete text preprocessing steps are\nas follows:\n\nIn[60]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figcf_HTML.gif)\n\n|\n\n_# Import the Tokenizer and pad sequences package_\n\n**from** tensorflow.keras.preprocessing.text **import** Tokenizer\n\n**from** tensorflow.keras.preprocessing.sequence **import** pad_sequences\n\n_# Create the token object_\n\nktoken = Tokenizer(lower= **True** )\n\nktoken.fit_on_texts(testD)\n\n_# Create the sequence utterance object_\n\nsutterance = ktoken.texts_to_sequences(testD)\n\nMLEN=7\n\n_# Pad the utterance sequences_\n\npseq_utterance = pad_sequences(sutterance, MLEN, padding=\"post\")\n\npseq_utterance  \n  \n---|---|---  \n  \nOut[60]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figcg_HTML.gif)  \n  \n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figch_HTML.gif)\n\n|\n\nTransform utterances into a token-IDs sequence for tokens vectorization so\nthat utterances will be ready to feed into neural network  \n  \n---|---  \n  \n### 14.10.1 Embedding Words\n\nTokens can be transformed into token vectors. Embedding tokens into vectors\noccurred via a lookup embedding table. Each row holds a token vector indexed\nby token-IDs, hence the flow of obtaining a token vector is as follows:\n\n  1. 1.\n\ntoken->token-ID: A token-ID is assigned with each token with Keras\u2019 Tokenizer\nin previous section. Tokenizer holds all vocabularies and maps each vocabulary\ntoken to an ID.\n\n  2. 2.\n\ntoken-ID->token vector: A token-ID is an integer that can be used as an index\nto embed table's rows. Each token-ID corresponds to one row and when a token\nvector is required, first obtain its token-ID and lookup in the embedding\ntable rows with this token-ID.\n\nA sample of embedding words into token vectors is shown in Fig. 14.7.\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Fig7_HTML.png)\n\nFig. 14.7\n\nA sample of embedding words into token vectors\n\nRemember when a list of utterances began in the previous section:\n\n  1. 1.\n\nEach utterance is divided into tokens and built a vocabulary with Keras\u2019\nTokenizer.\n\n  2. 2.\n\nThe Tokenizer object held a token index with a token->token-ID mapping.\n\n  3. 3.\n\nWhen a token-ID is obtained, lookup to embedding table rows with this token-ID\nto acquire a token vector.\n\n  4. 4.\n\nThis token vector is fed to neural network.\n\nThere are several steps to transform sentences into vectors as training a\nneural network is complex.\n\nA LSTM neural network architecture can be designed to perform model training\nafter these preliminary steps.\n\n## 14.11 Movie Sentiment Analysis with LTSM Using Keras and spaCy\n\nThis section will demonstrate the design of LSTM-based RNN text classifier for\nsentiment analysis with steps below:\n\n  1. 1.\n\nData retrieve and preprocessing.\n\n  2. 2.\n\nTokenize review utterances with padding.\n\n  3. 3.\n\nCreate utterances pad sequence and put into Input Layer.\n\n  4. 4.\n\nVectorize each token and verified by token-ID in Embedding Layer.\n\n  5. 5.\n\nInput token vectors into LSTM.\n\n  6. 6.\n\nTrain LSTM network.\n\nLet us start by recalling the dataset again.\n\n### 14.11.1 Step 1: Dataset\n\nIMDB movie reviews identical dataset from sentiment analysis with spaCy\nsection will be used. They had already processed with _pandas_ and condensed\ninto two columns with binary labels.\n\nReload reviews table and perform data preprocessing as done in previous\nsection to ensure the data is up to date:\n\nIn[61]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd8660e1-811b-4090-bf77-608d17849b66": {"__data__": {"id_": "fd8660e1-811b-4090-bf77-608d17849b66", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1a7e625-d391-4b87-9d1a-f1e1efb00005", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "5280721825cff12bb276326b3715a305b32b24f7bd4dfb738e1f0c6aa983a5d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46f079f5-0da9-4c9d-9f7a-b37d9b3762f8", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "5a2866896927d16c1a4d7861bceb0b28754a5bf65a7ea31819c19e8a7294d58d", "class_name": "RelatedNodeInfo"}}, "hash": "e0e63889c99e1ebd8bd569b04cea8c11310ec296e6c1fd232aae2df0111414c3", "text": "1.\n\nData retrieve and preprocessing.\n\n  2. 2.\n\nTokenize review utterances with padding.\n\n  3. 3.\n\nCreate utterances pad sequence and put into Input Layer.\n\n  4. 4.\n\nVectorize each token and verified by token-ID in Embedding Layer.\n\n  5. 5.\n\nInput token vectors into LSTM.\n\n  6. 6.\n\nTrain LSTM network.\n\nLet us start by recalling the dataset again.\n\n### 14.11.1 Step 1: Dataset\n\nIMDB movie reviews identical dataset from sentiment analysis with spaCy\nsection will be used. They had already processed with _pandas_ and condensed\ninto two columns with binary labels.\n\nReload reviews table and perform data preprocessing as done in previous\nsection to ensure the data is up to date:\n\nIn[61]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figci_HTML.gif)\n\n|\n\n**import** pandas **as** pd\n\n**import** matplotlib.pyplot **as** plt\n\n**%** matplotlib inline\n\n_# Create the movie comment DataFrame and display the statistics_\n\nmcommentDF=pd.read_csv('imdb_5000.csv')\n\nmcommentDF = mcommentDF[['Review','Sentiment']].dropna()\n\naxplot=mcommentDF.Sentiment.value_counts().plot(kind='bar', colormap=\"Paired\")\n\nplt.show()  \n  \n---|---|---  \n  \nOut[61]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figcj_HTML.gif)  \n  \nHere is how _mcommentDF_ dataset should look:\n\nIn[62]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figck_HTML.gif)\n\n|\n\nmcommentDF.head()  \n  \n---|---|---  \n  \nOut[62]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figcl_HTML.gif)  \n  \nNext, extract review text and review label from each dataset row and add them\ninto Python lists:\n\nIn[63]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figcm_HTML.gif)\n\n|\n\n# Import spaCy\n\n**import** spacy\n\n# Load the spaCy NLP model\n\nnlp = spacy.load(\"en_core_web_md\")  \n  \n---|---|---  \n  \nIn[64]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figcn_HTML.gif)\n\n|\n\n_# Create movie comment sample and categories objects_\n\nmovie_comment_exp = []\n\ncategories = []\n\n_# Perform Tokenization_\n\n**for** idx, rw **in** mcommentDF.iterrows():\n\ncomments = rw[\"Review\"]\n\nrating = rw[\"Sentiment\"]\n\ncategories.append(rating)\n\nmtoks = [token.text **for** token **in** nlp(comments)]\n\nmovie_comment_exp.append(mtoks)  \n  \n---|---|---  \n  \nIn[65]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figco_HTML.gif)\n\n|\n\nmovie_comment_exp[0]  \n  \n---|---|---  \n  \nOut[65]\n\n|\n\n['*', '*', 'Possible', 'Spoilers', '*', '*']  \n  \n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figcp_HTML.gif)\n\n|\n\nNote that a list of words to _movie_comment_exp_ has added, hence each element\nof this list is a list of tokens. Next, invoke Keras' Tokenizer on this tokens\nlist to build vocabulary  \n  \n---|---  \n  \n### 14.11.2 Step 2: Data and Vocabulary Preparation\n\nSince the dataset had already processed, tokenize dataset sentences and build\na vocabulary.\n\n  1. 1.\n\nImport necessary Python packages.\n\nIn[66]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figcq_HTML.gif)\n\n|\n\n# Import Tokenizer, pad_sequences\n\n**from** tensorflow.keras.preprocessing.text **import** Tokenizer\n\n**from** tensorflow.keras.preprocessing.sequence **import** pad_sequences\n\n**import** numpy **as** np  \n  \n---|---|---  \n  \n  1. 2.\n\nFeed ktoken into token list and convert them into IDs by calling\ntexts_to_sequences:\n\nIn[67]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "46f079f5-0da9-4c9d-9f7a-b37d9b3762f8": {"__data__": {"id_": "46f079f5-0da9-4c9d-9f7a-b37d9b3762f8", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd8660e1-811b-4090-bf77-608d17849b66", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "e0e63889c99e1ebd8bd569b04cea8c11310ec296e6c1fd232aae2df0111414c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b04af3bf-0277-41a5-823b-d60bd723bc33", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8a670da308df4764199f660dd9c2b6829fc921d88d56b9936427d720b2870cd1", "class_name": "RelatedNodeInfo"}}, "hash": "5a2866896927d16c1a4d7861bceb0b28754a5bf65a7ea31819c19e8a7294d58d", "text": "Next, invoke Keras' Tokenizer on this tokens\nlist to build vocabulary  \n  \n---|---  \n  \n### 14.11.2 Step 2: Data and Vocabulary Preparation\n\nSince the dataset had already processed, tokenize dataset sentences and build\na vocabulary.\n\n  1. 1.\n\nImport necessary Python packages.\n\nIn[66]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figcq_HTML.gif)\n\n|\n\n# Import Tokenizer, pad_sequences\n\n**from** tensorflow.keras.preprocessing.text **import** Tokenizer\n\n**from** tensorflow.keras.preprocessing.sequence **import** pad_sequences\n\n**import** numpy **as** np  \n  \n---|---|---  \n  \n  1. 2.\n\nFeed ktoken into token list and convert them into IDs by calling\ntexts_to_sequences:\n\nIn[67]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figcr_HTML.gif)\n\n|\n\n_# Create ktoken and perform tokenization_\n\nktoken = Tokenizer(lower= **True** )\n\nktoken.fit_on_texts(movie_comment_exp)\n\n_# Create utterance sequences object_\n\nseq_utterance = ktoken.texts_to_sequences(movie_comment_exp)  \n  \n---|---|---  \n  \n  1. 3.\n\nPad short utterance sequences to a maximum length of 50. This will truncate\nlong reviews to 50 words:\n\nIn[68]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figcs_HTML.gif)\n\n|\n\n_# Set the max length to 50_\n\nMLEN = 50\n\n_# Create pad utterance sequence object_\n\nps_utterance = pad_sequences(seq_utterance, MLEN, padding=\"post\")  \n  \n---|---|---  \n  \n  1. 4.\n\nConvert this list of reviews and labels to numpy arrays:\n\nIn[69]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figct_HTML.gif)\n\n|\n\n_# Convert the ps_utterance into numpy arrays_\n\nps_utterance = np.array(ps_utterance)\n\n_# Create the category list (catlist)_\n\ncatlist = np.array(categories)  \n  \n---|---|---  \n  \nIn[70]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figcu_HTML.gif)\n\n|\n\ncatlist = catlist.reshape(catlist.shape[0] , 1)  \n  \n---|---|---  \n  \nIn[71]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figcv_HTML.gif)\n\n|\n\ncatlist.shape  \n  \n---|---|---  \n  \nOut[71]\n\n|\n\n(5000, 1)  \n  \nAll basic preparation works are completed at present to create a LSTM network\nand input data.\n\nLoad TensorFlow Keras related modules:\n\nIn[72]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figcw_HTML.gif)\n\n|\n\n_# Import the_ _LSTM_ _model and the optimizers_\n\n**from** tensorflow.keras.models **import** Model\n\n**from** tensorflow.keras.layers **import** Input, LSTM, Dense, Embedding\n\n**from** tensorflow.keras **import** optimizers  \n  \n---|---|---  \n  \n### 14.11.3 Step 3: Implement the Input Layer\n\nIn[73]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figcx_HTML.gif)\n\n|\n\nutterance_input = Input(shape=( **None** ,))  \n  \n---|---|---  \n  \n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figcy_HTML.gif)\n\n|\n\nDo not confuse _**None**_ as input shape. Here, _None_ means that dimension\ncan be any scalar number. So, use this expression when Keras infers the input\nshape  \n  \n---|---  \n  \n### 14.11.4 Step 4: Implement the Embedding Layer\n\nCreate an Embedding Layer as follows:\n\nIn[74]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figcz_HTML.gif)\n\n|\n\n_# Create the Embedding_Layer_\n\nembedding = Embedding(input_dim = len(ktoken.word_index)+1,\n\noutput_dim = 100)(utterance_input)  \n  \n---|---|---  \n  \n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b04af3bf-0277-41a5-823b-d60bd723bc33": {"__data__": {"id_": "b04af3bf-0277-41a5-823b-d60bd723bc33", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46f079f5-0da9-4c9d-9f7a-b37d9b3762f8", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "5a2866896927d16c1a4d7861bceb0b28754a5bf65a7ea31819c19e8a7294d58d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec77d0b1-8f3c-4f00-9230-e6b127533479", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9ba38e67066737ce18cafda259f7dd41bc47436232562ed87ae2f165faeee455", "class_name": "RelatedNodeInfo"}}, "hash": "8a670da308df4764199f660dd9c2b6829fc921d88d56b9936427d720b2870cd1", "text": "[](../images/533412_1_En_14_Chapter/533412_1_En_14_Figcy_HTML.gif)\n\n|\n\nDo not confuse _**None**_ as input shape. Here, _None_ means that dimension\ncan be any scalar number. So, use this expression when Keras infers the input\nshape  \n  \n---|---  \n  \n### 14.11.4 Step 4: Implement the Embedding Layer\n\nCreate an Embedding Layer as follows:\n\nIn[74]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figcz_HTML.gif)\n\n|\n\n_# Create the Embedding_Layer_\n\nembedding = Embedding(input_dim = len(ktoken.word_index)+1,\n\noutput_dim = 100)(utterance_input)  \n  \n---|---|---  \n  \n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figda_HTML.gif)\n\n|\n\n1\\. When defining embedding layer, input dimension should always be tokens\nnumber in the vocabulary (+1 because the indices start from 1 and not 0. Index\n0 is reserved for padding value)\n\n2\\. Here, 100 is selected as the output shape, hence token vectors for\nvocabulary tokens will be 100-dimensional. Popular numbers for token vector\ndimensions are 50, 100, and 200 depending on task complexity  \n  \n---|---  \n  \n### 14.11.5 Step 5: Implement the LSTM Layer\n\nCreate LSTM_Layer:\n\nIn[75]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figdb_HTML.gif)\n\n|\n\n_# Create the_ _LSTM_ __Layer_\n\nLSTM_layer = LSTM(units=256)(embedding)  \n  \n---|---|---  \n  \n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figdc_HTML.gif)\n\n|\n\nHere, _units_ = 256 is the dimension of hidden state. LSTM output shape and\nhidden state shape are identical due to LSTM architecture.  \n  \n---|---  \n  \n### 14.11.6 Step 6: Implement the Output Layer\n\nWhen a 256-dimensional vector from LSTM layer has obtained, it will be\ncondensed into a 1-dimensional vector (possible values of this vector are 0\nand 1, which are class labels):\n\nIn[76]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figdd_HTML.gif)\n\n|\n\n_# Create the Output Layer_\n\noutlayer = Dense(1, activation=\"sigmoid\")(LSTM_layer)  \n  \n---|---|---  \n  \n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figde_HTML.gif)\n\n|\n\nA sigmoid function is an S-shaped function used as an activation function to\nmap its input to a [0-1] range in output layer. It is commonly used in many\nneural networks  \n  \n---|---  \n  \n### 14.11.7 Step 7: System Compilation\n\nAfter the model has defined, it required to compile with an optimizer, a loss\nfunction, and an evaluation metric:\n\nIn[77]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figdf_HTML.gif)\n\n|\n\n_# Create the_ _IMDB_ _User Review_ _LSTM_ _Model (imdb_mdl)_\n\nimdb_mdl = Model(inputs=[utterance_input],outputs=[outlayer])  \n  \n---|---|---  \n  \nLet us look at an _imdb_mdl_ model setup:\n\nIn[78]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figdg_HTML.gif)\n\n|\n\nimdb_mdl.summary()  \n  \n---|---|---  \n  \nOut[78]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figdh_HTML.gif)  \n  \nNext, invoke model compilation:\n\nIn[79]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figdi_HTML.gif)\n\n|\n\nimdb_mdl.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",\nmetrics=[\"accuracy\"])  \n  \n---|---|---  \n  \n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figdj_HTML.gif)\n\n|\n\n1\\.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec77d0b1-8f3c-4f00-9230-e6b127533479": {"__data__": {"id_": "ec77d0b1-8f3c-4f00-9230-e6b127533479", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b04af3bf-0277-41a5-823b-d60bd723bc33", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8a670da308df4764199f660dd9c2b6829fc921d88d56b9936427d720b2870cd1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81313276-cf58-464d-9b71-d5eba1a3fba1", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "69824f22155cf0470fe36367f9f33238932c9a921bca594409de4287cafa213f", "class_name": "RelatedNodeInfo"}}, "hash": "9ba38e67066737ce18cafda259f7dd41bc47436232562ed87ae2f165faeee455", "text": "[](../images/533412_1_En_14_Chapter/533412_1_En_14_Figdg_HTML.gif)\n\n|\n\nimdb_mdl.summary()  \n  \n---|---|---  \n  \nOut[78]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figdh_HTML.gif)  \n  \nNext, invoke model compilation:\n\nIn[79]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figdi_HTML.gif)\n\n|\n\nimdb_mdl.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",\nmetrics=[\"accuracy\"])  \n  \n---|---|---  \n  \n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figdj_HTML.gif)\n\n|\n\n1\\. Use ADAM (adaptive moment estimation) as optimizer for LSTM training for\nimdb_mdl LSTM model\n\n2\\. Use binary cross-entropy as loss function\n\n3\\. A list of supported performance metrics can be found in Keras official\nsite (Keras 2022)  \n  \n---|---  \n  \n### 14.11.8 Step 8: Model Fitting and Experiment Evaluation\n\nFeed _imdb_mdl_ model to data with 5 epochs to reduce time:\n\nIn[80]\n\n|\n\n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figdk_HTML.gif)\n\n|\n\n_# Model fitting by using 5 epochs_\n\nimdb_mdl.fit(x=ps_utterance,\n\ny=catlist,\n\nbatch_size=64,\n\nepochs=5,\n\nvalidation_split=0.3)  \n  \n---|---|---  \n  \nOut[80]\n\n|\n\nEpoch 1/5\n\n55/55 [=====] - 30s 528ms/step - loss: 0.6062 - accuracy: 0.6591 - val_loss:\n0.5078 - val_accuracy: 0.7427\n\nEpoch 2/5\n\n55/55 [=====] - 28s 501ms/step - loss: 0.2577 - accuracy: 0.9054 - val_loss:\n0.4609 - val_accuracy: 0.7893\n\nEpoch 3/5\n\n55/55 [=====] - 26s 465ms/step - loss: 0.0940 - accuracy: 0.9737 - val_loss:\n0.8353 - val_accuracy: 0.7580\n\nEpoch 4/5\n\n55/55 [=====] - 24s 432ms/step - loss: 0.0357 - accuracy: 0.9911 - val_loss:\n0.9209 - val_accuracy: 0.7633\n\nEpoch 5/5\n\n55/55 [=====] - 24s 438ms/step - loss: 0.0314 - accuracy: 0.9917 - val_loss:\n0.5480 - val_accuracy: 0.7793\n\n<keras.callbacks.History at 0x26289a53f10>  \n  \n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figdl_HTML.gif)\n\n|\n\n1\\. x is a list of _ps_utterance_ for network training and y is the list of\ncategories (catlist). The epochs parameter is set to 5 to process 5 passes\nover the data\n\n2\\. Data has processed 5 times in parameter batch_size = 64 means a batch of\n64 training utterances are fed into the memory at a time due to memory\nlimitations\n\n3\\. The _validation_split_ = 0.3 means 70% of dataset are used for training\nand 30% are used for system validation\n\n4\\. An experiment validation accuracy rate 0.7793 is acceptable for a basic\nLSTM network training for 5 epochs only  \n  \n---|---  \n  \n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figdm_HTML.gif)\n\n|\n\n**Workshop 5.2 Further Exploration of** **LSTM** **model on Movie Sentiment\nAnalysis**\n\n1\\. Follow Workshop 14.1 logic and use rating (0\u201310) field of IMDB movie\nreviews dataset to reconstruct a LSTM for sentiment analysis into 3\ncategories: Positive, Neutral and Negative\n\n2\\. Verify training performance\n\n3\\. Experiment with the code further by placing dropout layers at different\nlocations such as after embedding layer or, after LSTM layer\n\n4\\.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81313276-cf58-464d-9b71-d5eba1a3fba1": {"__data__": {"id_": "81313276-cf58-464d-9b71-d5eba1a3fba1", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec77d0b1-8f3c-4f00-9230-e6b127533479", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9ba38e67066737ce18cafda259f7dd41bc47436232562ed87ae2f165faeee455", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9029a78-c1dc-4828-bfee-7900a3378184", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f353b99e7732ac39389aa5f76fdcc66c8c581b83d1d2f26f6141ffa801124554", "class_name": "RelatedNodeInfo"}}, "hash": "69824f22155cf0470fe36367f9f33238932c9a921bca594409de4287cafa213f", "text": "An experiment validation accuracy rate 0.7793 is acceptable for a basic\nLSTM network training for 5 epochs only  \n  \n---|---  \n  \n![](../images/533412_1_En_14_Chapter/533412_1_En_14_Figdm_HTML.gif)\n\n|\n\n**Workshop 5.2 Further Exploration of** **LSTM** **model on Movie Sentiment\nAnalysis**\n\n1\\. Follow Workshop 14.1 logic and use rating (0\u201310) field of IMDB movie\nreviews dataset to reconstruct a LSTM for sentiment analysis into 3\ncategories: Positive, Neutral and Negative\n\n2\\. Verify training performance\n\n3\\. Experiment with the code further by placing dropout layers at different\nlocations such as after embedding layer or, after LSTM layer\n\n4\\. Try different values for embedding dimensions such as 50, 150, and 200 to\nobserve change in accuracy\n\n5\\. Experiment with different values instead of 256 at LSTM layer's hidden\ndimension. Try different parameters for each to perform simulations and see\nwhether the best configuration can be found  \n  \n---|---  \n  \nReferences\n\n  1. Agarwal, B. (2020) Deep Learning-Based Approaches for Sentiment Analysis (Algorithms for Intelligent Systems). Springer.[Crossref](https://doi.org/10.1007/978-981-15-1216-2)\n\n  2. Albrecht, J., Ramachandran, S. and Winkler, C. (2020) Blueprints for Text Analytics Using Python: Machine Learning-Based Solutions for Common Real World (NLP) Applications. O\u2019Reilly Media.\n\n  3. Altinok, D. (2021) Mastering spaCy: An end-to-end practical guide to implementing NLP applications using the Python ecosystem. Packt Publishing.\n\n  4. Arumugam, R., & Shanmugamani, R. (2018). Hands-on natural language processing with python. Packt Publishing.\n\n  5. Bird, S., Klein, E., and Loper, E. (2009). Natural language processing with python. O'Reilly.[zbMATH](http://www.emis.de/MATH-item?1187.68630)\n\n  6. Ekman, M. (2021) Learning Deep Learning: Theory and Practice of Neural Networks, Computer Vision, Natural Language Processing, and Transformers Using TensorFlow. Addison-Wesley Professional.\n\n  7. George, A. (2022) Python Text Mining: Perform Text Processing, Word Embedding, Text Classification and Machine Translation. BPB Publications.\n\n  8. G\u00e9ron, A. (2019) Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O\u2019Reilly Media.\n\n  9. Kaggle (2022) IMDB Large Movie Review Dataset from Kaggle. [https://\u200bwww.\u200bkaggle.\u200bcom/\u200bcode/\u200bnisargchodavadiy\u200ba/\u200bmovie-review-analytics-sentiment-ratings/\u200bdata](https://www.kaggle.com/code/nisargchodavadiya/movie-review-analytics-sentiment-ratings/data). Accessed 23 June 2022.\n\n  10. Kedia, A. and Rasu, M. (2020) Hands-On Python Natural Language Processing: Explore tools and techniques to analyze and process text with a view to building real-world NLP applications. Packt Publishing.\n\n  11. Keras (2022) Keras official site performance metrics. [https://\u200bkeras.\u200bio/\u200bapi/\u200bmetrics](https://keras.io/api/metrics). Accessed 23 June 2022.\n\n  12. Korstanje, J. (2021) Advanced Forecasting with Python: With State-of-the-Art-Models Including LSTMs, Facebook\u2019s Prophet, and Amazon\u2019s DeepAR. Apress.[Crossref](https://doi.org/10.1007/978-1-4842-7150-6)\n\n  13. Perkins, J. (2014). Python 3 text processing with NLTK 3 cookbook. Packt Publishing Ltd.\n\n  14. Pozzi, F., Fersini, E., Messina, E. and Liu, B. (2016) Sentiment Analysis in Social Networks. Morgan Kaufmann.\n\n  15. SpaCy (2022) spaCy official site.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9029a78-c1dc-4828-bfee-7900a3378184": {"__data__": {"id_": "b9029a78-c1dc-4828-bfee-7900a3378184", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81313276-cf58-464d-9b71-d5eba1a3fba1", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "69824f22155cf0470fe36367f9f33238932c9a921bca594409de4287cafa213f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "88fc90ea-8ffc-466f-90c0-fc10e285d819", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "90645505b9fa15dc7d9eccaa335b698b9a44aa7acbf77c68a89c8fb41bcbd49c", "class_name": "RelatedNodeInfo"}}, "hash": "f353b99e7732ac39389aa5f76fdcc66c8c581b83d1d2f26f6141ffa801124554", "text": "Accessed 23 June 2022.\n\n  12. Korstanje, J. (2021) Advanced Forecasting with Python: With State-of-the-Art-Models Including LSTMs, Facebook\u2019s Prophet, and Amazon\u2019s DeepAR. Apress.[Crossref](https://doi.org/10.1007/978-1-4842-7150-6)\n\n  13. Perkins, J. (2014). Python 3 text processing with NLTK 3 cookbook. Packt Publishing Ltd.\n\n  14. Pozzi, F., Fersini, E., Messina, E. and Liu, B. (2016) Sentiment Analysis in Social Networks. Morgan Kaufmann.\n\n  15. SpaCy (2022) spaCy official site. [https://\u200bspacy.\u200bio/\u200b](https://spacy.io/). Accessed 16 June 2022.\n\n  16. Sarkar, D. (2019) Text Analytics with Python: A Practitioner\u2019s Guide to Natural Language Processing. Apress.[Crossref](https://doi.org/10.1007/978-1-4842-4354-1)\n\n  17. Siahaan, V. and Sianipar, R. H. (2022) Text Processing and Sentiment Analysis using Machine Learning and Deep Learning with Python GUI. Balige Publishing.\n\n  18. Srinivasa-Desikan, B. (2018). Natural language processing and computational linguistics: A practical guide to text analysis with python, gensim, SpaCy, and keras. Packt Publishing Limited.\n\n  19. TensorFlow (2022) TensorFlow official site. [https://\u200btensorflow.\u200borg](https://tensorflow.org)/. Accessed 22 June 2022.\n\n  20. Vasiliev, Y. (2020) Natural Language Processing with Python and spaCy: A Practical Introduction. No Starch Press.\n\n\n\u00a9 The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.\n2024\n\nR. S. T. Lee Natural Language Processing\n<https://doi.org/10.1007/978-981-99-1999-4_15>\n\n# 15\\. Workshop#6 Transformers with spaCy and TensorFlow (Hour 11\u201312)\n\nRaymond S. T. Lee 1\n\n(1)\n\nUnited International College, Beijing Normal University-Hong Kong Baptist\nUniversity, Zhuhai, China\n\n## 15.1 Introduction\n\nIn Chap. [8](533412_1_En_8_Chapter.xhtml), the basic concept about Transfer\nLearning, its motivation and related background knowledge such as Recurrent\nNeural Networks (RNN) with Transformer Technology and BERT model are\nintroduced.\n\nThis workshop will learn about the latest topic Transformers in NLP, how to\nuse them with TensorFlow and spaCy. First, we will learn about Transformers\nand Transfer learning. Second, we will learn about a commonly used Transformer\narchitecture\u2014Bidirectional Encoder Representations from Transformers (BERT) as\nwell as to how BERT Tokenizer and WordPiece algorithms work.\n\nFurther, we will learn how to start with pre-trained transformer models of\nHuggingFace library (HuggingFace 2022) and practice to fine-tune HuggingFace\nTransformers with TensorFlow and Keras (TensorFlow 2022; Keras 2022) followed\nby how spaCy v3.0 (spaCy 2022) integrates transformer models as pre-trained\npipelines. These techniques and tools will be used in the last workshop for\nbuilding a Q&A chatbot.\n\nHence, this workshop will cover the following topics:\n\n  * Transformers and Transfer Learning.\n\n  * Understanding BERT.\n\n  * Transformers and TensorFlow.\n\n  * Transformers and spaCy.\n\n## 15.2 Technical Requirements\n\nTransformers, TensorFlow and spaCy (TensorFlow 2022; spaCy 2022) are to be\ninstalled in own PC/notebook computer.\n\nAll source codes for this workshop can be downloaded from GitHub archive on\nNLPWorkshop6 (NLPWorkshop6 2022).\n\nUse pip install commands to install the following packages:\n\n  * pip install spacy.\n\n  * pip install TensorFlow (note: version 2.2 or above).\n\n  * pip install transformers.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88fc90ea-8ffc-466f-90c0-fc10e285d819": {"__data__": {"id_": "88fc90ea-8ffc-466f-90c0-fc10e285d819", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9029a78-c1dc-4828-bfee-7900a3378184", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f353b99e7732ac39389aa5f76fdcc66c8c581b83d1d2f26f6141ffa801124554", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98f79b34-99f6-42d4-a905-f611eb0ec699", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f7c3d89d24b344f6b3b1fc844ea60ec52530916517b1c47d52eee197d4871bcc", "class_name": "RelatedNodeInfo"}}, "hash": "90645505b9fa15dc7d9eccaa335b698b9a44aa7acbf77c68a89c8fb41bcbd49c", "text": "These techniques and tools will be used in the last workshop for\nbuilding a Q&A chatbot.\n\nHence, this workshop will cover the following topics:\n\n  * Transformers and Transfer Learning.\n\n  * Understanding BERT.\n\n  * Transformers and TensorFlow.\n\n  * Transformers and spaCy.\n\n## 15.2 Technical Requirements\n\nTransformers, TensorFlow and spaCy (TensorFlow 2022; spaCy 2022) are to be\ninstalled in own PC/notebook computer.\n\nAll source codes for this workshop can be downloaded from GitHub archive on\nNLPWorkshop6 (NLPWorkshop6 2022).\n\nUse pip install commands to install the following packages:\n\n  * pip install spacy.\n\n  * pip install TensorFlow (note: version 2.2 or above).\n\n  * pip install transformers.\n\n## 15.3 Transformers and Transfer Learning in a Nutshell\n\nTransformers in NLP is an innovative idea which aims to solve sequential\nmodelling tasks and targets problems introduced by Long-Short-Term-Memory\n(LSTM) architecture (Ekman 2021; Korstanje 2021).\n\nIt is a contemporary machine learning concept and architecture introduced by\nVaswani et al. (2017) in research paper _Attention Is All You Need_. It\nexplained that \u201cThe Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output\nwithout using sequence-aligned RNNs or convolution.\u201d\n\nTransduction in this context means transforming input words to output words by\ntransforming input words and sentences into vectors. A transformer is trained\non a large corpus such as Wiki or news. These vectors will be used to convey\ninformation regarding word semantics, sentence structures, and sentence\nsemantics for downstream tasks.\n\nWord vectors like Glove and FastText are already trained on Wikipedia corpus\nthat can be used in semantic similarity calculations, hence, Transfer Learning\nmeans to import knowledge from pre-trained word vectors or pre-trained\nstatistical models.\n\nTransformers offer many pre-trained models to perform NLP tasks such as text\nclassification, text summarization, question answering, machine translation,\nand natural language generation in over 100 languages. It aims to make state-\nof-the-art NLP accessible to everyone (Bansal 2021; Rothman 2022; Tunstall et\nal. 2022; Y\u0131ld\u0131r\u0131m and Asgari-Chenaghlu 2021).\n\nA list of Transformer models provided by HuggingFace ( 2022) is shown in Fig.\n15.1. Each model is named with a combination of architecture names such as\nBERT or DistilBert, possibly a language code, i.e. en, de, multilingual, which\nis located at the left side of the figure, and information regarding whether\nthe model is cased or uncased i.e. distinguish between uppercase and lowercase\ncharacters.\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Fig1_HTML.png)\n\nFig. 15.1\n\nSample Input Texts and their corresponding Output Class Labels\n\nTask names are also listed at the left-hand side. Each model is labeled with a\ntask name such as text classification or machine translation for Q&A chatbot.\n\n## 15.4 Why Transformers?\n\nLet us review text classification with spaCy in LSTM architecture.\n\nLSTMs work for modelling text effectively, but there are shortcomings:\n\n  * LSTM architecture has difficulties in learning long texts sometimes. Statistical dependencies in a long text have problems represented by LSTM because it can fail to recall words processed earlier as time steps progress.\n\n  * LSTMs are sequential which means that a single word can process at each time step but is impossible to parallelizing learning process causing bottleneck.\n\nTransformers address these problems by not using recurrent layers at all,\ntheir architecture is different from LSTM architecture (Bansal 2021; Rothman\n2022; Tunstall et al. 2022; Y\u0131ld\u0131r\u0131m and Asgari-Chenaghlu 2021). A Transformer\narchitecture has an input encoder block at the left, called Encoder, and an\noutput decoder at the right called Decoder as shown in Fig. 15.2.\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Fig2_HTML.png)\n\nFig. 15.2\n\nTransformer architecture\n\nThe architecture is catered for a machine translation task, input is a\nsequence of words from source language, and output is a sequence of words in\ntarget language. Encoder generates a vector representation of input words and\npasses them to decoder where the word vector transfer is represented by an\narrow from encoder block to decoder block direction.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "98f79b34-99f6-42d4-a905-f611eb0ec699": {"__data__": {"id_": "98f79b34-99f6-42d4-a905-f611eb0ec699", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88fc90ea-8ffc-466f-90c0-fc10e285d819", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "90645505b9fa15dc7d9eccaa335b698b9a44aa7acbf77c68a89c8fb41bcbd49c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6210a687-7ffb-478e-9cd9-5c68fc6c3f2a", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "0b2703b363783045bba68f7cb8380328b97f32db5e776e235914d6220c642aa2", "class_name": "RelatedNodeInfo"}}, "hash": "f7c3d89d24b344f6b3b1fc844ea60ec52530916517b1c47d52eee197d4871bcc", "text": "2022; Y\u0131ld\u0131r\u0131m and Asgari-Chenaghlu 2021). A Transformer\narchitecture has an input encoder block at the left, called Encoder, and an\noutput decoder at the right called Decoder as shown in Fig. 15.2.\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Fig2_HTML.png)\n\nFig. 15.2\n\nTransformer architecture\n\nThe architecture is catered for a machine translation task, input is a\nsequence of words from source language, and output is a sequence of words in\ntarget language. Encoder generates a vector representation of input words and\npasses them to decoder where the word vector transfer is represented by an\narrow from encoder block to decoder block direction. Decoder extracts input\nword vectors, transforms output words into word vectors, and generates the\nprobability of each output word.\n\nThere are feedforward layers, which are dense layers in encoder and decoder\nblocks used for text classification with spaCy. The innovative transformers\ncan place in a Multi-Head Attention block to create a dense representation for\neach word with self-attention mechanism. This mechanism relates each word in\ninput sentence to other words in the input sentence. Word embedding is\ncalculated by taking a weighted average of other words\u2019 embeddings, and each\nword significance can be calculated in input sentence to enable the\narchitecture focus on each input word sequentially.\n\nA self-attention mechanism of how input words at the left-hand side attend\ninput word _it_ at the right-hand side is shown in Fig. 15.3. Dark colors\nrepresented relevance, phrase _the animal_ are related to _it_ than other\nwords in the sentence. This signified transformer can resolve many semantic\ndependencies in a sentence and used in different tasks such as text\nclassification and machine translation since they have several architectures\ndepending on tasks. BERT is a popular architecture to be used.\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Fig3_HTML.png)\n\nFig. 15.3\n\nIllustration of the self-attention mechanism\n\n## 15.5 An Overview of BERT Technology\n\n### 15.5.1 What Is BERT?\n\nBERT is introduced in a Google\u2019s original research paper published by Devlin\net al. (2019), the complete Google BERT model can be downloaded from Google\u2019s\nGitHub archive (GoogleBert 2022).\n\nIt has the following output features (Bansal 2021; Rothman 2022; Tunstall et\nal. 2022; Y\u0131ld\u0131r\u0131m and Asgari-Chenaghlu 2021):\n\n  * Bidirectional: Each input sentence text data training is processed from left to right and from right to left.\n\n  * Encoder: An encoder encodes input sentence.\n\n  * Representations: A representation is a word vector.\n\n  * Transformers: A transformer-based architecture.\n\nBERT is a trained transformer encoder stack. The input is a sentence, and the\noutput is a sequence of word vectors. Word vectors are contextual which means\nthat a word vector is assigned to a word based on input sentence. In short,\nBERT outputs contextual word representations as shown in Fig. 15.4.\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Fig4_HTML.png)\n\nFig. 15.4\n\nWord vector for the word \u201cbank\u201d\n\nIt is noted that word _bank_ has different meanings in these two sentences,\nword vectors are the same because Glove and FastText are static. Each word has\nonly one vector and vectors are saved to a file after training. Then, these\npre-trained vectors can be downloaded to our application. BERT word vectors\nare dynamic on the contrary. It can generate different word vectors for the\nsame word depending on input sentence. Word vectors generated by BERT are\nshown in Fig. 15.5 against the counterpart shown in Fig. 15.4:\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Fig5_HTML.png)\n\nFig. 15.5\n\nTwo distinct word vectors generated by BERT for the same word _bank_ in two\ndifferent contexts\n\n### 15.5.2 BERT Architecture\n\nBERT is a transformer encoder stack, which means several encoder layers are\nstacked on top of each other. The first layer initializes word vectors\nrandomly, and then each encoder layer transforms output of the previous\nencoder layer. Figure 15.6 illustrates two BERT model sizes: BERT Base and\nBERT Large.\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6210a687-7ffb-478e-9cd9-5c68fc6c3f2a": {"__data__": {"id_": "6210a687-7ffb-478e-9cd9-5c68fc6c3f2a", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "98f79b34-99f6-42d4-a905-f611eb0ec699", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f7c3d89d24b344f6b3b1fc844ea60ec52530916517b1c47d52eee197d4871bcc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2807e937-7ce2-4d9a-a8db-66b5af9d3cbd", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "59b633c35d627c87118ebe2b4342719fba952e0c4f20178ef670062f1e5da296", "class_name": "RelatedNodeInfo"}}, "hash": "0b2703b363783045bba68f7cb8380328b97f32db5e776e235914d6220c642aa2", "text": "BERT word vectors\nare dynamic on the contrary. It can generate different word vectors for the\nsame word depending on input sentence. Word vectors generated by BERT are\nshown in Fig. 15.5 against the counterpart shown in Fig. 15.4:\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Fig5_HTML.png)\n\nFig. 15.5\n\nTwo distinct word vectors generated by BERT for the same word _bank_ in two\ndifferent contexts\n\n### 15.5.2 BERT Architecture\n\nBERT is a transformer encoder stack, which means several encoder layers are\nstacked on top of each other. The first layer initializes word vectors\nrandomly, and then each encoder layer transforms output of the previous\nencoder layer. Figure 15.6 illustrates two BERT model sizes: BERT Base and\nBERT Large.\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Fig6_HTML.png)\n\nFig. 15.6\n\nBERT Base and Large architectures (having 12 and 24 encoder layers,\nrespectively)\n\nBERT Base and BERT Large have 12 and 24 encoder layers to generate word\nvectors sizes of 768 and 1024 comparatively.\n\nBERT outputs word vectors for each input word. A high-level overview of BERT\ninputs and outputs is illustrated in Fig. 15.7. It showed that BERT input\nshould be in a special format to include special tokens such as CLS.\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Fig7_HTML.png)\n\nFig. 15.7\n\nBERT model input word and output word vectors\n\n### 15.5.3 BERT Input Format\n\nAfter learning BERT basic architecture, let us look at how to generate output\nvectors using BERT.\n\nBERT input format can represent a single sentence and a pair of sentences in a\nsingle sequence of tokens (for tasks such as question answering and semantic\nsimilarity, we input two sentences to the model).\n\nBERT works with a special tokens class and a special tokenization algorithm\ncalled WordPiece.\n\nThere are several types of special tokens [CLS], [SEP], and [PAD]:\n\n  * [CLS] is the first special token type for every input sequence. This token is a quantity of input sentence for classification tasks but disregard non-classification tasks.\n\n  * [SEP] is a sentence separator. If the input is a single sentence, this token will be placed at the end of sentence, i.e. [CLS] sentence [SEP], or to separate two sentences, i.e. [CLS] sentence1 [SEP] sentence2 [SEP].\n\n  * [PAD] is a special token for padding. The padding values can generate sentences from dataset with equal length. BERT receives sentences with fixed length only, hence, padding short sentences is required prior feeding to BERT. The tokens maximum length can feed to BERT is 512.\n\nIt was learnt that a sentence can feed to Keras model one word at a time,\ninput sentences can be tokenized into words using spaCy tokenizer, but BERT\nworks differently as it uses WordPiece tokenization. A _word piece_ is\nliterally a piece of a word.\n\nWordPiece algorithm breaks down words into several subwords, its logic behind\nis to break down complex/long tokens into tokens, e.g. the word _playing_ is\ntokenized as play + ##ing. A ## character is placed before every word piece to\nindicate that this token is not a word from language\u2019s vocabulary but is a\nword piece.\n\nLet us look at some examples:\n\nplaying play, ##ing\n\nplayed play, ##ed\n\ngoing go, ##ing\n\nvocabulary = [play, go, ##ing, ##ed]  \n  \n---  \n  \nIt can concise language vocabulary as WordPiece groups common subwords.\n\nWordPiece tokenization can divide rare/unseen words into their subwords.\n\nAfter input sentence is tokenized and special tokens are added, each token is\nconverted to its ID and feed token-ID sequences to BERT.\n\nAn input sentence transformed into BERT input format is illustrated in Fig.\n15.8.\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Fig8_HTML.png)\n\nFig. 15.8\n\nTransforming an input sentence into BERT input format\n\nBERT Tokenizer has several methods to perform above tasks but it has an\nencoding method that combines these steps into a single step.\n\n### 15.5.4 How to Train BERT?", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2807e937-7ce2-4d9a-a8db-66b5af9d3cbd": {"__data__": {"id_": "2807e937-7ce2-4d9a-a8db-66b5af9d3cbd", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6210a687-7ffb-478e-9cd9-5c68fc6c3f2a", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "0b2703b363783045bba68f7cb8380328b97f32db5e776e235914d6220c642aa2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f200de91-2ef5-48bc-8ba8-e374b3f44f01", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "39b73572bea0ca6bfe5235b1c151813918a13926c2d3857552b309e8b76c03b1", "class_name": "RelatedNodeInfo"}}, "hash": "59b633c35d627c87118ebe2b4342719fba952e0c4f20178ef670062f1e5da296", "text": "WordPiece tokenization can divide rare/unseen words into their subwords.\n\nAfter input sentence is tokenized and special tokens are added, each token is\nconverted to its ID and feed token-ID sequences to BERT.\n\nAn input sentence transformed into BERT input format is illustrated in Fig.\n15.8.\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Fig8_HTML.png)\n\nFig. 15.8\n\nTransforming an input sentence into BERT input format\n\nBERT Tokenizer has several methods to perform above tasks but it has an\nencoding method that combines these steps into a single step.\n\n### 15.5.4 How to Train BERT?\n\nBERT originators stated that \u201cWe then train a large model (12-layers to\n24-layers Transformer) on a large corpus (Wikipedia + BookCorpus) for a long\ntime (1 M update steps), and that is BERT\u201d in Google Research\u2019s BERT GitHub\nrepository (GoogleBert 2022).\n\nBERT is trained by masked language model (MLM) and next sentence prediction\n(NSP).\n\nLanguage modelling is the task of predicting the next token given the sequence\nof previous tokens. For example, given the sequence of words _Yesterday I\nvisited_ , a language model can predict the next token as one of the tokens\n_church_ , _hospital_ , _school_ , and so on.\n\nMLM is different. A percentage of tokens are masked randomly to replace a\n[MASK] token and presume MLM predicts the masked words.\n\nBERT\u2019s masked language model is implemented as follows:\n\n  1. 1.\n\nselect 15 input tokens randomly.\n\n  2. 2.\n\n80% of selected tokens are replaced by [MASK].\n\n  3. 3.\n\n10% of selected tokens are replaced by another token from vocabulary.\n\n  4. 4.\n\n10% remain unchanged.\n\nA training sentence to LMM example is as follows:\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figa_HTML.png)\n\nNSP is the task of predicting the next sentence given by an input sentence.\nThere are two sentences fed to BERT and presume BERT predicts sentences order\nif second sentence is followed by first sentence.\n\nAn input of two sentences separated by a [SEP] token to NSP example is as\nfollows:\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figb_HTML.png)\n\nIt showed that second sentence can follow first sentence, hence, the predicted\nlabel is IsNext.\n\nHere is another example:\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figc_HTML.png)\n\nThis example showed that the pair of sentences generate a _NotNext_ label\nwithout contextual or semantical relevance.\n\n## 15.6 Transformers with TensorFlow\n\nPre-trained transformer models are provided to program developers in open\nsources by many organizations including Google (GoogleBert 2022), Facebook\n(Facebook-transformer 2022), and HuggingFace (HuggingFace_transformer 2022).\n\nHuggingFace is an AI company focuses on NLP apportioned to open source.\n\nThese pre-trained models and agreeable interfaces can integrate transformers\ninto Python code, as interfaces are compatible with either PyTorch or\nTensorFlow or both. HuggingFace\u2018s pre-trained transformers and their\nTensorFlow interface to transformer models will be used in this workshop.\n\n### 15.6.1 HuggingFace Transformers\n\nThis section will explore HuggingFace\u2018s pre-trained models, TensorFlow\ninterface and its conventions. HuggingFace offers several models as in Fig.\n15.1. Each model is dedicated to a task such as text classification, question\nanswering, and sequence-to-sequence modelling.\n\nA HuggingFace documentation of a distilbert-base-uncased-distilled-squad model\nis shown in Fig. 15.9. A Question-Answering task tag is assigned to the upper-\nleft corner in the documentation followed by supporting deep learning\nlibraries PyTorch, TensorFlow, TFLite, TFSavedModel, training dataset, e.g.\nsquad, model language, e.g. en for English; the license and base model\u2019s name,\ne.g. DistilBERT.\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Fig9_HTML.png)\n\nFig. 15.9\n\nDocumentation of the distilbert-base-uncased-distilled-squad model\n\nSome models are trained with similar algorithms that belong to identical model\nfamily.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f200de91-2ef5-48bc-8ba8-e374b3f44f01": {"__data__": {"id_": "f200de91-2ef5-48bc-8ba8-e374b3f44f01", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2807e937-7ce2-4d9a-a8db-66b5af9d3cbd", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "59b633c35d627c87118ebe2b4342719fba952e0c4f20178ef670062f1e5da296", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93aa9ea2-be05-4aeb-9c07-be8880329809", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "73cf59a42abce127d2ee6e32229cb320bc7a8e8b98b0b4e59495946c025e314b", "class_name": "RelatedNodeInfo"}}, "hash": "39b73572bea0ca6bfe5235b1c151813918a13926c2d3857552b309e8b76c03b1", "text": "A HuggingFace documentation of a distilbert-base-uncased-distilled-squad model\nis shown in Fig. 15.9. A Question-Answering task tag is assigned to the upper-\nleft corner in the documentation followed by supporting deep learning\nlibraries PyTorch, TensorFlow, TFLite, TFSavedModel, training dataset, e.g.\nsquad, model language, e.g. en for English; the license and base model\u2019s name,\ne.g. DistilBERT.\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Fig9_HTML.png)\n\nFig. 15.9\n\nDocumentation of the distilbert-base-uncased-distilled-squad model\n\nSome models are trained with similar algorithms that belong to identical model\nfamily. For example, the DistilBERT family has many models such as distilbert-\nbase-uncased and distilbert-multilingual-cased. Each model name includes\ninformation such as casing to distinguish uppercase/lowercase or model\nlanguage such as en, de, or multilingual.\n\nHuggingFace documentation provides information about each model family with\nindividual model\u2019s API in detail. Lists of available models and BERT model\narchitecture variations are shown in Fig. 15.10.\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Fig10_HTML.png)\n\nFig. 15.10\n\nLists of the available models (left-hand side) and BERT model variations\n(right-hand side)\n\nBERT model has many tasks variations such as text classification, question\nanswering, and next sentence prediction.\n\nEach of these models is obtained by placing extra layers atop of BERT output\nas these outputs are a sequence of word vectors for each word of input\nsentences.\n\nFor example, a BERTForSequenceClassification model is obtained by placing a\ndense layer atop of BERT word vectors.\n\n### 15.6.2 Using the BERT Tokenizer\n\nBERT uses WordPiece algorithm for tokenization to ensure that each input word\nis divided into subwords.\n\nLet us look at how to prepare input data with HuggingFace library.\n\nIn[1]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figd_HTML.gif)\n\n|\n\n_# Import transformer package_\n\n**from** transformers **import** BertTokenizer\n\n_# Create bert_tokenizer and sample utterance (utt1) and tokens (tok1)_\n\nbtokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\nutt1 = \"He lived characteristically idle and romantic.\"\n\nutt1 = \"[CLS] \" + utt1 + \" [SEP]\"\n\ntok1 = btokenizer.tokenize(utt1)  \n  \n---|---|---  \n  \nIn[2]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Fige_HTML.gif)\n\n|\n\n_# Display bert tokens_\n\ntok1  \n  \n---|---|---  \n  \nOut[2]\n\n|\n\n['[CLS]', 'he', 'lived', 'characteristic', '##ally', 'idle', 'and',\n'romantic',\n\n'.', '[SEP]']  \n  \nIn[3]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figf_HTML.gif)\n\n|\n\n_# Convert bert tokens to ids (id1)_\n\nid1 = btokenizer.convert_tokens_to_ids(tok1)\n\nid1  \n  \n---|---|---  \n  \nOut[3]\n\n|\n\n[101, 2002, 2973, 8281, 3973, 18373, 1998, 6298, 1012, 102]  \n  \n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figg_HTML.gif)\n\n|\n\n1\\. Import BertTokenizer. Note that different models have different\ntokenizers, e.g. XLNet model\u2019s tokenizer is called XLNetTokenizer.\n\n2\\. Call from_pretrained method on tokenizer object and provide model\u2019s name.\nNeedless to download pre-trained bert-base-uncased (or model) as this method\ndownloads model by itself.\n\n3\\. Call tokenize method. It tokenizes sentence by dividing all words into\nsubwords.\n\n4\\. Print tokens to examine subwords. The words _he_ , _lived_ , _idle_ , that\nexist in Tokenizer\u2019s vocabulary are to be remained. _Characteristically_ is a\nrare word does not exist in Tokenizer\u2019s vocabulary.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93aa9ea2-be05-4aeb-9c07-be8880329809": {"__data__": {"id_": "93aa9ea2-be05-4aeb-9c07-be8880329809", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f200de91-2ef5-48bc-8ba8-e374b3f44f01", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "39b73572bea0ca6bfe5235b1c151813918a13926c2d3857552b309e8b76c03b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19344792-e779-4473-89f4-e6ac9074a9df", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "78a77022917ddd853b8e8fd6e55552ae96806b8cf4edbfb41b2b48da0ee32c2b", "class_name": "RelatedNodeInfo"}}, "hash": "73cf59a42abce127d2ee6e32229cb320bc7a8e8b98b0b4e59495946c025e314b", "text": "[](../images/533412_1_En_15_Chapter/533412_1_En_15_Figg_HTML.gif)\n\n|\n\n1\\. Import BertTokenizer. Note that different models have different\ntokenizers, e.g. XLNet model\u2019s tokenizer is called XLNetTokenizer.\n\n2\\. Call from_pretrained method on tokenizer object and provide model\u2019s name.\nNeedless to download pre-trained bert-base-uncased (or model) as this method\ndownloads model by itself.\n\n3\\. Call tokenize method. It tokenizes sentence by dividing all words into\nsubwords.\n\n4\\. Print tokens to examine subwords. The words _he_ , _lived_ , _idle_ , that\nexist in Tokenizer\u2019s vocabulary are to be remained. _Characteristically_ is a\nrare word does not exist in Tokenizer\u2019s vocabulary. Thus, tokenizer splits\nthis word into subwords _characteristic_ and _##ally_. Notice that _##ally_\nstarts with characters _##_ to emphasize that this is a piece of word.\n\n5\\. Call convert_tokens_to_ids.  \n  \n---|---  \n  \nSince [CLS] and [SEP] tokens must add to the beginning and end of input\nsentence, it required to add them manually for the preceding code, but these\npreprocessing steps can perform in a single step.\n\nBERT provides a method called encode that can:\n\n  * add CLS and SEP tokens to input sentence\n\n  * tokenize sentence by dividing tokens into subwords\n\n  * converts tokens to their token-IDs\n\nCall encode method on input sentence directly as follows:\n\nIn[4]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figh_HTML.gif)\n\n|\n\n**from** transformers **import** BertTokenizer\n\nbtokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\nutt2 = \"He lived characteristically idle and romantic.\"\n\nid2 = btokenizer.encode(utt2)\n\nprint(id2)  \n  \n---|---|---  \n  \nOut[4]\n\n|\n\n[101, 2002, 2973, 8281, 3973, 18373, 1998, 6298, 1012, 102]  \n  \n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figi_HTML.gif)\n\n|\n\nThis code segment outputs token-IDs in a single step instead of step-by-step.\nThe result is a Python list.  \n  \n---|---  \n  \nSince all input sentences in a dataset must have equal length because BERT\ncannot process variable-length sentences, padding the longest sentence from\ndataset into short sentences is required using the parameter\n\"padding='longest'\".\n\nWriteup conversion codes are also required if a TensorFlow tensor is used\ninstead of a plain list. HuggingFace library provides _encode_plus_ to combine\nall these steps into single method as follows:\n\nIn[5]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figj_HTML.gif)\n\n|\n\n**from** transformers **import** BertTokenizer\n\nbtokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\nutt3 = \"He lived characteristically idle and romantic.\"\n\nencoded = btokenizer.encode_plus(\n\ntext=utt3,\n\nadd_special_tokens= **True** ,\n\npadding='longest',\n\nreturn_tensors=\"tf\"\n\n)\n\nid3 = encoded[\"input_ids\"]\n\nprint(id3)  \n  \n---|---|---  \n  \nOut[5]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figk_HTML.gif)  \n  \nCall _encode_plus_ to input sentence directly. It is padded to a length of 10\nincluding special tokens [CLS] and [SEP]. The output is a direct TensorFlow\ntensor with token-IDs.\n\nVerify parameter list of _encode_plus()_ by:\n\nIn[6]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figl_HTML.gif)\n\n|\n\nbtokenizer.encode_plus **?**  \n  \n---|---|---  \n  \n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figm_HTML.gif)\n\n|\n\nBERT Tokenizer provides several methods on input sentences. Data preparation\nis not straightforward, but practice makes perfect. Try out code examples with\nown text.  \n  \n---|---  \n  \nIt is ready to process transformed input sentences to BERT model and obtain\nBERT word vectors.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19344792-e779-4473-89f4-e6ac9074a9df": {"__data__": {"id_": "19344792-e779-4473-89f4-e6ac9074a9df", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93aa9ea2-be05-4aeb-9c07-be8880329809", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "73cf59a42abce127d2ee6e32229cb320bc7a8e8b98b0b4e59495946c025e314b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a02f0b2-75af-49a8-bf12-a4e8b039e072", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "cc8cb93401e458d207149cfada843f39596434f4efd649456e6fe65115d92dab", "class_name": "RelatedNodeInfo"}}, "hash": "78a77022917ddd853b8e8fd6e55552ae96806b8cf4edbfb41b2b48da0ee32c2b", "text": "It is padded to a length of 10\nincluding special tokens [CLS] and [SEP]. The output is a direct TensorFlow\ntensor with token-IDs.\n\nVerify parameter list of _encode_plus()_ by:\n\nIn[6]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figl_HTML.gif)\n\n|\n\nbtokenizer.encode_plus **?**  \n  \n---|---|---  \n  \n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figm_HTML.gif)\n\n|\n\nBERT Tokenizer provides several methods on input sentences. Data preparation\nis not straightforward, but practice makes perfect. Try out code examples with\nown text.  \n  \n---|---  \n  \nIt is ready to process transformed input sentences to BERT model and obtain\nBERT word vectors.\n\n### 15.6.3 Word Vectors in BERT\n\nThis section will examine BERT model output as they are a sequence of word\nvectors assigned by one vector per input word. BERT has a special output\nformat. Let\u2019 us look at the code first.\n\nIn[7]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Fign_HTML.gif)\n\n|\n\n**from** transformers **import** BertTokenizer, TFBertModel\n\nbtokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\nbmodel = TFBertModel.from_pretrained(\"bert-base-uncased\")\n\nutt4 = \"He was idle.\"\n\nencoded = btokenizer.encode_plus(\n\ntext=utt4,\n\nadd_special_tokens= **True** ,\n\npadding='longest',\n\nmax_length=10,\n\nreturn_attention_mask= **True** ,\n\nreturn_tensors=\"tf\"\n\n)\n\nid4 = encoded[\"input_ids\"]\n\noutputs = bmodel(id4)  \n  \n---|---|---  \n  \n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figo_HTML.gif)\n\n|\n\n\u2022 Import TFBertModel\n\n\u2022 Initialize out BERT model with a bert-base-uncased pre-trained model\n\n\u2022 Transform input sentence to BERT input format with encode_plus, and capture\nresult tf.tensor in the input variable\n\n\u2022 Feed sentence to BERT model and capture output with the output\u2019s variables  \n  \n---|---  \n  \nBERT model output is a tuple of two elements. Let us print the shapes of\noutput pair:\n\nIn[8]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figp_HTML.gif)\n\n|\n\nprint(outputs[0].shape)  \n  \n---|---|---  \n  \nOut[8]\n\n|\n\n(1, 6, 768)  \n  \nIn[9]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figq_HTML.gif)\n\n|\n\nprint(outputs[1].shape)  \n  \n---|---|---  \n  \nOut[9]\n\n|\n\n(1, 768)  \n  \n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figr_HTML.gif)\n\n|\n\n1\\. Shape, i.e. batch size, sequence length, hidden size is the first element\nof output. A batch size is the numbers of sentences that can feed to model\ninstantly. When one sentence is fed, the batch size is 1. Sequence length is\n10 because sentence is fed max_length = 10 to the tokenizer and padded to\nlength of 10. Hidden_size is a BERT parameter. BERT architecture has 768\nhidden layers size to produce word vectors with 768 dimensions. Hence, the\nfirst output element contains 768-dimensional vectors per word means it\ncontains 10 words \u00d7 768-dimensional vectors\n\n2\\. The second output is only one vector of 768-dimension. This vector is the\nword embedding of [CLS] token. Since [CLS] token is an aggregate of the whole\nsentence, this token embedding is regarded as embeddings pooled version of all\nwords in the sentence. The shape of output tuple is always the batch size,\nhidden_size. It is to collect [CLS] token\u2019s embedding per input sentence\nbasically  \n  \n---|---  \n  \nWhen BERT embeddings are extracted, they can be used to train text\nclassification model with TensorFlow and tf.keras.\n\n## 15.7 Revisit Text Classification Using BERT\n\nSome of the codes will be used from previous workshop, but this time the code\nis shorter because the embedding and LSTM layers will be replaced by BERT to\ntrain a binary text classifier and tf.keras.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a02f0b2-75af-49a8-bf12-a4e8b039e072": {"__data__": {"id_": "2a02f0b2-75af-49a8-bf12-a4e8b039e072", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19344792-e779-4473-89f4-e6ac9074a9df", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "78a77022917ddd853b8e8fd6e55552ae96806b8cf4edbfb41b2b48da0ee32c2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec48e832-eb08-4475-a762-65bd6f32d68b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "35e39efb48c163d116f16ea539fd5db19a530ec73efbec566da4812c4ee0d59f", "class_name": "RelatedNodeInfo"}}, "hash": "cc8cb93401e458d207149cfada843f39596434f4efd649456e6fe65115d92dab", "text": "The second output is only one vector of 768-dimension. This vector is the\nword embedding of [CLS] token. Since [CLS] token is an aggregate of the whole\nsentence, this token embedding is regarded as embeddings pooled version of all\nwords in the sentence. The shape of output tuple is always the batch size,\nhidden_size. It is to collect [CLS] token\u2019s embedding per input sentence\nbasically  \n  \n---|---  \n  \nWhen BERT embeddings are extracted, they can be used to train text\nclassification model with TensorFlow and tf.keras.\n\n## 15.7 Revisit Text Classification Using BERT\n\nSome of the codes will be used from previous workshop, but this time the code\nis shorter because the embedding and LSTM layers will be replaced by BERT to\ntrain a binary text classifier and tf.keras.\n\nThis section will use an email log dataset _emails.csv_ for spam mail\nclassification found in NLP Workshop6 GitHub repository (NLPWorkshop6 2022).\n\n### 15.7.1 Data Preparation\n\nBefore Text Classification model using BERT is created, let us prepare the\ndata first just like being learnt in the previous workshop:\n\n#### 15.7.1.1 Import Related Modules\n\nIn[10]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figs_HTML.gif)\n\n|\n\n**import** pandas **as** pd\n\n**import** numpy **as** np\n\n**import** tensorflow\n\n**from** tensorflow.keras.layers **import** Dense, Input\n\n**from** tensorflow.keras.models **import** Model  \n  \n---|---|---  \n  \n#### 15.7.1.2 Read emails.csv Datafile\n\nIn[11]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figt_HTML.gif)\n\n|\n\nemails=pd.read_csv(\"emails.csv\",encoding='ISO-8859-1')\n\nemails.head()  \n  \n---|---|---  \n  \nOut[11]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figu_HTML.gif)  \n  \n#### 15.7.1.3 Use dropna() to Remove Record with Missing Contents\n\nIn[12]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figv_HTML.gif)\n\n|\n\nemails=emails.dropna()\n\nemails=emails.reset_index(drop= **True** )\n\nemails.columns = ['text','label']\n\nemails.head()  \n  \n---|---|---  \n  \nOut[12]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figw_HTML.gif)  \n  \n### 15.7.2 Start the BERT Model Construction\n\n#### 15.7.2.1 Import BERT Models and Tokenizer\n\nIn[13]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figx_HTML.gif)\n\n|\n\n**from** transformers **import** BertTokenizer, TFBertModel, BertConfig,\nTFBertForSequenceClassification\n\nbert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\nbmodel = TFBertModel.from_pretrained(\"bert-base-uncased\")  \n  \n---|---|---  \n  \n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figy_HTML.gif)\n\n|\n\n1\\. Import BertTokenizer and BERT model, TFBertModel\n\n2\\. Initialize both tokenizer and BERT model with a pre-trained bert-base-\nuncased model. Note that model\u2019s name starts with TF as names of all\nHuggingFace pre-trained models for TensorFlow start with TF. Please pay\nattention to this when using other transformer models  \n  \n---|---  \n  \n#### 15.7.2.2 Process Input Data with BertTokenizer\n\nIn[14]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figz_HTML.gif)\n\n|\n\nemails.head()  \n  \n---|---|---  \n  \nOut[14]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figaa_HTML.gif)  \n  \n#### 15.7.2.3 Double Check Databank to See Whether Data Has\n\nIn[15]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec48e832-eb08-4475-a762-65bd6f32d68b": {"__data__": {"id_": "ec48e832-eb08-4475-a762-65bd6f32d68b", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a02f0b2-75af-49a8-bf12-a4e8b039e072", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "cc8cb93401e458d207149cfada843f39596434f4efd649456e6fe65115d92dab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5801ae0e-e6f1-4b82-a4e5-7ade293997bb", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "430caa4dfe2c3345977167395aee2bd65915b9001cef2e52de6f7ac6d7892628", "class_name": "RelatedNodeInfo"}}, "hash": "35e39efb48c163d116f16ea539fd5db19a530ec73efbec566da4812c4ee0d59f", "text": "Initialize both tokenizer and BERT model with a pre-trained bert-base-\nuncased model. Note that model\u2019s name starts with TF as names of all\nHuggingFace pre-trained models for TensorFlow start with TF. Please pay\nattention to this when using other transformer models  \n  \n---|---  \n  \n#### 15.7.2.2 Process Input Data with BertTokenizer\n\nIn[14]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figz_HTML.gif)\n\n|\n\nemails.head()  \n  \n---|---|---  \n  \nOut[14]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figaa_HTML.gif)  \n  \n#### 15.7.2.3 Double Check Databank to See Whether Data Has\n\nIn[15]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figab_HTML.gif)\n\n|\n\nmessages=emails['text']\n\nlabels=emails['label']\n\nlen(messages),len(labels)  \n  \n---|---|---  \n  \nOut[15]\n\n|\n\n(5728, 5728)  \n  \n#### 15.7.2.4 Use BERT Tokenizer\n\nIn[16]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figac_HTML.gif)\n\n|\n\ninput_ids=[]\n\nattention_masks=[]\n\n**for** msg **in** messages:\n\nbert_inp=bert_tokenizer.encode_plus(msg,add_special_tokens = **True** ,\nmax_length =64,pad_to_max_length = **True** , return_attention_mask = **True**\n)\n\ninput_ids.append(bert_inp['input_ids'])\n\nattention_masks.append(bert_inp['attention_mask'])\n\ninput_ids=np.asarray(input_ids)\n\nattention_masks=np.array(attention_masks)\n\nlabels=np.array(labels)  \n  \n---|---|---  \n  \n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figad_HTML.gif)\n\n|\n\nThis code segment will generate token-IDs for each input sentence of the\ndataset and append to a list. They are list of class labels consist of 0 and 1\ns. convert python lists, input_ids, label to numpy arrays and feed them to\nKeras model  \n  \n---|---  \n  \n#### 15.7.2.5 Define Keras Model Using the Following Lines\n\nIn[17]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figae_HTML.gif)\n\n|\n\ninputs = Input(shape=(64,), dtype=\"int32\")\n\nbert = bmodel(inputs)\n\nbert = bert[1]\n\noutputs = Dense(units=1, activation=\"sigmoid\")(bert)\n\nmodel = Model(inputs, outputs)\n\nadam = tensorflow.keras.optimizers.Adam (learning_rate=2e-5,epsilon=1e-08)\n\nmodel.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"],\noptimizer=adam)  \n  \n---|---|---  \n  \n#### 15.7.2.6 Perform Model Fitting and Use 1 Epoch to Save Time\n\nIn[18]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figaf_HTML.gif)\n\n|\n\nmodel.fit **?**  \n  \n---|---|---  \n  \nIn[19]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figag_HTML.gif)\n\n|\n\nhistory=model.fit(input_ids,labels,batch_size=1,epochs=1)  \n  \n---|---|---  \n  \nOut[19]\n\n|\n\n5728/5728 [==============================] - 8675s 2s/step - loss: 0.0950 -\naccuracy: 0.9663  \n  \n#### 15.7.2.7 Review Model Summary\n\nIn[20]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figah_HTML.gif)\n\n|\n\nbmodel.summary()  \n  \n---|---|---  \n  \nOut[20]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figai_HTML.gif)  \n  \n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figaj_HTML.gif)\n\n|\n\nA BERT-based text classifier using less than 10 lines of code is to:\n\n1\\. Define input layer to inputs sentences to model. The shape is 64 because\neach input sentence has 64 tokens in length.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5801ae0e-e6f1-4b82-a4e5-7ade293997bb": {"__data__": {"id_": "5801ae0e-e6f1-4b82-a4e5-7ade293997bb", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec48e832-eb08-4475-a762-65bd6f32d68b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "35e39efb48c163d116f16ea539fd5db19a530ec73efbec566da4812c4ee0d59f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46407f5c-13b7-48b6-b3d1-0916776cb806", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "ea1686c03bbec8e6e51ca7847f54ec097971118c2fc5b0bdd6106e28dbbf3e72", "class_name": "RelatedNodeInfo"}}, "hash": "430caa4dfe2c3345977167395aee2bd65915b9001cef2e52de6f7ac6d7892628", "text": "[](../images/533412_1_En_15_Chapter/533412_1_En_15_Figah_HTML.gif)\n\n|\n\nbmodel.summary()  \n  \n---|---|---  \n  \nOut[20]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figai_HTML.gif)  \n  \n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figaj_HTML.gif)\n\n|\n\nA BERT-based text classifier using less than 10 lines of code is to:\n\n1\\. Define input layer to inputs sentences to model. The shape is 64 because\neach input sentence has 64 tokens in length. Pad each sentence to 64 tokens\nwhen encode_plus method is called\n\n2\\. Feed input sentences to BERT model\n\n3\\. Extract second output of BERT output at the third line. Since BERT model\u2019s\noutput is a tuple, the first element of output tuple is a sequence of word\nvectors, and the second element is a single vector that represents the whole\nsentence called pooled output vector. bert[1] extracts pooled output vector\nwhich is a vector of shape (1, 768)\n\n4\\. Squash pooled output vector to a vector of shape 1 by a sigmoid function\nwhich is the class label\n\n5\\. Define Keras model with inputs and outputs\n\n6\\. Compile model\n\n7\\. Fit Keras model  \n  \n---|---  \n  \nBERT model accepts one line only but can transfer enormous knowledge of Wiki\ncorpus to model. This model obtains an accuracy of 0.96 at the end of the\ntraining. A single epoch is usually fitted to the model due to BERT overfits a\nmoderate size corpus.\n\nThe rest of the code handles compiling and fitting Keras model as BERT has a\nhuge memory requirement as can be seen by RAM requirements of Google\nResearch\u2019s GitHub archive (GoogleBert-Memory 2022).\n\nThe training code operates for about an hour in local machine, where bigger\ndatasets require more time even for one epoch.\n\nThis section will learn how to train a Keras model with BERT from scratch.\n\n## 15.8 Transformer Pipeline Technology\n\nHuggingFace Transformers library provide pipelines to assist program\ndevelopers and benefit from transformer code immediately without custom\ntraining. A pipeline is a combination of a tokenizer and a pre-trained model.\n\nHuggingFace provides models for various NLP tasks, its HuggingFace pipelines\noffer:\n\n  * Sentiment analysis (Agarwal 2020; Siahaan and Sianipar 2022)\n\n  * Question answering (Rothman 2022; Tunstall et al. 2022)\n\n  * Text summarization (Albrecht et al. 2020; Kedia and Rasu 2020)\n\n  * Translation (Arumugam and Shanmugamani 2018; G\u00e9ron 2019)\n\nThis section will explore pipelines for sentiment analysis and question\nanswering.\n\n### 15.8.1 Transformer Pipeline for Sentiment Analysis\n\nLet us start examples on sentiment analysis:\n\nIn[21]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figak_HTML.gif)\n\n|\n\nfrom transformers import pipeline\n\nnlp = pipeline(\"sentiment-analysis\")\n\nutt5 = \"I hate I am being a worker in the desert.\"\n\nutt6 = \"I like you who are beautiful and kind.\"\n\nresult1 = nlp(utt5)\n\nresult2 = nlp(utt6)  \n  \n---|---|---  \n  \n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figal_HTML.gif)\n\n|\n\nThe following steps are taken in the preceding code snippet:\n\n1\\. Import pipeline function from transformers\u2019 library. This function creates\npipeline objects with task name given as a parameter. Hence, a sentiment\nanalysis pipeline object nlp is created by calling this function on the second\nline\n\n2\\. Define two example sentences with negative and positive sentiments. Then\nfeed these sentences to the pipeline object nlp.  \n  \n---|---  \n  \nCheck outputs:\n\nIn[22]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figam_HTML.gif)\n\n|\n\nresult1  \n  \n---|---|---  \n  \nOut[22]\n\n|\n\n[{'label': 'NEGATIVE', 'score': 0.9276903867721558}]  \n  \nIn[23]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "46407f5c-13b7-48b6-b3d1-0916776cb806": {"__data__": {"id_": "46407f5c-13b7-48b6-b3d1-0916776cb806", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5801ae0e-e6f1-4b82-a4e5-7ade293997bb", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "430caa4dfe2c3345977167395aee2bd65915b9001cef2e52de6f7ac6d7892628", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45a74346-fb00-4e7a-98ed-4d61bd6ba7f0", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "4a6f83e727e755f45900ad34f270a90c5b7a6bce1518336a300d45db8590ffba", "class_name": "RelatedNodeInfo"}}, "hash": "ea1686c03bbec8e6e51ca7847f54ec097971118c2fc5b0bdd6106e28dbbf3e72", "text": "Import pipeline function from transformers\u2019 library. This function creates\npipeline objects with task name given as a parameter. Hence, a sentiment\nanalysis pipeline object nlp is created by calling this function on the second\nline\n\n2\\. Define two example sentences with negative and positive sentiments. Then\nfeed these sentences to the pipeline object nlp.  \n  \n---|---  \n  \nCheck outputs:\n\nIn[22]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figam_HTML.gif)\n\n|\n\nresult1  \n  \n---|---|---  \n  \nOut[22]\n\n|\n\n[{'label': 'NEGATIVE', 'score': 0.9276903867721558}]  \n  \nIn[23]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figan_HTML.gif)\n\n|\n\nresult2  \n  \n---|---|---  \n  \nOut[23]\n\n|\n\n[{'label': 'POSITIVE', 'score': 0.9998767375946045}]  \n  \n### 15.8.2 Transformer Pipeline for QA System\n\nNext, we will perform on question answering. Let us see the code:\n\nIn[24]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figao_HTML.gif)\n\n|\n\n**from** transformers **import** pipeline\n\nnlp = pipeline(\"question-answering\")\n\nres = nlp({\n\n'question': 'What is the name of this book ?',\n\n'context': \"I'll publish my new book Natural Language Processing soon.\"\n\n})\n\nprint(res)  \n  \n---|---|---  \n  \nOut[24]\n\n|\n\n{'score': 0.9857430458068848, 'start': 25, 'end': 52, 'answer': 'Natural\nLanguage Processing'}  \n  \n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figap_HTML.gif)\n\n|\n\nAgain, import pipeline function to create a pipeline object nlp. A context\nwhich has identical background information for the model is required for\nquestion-answering tasks to the model\n\n\u2022 Request the model about this book\u2019s name after giving information of _this\nnew publication will be available soon_\n\n\u2022 The answer is _natural language processing_ , as expected\n\n\u2022 Try your own examples as simple exercise  \n  \n---|---  \n  \nHuggingFace transformers studies are completed. Let us move on to final\nsection to see what spaCy offers on transformers.\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figaq_HTML.gif)\n\n|\n\n**Workshop 6.1 revisit sentiment analysis using** **Transformer**\n**technology**\n\n1\\. Use either previous workshop databank or another to import databank for\nsentiment analysis\n\n2\\. Try to implement sentiment analysis using previous and Transformer\ntechnology learnt in this workshop\n\n3\\. Compare performances and analysis (bonus)  \n  \n---|---  \n  \n## 15.9 Transformer and spaCy\n\nSpaCy v3.0 had released new features and components. It has integrated\ntransformers into spaCy NLP pipeline to introduce one more pipeline component\ncalled Transformer. This component allows users to use all HuggingFace models\nwith spaCy pipelines. A spaCy NLP pipeline without transformers is illustrated\nin Fig. 15.11.\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Fig11_HTML.png)\n\nFig. 15.11\n\nVector-based spaCy pipeline components\n\nWith the release of v3.0, v2 style spaCy models are still supported and\ntransformer-based models introduced. A transformer-based pipeline component\nlooks like the following as illustrated in Fig. 15.12:\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Fig12_HTML.png)\n\nFig. 15.12\n\nTransformed-based spaCy pipeline components\n\nTransformer-based models and v2 style models are listed under Models page of\nthe documentation (spaCy-model 2022) in English model for each supported\nlanguage. Transformer-based models have various sizes and pipeline components\nlike v2 style models. Also, each model has corpus and genre information like\nv2 style models. An example of an English transformer-based language model\nfrom Models page is shown in Fig. 15.13.\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Fig13_HTML.png)\n\nFig.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45a74346-fb00-4e7a-98ed-4d61bd6ba7f0": {"__data__": {"id_": "45a74346-fb00-4e7a-98ed-4d61bd6ba7f0", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46407f5c-13b7-48b6-b3d1-0916776cb806", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "ea1686c03bbec8e6e51ca7847f54ec097971118c2fc5b0bdd6106e28dbbf3e72", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed15d0cb-37d6-4221-aa62-c438fb0baf9b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "da54d3c566dc549c83deb219f3a0be3f068d7412be6fc7cc5a73b738ad4e439a", "class_name": "RelatedNodeInfo"}}, "hash": "4a6f83e727e755f45900ad34f270a90c5b7a6bce1518336a300d45db8590ffba", "text": "A transformer-based pipeline component\nlooks like the following as illustrated in Fig. 15.12:\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Fig12_HTML.png)\n\nFig. 15.12\n\nTransformed-based spaCy pipeline components\n\nTransformer-based models and v2 style models are listed under Models page of\nthe documentation (spaCy-model 2022) in English model for each supported\nlanguage. Transformer-based models have various sizes and pipeline components\nlike v2 style models. Also, each model has corpus and genre information like\nv2 style models. An example of an English transformer-based language model\nfrom Models page is shown in Fig. 15.13.\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Fig13_HTML.png)\n\nFig. 15.13\n\nspaCy English transformer-based language models\n\nIt showed that the first pipeline component is a transformer that generates\nword representations and deals with WordPiece algorithm to tokenize words into\nsubwords. Word vectors are fed to the rest of the pipeline.\n\nDownloading, loading, and using transformer-based models are identical to v2\nstyle models.\n\nEnglish has two pre-trained transformer-based models, en_core_web_trf and\nen_core_web_lg currently. Let us start by downloading the en_core_web_trf\nmodel:\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figar_HTML.png)\n\nImport spaCy module and transformer-based model:\n\nIn[25]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figas_HTML.gif)\n\n|\n\n**import** spacy\n\n**import** torch\n\n**import** spacy_transformers\n\nnlp = spacy.load(\"en_core_web_trf\")  \n  \n---|---|---  \n  \nAfter loading model and initializing pipeline, use this model the same way as\nin v2 style models:\n\nIn[26]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figat_HTML.gif)\n\n|\n\nutt7 = nlp(\"I visited my friend Betty at her house.\")\n\nutt7.ents  \n  \n---|---|---  \n  \nOut[26]\n\n|\n\n(Betty,)  \n  \nIn[27]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figau_HTML.gif)\n\n|\n\n**for** word **in** utt7:\n\nprint(word.pos_, word.lemma_)  \n  \n---|---|---  \n  \nOut[27]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figav_HTML.gif)  \n  \nThese features related to the transformer component can be accessed by _\n_.trf_data_ which contain word pieces, input ids, and vectors generated by the\ntransformer.\n\nLet us examine the features one by one:\n\nIn[28]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figaw_HTML.gif)\n\n|\n\nutt8 = nlp(\"It went there unwillingly.\")  \n  \n---|---|---  \n  \nIn[29]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figax_HTML.gif)\n\n|\n\nutt8._.trf_data.wordpieces  \n  \n---|---|---  \n  \nOut[29]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figay_HTML.gif)  \n  \nThere are five elements: word pieces, input IDs, attention masks, lengths, and\ntoken type IDs in the preceding output.\n\nWord pieces are subwords generated by WordPiece algorithm. The word pieces of\nthis sentence are as follows:\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figaz_HTML.png)\n\nThe first and last tokens are special tokens used at the beginning and end of\nthe sentence. The word _unwillingly_ is divided into three subwords\u2014 _unw_ ,\n_ill_ , and _ingly_. A G character is used to mark word boundaries. Tokens\nwithout G are subwords, such as _ill_ and _ingly_ in the preceding word piece\nlist, except first word in the sentence marked by <'s'>.\n\nInput IDs have identical meanings which are subword IDs assigned by the\ntransformer\u2019s tokenizer.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed15d0cb-37d6-4221-aa62-c438fb0baf9b": {"__data__": {"id_": "ed15d0cb-37d6-4221-aa62-c438fb0baf9b", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45a74346-fb00-4e7a-98ed-4d61bd6ba7f0", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "4a6f83e727e755f45900ad34f270a90c5b7a6bce1518336a300d45db8590ffba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6cc2bfa2-a695-4bd9-aadf-9bfea6e71547", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "26e3a6f93fd27d5c3120868b76b4a3ff3c1d52c61ffeeaaadd4b71b19de4f354", "class_name": "RelatedNodeInfo"}}, "hash": "da54d3c566dc549c83deb219f3a0be3f068d7412be6fc7cc5a73b738ad4e439a", "text": "Word pieces are subwords generated by WordPiece algorithm. The word pieces of\nthis sentence are as follows:\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figaz_HTML.png)\n\nThe first and last tokens are special tokens used at the beginning and end of\nthe sentence. The word _unwillingly_ is divided into three subwords\u2014 _unw_ ,\n_ill_ , and _ingly_. A G character is used to mark word boundaries. Tokens\nwithout G are subwords, such as _ill_ and _ingly_ in the preceding word piece\nlist, except first word in the sentence marked by <'s'>.\n\nInput IDs have identical meanings which are subword IDs assigned by the\ntransformer\u2019s tokenizer.\n\nThe attention mask is a list of 0 s and 1 s for pointing the transformer to\ntokens it should notice. 0 corresponds to PAD tokens, while all other tokens\nshould have a corresponding 1.\n\nLengths refer to the length of sentence after dividing into subwords. Here is\n9 but notice that len(doc) outputs is 5, while spaCy always operates on\nlinguistic words.\n\n_token_type_ids_ are used by transformer tokenizers to mark sentence\nboundaries of two sentences input tasks such as question and answering. Since\nthere is only one text provided, this feature is inapplicable.\n\nToken vectors are generated by transformer, doc._.trf_data.tensors which\ncontain transformer output, a sequence of word vectors per word, and the\npooled output vector. Please refer to Obtaining BERT word vectors section if\nnecessary.\n\nIn[30]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figba_HTML.gif)\n\n|\n\nutt8._.trf_data.tensors[0].shape  \n  \n---|---|---  \n  \nOut[30]\n\n|\n\n(1, 9, 768)  \n  \nIn[31]\n\n|\n\n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figbb_HTML.gif)\n\n|\n\nutt8._.trf_data.tensors[1].shape  \n  \n---|---|---  \n  \nOut[31]\n\n|\n\n(1, 768)  \n  \n![](../images/533412_1_En_15_Chapter/533412_1_En_15_Figbc_HTML.gif)\n\n|\n\nThe first element of tuple is the vectors for tokens. Each vector is\n768-dimensional, hence 9 words produce 9 x 768-dimensional vectors. The second\nelement of tuple is the pooled output vector which is an aggregate\nrepresentation for input sentence, and the shape is 1 x 768  \n  \n---|---  \n  \nspaCy provides user-friendly API and packaging for complicated models such as\ntransformers. Transformer integration is a validation of using spaCy for NLP.\n\nReferences\n\n  1. Agarwal, B. (2020) Deep Learning-Based Approaches for Sentiment Analysis (Algorithms for Intelligent Systems). Springer.[Crossref](https://doi.org/10.1007/978-981-15-1216-2)\n\n  2. Albrecht, J., Ramachandran, S. and Winkler, C. (2020) Blueprints for Text Analytics Using Python: Machine Learning-Based Solutions for Common Real World (NLP) Applications. O\u2019Reilly Media.\n\n  3. Arumugam, R., & Shanmugamani, R. (2018). Hands-on natural language processing with python. Packt Publishing.\n\n  4. Bansal, A. (2021) Advanced Natural Language Processing with TensorFlow 2: Build effective real-world NLP applications using NER, RNNs, seq2seq models, Transformers, and more. Packt Publishing.\n\n  5. Devlin, J., Chang, M. W., Lee, K. and Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Archive: [https://\u200barxiv.\u200borg/\u200bpdf/\u200b1810.\u200b04805.\u200bpdf](https://arxiv.org/pdf/1810.04805.pdf).\n\n  6. Ekman, M. (2021) Learning Deep Learning: Theory and Practice of Neural Networks, Computer Vision, Natural Language Processing, and Transformers Using TensorFlow. Addison-Wesley Professional.\n\n  7. Facebook-transformer (2022) Facebook Transformer Model archive.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6cc2bfa2-a695-4bd9-aadf-9bfea6e71547": {"__data__": {"id_": "6cc2bfa2-a695-4bd9-aadf-9bfea6e71547", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed15d0cb-37d6-4221-aa62-c438fb0baf9b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "da54d3c566dc549c83deb219f3a0be3f068d7412be6fc7cc5a73b738ad4e439a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6f043b5-6131-4d29-8d23-5c00d56e2ae8", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "e0438b7d72b9fec83654d566c69dc53db28374a6c60303a514ea85b176d0368f", "class_name": "RelatedNodeInfo"}}, "hash": "26e3a6f93fd27d5c3120868b76b4a3ff3c1d52c61ffeeaaadd4b71b19de4f354", "text": "4. Bansal, A. (2021) Advanced Natural Language Processing with TensorFlow 2: Build effective real-world NLP applications using NER, RNNs, seq2seq models, Transformers, and more. Packt Publishing.\n\n  5. Devlin, J., Chang, M. W., Lee, K. and Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Archive: [https://\u200barxiv.\u200borg/\u200bpdf/\u200b1810.\u200b04805.\u200bpdf](https://arxiv.org/pdf/1810.04805.pdf).\n\n  6. Ekman, M. (2021) Learning Deep Learning: Theory and Practice of Neural Networks, Computer Vision, Natural Language Processing, and Transformers Using TensorFlow. Addison-Wesley Professional.\n\n  7. Facebook-transformer (2022) Facebook Transformer Model archive. [https://\u200bgithub.\u200bcom/\u200bpytorch/\u200bfairseq/\u200bblob/\u200bmaster/\u200bexamples/\u200blanguage_\u200bmodel/\u200bREADME.\u200bmd](https://github.com/pytorch/fairseq/blob/master/examples/language_model/README.md). Accessed 24 June 2022.\n\n  8. G\u00e9ron, A. (2019) Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O\u2019Reilly Media.\n\n  9. GoogleBert (2022) Google Bert Model Github archive. [https://\u200bgithub.\u200bcom/\u200bgoogle-research/\u200bbert](https://github.com/google-research/bert). Accessed 24 June 2022. Accessed 24 June 2022.\n\n  10. GoogleBert-Memory (2022) GoogleBert Memory Requirement. [https://\u200bgithub.\u200bcom/\u200bgoogle-research/\u200bbert#out-of-memory-issues](https://github.com/google-research/bert#out-of-memory-issues).\n\n  11. HuggingFace (2022) Hugging Face official site. [https://\u200bhuggingface.\u200bco/\u200b](https://huggingface.co/). Accessed 24 June 2022.\n\n  12. HuggingFace_transformer (2022) HuggingFace Transformer Model archive. [https://\u200bgithub.\u200bcom/\u200bhuggingface/\u200btransformers](https://github.com/huggingface/transformers). Accessed 24 June 2022.\n\n  13. Kedia, A. and Rasu, M. (2020) Hands-On Python Natural Language Processing: Explore tools and techniques to analyze and process text with a view to building real-world NLP applications. Packt Publishing.\n\n  14. Keras (2022) Keras official site. [https://\u200bkeras.\u200bio/\u200b](https://keras.io/). Accessed 24 June 2022.\n\n  15. Korstanje, J. (2021) Advanced Forecasting with Python: With State-of-the-Art-Models Including LSTMs, Facebook\u2019s Prophet, and Amazon\u2019s DeepAR. Apress.[Crossref](https://doi.org/10.1007/978-1-4842-7150-6)\n\n  16. NLPWorkshop6 (2022) NLP Workshop 6 GitHub archive. [https://\u200bgithub.\u200bcom/\u200braymondshtlee/\u200bNLP/\u200btree/\u200bmain/\u200bNLPWorkshop6](https://github.com/raymondshtlee/NLP/tree/main/NLPWorkshop6). Accessed 24 June 2022.\n\n  17. Rothman, D. (2022) Transformers for Natural Language Processing: Build, train, and fine-tune deep neural network architectures for NLP with Python, PyTorch, TensorFlow, BERT, and GPT-3. Packt Publishing.\n\n  18. SpaCy (2022) spaCy official site. [https://\u200bspacy.\u200bio/\u200b](https://spacy.io/). Accessed 24 June 2022.\n\n  19. SpaCy-model (2022) spaCy English Pipeline Model. [https://\u200bspacy.\u200bio/\u200bmodels/\u200ben](https://spacy.io/models/en). Accessed 24 June 2022.\n\n  20. Siahaan, V. and Sianipar, R. H.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6f043b5-6131-4d29-8d23-5c00d56e2ae8": {"__data__": {"id_": "e6f043b5-6131-4d29-8d23-5c00d56e2ae8", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6cc2bfa2-a695-4bd9-aadf-9bfea6e71547", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "26e3a6f93fd27d5c3120868b76b4a3ff3c1d52c61ffeeaaadd4b71b19de4f354", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e79feec-40be-4184-837d-355b8ea6d3ef", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3ff429ba561f5fdb5dbc8d5def6bfe97e73efb522fa1a28a8c00caead9905de3", "class_name": "RelatedNodeInfo"}}, "hash": "e0438b7d72b9fec83654d566c69dc53db28374a6c60303a514ea85b176d0368f", "text": "Accessed 24 June 2022.\n\n  17. Rothman, D. (2022) Transformers for Natural Language Processing: Build, train, and fine-tune deep neural network architectures for NLP with Python, PyTorch, TensorFlow, BERT, and GPT-3. Packt Publishing.\n\n  18. SpaCy (2022) spaCy official site. [https://\u200bspacy.\u200bio/\u200b](https://spacy.io/). Accessed 24 June 2022.\n\n  19. SpaCy-model (2022) spaCy English Pipeline Model. [https://\u200bspacy.\u200bio/\u200bmodels/\u200ben](https://spacy.io/models/en). Accessed 24 June 2022.\n\n  20. Siahaan, V. and Sianipar, R. H. (2022) Text Processing and Sentiment Analysis using Machine Learning and Deep Learning with Python GUI. Balige Publishing.\n\n  21. TensorFlow (2022) TensorFlow official site> [https://\u200btensorflow.\u200borg](https://tensorflow.org) /. Accessed 24 June 2022.\n\n  22. Tunstall, L, Werra, L. and Wolf, T. (2022) Natural Language Processing with Transformers: Building Language Applications with Hugging Face. O\u2019Reilly Media.\n\n  23. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30. [https://\u200barxiv.\u200borg/\u200babs/\u200b1706.\u200b03762](https://arxiv.org/abs/1706.03762).\n\n  24. Y\u0131ld\u0131r\u0131m, S, Asgari-Chenaghlu, M. (2021) Mastering Transformers: Build state-of-the-art models from scratch with advanced natural language processing techniques. Packt Publishing.\n\n\n\u00a9 The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.\n2024\n\nR. S. T. Lee Natural Language Processing\n<https://doi.org/10.1007/978-981-99-1999-4_16>\n\n# 16\\. Workshop#7 Building Chatbot with TensorFlow and Transformer Technology\n(Hour 13\u201314)\n\nRaymond S. T. Lee 1\n\n(1)\n\nUnited International College, Beijing Normal University-Hong Kong Baptist\nUniversity, Zhuhai, China\n\n## 16.1 Introduction\n\nIn previous 6 NLP workshops, we studied NLP implementation tools and\ntechniques ranging from tokenization, N-gram generation to semantic and\nsentiment analysis with various key NLP Python enabling technologies: NLTK,\nspaCy, TensorFlow, and contemporary Transformer Technology. This final\nworkshop will explore how to integrate them for the design and implementation\nof a live domain-based chatbot system on a movie domain.\n\nThis workshop will explore:\n\n  1. 1.\n\nTechnical requirements for chatbot system.\n\n  2. 2.\n\nKnowledge domain\u2014the Cornell Large Movie Conversation Dataset is a well-known\nconversation dataset with over 200,000 movie dialogs of 10,000+ movie\ncharacters (Cornell 2022; Cornell_Movie_Corpus 2022)\n\n  3. 3.\n\nA step-by-step Movie Chatbot system implementation which involves movie dialog\npreprocessing, model construction, attention learning, system integration with\nspaCy, TensorFlow, Keras and Transformer Technology, an important tool in NLP\nsystem implementation (Bansal 2021; Devlin et al. 2019; G\u00e9ron 2019; Rothman\n2022; Tunstall et al. 2022; Y\u0131ld\u0131r\u0131m and Asgari-Chenaghlu 2021).\n\n  4. 4.\n\nEvaluation metrics with live chat examples.\n\n## 16.2 Technical Requirements\n\nTransformers, Tensorflow, and spaCy and Python modules include numpy and\nscikit-learn that are to be installed in machine.\n\nUse pip install commands to:\n\n  * pip install spacy\n\n  * pip install tensorflow (note: version 2.2 or above)\n\n  * pip install transformers\n\n  * pip install scikit-learn\n\n  * pip install numpy\n\nThe data files used in this workshop can be found in DATA sub-directory of\nNLPWorkshop7 directory of JupyterHub Server (NLPWorkshop7 2022).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e79feec-40be-4184-837d-355b8ea6d3ef": {"__data__": {"id_": "4e79feec-40be-4184-837d-355b8ea6d3ef", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6f043b5-6131-4d29-8d23-5c00d56e2ae8", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "e0438b7d72b9fec83654d566c69dc53db28374a6c60303a514ea85b176d0368f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2f59b53-2c62-45bc-a8a8-deb071cbefb4", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "aeb1b54b2adf6419509f895581a6028a9bc5456f3bb5a20cb9ee194ccec25e39", "class_name": "RelatedNodeInfo"}}, "hash": "3ff429ba561f5fdb5dbc8d5def6bfe97e73efb522fa1a28a8c00caead9905de3", "text": "2019; G\u00e9ron 2019; Rothman\n2022; Tunstall et al. 2022; Y\u0131ld\u0131r\u0131m and Asgari-Chenaghlu 2021).\n\n  4. 4.\n\nEvaluation metrics with live chat examples.\n\n## 16.2 Technical Requirements\n\nTransformers, Tensorflow, and spaCy and Python modules include numpy and\nscikit-learn that are to be installed in machine.\n\nUse pip install commands to:\n\n  * pip install spacy\n\n  * pip install tensorflow (note: version 2.2 or above)\n\n  * pip install transformers\n\n  * pip install scikit-learn\n\n  * pip install numpy\n\nThe data files used in this workshop can be found in DATA sub-directory of\nNLPWorkshop7 directory of JupyterHub Server (NLPWorkshop7 2022).\n\n## 16.3 AI Chatbot in a Nutshell\n\n### 16.3.1 What Is a Chatbot?\n\nConversational artificial intelligence (conversational AI) is a field of\nmachine learning that aims to create technology and enables users to have text\nor speech-based interactions with machines. Chatbots, virtual assistants, and\nvoice assistants are typical conversational AI products (Batish 2018; Freed\n2021; Janarthanam 2017; Raj 2018).\n\nA chatbot is a software application designed to make conversations with\nhumans.\n\nChatbots are widely used in human resources, marketing and sales, banking,\nhealthcare, and non-commercial areas such as personal conversations. They\ninclude:\n\n  * Amazon Alexa is a voice-based virtual assistant to perform tasks per user requests or inquiries, i.e. play music, podcasts, set alarms, read audiobooks, provide real-time weather, traffic, and other information. Alexa Home can connect smart home devices to oversee premises and electrical appliances.\n\n  * Facebook Messenger and Telegram instant messaging services provide interfaces and API documentations (Facebook 2022; Telegram 2022) for developers to connect bots.\n\n  * Google Assistant provides real-time weather, flight, traffic information, send and receive text messages, email services, device information, set alarms and integrate with smart home devices, etc. available on Google Maps, Google Search, and standalone Android and iOS applications.\n\n  * IKEA provides customer service chatbot called Anna, AccuWeather, and FAQ chatbots.\n\n  * Sephora has virtual make-up artist and customer service chatbots at Facebook messenger.\n\n  * Siri integrates with iPhone, iPad, iPod, and macOS to initiate, answer calls, send, receive text messages and WhatsApp messages at iPhone.\n\nOther virtual assistants include AllGenie, Bixby, Celia, Cortana, Duer, and\nXiaowei.\n\n### 16.3.2 What Is a Wake Word in Chatbot?\n\nA wake word is the gateway between user and user\u2019s digital assistant/Chatbot.\nVoice assistants such as Alexa and Siri are powered by AI with word detection\nabilities to queries response and commands.\n\nCommon wake words include Hey, Google, Alexa, and Hey Siri.\n\nToday\u2019s wake word performance and speech recognition are operated by machine\nlearning or AI with cloud processing.\n\nSensory\u2019s wake word and phrase recognition engines use deep neural networks to\nprovide an embedded or on-device wake word and phrase recognition engine (Fig.\n16.1).\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Fig1_HTML.png)\n\nFig. 16.1\n\nWake word to invoke Chatbot (Tuchong 2022)\n\n#### 16.3.2.1 Tailor-Made Wake Word\n\nWake words like Alexa, Siri, and Google are associated with highly valued and\ntechnical products experiences, other companies had created tailor-made wake\nword and uniqueness to their products, i.e. Hi Toyota had opened a doorway to\nvoice user interface to strengthen the relationship between customers and the\nbrand.\n\n#### 16.3.2.2 Why Embedded Word Detection?\n\nWake word technology has been used in cases beyond mobile applications. Some\nbattery powered devices like Bluetooth headphones, smart watches, cameras, and\nemergency alert devices.\n\nChatbot allow users to utter commands naturally. Queries like _what time is\nit?_ or _how many steps have I taken?_ are phrases examples that a chatbot can\nprocess zero latency with high accuracy.\n\nWake word technology can integrate with voice recognition applications like\ntouch screen food ordering, voice-control microwaves, or user identification\nsettings at televisions or vehicles.\n\n### 16.3.3 NLP Components in a Chatbot\n\nA typical chatbot consists of major components:\n\n  1. 1.\n\nSpeech-to-text converts user speech into text.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2f59b53-2c62-45bc-a8a8-deb071cbefb4": {"__data__": {"id_": "a2f59b53-2c62-45bc-a8a8-deb071cbefb4", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e79feec-40be-4184-837d-355b8ea6d3ef", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3ff429ba561f5fdb5dbc8d5def6bfe97e73efb522fa1a28a8c00caead9905de3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e0bab5e1-8d68-4a58-9bd8-3a514e56ffb0", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "c58a14ddbbf045ca25a514a5888f4378f3976a86ab325f6d07299a0724a93479", "class_name": "RelatedNodeInfo"}}, "hash": "aeb1b54b2adf6419509f895581a6028a9bc5456f3bb5a20cb9ee194ccec25e39", "text": "Hi Toyota had opened a doorway to\nvoice user interface to strengthen the relationship between customers and the\nbrand.\n\n#### 16.3.2.2 Why Embedded Word Detection?\n\nWake word technology has been used in cases beyond mobile applications. Some\nbattery powered devices like Bluetooth headphones, smart watches, cameras, and\nemergency alert devices.\n\nChatbot allow users to utter commands naturally. Queries like _what time is\nit?_ or _how many steps have I taken?_ are phrases examples that a chatbot can\nprocess zero latency with high accuracy.\n\nWake word technology can integrate with voice recognition applications like\ntouch screen food ordering, voice-control microwaves, or user identification\nsettings at televisions or vehicles.\n\n### 16.3.3 NLP Components in a Chatbot\n\nA typical chatbot consists of major components:\n\n  1. 1.\n\nSpeech-to-text converts user speech into text. The input is a wav/mp3 file and\nthe output is a text file containing user\u2019s utterance.\n\n  2. 2.\n\nConversational NLU performs intent recognition and entity extraction on user\u2019s\nutterance text. The output is the user\u2019s intent with a list of entities.\nResolving references in the current to previous utterances is processed by\nthis component.\n\n  3. 3.\n\nDialog manager retains conversation memory to generate a meaningful and\ncoherent chat. This component is regarded as dialog memory in conversational\nstate hitherto entities and intents appeared. Hence, the input is the previous\ndialog state for current user to parse intent and entities to a new dialog\nstate output.\n\n  4. 4.\n\nAnswer generator gives all inputs from previous stages to generate answers to\nuser\u2019s utterance.\n\n  5. 5.\n\nText-to-speech generates a speech file (WAV or mp3) from system\u2019s answers\n\nEach of these components is trained and evaluated separately, e.g. speech-to-\ntext training is performed by speech files and corresponding transcriptions on\nan annotated speech corpus.\n\n## 16.4 Building Movie Chatbot by Using TensorFlow and Transformer Technology\n\nThis workshop will integrate the learnt technologies including: TensorFlow\n(Bansal 2021; Ekman 2021; TensorFlow 2022), Keras (G\u00e9ron 2019; Keras 2022a),\nTransformer technology with Attention Learning Scheme (Ekman 2021; Kedia and\nRasu 2020; Rothman 2022; Tunstall et al. 2022; Vaswani et al. 2017; Y\u0131ld\u0131r\u0131m\nand Asgari-Chenaghlu 2021) to build a live domain-based chatbot system. The\nCornell Large Movie Dialog Corpus (Cornell 2022) will be used as conversation\ndataset for system training. The movie dataset can be downloaded either from\nCornell databank (2022) or Kaggle\u2019s Cornell Movie Corpus archive (2022).\n\nUse pip install command to invoke TensorFlow package and install its dataset:\n\nIn[1]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figa_HTML.gif)\n\n|\n\n**import** tensorflow **as** tflow\n\ntflow.random.set_seed(1234)\n\n_# !pip install tensorflow-datasets==1.2.0_\n\n**import** tensorflow_datasets as tflowDS\n\n**import** re\n\n**import** matplotlib.pyplot **as** pyplt  \n  \n---|---|---  \n  \n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figb_HTML.gif)\n\n|\n\n1\\. Install and import TensorFlow-datasets in addition to TensorFlow package.\nPlease use pip install command as script if not installed already\n\n2\\. Use _random.set_seed()_ method to set all random seeds required to\nreplicate TensorFlow codes  \n  \n---|---  \n  \n### 16.4.1 The Chatbot Dataset\n\nThe Cornell Movie Dialogs corpus is used in this project. This dataset,\nmovie_conversations.txt contains lists of conversation IDs and movie_lines.txt\nassociative conversation ID. It has generated 220,579 conversations and 10,292\nmovie characters amongst movies.\n\n### 16.4.2 Movie Dialog Preprocessing\n\nThe maximum numbers of conversations (MAX_CONV) and the maximum length of\nutterance (MLEN) are set for 50,000 and 40 for system training, respectively.\n\nPreprocessing data procedure (PP) involves the following steps:\n\n  1. 1.\n\nObtain 50,000 movie dialog pairs from dataset.\n\n  2. 2.\n\nPP each utterance by special and control characters removal.\n\n  3. 3.\n\nConstruct tokenizer.\n\n  4. 4.\n\nTokenize each utterance.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e0bab5e1-8d68-4a58-9bd8-3a514e56ffb0": {"__data__": {"id_": "e0bab5e1-8d68-4a58-9bd8-3a514e56ffb0", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2f59b53-2c62-45bc-a8a8-deb071cbefb4", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "aeb1b54b2adf6419509f895581a6028a9bc5456f3bb5a20cb9ee194ccec25e39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd5c76d0-e34e-4835-9d4d-0ffdb141d1e4", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "2c3d385de98d46883c40d14f86f986f055bb4ce79fda00828b083ee75cadb4a9", "class_name": "RelatedNodeInfo"}}, "hash": "c58a14ddbbf045ca25a514a5888f4378f3976a86ab325f6d07299a0724a93479", "text": "This dataset,\nmovie_conversations.txt contains lists of conversation IDs and movie_lines.txt\nassociative conversation ID. It has generated 220,579 conversations and 10,292\nmovie characters amongst movies.\n\n### 16.4.2 Movie Dialog Preprocessing\n\nThe maximum numbers of conversations (MAX_CONV) and the maximum length of\nutterance (MLEN) are set for 50,000 and 40 for system training, respectively.\n\nPreprocessing data procedure (PP) involves the following steps:\n\n  1. 1.\n\nObtain 50,000 movie dialog pairs from dataset.\n\n  2. 2.\n\nPP each utterance by special and control characters removal.\n\n  3. 3.\n\nConstruct tokenizer.\n\n  4. 4.\n\nTokenize each utterance.\n\n  5. 5.\n\nCap the max utterance length to MLEN.\n\n  6. 6.\n\nFilter and pad utterances.\n\nIn[2]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figc_HTML.gif)\n\n|\n\n_# Set the maximum number of training conversation_\n\nMAX_CONV = 50000\n\n_# Preprocess all utterances_\n\n**def** pp_utterance+(utterance):\n\nutterance = utterance.lower().strip()\n\n_# Add a space to the following special characters_\n\nutterance = re.sub(r\"([?.!,])\", r\" \\1 \", utterance)\n\n_# Delete extrac spaces_\n\nutterance = re.sub(r'[\" \"]+', \" \", utterance)\n\n_# Other than below characters, the other character replace by spaces_\n\nutterance = re.sub(r\"[^a-zA-Z?.,!]+\", \" \", utterance)\n\nutterance = utterance.strip()\n\n**return** utterance\n\n**def** get_dialogs():\n\n_# Create the dialog object (dlogs)_\n\nid2dlogs = {}\n\n_# Open the movie_lines text file_\n\n**with** open('data/movie_lines.txt', encoding = 'utf-8', errors = 'ignore')\n**as** f_dlogs:\n\ndlogs = f_dlogs.readlines()\n\n**for** dlog **in** dlogs:\n\nsections = dlog.replace('\\n', '').split(' +++$+++ ')\n\nid2dlogs[sections[0]] = sections[4]\n\nquery, ans = [], []\n\n**with** open('data/movie_conversations.txt',\n\nencoding = 'utf-8', errors = 'ignore') **as** f_conv:\n\nconvs = f_conv.readlines()\n\n**for** conv **in** convs:\n\nsections = conv.replace('\\n', '').split(' +++$+++ ')\n\n_# Create movie conservation object m_conv as a list_\n\nm_conv = [conv[1:-1] **for** conv **in** sections[3][1:-1].split(', ')]\n\n**for** i **in** range(len(m_conv) - 1):\n\nquery.append(pp_utterance(id2dlogs[m_conv[i]]))\n\nans.append(pp_utterance(id2dlogs[m_conv[i + 1]]))\n\n**if** len(query) >= MAX_CONV:\n\n**return** query, ans\n\n**return** query, ans\n\nqueries, responses = get_dialogs()  \n  \n---|---|---  \n  \nSelect query 13 and verify response:\n\nIn[3]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figd_HTML.gif)\n\n|\n\nprint('Query 13: {}'.format(queries[13]))\n\nprint('Response 13: {}'.format(responses[13]))  \n  \n---|---|---  \n  \nOut[3]\n\n|\n\nQuery 13: that s because it s such a nice one .\n\nResponse 13: forget french .  \n  \nSelect query 100 and verify response:\n\nIn[4]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Fige_HTML.gif)\n\n|\n\nprint('Query 100: {}'.format(queries[100]))\n\nprint('Response 100: {}'.format(responses[100]))  \n  \n---|---|---  \n  \nOut[4]\n\n|\n\nQuery 100: you set me up .\n\nResponse 100: i just wanted  \n  \nVerify queries (responses) size to see whether it situates within MAX_CONV:\n\nIn[5]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figf_HTML.gif)\n\n|\n\nlen(queries)  \n  \n---|---|---  \n  \nOut[5]\n\n|\n\n50000  \n  \nIn[6]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figg_HTML.gif)\n\n|\n\nLen(responses)  \n  \n---|---|---  \n  \nOut[6]\n\n|\n\n50000  \n  \n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd5c76d0-e34e-4835-9d4d-0ffdb141d1e4": {"__data__": {"id_": "cd5c76d0-e34e-4835-9d4d-0ffdb141d1e4", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e0bab5e1-8d68-4a58-9bd8-3a514e56ffb0", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "c58a14ddbbf045ca25a514a5888f4378f3976a86ab325f6d07299a0724a93479", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e971391-34b4-417f-b177-0719365b2a88", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "df1cfddfd0dac16a56985e308cc03e3b14bb993042026f5988e5107f998f4a63", "class_name": "RelatedNodeInfo"}}, "hash": "2c3d385de98d46883c40d14f86f986f055bb4ce79fda00828b083ee75cadb4a9", "text": "Response 100: i just wanted  \n  \nVerify queries (responses) size to see whether it situates within MAX_CONV:\n\nIn[5]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figf_HTML.gif)\n\n|\n\nlen(queries)  \n  \n---|---|---  \n  \nOut[5]\n\n|\n\n50000  \n  \nIn[6]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figg_HTML.gif)\n\n|\n\nLen(responses)  \n  \n---|---|---  \n  \nOut[6]\n\n|\n\n50000  \n  \n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figh_HTML.gif)\n\n|\n\n1\\. After max 50,000 movie conversations had obtained to perform basic\npreprocessing, it is sufficient for model training\n\n2\\. Perform tokenization procedure to add START and END tokens using commands\nbelow  \n  \n---|---  \n  \n### 16.4.3 Tokenization of Movie Conversation\n\nIn[7]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figi_HTML.gif)\n\n|\n\n_# Define the Movie Token object_\n\nm_token = tflowDS.deprecated.text.SubwordTextEncoder.build_from_corpus(queries\n+ responses, target_vocab_size = 2**13)\n\n_# Define the Start and End tokens_\n\nSTART_TOKEN, END_TOKEN = [m_token.vocab_size], [m_token.vocab_size + 1]\n\n_# Define the size of Vocab (SVCAB)_\n\nSVCAB = m_token.vocab_size + 2  \n  \n---|---|---  \n  \nVerify movie token lists for conv 13 and 100:\n\nIn[8]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figj_HTML.gif)\n\n|\n\nprint('The movie token of conv 13: {}'.format(m_token.encode(queries[13])))  \n  \n---|---|---  \n  \nOut[8]\n\n|\n\nThe movie token of conv 13: [15, 8, 151, 12, 8, 354, 10, 347, 188, 1]  \n  \nIn[9]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figk_HTML.gif)\n\n|\n\nprint('The movie token of conv 100: {}'.format(m_token.encode(queries[100])))  \n  \n---|---|---  \n  \nOut[9]\n\n|\n\nThe movie token of conv 100: [5, 539, 36, 119, 1]  \n  \n### 16.4.4 Filtering and Padding Process\n\nCap utterance max length (MLEN) to 40, perform filtering and padding:\n\nIn[10]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figl_HTML.gif)\n\n|\n\n_# Set the maximum length of each utterance MLEN to 40_\n\nMLEN = 40\n\n_# Performs the filtering and padding of each utterance_\n\n**def** filter_pad (qq, aa):\n\nm_token_qq, m_token_aa = [], []\n\n**for** (utterance1, utterance2) **in** zip(qq, aa):\n\nutterance1 = START_TOKEN + m_token.encode(utterance1) + END_TOKEN\n\nutterance2 = START_TOKEN + m_token.encode(utterance2) + END_TOKEN\n\n**if** len(utterance1) <= MLEN **and** len(utterance2) <= MLEN:\n\nm_token_qq.append(utterance1)\n\nm_token_aa.append(utterance2)\n\n_# pad tokenized sentences_\n\nm_token_qq = tflow.keras.preprocessing.sequence.pad_sequences(m_token_qq,\nmaxlen=MLEN, padding = 'post')\n\nm_token_aa = tflow.keras.preprocessing.sequence.pad_sequences(m_token_aa,\nmaxlen=MLEN, padding = 'post')\n\n**return** m_token_qq, m_token_aa\n\nqueries, responses = filter_pad (queries, responses)  \n  \n---|---|---  \n  \nReview the size of movie vocab (SVCAB) and total number of conversation\n(conv):\n\nIn[11]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figm_HTML.gif)\n\n|\n\nprint('Size of vocab: {}'.format(SVCAB))\n\nprint('Total number of conv: {}'.format(len(queries)))  \n  \n---|---|---  \n  \nOut[11]\n\n|\n\nSize of vocab: 8333\n\nTotal number of conv: 44095  \n  \n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8e971391-34b4-417f-b177-0719365b2a88": {"__data__": {"id_": "8e971391-34b4-417f-b177-0719365b2a88", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd5c76d0-e34e-4835-9d4d-0ffdb141d1e4", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "2c3d385de98d46883c40d14f86f986f055bb4ce79fda00828b083ee75cadb4a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "20b2460b-100f-4f1d-9af9-05304a0cc4e0", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "344f37728a6aa9990e7967a4fa461b96fbcc50e5e7374a47f43bea340543c889", "class_name": "RelatedNodeInfo"}}, "hash": "df1cfddfd0dac16a56985e308cc03e3b14bb993042026f5988e5107f998f4a63", "text": "[](../images/533412_1_En_16_Chapter/533412_1_En_16_Figm_HTML.gif)\n\n|\n\nprint('Size of vocab: {}'.format(SVCAB))\n\nprint('Total number of conv: {}'.format(len(queries)))  \n  \n---|---|---  \n  \nOut[11]\n\n|\n\nSize of vocab: 8333\n\nTotal number of conv: 44095  \n  \n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Fign_HTML.gif)\n\n|\n\n1\\. Note that the total number of conversations after filtering and padding\nprocess is 44,095 which is less than previous max conv size 50,000 as some\nconversations are filtered out\n\n2\\. SVCAB size is around 8000 which makes sense as the total numbers of\nconversation are around 44,000 lines, the number of vocabulary used is between\n5000 and 10,000  \n  \n---|---  \n  \n### 16.4.5 Creation of TensorFlow Movie Dataset Object (mDS)\n\nTensorFlow dataset object is created by using Dataset.from_tensor_slices()\nmethod of TensorFlow Data class as below:\n\nIn[12]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figo_HTML.gif)\n\n|\n\ntflow.data.Dataset.from_tensor_slices **?**  \n  \n---|---|---  \n  \nIn[13]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figp_HTML.gif)\n\n|\n\n_# Define the Batch and Buffer size_\n\nsBatch = 64\n\nsBuffer = 20000\n\n_# Create mDS object from_ _TensorFlow_ _class_\n\nmDS = tflow.data.Dataset.from_tensor_slices(({'inNodes':queries,\n'decNodes':responses[:, :-1]},{'outNodes':responses[:, 1:]}))\n\nmDS = mDS.cache()\n\nmDS = mDS.shuffle(sBuffer)\n\nmDS = mDS.batch(sBatch)\n\nmDS = mDS.prefetch(tflow.data.experimental.AUTOTUNE)  \n  \n---|---|---  \n  \n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figq_HTML.gif)\n\n|\n\n1\\. Create a TensorFlow Dataset object first to define Batch and Buffer size\n\n2\\. Define three layers of Transformer Model: a. Input node layer (inNodes) \u2013\nQueries b. Decoder input node layer (decNodes) \u2013 Responses c. Output node\nlayer (outNodes) - Responses\n\n3\\. Define prefetch scheme\u2014AUTOTUNE in our project  \n  \n---|---  \n  \n### 16.4.6 Calculate Attention Learning Weights\n\nThe main concept of Transformer Technology is Attention Learning technique,\nwhich aimed at network capability to focus _attention_ to various parts of\ntraining sequence during recurrent network learning. AI chatbot corresponds to\n_self-attention_ learning on movie dialogs, in which the network has attention\nability to different positions of dialog token sequences to compute utterances\nrepresentation. A system architecture of Attention Learning model with\nTransformer Technology is shown in Fig. 16.2. Implement Attention Equation to\ncalculate the attention weight is given by\n\n![$$ \\\\mathrm{Attention}\\\\left\\(Q,K,V\\\\right\\)=\nsoft{\\\\max}_k\\\\left\\(\\\\frac{Q{K}^T}{\\\\sqrt{d_k}}\\\\right\\)\n$$](../images/533412_1_En_16_Chapter/533412_1_En_16_Chapter_TeX_Equ1.png)\n\n(16.1)\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Fig2_HTML.png)\n\nFig. 16.2\n\nAttention Learning with Transformer Technology\n\nAttention Equation is a typical scaled-dot-product attention function in\ntransformer object Query ( _Q_ ), _K_ (Key), and _V_ (Value) Value and Python\nimplementation is given below:\n\nIn[14]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "20b2460b-100f-4f1d-9af9-05304a0cc4e0": {"__data__": {"id_": "20b2460b-100f-4f1d-9af9-05304a0cc4e0", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e971391-34b4-417f-b177-0719365b2a88", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "df1cfddfd0dac16a56985e308cc03e3b14bb993042026f5988e5107f998f4a63", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89f70779-3fed-4981-affc-2c3464eaa30c", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9be373c6d6a17b803e4b0351b4eee95045bab8e63b4f8bb2ffc476cd1166579a", "class_name": "RelatedNodeInfo"}}, "hash": "344f37728a6aa9990e7967a4fa461b96fbcc50e5e7374a47f43bea340543c889", "text": "16.2. Implement Attention Equation to\ncalculate the attention weight is given by\n\n![$$ \\\\mathrm{Attention}\\\\left\\(Q,K,V\\\\right\\)=\nsoft{\\\\max}_k\\\\left\\(\\\\frac{Q{K}^T}{\\\\sqrt{d_k}}\\\\right\\)\n$$](../images/533412_1_En_16_Chapter/533412_1_En_16_Chapter_TeX_Equ1.png)\n\n(16.1)\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Fig2_HTML.png)\n\nFig. 16.2\n\nAttention Learning with Transformer Technology\n\nAttention Equation is a typical scaled-dot-product attention function in\ntransformer object Query ( _Q_ ), _K_ (Key), and _V_ (Value) Value and Python\nimplementation is given below:\n\nIn[14]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figr_HTML.gif)\n\n|\n\n_# Calculate the Attention Weight, Query (q), Key(k), Value(v), Mask(m)_\n\n**def** calc_attention(q, k, v, m):\n\nqk = tflow.matmul(q, k, transpose_b = **True** )\n\ndep = tflow.cast(tflow.shape(k)[-1], tflow.float32)\n\nmlogs = qk / tflow.math.sqrt(dep)\n\n_# Use the masking for padding_\n\n**if** m **is not None** :\n\nmlogs += (m * -1e9)\n\n_# Apply softmax on the final axis of the utterance sequence_\n\natt_wts = tflow.nn.softmax(mlogs, axis = -1)\n\n_# Apply matmul() operation_\n\nout_wts = tflow.matmul(att_wts, v)\n\nreturn out_wts  \n  \n---|---|---  \n  \n### 16.4.7 Multi-Head-Attention (MHAttention)\n\nMulti-Head-Attention (MHAttention) consists of the following steps:\n\n  1. 1.\n\nConstruct linear layers.\n\n  2. 2.\n\nPerform head-splitting.\n\n  3. 3.\n\nCalculate attention weights.\n\n  4. 4.\n\nCombine heads.\n\n  5. 5.\n\nCondense layers\n\nMHAttention is implemented as follows:\n\nIn[15]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figs_HTML.gif)\n\n|\n\n**class** MHAttention(tflow.keras.layers.Layer):\n\n**def** __init__(self, dm, nhd, name=\"MHAttention\"):\n\nsuper(MHAttention, self).__init__(name=name)\n\nself.nhd = nhd\n\nself.dm = dm\n\nassert dm **%** self.nhd == 0\n\nself.dep = dm **//** self.nhd\n\nself.qdes = tflow.keras.layers.Dense(units=dm)\n\nself.kdes = tflow.keras.layers.Dense(units=dm)\n\nself.vdes = tflow.keras.layers.Dense(units=dm)\n\nself.des = tflow.keras.layers.Dense(units=dm)\n\n**def** sheads(self, inNodes, bsize):\n\ninNodes = tflow.reshape(\n\ninNodes, shape=(bsize, -1, self.nhd, self.dep))\n\n**return** tflow.transpose(inNodes, perm=[0, 2, 1, 3])\n\n**def** call(self, inNodes):\n\nq, k, v, m = inNodes['q'], inNodes['k'], inNodes['v'], inNodes['m']\n\nbsize = tflow.shape(q)[0]\n\n_# 1. Construct Linear-layers_\n\nq = self.qdes(q)\n\nk = self.kdes(k)\n\nv = self.vdes(v)\n\n_# 2. Perform Head-splitting_\n\nq = self.sheads(q, bsize)\n\nk = self.sheads(k, bsize)\n\nv = self.sheads(v, bsize)\n\n_# 3. Calculate Attention Weights_\n\nsattention = calc_attention(q, k, v, m)\n\nsattention = tflow.transpose(sattention, perm=[0, 2, 1, 3])\n\n_# 4. Head Combining_\n\ncattention = tflow.reshape(sattention,\n\n(bsize, -1, self.dm))\n\n_# 5. Layer Condensation_\n\noutNodes = self.des(cattention)\n\nreturn outNodes  \n  \n---|---|---  \n  \n### 16.4.8 System Implementation\n\n#### 16.4.8.1 Step 1. Implement Masking\n\nImplement (1) Padding Mask and (2) Look_ahead Mask to mask token sequences.\n\nIn[16]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89f70779-3fed-4981-affc-2c3464eaa30c": {"__data__": {"id_": "89f70779-3fed-4981-affc-2c3464eaa30c", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20b2460b-100f-4f1d-9af9-05304a0cc4e0", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "344f37728a6aa9990e7967a4fa461b96fbcc50e5e7374a47f43bea340543c889", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5eeb2ea6-00e1-4972-9b01-03bbcb378058", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "1efe6a400b873c63ba10ebffe668b4d34144c870d8dca6ae34a4706917b2db1e", "class_name": "RelatedNodeInfo"}}, "hash": "9be373c6d6a17b803e4b0351b4eee95045bab8e63b4f8bb2ffc476cd1166579a", "text": "Perform Head-splitting_\n\nq = self.sheads(q, bsize)\n\nk = self.sheads(k, bsize)\n\nv = self.sheads(v, bsize)\n\n_# 3. Calculate Attention Weights_\n\nsattention = calc_attention(q, k, v, m)\n\nsattention = tflow.transpose(sattention, perm=[0, 2, 1, 3])\n\n_# 4. Head Combining_\n\ncattention = tflow.reshape(sattention,\n\n(bsize, -1, self.dm))\n\n_# 5. Layer Condensation_\n\noutNodes = self.des(cattention)\n\nreturn outNodes  \n  \n---|---|---  \n  \n### 16.4.8 System Implementation\n\n#### 16.4.8.1 Step 1. Implement Masking\n\nImplement (1) Padding Mask and (2) Look_ahead Mask to mask token sequences.\n\nIn[16]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figt_HTML.gif)\n\n|\n\n# Generate Padding Mask (gen_pmask)\n\n**def** gen_pmask(p):\n\npmask = tflow.cast(tflow.math.equal(p, 0), tflow.float32)\n\n**return** pmask[:, tflow.newaxis, tflow.newaxis, :]  \n  \n---|---|---  \n  \nIn[17]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figu_HTML.gif)\n\n|\n\n_# Generate Look_Ahead Mask (gen_lamask)_\n\n**def** gen_lamask(x):\n\nslen = tflow.shape(x)[1]\n\nlamask = 1- tflow.linalg.band_part(tflow.ones((slen, slen)), -1, 0)\n\npmask = gen_pmask(x)\n\n**return** tflow.maximum(lamask, pmask)  \n  \n---|---|---  \n  \nReview _lamask_ with a sample matrix:\n\nIn[18]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figv_HTML.gif)\n\n|\n\nprint(gen_lamask(tflow.constant([[1, 2, 0, 4, 5]])))  \n  \n---|---|---  \n  \nOut[18]\n\n|\n\ntf.Tensor(\n\n[[[[0. 1. 1. 1. 1.]\n\n[0. 0. 1. 1. 1.]\n\n[0. 0. 1. 1. 1.]\n\n[0. 0. 1. 0. 1.]\n\n[0. 0. 1. 0. 0.]]]], shape=(1, 1, 5, 5), dtype=float32)  \n  \n#### 16.4.8.2 Step 2. Implement Positional Encoding\n\nThe main function of positional encoding is to provide model with information\nabout the relative position of word tokens within utterance for attention\nlearning given by the following formula:\n\n![$$ {\\\\displaystyle\n\\\\begin{array}{c}{\\\\mathrm{PE}}_{\\\\left\\(\\\\mathrm{pos},2i\\\\right\\)}=\\\\sin\n\\\\left\\(\\\\mathrm{pos}/{10000}^{2i/{d}_{\\\\mathrm{model}}}\\\\right\\)\\\\\\\\\n{}{\\\\mathrm{PE}}_{\\\\left\\(\\\\mathrm{pos},2i+1\\\\right\\)}=\\\\cos\n\\\\left\\(\\\\mathrm{pos}/{10000}^{2i/{d}_{\\\\mathrm{model}}}\\\\right\\)\\\\end{array}}\n$$](../images/533412_1_En_16_Chapter/533412_1_En_16_Chapter_TeX_Equ2.png)\n\n(16.2)\n\nIn[19]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5eeb2ea6-00e1-4972-9b01-03bbcb378058": {"__data__": {"id_": "5eeb2ea6-00e1-4972-9b01-03bbcb378058", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89f70779-3fed-4981-affc-2c3464eaa30c", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9be373c6d6a17b803e4b0351b4eee95045bab8e63b4f8bb2ffc476cd1166579a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b37e3fcc-fd8a-446c-b8ec-1a44486c78f6", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "cc4dad7fb52e3d52c17754b3fbc28b8a2e4c3a171502d05d58800938a5c30753", "class_name": "RelatedNodeInfo"}}, "hash": "1efe6a400b873c63ba10ebffe668b4d34144c870d8dca6ae34a4706917b2db1e", "text": "[$$ {\\\\displaystyle\n\\\\begin{array}{c}{\\\\mathrm{PE}}_{\\\\left\\(\\\\mathrm{pos},2i\\\\right\\)}=\\\\sin\n\\\\left\\(\\\\mathrm{pos}/{10000}^{2i/{d}_{\\\\mathrm{model}}}\\\\right\\)\\\\\\\\\n{}{\\\\mathrm{PE}}_{\\\\left\\(\\\\mathrm{pos},2i+1\\\\right\\)}=\\\\cos\n\\\\left\\(\\\\mathrm{pos}/{10000}^{2i/{d}_{\\\\mathrm{model}}}\\\\right\\)\\\\end{array}}\n$$](../images/533412_1_En_16_Chapter/533412_1_En_16_Chapter_TeX_Equ2.png)\n\n(16.2)\n\nIn[19]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figw_HTML.gif)\n\n|\n\n_# Implementation of Positional Encoding Class (PEncoding)_\n\n**class** PEncoding(tflow.keras.layers.Layer):\n\n**def** __init__(self, pos, dm):\n\nsuper(PEncoding, self).__init__()\n\nself.pencode = self.pencods(pos, dm)\n\n**def** gdeg(self, pos, i, dm):\n\ndeg = 1 / tflow.pow(10000,(2 * (i // 2)) / tflow.cast(dm, tflow.float32))\n\n**return** pos * deg\n\n**def** pencods(self, pos, dm):\n\ndeg_rads = self.gdeg(pos = tflow.range(pos, dtype=tflow.float32)[:,\ntflow.newaxis], i=tflow.range(dm, dtype=tflow.float32)[tflow.newaxis, :], dm =\ndm)\n\nm_sin = tflow.math.sin(deg_rads[:, 1::2])\n\nm_cos = tflow.math.cos(deg_rads[:, 1::2])\n\npencode = tflow.concat([m_sin, m_cos], axis = -1)\n\npencode = pencode[tflow.newaxis, ...]\n\n**return** tflow.cast(pencode, tflow.float32)\n\n**def** call(self, inNodes):\n\n**return** inNodes + self.pencode[:, :tflow.shape(inNodes)[1], :]  \n  \n---|---|---  \n  \nTry to plot _PositionalEncoding_ diagram:\n\nIn[20]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figx_HTML.gif)\n\n|\n\n_# Create PositionalEncoding Sample_\n\npencoding_sample = PEncoding(50, 512)\n\npyplt.pcolormesh(pencoding_sample.pencode.numpy()[0], cmap = 'RdBu')\n\npyplt.xlabel('Depth')\n\npyplt.xlim((0, 512))\n\npyplt.ylabel('Position')\n\npyplt.colorbar()\n\npyplt.show()  \n  \n---|---|---  \n  \nOut[20]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figy_HTML.gif)  \n  \n---|---  \n  \n#### 16.4.8.3 Step 3. Implement Encoder Layer\n\nEncoder Layer (enclayer) implementation involves:\n\n  1. 1.\n\nCreate MHAttention object.\n\n  2. 2.\n\nTwo dense layers.\n\nDetails as shown below:\n\nIn[21]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figz_HTML.gif)\n\n|\n\n_# Implementation of Encoder Layer (enclayer)_\n\n**def** enclayer(i, dm, nhd, drop, name=\"enclayer\"):\n\ninNodes = tflow.keras.Input(shape=(None, dm), name=\"inNodes\")\n\npmask = tflow.keras.Input(shape=(1, 1, None), name=\"pmask\")\n\natt = MHAttention(\n\ndm, nhd, name=\"att\")({\n\n'q': inNodes,\n\n'k': inNodes,\n\n'v': inNodes,\n\n'm': pmask\n\n})\n\natt = tflow.keras.layers.Dropout(rate=drop)(att)\n\natt = tflow.keras.layers.LayerNormalization(\n\nepsilon=1e-6)(inNodes + att)\n\noutNodes = tflow.keras.layers.Dense(units=i, activation=\"relu\")(att)\n\noutNodes = tflow.keras.layers.Dense(units=dm)(outNodes)\n\noutNodes = tflow.keras.layers.Dropout(rate=drop)(outNodes)\n\noutNodes = tflow.keras.layers.LayerNormalization(\n\nepsilon=1e-6)(att + outNodes)\n\n**return** tflow.keras.Model(\n\ninputs=[inNodes, pmask], outputs=outNodes, name=name)  \n  \n---|---|---  \n  \n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b37e3fcc-fd8a-446c-b8ec-1a44486c78f6": {"__data__": {"id_": "b37e3fcc-fd8a-446c-b8ec-1a44486c78f6", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5eeb2ea6-00e1-4972-9b01-03bbcb378058", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "1efe6a400b873c63ba10ebffe668b4d34144c870d8dca6ae34a4706917b2db1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4cf1b79-17a9-4d8d-889b-d16e071b8af3", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "a62d890e1b471edec5cb73d0c73982485eae78af819c29d013bd406a70af5a30", "class_name": "RelatedNodeInfo"}}, "hash": "cc4dad7fb52e3d52c17754b3fbc28b8a2e4c3a171502d05d58800938a5c30753", "text": "[](../images/533412_1_En_16_Chapter/533412_1_En_16_Figaa_HTML.gif)\n\n|\n\n1\\. An Attention Learning object is defined and used at Encoder Layer\nimplementation class\n\n2\\. _relu_ function is used as default setting for Encoder Layer Activation\nFunction. Current research includes the modification (or change) of activation\nfunction for system enhancement  \n  \n---|---  \n  \nTry to display a sample Encoder Layer using Keras plot model():\n\nIn[22]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figab_HTML.gif)\n\n|\n\n_# Create a sample Encoder Layer and display object diagram_\n\nenclayer_sample = enclayer(i = 512, dm = 128, nhd = 4, drop = 0.3, name =\n\"enclayer_sample\")\n\ntflow.keras.utils.plot_model(enclayer_sample, to_file = 'enclayer.png',\nshow_shapes = **True** )  \n  \n---|---|---  \n  \nOut[22]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figac_HTML.gif)  \n  \n#### 16.4.8.4 Step 4. Implement Encoder\n\nEncoder implementation involves the following processes:\n\n  1. 1.\n\nEmbed inputs.\n\n  2. 2.\n\nPerform positional encoding scheme.\n\n  3. 3.\n\nEncode Num Layers\n\nIn[23]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figad_HTML.gif)\n\n|\n\n_# Implementation of Encoder Class (encoder)_\n\n**def** encoder(svcab,\n\nnlayers,\n\nx,\n\ndm,\n\nnhd,\n\ndrop,\n\nname=\"encoder\"):\n\ninNodes = tflow.keras.Input(shape=( **None** ,), name=\"inNodes\")\n\npmask = tflow.keras.Input(shape=(1, 1, **None** ), name=\"pmask\")\n\nembeddings = tflow.keras.layers.Embedding(svcab, dm)(inNodes)\n\nembeddings *= tflow.math.sqrt(tflow.cast(dm, tflow.float32))\n\nembeddings = PEncoding(svcab, dm)(embeddings)\n\noutNodes = tflow.keras.layers.Dropout(rate=drop)(embeddings)\n\n**for** i **in** range(nlayers):\n\noutNodes = enclayer(\n\ni=x,\n\ndm=dm,\n\nnhd=nhd,\n\ndrop=drop,\n\nname=\"enclayer_{}\".format(i),\n\n)([outNodes, pmask])\n\n**return** tflow.keras.Model(\n\ninputs=[inNodes, pmask], outputs=outNodes, name=name)  \n  \n---|---|---  \n  \nDisplay a sample Encoder using Keras Plot model:\n\nIn[24]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figae_HTML.gif)\n\n|\n\n_# Create a sample Encoder Sample and display object diagram_\n\nencoder_sample = encoder(svcab = 8192,\n\nnlayers = 2,\n\nx = 512,\n\ndm = 128,\n\nnhd = 4,\n\ndrop = 0.3,\n\nname = \"encoder_sample\")\n\ntflow.keras.utils.plot_model(encoder_sample, to_file='encoder_sample.png',\nshow_shapes = **True** )  \n  \n---|---|---  \n  \nOut[24]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figaf_HTML.gif)  \n  \n#### 16.4.8.5 Step 5. Implement Decoder Layer\n\nDecoder Layer implementation involves the following steps:\n\n  1. 1.\n\nMHAttention.\n\n  2. 2.\n\n2 Dense Decoder Layers with dropout.\n\nIn[25]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d4cf1b79-17a9-4d8d-889b-d16e071b8af3": {"__data__": {"id_": "d4cf1b79-17a9-4d8d-889b-d16e071b8af3", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b37e3fcc-fd8a-446c-b8ec-1a44486c78f6", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "cc4dad7fb52e3d52c17754b3fbc28b8a2e4c3a171502d05d58800938a5c30753", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62056447-7355-4d55-b8ce-1554f19bdc92", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3dbabbdf27b679f1a3b6fbb4b1ca3ab2b37457d80448f908973a55eed0271b60", "class_name": "RelatedNodeInfo"}}, "hash": "a62d890e1b471edec5cb73d0c73982485eae78af819c29d013bd406a70af5a30", "text": "[](../images/533412_1_En_16_Chapter/533412_1_En_16_Figaf_HTML.gif)  \n  \n#### 16.4.8.5 Step 5. Implement Decoder Layer\n\nDecoder Layer implementation involves the following steps:\n\n  1. 1.\n\nMHAttention.\n\n  2. 2.\n\n2 Dense Decoder Layers with dropout.\n\nIn[25]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figag_HTML.gif)\n\n|\n\n_# Implementation of Decoder Layer (declayer)_\n\n**def** declayer(i, dm, nhd, drop, name = \"declayer\"):\n\ninNodes = tflow.keras.Input(shape=(None, dm), name =\"inNodes\")\n\nencouts = tflow.keras.Input(shape=(None, dm), name=\"encouts\")\n\nlamask = tflow.keras.Input(shape=(1, None, None), name = \"lamask\")\n\npmask = tflow.keras.Input(shape=(1, 1, None), name = \"pmask\")\n\natt1 = MHAttention(dm, nhd, name=\"att1\")(inNodes={'q':inNodes,\n\n'k':inNodes,\n\n'v':inNodes,\n\n'm':lamask})\n\natt1 = tflow.keras.layers.LayerNormalization(epsilon=1e-6)(att1 + inNodes)\n\natt2 = MHAttention(dm,nhd, name = \"att2\")(inNodes={'q':att1,\n\n'k':encouts,\n\n'v':encouts,\n\n'm':pmask})\n\natt2 = tflow.keras.layers.Dropout(rate=drop)(att2)\n\natt2 = tflow.keras.layers.LayerNormalization(epsilon = 1e-6)(att2 + att1)\n\noutNodes = tflow.keras.layers.Dense(units=i, activation=\"relu\")(att2)\n\noutNodes = tflow.keras.layers.Dense(units=dm)(outNodes)\n\noutNodes = tflow.keras.layers.Dropout(rate=drop)(outNodes)\n\noutNodes = tflow.keras.layers.LayerNormalization(epsilon=1e-6)(outNodes +\natt2)\n\n**return** tflow.keras.Model(inputs=[inNodes, encouts, lamask, pmask],\n\noutputs = outNodes,\n\nname = name)  \n  \n---|---|---  \n  \n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figah_HTML.gif)\n\n|\n\n1\\. Encoder Layer implements single Attention Learning object, and Decoder\nLayer implements two Attention Learning objects att1 and att2 according to\nTransformer Learning model\n\n2\\. Again, _relu_ function is used as Activation Function. It can modify or\nadopt different Activation Function to improve network performance as studied\nin Sect. 16.1  \n  \n---|---  \n  \nDisplay sample Decoder Layer using Keras _plot_model()_ :\n\nIn[26]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figai_HTML.gif)\n\n|\n\n_# Create a decoder layer sample and show object association diagram_\n\ndeclayer_sample = declayer(i = 512, dm = 128, nhd = 4, drop = 0.3, name =\n\"declayer_sample\")\n\ntflow.keras.utils.plot_model(declayer_sample, to_file='declayer_sample.png',\nshow_shapes=True)  \n  \n---|---|---  \n  \nOut[26]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figaj_HTML.gif)  \n  \n#### 16.4.8.6 Step 6. Implement Decoder\n\nDecoder implementation involves the following processes:\n\n  1. 1.\n\nEmbed network outputs.\n\n  2. 2.\n\nLook ahead and pad masking.\n\n  3. 3.\n\nPositional encoding scheme.\n\n  4. 4.\n\nPerform N-decoder layers.\n\nIn[27]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62056447-7355-4d55-b8ce-1554f19bdc92": {"__data__": {"id_": "62056447-7355-4d55-b8ce-1554f19bdc92", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4cf1b79-17a9-4d8d-889b-d16e071b8af3", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "a62d890e1b471edec5cb73d0c73982485eae78af819c29d013bd406a70af5a30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a97a763b-9acb-4697-9a32-d91c65fdeb2b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "4f7d959c569405533e8815a3a5a23bdc006c59891fddcb3d264c0f5aade0fc00", "class_name": "RelatedNodeInfo"}}, "hash": "3dbabbdf27b679f1a3b6fbb4b1ca3ab2b37457d80448f908973a55eed0271b60", "text": "[](../images/533412_1_En_16_Chapter/533412_1_En_16_Figaj_HTML.gif)  \n  \n#### 16.4.8.6 Step 6. Implement Decoder\n\nDecoder implementation involves the following processes:\n\n  1. 1.\n\nEmbed network outputs.\n\n  2. 2.\n\nLook ahead and pad masking.\n\n  3. 3.\n\nPositional encoding scheme.\n\n  4. 4.\n\nPerform N-decoder layers.\n\nIn[27]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figak_HTML.gif)\n\n|\n\n_# Implementation of Decoder class (decoder)_\n\n**def** decoder(svcab,\n\nnlayers,\n\nx,\n\ndm,\n\nnhd,\n\ndrop,\n\nname='decoder'):\n\ninNodes = tflow.keras.Input(shape=( **None** ,), name=\"inNodes\")\n\nencouts = tflow.keras.Input(shape=( **None** , dm), name=\"encouts\")\n\nlamask = tflow.keras.Input(shape=(1, **None** , **None** ), name=\"lamask\")\n\npmask = tflow.keras.Input(shape=(1, 1, **None** ), name=\"pmask\")\n\nembeddings = tflow.keras.layers.Embedding(svcab, dm)(inNodes)\n\nembeddings *= tflow.math.sqrt(tflow.cast(dm, tflow.float32))\n\nembeddings = PEncoding(svcab, dm)(embeddings)\n\noutNodes = tflow.keras.layers.Dropout(rate=drop)(embeddings)\n\n**for** i **in** range(nlayers):\n\noutNodes = declayer(i = x,\n\ndm=dm,\n\nnhd=nhd,\n\ndrop=drop,\n\nname = 'declayer_{}'.format(i),)(inputs=[outNodes, encouts, lamask, pmask])\n\n**return** tflow.keras.Model(inputs=[inNodes, encouts, lamask, pmask],\n\noutputs = outNodes,\n\nname = name)  \n  \n---|---|---  \n  \nDisplay sample Decoder using Keras Plot_model:\n\nIn[28]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figal_HTML.gif)\n\n|\n\n_# Create a decoder sample and show object association diagram_\n\ndecoder_sample = decoder(svcab=8192,\n\nnlayers=2,\n\nx = 512,\n\ndm = 128,\n\nnhd = 4,\n\ndrop = 0.3,\n\nname = \"decoder_sample\")\n\ntflow.keras.utils.plot_model(decoder_sample, to_file='decoder_sample.png',\nshow_shapes = **True** )  \n  \n---|---|---  \n  \nOut[28]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figam_HTML.gif)  \n  \n#### 16.4.8.7 Step 7. Implement Transformer\n\nTransformer involves implementing Encoder, Decoder, and the final Linear\nLayer.\n\nTransformer Decoder output is input to Linear Layer as a Recurrent Neural\nNetwork (RNN) and output model is returned.\n\nIn[29]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a97a763b-9acb-4697-9a32-d91c65fdeb2b": {"__data__": {"id_": "a97a763b-9acb-4697-9a32-d91c65fdeb2b", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62056447-7355-4d55-b8ce-1554f19bdc92", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3dbabbdf27b679f1a3b6fbb4b1ca3ab2b37457d80448f908973a55eed0271b60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e79d1a1-6aca-4766-9adc-0ca5b90eb143", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "0634426708117cf6630be6a115257856f023fad76a3c9791bf8975a41402e5d2", "class_name": "RelatedNodeInfo"}}, "hash": "4f7d959c569405533e8815a3a5a23bdc006c59891fddcb3d264c0f5aade0fc00", "text": "[](../images/533412_1_En_16_Chapter/533412_1_En_16_Figam_HTML.gif)  \n  \n#### 16.4.8.7 Step 7. Implement Transformer\n\nTransformer involves implementing Encoder, Decoder, and the final Linear\nLayer.\n\nTransformer Decoder output is input to Linear Layer as a Recurrent Neural\nNetwork (RNN) and output model is returned.\n\nIn[29]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figan_HTML.gif)\n\n|\n\n_# Implementation of_ _Transformer_ _Class_\n\n**def** transformer(svcab, nlayers, x, dm, nhd, drop, name=\"transformer\"):\n\nqueries = tflow.keras.Input(shape=( **None** ,), name=\"inNodes\")\n\ndec_queries = tflow.keras.Input(shape=( **None** ,), name=\"decNodes\")\n\nenc_pmask = tflow.keras.layers.Lambda(\n\ngen_pmask, output_shape=(1, 1, **None** ),\n\nname=\"enc_pmask\")(queries)\n\n_# Perform Look Ahead Masking for Decoder Input for the Att1_\n\nlamask = tflow.keras.layers.Lambda(gen_lamask,\n\noutput_shape=(1, **None** , **None** ),\n\nname = \"lamask\")(dec_queries)\n\n_# Perform Padding Masking for Encoder Output for the Att2_\n\ndec_pmask = tflow.keras.layers.Lambda(gen_pmask,\n\noutput_shape=(1, 1, **None** ),\n\nname=\"dec_pmask\")(queries)\n\nencouts = encoder(svcab=svcab,\n\nnlayers = nlayers,\n\nx = x,\n\ndm = dm,\n\nnhd = nhd,\n\ndrop = drop,)(inputs = [queries, enc_pmask])\n\ndecouts = decoder(svcab=svcab,\n\nnlayers = nlayers,\n\nx = x,\n\ndm = dm,\n\nnhd = nhd,\n\ndrop=drop,)(inputs=[dec_queries, encouts, lamask, dec_pmask])\n\nresponses = tflow.keras.layers.Dense(units=svcab, name=\"outNodes\")(decouts)\n\n**return** tflow.keras.Model(inputs=[queries, dec_queries], outputs=responses,\nname=name)  \n  \n---|---|---  \n  \nDisplay sample transformer object using Keras Plot_model:\n\nIn[30]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figao_HTML.gif)\n\n|\n\n_# Create a transformer sample and display object diagram_\n\ntransformer_sample = transformer(svcab=8192, nlayers=4, x=512,\n\ndm=128, nhd = 4, drop=0.3, name=\"transformer_sample\")\n\ntflow.keras.utils.plot_model(transformer_sample,\nto_file=\"transformer_sample.png\", show_shapes= **True** )  \n  \n---|---|---  \n  \nOut[30]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figap_HTML.gif)  \n  \n#### 16.4.8.8 Step 8. Model Training\n\nParameters for nLayers, dm, and units (x) had reduced to speed up training\nprocess.\n\nIn[31]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figaq_HTML.gif)\n\n|\n\n_# Create_ _Transformer_ _Model_\n\ntflow.keras.backend.clear_session()\n\nmodel = transformer(svcab = SVCAB,\n\nnlayers=2,\n\nx=512,\n\ndm=256,\n\nnhd=8,\n\ndrop=0.1)  \n  \n---|---|---  \n  \n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figar_HTML.gif)\n\n|\n\n1\\. A Movie Chatbot Transformer Model consists of two layers with 512 units,\ndata-model size 256, head number 8 and dropout rate 0.1 according to\nTransformer Model as in Fig. 16.2\n\n2\\. It is recommended to modify these parameter settings to improve network\nperformance as discussed in Sect. 16.1  \n  \n---|---  \n  \n#### 16.4.8.9 Step 9. Implement Model Evaluation Function\n\nA loss function is implemented for system evaluation. It is important to apply\na padding mask when calculating the loss since target sequences are padded.\n\nIn[32]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e79d1a1-6aca-4766-9adc-0ca5b90eb143": {"__data__": {"id_": "4e79d1a1-6aca-4766-9adc-0ca5b90eb143", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a97a763b-9acb-4697-9a32-d91c65fdeb2b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "4f7d959c569405533e8815a3a5a23bdc006c59891fddcb3d264c0f5aade0fc00", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bee044a8-00b9-49fc-a02e-553193091c9b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "0832e2d4dfe18b9b684a9b8951d055b6cb255e05dbe539e54435632212404f87", "class_name": "RelatedNodeInfo"}}, "hash": "0634426708117cf6630be6a115257856f023fad76a3c9791bf8975a41402e5d2", "text": "[](../images/533412_1_En_16_Chapter/533412_1_En_16_Figar_HTML.gif)\n\n|\n\n1\\. A Movie Chatbot Transformer Model consists of two layers with 512 units,\ndata-model size 256, head number 8 and dropout rate 0.1 according to\nTransformer Model as in Fig. 16.2\n\n2\\. It is recommended to modify these parameter settings to improve network\nperformance as discussed in Sect. 16.1  \n  \n---|---  \n  \n#### 16.4.8.9 Step 9. Implement Model Evaluation Function\n\nA loss function is implemented for system evaluation. It is important to apply\na padding mask when calculating the loss since target sequences are padded.\n\nIn[32]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figas_HTML.gif)\n\n|\n\n_# Implementation of Evaluation Function (Loss Function)_\n\n**def** Eval_function(xtrue, xpred):\n\nxtrue = tflow.reshape(xtrue, shape=(-1, MLEN - 1))\n\nloss_val = tflow.keras.losses.SparseCategoricalCrossentropy(\n\nfrom_logits= **True** , reduction=\"none\")(xtrue, xpred)\n\nmask_val = tflow.cast(tflow.not_equal(xtrue, 0), tflow.float32)\n\nloss_val = tflow.multiply(loss_val, mask_val)\n\n**return** tflow.reduce_mean(loss_val)  \n  \n---|---|---  \n  \n#### 16.4.8.10 Step 10. Implement Customized Learning Rate\n\nAdam_Optimizer with customized learning rate is used with formula below:\n\n![$$ {I}_{\\\\mathrm{rate}}={d}_{\\\\mathrm{model}}^{-5}\\\\ast \\\\min \\\\left\\(\nstep\\\\_ nu{m}^{-0.5}, step\\\\_ nu m\\\\ast warmup\\\\_ step{s}^{-1.5}\\\\right\\)\n$$](../images/533412_1_En_16_Chapter/533412_1_En_16_Chapter_TeX_Equ3.png)\n\n(16.3)\n\nIn[33]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figat_HTML.gif)\n\n|\n\n_# Implementation of Customized Learning Rate_\n\n**class** CLearning(tflow.keras.optimizers.schedules.LearningRateSchedule):\n\n**def** __init__(self, dm, warmup_steps=4000):\n\nsuper(CLearning, self).__init__()\n\nself.dm = dm\n\nself.dm = tflow.cast(self.dm, tflow.float32)\n\nself.warmup_steps = warmup_steps\n\n**def** __call__(self, step):\n\n_# arg1 = tflow.math.rsqrt(step)_\n\narg1 = tflow.math.rsqrt(tflow.cast(step, tflow.float32))\n\narg2 = tflow.cast(step, tflow.float32) * (tflow.cast(self.warmup_steps,\ntflow.float32)**-1.5)\n\n**return** tflow.math.rsqrt(self.dm) * tflow.math.minimum(arg1, arg2)  \n  \n---|---|---  \n  \nPlot customized Learning Rate:\n\nIn[34]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figau_HTML.gif)\n\n|\n\n_# Create customized learning rate object and display performance_\n\nCLearning_sample = CLearning(dm=128)\n\npyplt.plot(CLearning_sample(tflow.range(200000, dtype=tflow.float32)))\n\npyplt.ylabel(\"Learning Rate\")\n\npyplt.xlabel(\"Train Step\")  \n  \n---|---|---  \n  \nOut[34]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figav_HTML.gif)  \n  \n#### 16.4.8.11 Step 11. Compile Chatbot Model\n\nIn[35]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bee044a8-00b9-49fc-a02e-553193091c9b": {"__data__": {"id_": "bee044a8-00b9-49fc-a02e-553193091c9b", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e79d1a1-6aca-4766-9adc-0ca5b90eb143", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "0634426708117cf6630be6a115257856f023fad76a3c9791bf8975a41402e5d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7710e3c-927f-468f-a50b-bfa09014a3eb", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "c53a6bf0d0ece3a901872f72265427cda550ff93ec33ee7d612579fb3e1e6b79", "class_name": "RelatedNodeInfo"}}, "hash": "0832e2d4dfe18b9b684a9b8951d055b6cb255e05dbe539e54435632212404f87", "text": "[](../images/533412_1_En_16_Chapter/533412_1_En_16_Figau_HTML.gif)\n\n|\n\n_# Create customized learning rate object and display performance_\n\nCLearning_sample = CLearning(dm=128)\n\npyplt.plot(CLearning_sample(tflow.range(200000, dtype=tflow.float32)))\n\npyplt.ylabel(\"Learning Rate\")\n\npyplt.xlabel(\"Train Step\")  \n  \n---|---|---  \n  \nOut[34]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figav_HTML.gif)  \n  \n#### 16.4.8.11 Step 11. Compile Chatbot Model\n\nIn[35]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figaw_HTML.gif)\n\n|\n\n_# Compile Movie Chatbot Model_\n\n_# Set the Customized Learning Rate_\n\ncLRate = CLearning(256)\n\n_# Set Adam Optimizers_\n\noptimizer = tflow.keras.optimizers.Adam(learning_rate=cLRate, beta_1=0.9,\nbeta_2=0.98, epsilon=1e-9)\n\n_# Implement Accuracy Evaluation Scheme_\n\ndef accuracy(xtrue, xpred):\n\nxtrue = tflow.reshape(xtrue, shape=(-1, MLEN - 1))\n\nreturn tflow.keras.metrics.sparse_categorical_accuracy(xtrue, xpred)\n\n_# Compile Chatbot Model_\n\nmodel.compile(optimizer=optimizer, loss=Eval_function, metrics=[accuracy])  \n  \n---|---|---  \n  \n#### 16.4.8.12 Step 12. System Training (Model Fitting)\n\nTrain Chatbot transformer model by calling _model.fit()_ for 20 epochs to save\ntime.\n\nIn[36]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figax_HTML.gif)\n\n|\n\nEPOCHS = 20\n\nmodel.fit(mDS, epochs = EPOCHS)  \n  \n---|---|---  \n  \nOut[36]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figay_HTML.gif)  \n  \n#### 16.4.8.13 Step 13. System Evaluation and Live Chatting\n\nSystem evaluation and live chatting implementation involve following steps:\n\n  1. 1.\n\nCreate Mining() method by performing data preprocessing of all utterances.\n\n  2. 2.\n\nPerform tokenization of utterances and padded with START and END tokens.\n\n  3. 3.\n\nPerform LookAhead and Padding Masks.\n\n  4. 4.\n\nConstruct Transformer model with Attention Learning.\n\n  5. 5.\n\nImplement chatting() method by decoder scheme.\n\n  6. 6.\n\nCombine chatted word sequences to decoder input.\n\n  7. 7.\n\nUse Transformer Model for system to predict responses based on previous\ntraining epochs.\n\nIn[37]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figaz_HTML.gif)\n\n|\n\n_# Implementation of Movie Chatting class - mchat_\n\ndef mchat(utterance):\n\n_# Utterance Preprocessing and add START AND END TOKENS_\n\nutterance = pp_utterance(utterance)\n\nutterance = tflow.expand_dims(START_TOKEN + m_token.encode(utterance) +\nEND_TOKEN, axis = 0)\n\n_# Create response object_\n\nresponse = tflow.expand_dims(START_TOKEN, 0)\n\n**for** i **in** range(MLEN):\n\nchatting = model(inputs = [utterance, response], training = **False** )\n\n_# Choose last_word from token sequence_\n\nchatting = chatting[:, -1:, :]\n\nchatted_id = tflow.cast(tflow.argmax(chatting, axis=-1), tflow.int32)\n\n_# Return with chattedID with ENDTOKEN_\n\n**if** tflow.equal(chatted_id, END_TOKEN[0]):\n\n**break**\n\n_# Combine CHATTEDID with utterance response_\n\nresponse = tflow.concat([response, chatted_id], axis=-1)\n\nreturn tflow.squeeze(response, axis = 0)\n\n_# Implementation of main class for Movie Chatting - mchatting_\n\ndef mchatting(utterance):\n\nmchatting = mchat(utterance)\n\nchatted_utterance = m_token.decode([i **for** i **in** mchatting **if** i <\nm_token.vocab_size])\n\nprint('Query: {}'.format(utterance))\n\nprint('Response: {}'.format(chatted_utterance))\n\n**return** chatted_utterance  \n  \n---|---|---  \n  \nTry some movie conversations to see whether it works:\n\nIn[38]\n\n|\n\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7710e3c-927f-468f-a50b-bfa09014a3eb": {"__data__": {"id_": "c7710e3c-927f-468f-a50b-bfa09014a3eb", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bee044a8-00b9-49fc-a02e-553193091c9b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "0832e2d4dfe18b9b684a9b8951d055b6cb255e05dbe539e54435632212404f87", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3e2c50f-7e64-455a-83e3-9a4d49b57115", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "6c700351b4b8bb9455e48c6c2b5a33ad93f8b59258aa14fc5740553d746dfde4", "class_name": "RelatedNodeInfo"}}, "hash": "c53a6bf0d0ece3a901872f72265427cda550ff93ec33ee7d612579fb3e1e6b79", "text": "[](../images/533412_1_En_16_Chapter/533412_1_En_16_Figba_HTML.gif)\n\n|\n\noutput = mchatting(\"Where have you been?\")  \n  \n---|---|---  \n  \nOut[38]\n\n|\n\nQuery: Where have you been?\n\nResponse: i m going to get my father .  \n  \nIn[39]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figbb_HTML.gif)\n\n|\n\noutput = mchatting(\"It's a trap\")  \n  \n---|---|---  \n  \nOut[39]\n\n|\n\nQuery: It's a trap\n\nResponse: i don t know what to do . it s just that way .  \n  \nIn[40]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figbc_HTML.gif)\n\n|\n\noutput = mchatting(\"Do you need help?\")  \n  \n---|---|---  \n  \nOut[40]\n\n|\n\nQuery: Do you need help?\n\nResponse: no .  \n  \nIn[41]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figbd_HTML.gif)\n\n|\n\noutput = mchatting(\"What do you think?\")  \n  \n---|---|---  \n  \nOut[41]\n\n|\n\nQuery: What do you think?\n\nResponse: i don t know . i don t know . i m not sure . i just had to see what\ni m saying .  \n  \nIn[42]\n\n|\n\n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figbe_HTML.gif)\n\n|\n\noutput = mchatting(\"Are you happy?\")  \n  \n---|---|---  \n  \nOut[42]\n\n|\n\nQuery: Are you happy?\n\nResponse: no . but you re not . you re not sure ?  \n  \n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figbf_HTML.gif)\n\n|\n\n1\\. Training showed that epochs 1\u201320 are rather slow but increased in accuracy\nand decreased in loss rate\n\n2\\. Two chatbots experiments with one used 2 epochs and the other used 20\nepochs. Results showed that performance on 20 epochs has satisfactory\nperformance than the one with 2 epochs\n\n3\\. Increase epochs, say up to 50 epochs to review whether accuracy has\ncontinuous improvement. It is natural to require more time unless there are\nsufficient GPUs  \n  \n---|---  \n  \n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figbg_HTML.gif)\n\n|\n\n**Workshop 7.1 Fine-tune Chatbot Model**\n\nTensorFlow and Transformer Technology are used to develop a domain-based\nChatbot system\n\nThere are rooms to fine-tune model performance like any AI model. It can be\nconducted by:\n\n1\\. Dataset Level\n\n\u2013 Enhance preprocessing process\n\n\u2013 Improve data record selection scheme, e.g. sample size, utterance max\nlength, etc.\n\n2\\. Network Model Level\n\n\u2013 Fine-tune system parameters, e.g. Learning Rate and Method, etc.\n\n\u2013 Fine-tune Transformer Model by modifying Attention Function etc. Compare\nperformances (MUST) and analysis (bonus)\n\nFine-tune Movie Chatbot model and compare with original version  \n  \n---|---  \n  \n![](../images/533412_1_En_16_Chapter/533412_1_En_16_Figbh_HTML.gif)\n\n|\n\n**Workshop 7.2 Mini Project - Build a Semantic-Level AI Chatbot System**\n\nExtend Character-level and Word-level NLU to a Semantic-Level NLU\n\n1\\. Modify codes of AI Chatbot learnt in this section to implement a Semantic-\nlevel AI Chatbot system\n\n2\\. Compare system performance of this revised system with previous Character-\nlevel and Word-level AI Chatbot system  \n  \n---|---  \n  \n## 16.5 Related Works\n\nThis workshop had integrated all NLP related implementation techniques\nincluding TensorFlow and Keras with Transformer Technology to design an AI-\nbased NLP application chatbot system. It is a step-by-step implementation\nconsisting of data preprocessing, model construction, system training, testing\nevaluation process; and Attention Learning and Transformer Technology with\nTensorFlow and Keras implementation platform easily applied to other chatbot\ndomain and interactive QA systems using Cornell Large Movie dataset with over\n200,000 movie conversations with 10,000+ movie characters.\n\nNevertheless, it is only the dawn of journey. There are regular new R&D\nprevalence and usage in NLP applications.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3e2c50f-7e64-455a-83e3-9a4d49b57115": {"__data__": {"id_": "a3e2c50f-7e64-455a-83e3-9a4d49b57115", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7710e3c-927f-468f-a50b-bfa09014a3eb", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "c53a6bf0d0ece3a901872f72265427cda550ff93ec33ee7d612579fb3e1e6b79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbe4f542-7fd0-42fb-9be4-cb5001c79bba", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "6a12eefcac94b05293ef5b49c650671088a763be7cd8b6ba8ead0a1088dfd83f", "class_name": "RelatedNodeInfo"}}, "hash": "6c700351b4b8bb9455e48c6c2b5a33ad93f8b59258aa14fc5740553d746dfde4", "text": "Modify codes of AI Chatbot learnt in this section to implement a Semantic-\nlevel AI Chatbot system\n\n2\\. Compare system performance of this revised system with previous Character-\nlevel and Word-level AI Chatbot system  \n  \n---|---  \n  \n## 16.5 Related Works\n\nThis workshop had integrated all NLP related implementation techniques\nincluding TensorFlow and Keras with Transformer Technology to design an AI-\nbased NLP application chatbot system. It is a step-by-step implementation\nconsisting of data preprocessing, model construction, system training, testing\nevaluation process; and Attention Learning and Transformer Technology with\nTensorFlow and Keras implementation platform easily applied to other chatbot\ndomain and interactive QA systems using Cornell Large Movie dataset with over\n200,000 movie conversations with 10,000+ movie characters.\n\nNevertheless, it is only the dawn of journey. There are regular new R&D\nprevalence and usage in NLP applications. Below are lists of renowned domains\nand resources related to chatbot systems for reference.\n\nDatasets for Chatbot Systems:\n\n  * Taskmaster from Google Research (GoogleResearch 2022a).\n\n  * Simulated Dialogue dataset from Google Research (GoogleResearch 2022b).\n\n  * Dialog Challenge dataset from Microsoft (MicrosoftDialog 2022).\n\n  * Dialog State Tracking Challenge dataset (DSTC 2022).\n\nKeras Modules and Optimizer:\n\n  * Keras layers (Keras 2022a).\n\n  * Keras optimizers (Keras 2022b).\n\n  * An overview of optimizers (Ruder 2022).\n\n  * Adam optimizer (Adam 2022).\n\nFamous Chatbot System:\n\n  * Amazon Alexa developer blog (Alexa 2022).\n\n  * Apple Siri Developer (AppleSiri 2022).\n\n  * Duer from Baidu (Duer 2022).\n\n  * Google Assistant (GoogleAssistant 2022).\n\n  * Microsoft Cortana Developer (MicrosoftCortana 2022).\n\n  * Samsung Bixby Developer (SamsungBixby 2022).\n\n  * Xiaowei from Tencent (Xiaowei 2022).\n\nReferences\n\n  1. Adam (2022) Adam optimizer: [https://\u200barxiv.\u200borg/\u200babs/\u200b1412.\u200b6980](https://arxiv.org/abs/1412.6980). Accessed 29 June 2022.\n\n  2. Alexa (2022) Amazon Alexa developer blog: [https://\u200bdeveloper.\u200bamazon.\u200bcom/\u200bblogs/\u200bhome/\u200btag/\u200bAlexa](https://developer.amazon.com/blogs/home/tag/Alexa). Accessed 29 June 2022.\n\n  3. AppleSiri (2022) Apple Siri Developer: [https://\u200bdeveloper.\u200bapple.\u200bcom/\u200bsiri/\u200b](https://developer.apple.com/siri/). Accessed 29 June 2022.\n\n  4. Bansal, A. (2021) Advanced Natural Language Processing with TensorFlow 2: Build effective real-world NLP applications using NER, RNNs, seq2seq models, Transformers, and more. Packt Publishing.\n\n  5. Batish, R. (2018) Voicebot and Chatbot Design: Flexible conversational interfaces with Amazon Alexa, Google Home, and Facebook Messenger. Packt Publishing.\n\n  6. Cornell (2022) [https://\u200bwww.\u200bcs.\u200bcornell.\u200bedu/\u200b~cristian/\u200bChameleons_\u200bin_\u200bimagined_\u200bconversations.\u200bhtml](https://www.cs.cornell.edu/~cristian/Chameleons_in_imagined_conversations.html). Accessed 29 June 2022.\n\n  7. Cornell_Movie_Corpus (2022) Cornell Movie Corpus archive. [https://\u200bwww.\u200bkaggle.\u200bcom/\u200bdatasets/\u200bCornell-University/\u200bmovie-dialog-corpus](https://www.kaggle.com/datasets/Cornell-University/movie-dialog-corpus). Accessed 29 June 2022.\n\n  8. Devlin, J., Chang, M. W., Lee, K. and Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Archive: [https://\u200barxiv.\u200borg/\u200bpdf/\u200b1810.\u200b04805.\u200bpdf](https://arxiv.org/pdf/1810.04805.pdf).\n\n  9. Duer (2022) Duer Baidu AI Chatbot.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbe4f542-7fd0-42fb-9be4-cb5001c79bba": {"__data__": {"id_": "bbe4f542-7fd0-42fb-9be4-cb5001c79bba", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3e2c50f-7e64-455a-83e3-9a4d49b57115", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "6c700351b4b8bb9455e48c6c2b5a33ad93f8b59258aa14fc5740553d746dfde4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5c32338-016e-4986-9a9e-17a9c04475ea", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "7f7145f0105ae91c03ac6f6e1b4d74f57fa770026305aa0cc8ca26721fe2357b", "class_name": "RelatedNodeInfo"}}, "hash": "6a12eefcac94b05293ef5b49c650671088a763be7cd8b6ba8ead0a1088dfd83f", "text": "Accessed 29 June 2022.\n\n  7. Cornell_Movie_Corpus (2022) Cornell Movie Corpus archive. [https://\u200bwww.\u200bkaggle.\u200bcom/\u200bdatasets/\u200bCornell-University/\u200bmovie-dialog-corpus](https://www.kaggle.com/datasets/Cornell-University/movie-dialog-corpus). Accessed 29 June 2022.\n\n  8. Devlin, J., Chang, M. W., Lee, K. and Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Archive: [https://\u200barxiv.\u200borg/\u200bpdf/\u200b1810.\u200b04805.\u200bpdf](https://arxiv.org/pdf/1810.04805.pdf).\n\n  9. Duer (2022) Duer Baidu AI Chatbot. [http://\u200bduer.\u200bbaidu.\u200bcom/\u200ben/\u200bindex.\u200bhtml](http://duer.baidu.com/en/index.html). Accessed 29 June 2022.\n\n  10. DSTC (2022) Dialog State Tracking Challenge dataset: [https://\u200bgithub.\u200bcom/\u200bmatthen/\u200bdstc](https://github.com/matthen/dstc). Accessed 29 June 2022.\n\n  11. Ekman, M. (2021) Learning Deep Learning: Theory and Practice of Neural Networks, Computer Vision, Natural Language Processing, and Transformers Using TensorFlow. Addison-Wesley Professional.\n\n  12. Facebook (2022) Facebook Messenger API documentation. [https://\u200bdevelopers.\u200bfacebook.\u200bcom/\u200bdocs/\u200bmessenger-platform/\u200bgetting-started/\u200bquick-start/\u200b](https://developers.facebook.com/docs/messenger-platform/getting-started/quick-start/). Accessed 29 June 2022.\n\n  13. Freed, A. (2021) Conversational AI: Chatbots that work. Manning.\n\n  14. G\u00e9ron, A. (2019) Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O\u2019Reilly Media.\n\n  15. GoogleAssistant (2022) Google Assistant: [https://\u200bassistant.\u200bgoogle.\u200bcom/\u200b](https://assistant.google.com/). Accessed 29 June 2022.\n\n  16. GoogleResearch (2022a) Taskmaster from Google Research. [https://\u200bgithub.\u200bcom/\u200bgoogle-research-datasets/\u200bTaskmaster/\u200btree/\u200bmaster/\u200bTM-1-2019](https://github.com/google-research-datasets/Taskmaster/tree/master/TM-1-2019). Accessed 29 June 2022.\n\n  17. GoogleResearch (2022b) Simulated Dialogue dataset from Google Research. [https://\u200bgithub.\u200bcom/\u200bgoogle-research-datasets/\u200bsimulated-dialogue](https://github.com/google-research-datasets/simulated-dialogue). Accessed 29 June 2022.\n\n  18. Janarthanam, S. (2017) Hands-On Chatbots and Conversational UI Development: Build chatbots and voice user interfaces with Chatfuel, Dialogflow, Microsoft Bot Framework, Twilio, and Alexa Skills. Packt Publishing.\n\n  19. Kedia, A. and Rasu, M. (2020) Hands-On Python Natural Language Processing: Explore tools and techniques to analyze and process text with a view to building real-world NLP applications. Packt Publishing.\n\n  20. Keras (2022a) Keras official sites: [https://\u200bkeras.\u200bio](https://keras.io). Accessed 29 June 2022.\n\n  21. Keras (2022b) Keras optimizers: [https://\u200bkeras.\u200bio/\u200bapi/\u200boptimizers/\u200b](https://keras.io/api/optimizers/). Accessed 29 June 2022.\n\n  22. MicrosoftCortana (2022) Microsoft Cortana Developer: [https://\u200bwww.\u200bmicrosoft.\u200bcom/\u200ben-us/\u200bcortana/\u200b](https://www.microsoft.com/en-us/cortana/). Accessed 29 June 2022.\n\n  23. MicrosoftDialog (2022) [https://\u200bgithub.\u200bcom/\u200bxiul-msr/\u200be2e_\u200bdialog_\u200bchallenge](https://github.com/xiul-msr/e2e_dialog_challenge).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a5c32338-016e-4986-9a9e-17a9c04475ea": {"__data__": {"id_": "a5c32338-016e-4986-9a9e-17a9c04475ea", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbe4f542-7fd0-42fb-9be4-cb5001c79bba", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "6a12eefcac94b05293ef5b49c650671088a763be7cd8b6ba8ead0a1088dfd83f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd6b3eaf-6a94-4ccc-8a83-6d4c32083257", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "18a4d7f587e6157e81ce5d0142d37a58eae12c412859ed61c13777466a4b797a", "class_name": "RelatedNodeInfo"}}, "hash": "7f7145f0105ae91c03ac6f6e1b4d74f57fa770026305aa0cc8ca26721fe2357b", "text": "Accessed 29 June 2022.\n\n  21. Keras (2022b) Keras optimizers: [https://\u200bkeras.\u200bio/\u200bapi/\u200boptimizers/\u200b](https://keras.io/api/optimizers/). Accessed 29 June 2022.\n\n  22. MicrosoftCortana (2022) Microsoft Cortana Developer: [https://\u200bwww.\u200bmicrosoft.\u200bcom/\u200ben-us/\u200bcortana/\u200b](https://www.microsoft.com/en-us/cortana/). Accessed 29 June 2022.\n\n  23. MicrosoftDialog (2022) [https://\u200bgithub.\u200bcom/\u200bxiul-msr/\u200be2e_\u200bdialog_\u200bchallenge](https://github.com/xiul-msr/e2e_dialog_challenge). Accessed 29 June 2022.\n\n  24. NLPWorkshop7 (2022) NLP Workshop 6 GitHub archive. [https://\u200bgithub.\u200bcom/\u200braymondshtlee/\u200bNLP/\u200btree/\u200bmain/\u200bNLPWorkshop7](https://github.com/raymondshtlee/NLP/tree/main/NLPWorkshop7). Accessed 29 June 2022.\n\n  25. Raj, S. (2018) Building Chatbots with Python: Using Natural Language Processing and Machine Learning. Apress.\n\n  26. Rothman, D. (2022) Transformers for Natural Language Processing: Build, train, and fine-tune deep neural network architectures for NLP with Python, PyTorch, TensorFlow, BERT, and GPT-3. Packt Publishing.\n\n  27. Ruder (2022) An overview of optimizers: [https://\u200bruder.\u200bio/\u200boptimizing-gradient-descent/\u200b](https://ruder.io/optimizing-gradient-descent/). Accessed 29 June 2022.\n\n  28. SamsungBixby (2022) Samsung Bixby Developer: [https://\u200bdeveloper.\u200bsamsung.\u200bcom/\u200bbixby](https://developer.samsung.com/bixby). Accessed 29 June 2022.\n\n  29. Telegram (2022) Telegram bot API documentation: ([https://\u200bcore.\u200btelegram.\u200borg/\u200bbots](https://core.telegram.org/bots). Accessed 29 June 2022.\n\n  30. TensorFlow (2022) TensorFlow official site> [https://\u200btensorflow.\u200borg](https://tensorflow.org) /. Accessed 29 June 2022.\n\n  31. Tuchong (2022) Wake word to invoke your Chatbot. [https://\u200bstock.\u200btuchong.\u200bcom/\u200bimage/\u200bdetail?\u200bimageId=\u200b9184951802606387\u200b96](https://stock.tuchong.com/image/detail?imageId=918495180260638796). Accessed 29 June 2022.\n\n  32. Tunstall, L, Werra, L. and Wolf, T. (2022) Natural Language Processing with Transformers: Building Language Applications with Hugging Face. O\u2019Reilly Media.\n\n  33. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30. [https://\u200barxiv.\u200borg/\u200babs/\u200b1706.\u200b03762](https://arxiv.org/abs/1706.03762).\n\n  34. Xiaowei (2022) Xiaowei chatbot system from Tencent. [https://\u200bxiaowei.\u200btencent.\u200bcom/\u200b](https://xiaowei.tencent.com/). Accessed 29 June 2022.\n\n  35. Y\u0131ld\u0131r\u0131m, S, Asgari-Chenaghlu, M. (2021) Mastering Transformers: Build state-of-the-art models from scratch with advanced natural language processing techniques. Packt Publishing.\n\n\nIndex\n\nA\n\nAbstractive Text Summary (ATS)[212](533412_1_En_9_Chapter.xhtml#PB212)\n\nAdd- _k_ smoothing\n\nAdjectives[43](533412_1_En_3_Chapter.xhtml#PB43),\n[44](533412_1_En_3_Chapter.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd6b3eaf-6a94-4ccc-8a83-6d4c32083257": {"__data__": {"id_": "dd6b3eaf-6a94-4ccc-8a83-6d4c32083257", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5c32338-016e-4986-9a9e-17a9c04475ea", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "7f7145f0105ae91c03ac6f6e1b4d74f57fa770026305aa0cc8ca26721fe2357b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5bcdf5e3-0748-48d9-85bb-1b75a76ab3fa", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "69451738f07e642db82effa7098dc6c64278f837a8681f9c81b5753d8b0ca1da", "class_name": "RelatedNodeInfo"}}, "hash": "18a4d7f587e6157e81ce5d0142d37a58eae12c412859ed61c13777466a4b797a", "text": "34. Xiaowei (2022) Xiaowei chatbot system from Tencent. [https://\u200bxiaowei.\u200btencent.\u200bcom/\u200b](https://xiaowei.tencent.com/). Accessed 29 June 2022.\n\n  35. Y\u0131ld\u0131r\u0131m, S, Asgari-Chenaghlu, M. (2021) Mastering Transformers: Build state-of-the-art models from scratch with advanced natural language processing techniques. Packt Publishing.\n\n\nIndex\n\nA\n\nAbstractive Text Summary (ATS)[212](533412_1_En_9_Chapter.xhtml#PB212)\n\nAdd- _k_ smoothing\n\nAdjectives[43](533412_1_En_3_Chapter.xhtml#PB43),\n[44](533412_1_En_3_Chapter.xhtml#PB44)\n\nAdverbs[43](533412_1_En_3_Chapter.xhtml#PB43),\n[44](533412_1_En_3_Chapter.xhtml#PB44), [51](533412_1_En_3_Chapter.xhtml#PB51)\n\nAI Tutor[3](533412_1_En_1_Chapter.xhtml#PB3),\n[4](533412_1_En_1_Chapter.xhtml#PB4), [15](533412_1_En_1_Chapter.xhtml#PB15)\n\nAlexa[402](533412_1_En_16_Chapter.xhtml#PB402),\n[403](533412_1_En_16_Chapter.xhtml#PB403), 431\n\nAliMe QA\nSystem[231](533412_1_En_9_Chapter.xhtml#PB231)\u2013[232](533412_1_En_9_Chapter.xhtml#PB232)\n\nA Lite BERT (ALBERT)[195](533412_1_En_8_Chapter.xhtml#PB195)\n\nAllGenie[402](533412_1_En_16_Chapter.xhtml#PB402)\n\nAnaphora[151](533412_1_En_7_Chapter.xhtml#PB151),\n[155](533412_1_En_7_Chapter.xhtml#PB155),\n[158](533412_1_En_7_Chapter.xhtml#PB158),\n[169](533412_1_En_7_Chapter.xhtml#PB169)\n\nAntecedent[151](533412_1_En_7_Chapter.xhtml#PB151),\n[164](533412_1_En_7_Chapter.xhtml#PB164)\u2013[166](533412_1_En_7_Chapter.xhtml#PB166),\n[169](533412_1_En_7_Chapter.xhtml#PB169),\n[170](533412_1_En_7_Chapter.xhtml#PB170)\n\nAntonyms[121](533412_1_En_6_Chapter.xhtml#PB121)\u2013[122](533412_1_En_6_Chapter.xhtml#PB122)\n\nArtificial Intelligence Markup (AIML)[226](533412_1_En_9_Chapter.xhtml#PB226)\n\nArtificial neural networks (ANNs)[180](533412_1_En_8_Chapter.xhtml#PB180),\n[335](533412_1_En_14_Chapter.xhtml#PB335), 357\n\nAttention equation410\n\nAugmented translation network (ATN)[9](533412_1_En_1_Chapter.xhtml#PB9)\n\nB\n\nBackoff and Interpolation (B&I)[39](533412_1_En_2_Chapter.xhtml#PB39)\n\nBASEBALL system[8](533412_1_En_1_Chapter.xhtml#PB8)\n\nBidirectional[373](533412_1_En_15_Chapter.xhtml#PB373),\n[377](533412_1_En_15_Chapter.xhtml#PB377)\n\nBidirectional Encoder Representation from Transformer\n(BERT)[192](533412_1_En_8_Chapter.xhtml#PB192)\u2013[196](533412_1_En_8_Chapter.xhtml#PB196),\n[201](533412_1_En_9_Chapter.xhtml#PB201),\n[204](533412_1_En_9_Chapter.xhtml#PB204),", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5bcdf5e3-0748-48d9-85bb-1b75a76ab3fa": {"__data__": {"id_": "5bcdf5e3-0748-48d9-85bb-1b75a76ab3fa", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd6b3eaf-6a94-4ccc-8a83-6d4c32083257", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "18a4d7f587e6157e81ce5d0142d37a58eae12c412859ed61c13777466a4b797a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d692038f-f6ca-42c0-bac5-79bad48a0640", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "bbe6784874019d652b5f759a246918f5277c6c66d12b2a16adc57148b9f4cf36", "class_name": "RelatedNodeInfo"}}, "hash": "69451738f07e642db82effa7098dc6c64278f837a8681f9c81b5753d8b0ca1da", "text": "xhtml#PB9)\n\nB\n\nBackoff and Interpolation (B&I)[39](533412_1_En_2_Chapter.xhtml#PB39)\n\nBASEBALL system[8](533412_1_En_1_Chapter.xhtml#PB8)\n\nBidirectional[373](533412_1_En_15_Chapter.xhtml#PB373),\n[377](533412_1_En_15_Chapter.xhtml#PB377)\n\nBidirectional Encoder Representation from Transformer\n(BERT)[192](533412_1_En_8_Chapter.xhtml#PB192)\u2013[196](533412_1_En_8_Chapter.xhtml#PB196),\n[201](533412_1_En_9_Chapter.xhtml#PB201),\n[204](533412_1_En_9_Chapter.xhtml#PB204),\n[206](533412_1_En_9_Chapter.xhtml#PB206),\n[227](533412_1_En_9_Chapter.xhtml#PB227), 228,\n[243](533412_1_En_10_Chapter.xhtml#PB243),\n[373](533412_1_En_15_Chapter.xhtml#PB373),\n[374](533412_1_En_15_Chapter.xhtml#PB374),\n[377](533412_1_En_15_Chapter.xhtml#PB377)\u2013[392](533412_1_En_15_Chapter.xhtml#PB392),\n397\n\nBidirectional Recurrent Neural Network\n(BRNN)[186](533412_1_En_8_Chapter.xhtml#PB186)\u2013[188](533412_1_En_8_Chapter.xhtml#PB188)\n\nBigrams[21](533412_1_En_2_Chapter.xhtml#PB21),\n[27](533412_1_En_2_Chapter.xhtml#PB27)\u2013[32](533412_1_En_2_Chapter.xhtml#PB32),\n[37](533412_1_En_2_Chapter.xhtml#PB37),\n[38](533412_1_En_2_Chapter.xhtml#PB38), 269, 271\u2013273, 275\n\nBixby[402](533412_1_En_16_Chapter.xhtml#PB402), 431\n\nBottom-up\nparser[82](533412_1_En_4_Chapter.xhtml#PB82)\u2013[84](533412_1_En_4_Chapter.xhtml#PB84)\n\nBrill, E.[61](533412_1_En_3_Chapter.xhtml#PB61)\n\nBrill\nTaggers[61](533412_1_En_3_Chapter.xhtml#PB61)\u2013[63](533412_1_En_3_Chapter.xhtml#PB63)\n\nBrown Corpus[22](533412_1_En_2_Chapter.xhtml#PB22),\n[24](533412_1_En_2_Chapter.xhtml#PB24), [56](533412_1_En_3_Chapter.xhtml#PB56)\n\nC\n\nCanonical form[101](533412_1_En_5_Chapter.xhtml#PB101),\n[102](533412_1_En_5_Chapter.xhtml#PB102),\n[112](533412_1_En_5_Chapter.xhtml#PB112)\n\nCarroll, L.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d692038f-f6ca-42c0-bac5-79bad48a0640": {"__data__": {"id_": "d692038f-f6ca-42c0-bac5-79bad48a0640", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5bcdf5e3-0748-48d9-85bb-1b75a76ab3fa", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "69451738f07e642db82effa7098dc6c64278f837a8681f9c81b5753d8b0ca1da", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eaafd9d7-add8-438d-8f95-8311dd98b3b6", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "d8d7fa570bc01012a920476b1a6337d599db000d1015a166f684a8cbdaa491d4", "class_name": "RelatedNodeInfo"}}, "hash": "bbe6784874019d652b5f759a246918f5277c6c66d12b2a16adc57148b9f4cf36", "text": "xhtml#PB84)\n\nBrill, E.[61](533412_1_En_3_Chapter.xhtml#PB61)\n\nBrill\nTaggers[61](533412_1_En_3_Chapter.xhtml#PB61)\u2013[63](533412_1_En_3_Chapter.xhtml#PB63)\n\nBrown Corpus[22](533412_1_En_2_Chapter.xhtml#PB22),\n[24](533412_1_En_2_Chapter.xhtml#PB24), [56](533412_1_En_3_Chapter.xhtml#PB56)\n\nC\n\nCanonical form[101](533412_1_En_5_Chapter.xhtml#PB101),\n[102](533412_1_En_5_Chapter.xhtml#PB102),\n[112](533412_1_En_5_Chapter.xhtml#PB112)\n\nCarroll, L.[247](533412_1_En_10_Chapter.xhtml#PB247)\n\nCase roles analysis[106](533412_1_En_5_Chapter.xhtml#PB106)\n\nCataphor[151](533412_1_En_7_Chapter.xhtml#PB151)\n\nCELEX[47](533412_1_En_3_Chapter.xhtml#PB47),\n[51](533412_1_En_3_Chapter.xhtml#PB51)\u2013[56](533412_1_En_3_Chapter.xhtml#PB56)\n\nCentering\nalgorithm[166](533412_1_En_7_Chapter.xhtml#PB166)\u2013[169](533412_1_En_7_Chapter.xhtml#PB169)\n\nCentering theory\n(CT)[166](533412_1_En_7_Chapter.xhtml#PB166)\u2013[169](533412_1_En_7_Chapter.xhtml#PB169)\n\nCFG parsing[67](533412_1_En_4_Chapter.xhtml#PB67),\n[79](533412_1_En_4_Chapter.xhtml#PB79), [93](533412_1_En_4_Chapter.xhtml#PB93)\n\nChain\nrule[24](533412_1_En_2_Chapter.xhtml#PB24)\u2013[25](533412_1_En_2_Chapter.xhtml#PB25),\n[40](533412_1_En_2_Chapter.xhtml#PB40)\n\nChatbots[3](533412_1_En_1_Chapter.xhtml#PB3),\n[4](533412_1_En_1_Chapter.xhtml#PB4), [9](533412_1_En_1_Chapter.xhtml#PB9),\n[10](533412_1_En_1_Chapter.xhtml#PB10),\n[14](533412_1_En_1_Chapter.xhtml#PB14),\n[401](533412_1_En_16_Chapter.xhtml#PB401),\n[402](533412_1_En_16_Chapter.xhtml#PB402),\n[404](533412_1_En_16_Chapter.xhtml#PB404), 410, 430\n\nChomsky, N.[8](533412_1_En_1_Chapter.xhtml#PB8)\n\nCOBUILD[51](533412_1_En_3_Chapter.xhtml#PB51)\n\nCoherence[151](533412_1_En_7_Chapter.xhtml#PB151)\u2013[154](533412_1_En_7_Chapter.xhtml#PB154),\n[159](533412_1_En_7_Chapter.xhtml#PB159)\u2013[162](533412_1_En_7_Chapter.xhtml#PB162),\n[167](533412_1_En_7_Chapter.xhtml#PB167),\n[171](533412_1_En_7_Chapter.xhtml#PB171)\n\nCoherence relation[159](533412_1_En_7_Chapter.xhtml#PB159)\n\nCohesion[154](533412_1_En_7_Chapter.xhtml#PB154)\n\nCollocation263\u2013264\n\nCommon sense[116](533412_1_En_6_Chapter.xhtml#PB116),\n[124](533412_1_En_6_Chapter.xhtml#PB124)\n\nworld knowledge[21](533412_1_En_2_Chapter.xhtml#PB21),\n[29](533412_1_En_2_Chapter.xhtml#PB29)\n\nCompositional semantic\nanalysis[117](533412_1_En_6_Chapter.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eaafd9d7-add8-438d-8f95-8311dd98b3b6": {"__data__": {"id_": "eaafd9d7-add8-438d-8f95-8311dd98b3b6", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d692038f-f6ca-42c0-bac5-79bad48a0640", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "bbe6784874019d652b5f759a246918f5277c6c66d12b2a16adc57148b9f4cf36", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "459179b4-bab5-454d-8a21-c47c762503ec", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "c717054e4eae91169be4fe519388d009076f92390d493a9b6cb0e17bd26330fb", "class_name": "RelatedNodeInfo"}}, "hash": "d8d7fa570bc01012a920476b1a6337d599db000d1015a166f684a8cbdaa491d4", "text": "[167](533412_1_En_7_Chapter.xhtml#PB167),\n[171](533412_1_En_7_Chapter.xhtml#PB171)\n\nCoherence relation[159](533412_1_En_7_Chapter.xhtml#PB159)\n\nCohesion[154](533412_1_En_7_Chapter.xhtml#PB154)\n\nCollocation263\u2013264\n\nCommon sense[116](533412_1_En_6_Chapter.xhtml#PB116),\n[124](533412_1_En_6_Chapter.xhtml#PB124)\n\nworld knowledge[21](533412_1_En_2_Chapter.xhtml#PB21),\n[29](533412_1_En_2_Chapter.xhtml#PB29)\n\nCompositional semantic\nanalysis[117](533412_1_En_6_Chapter.xhtml#PB117)\u2013[118](533412_1_En_6_Chapter.xhtml#PB118),\n[145](533412_1_En_6_Chapter.xhtml#PB145)\n\nCompositional semantics[118](533412_1_En_6_Chapter.xhtml#PB118)\n\nComputational linguistics (CL)[5](533412_1_En_1_Chapter.xhtml#PB5),\n[10](533412_1_En_1_Chapter.xhtml#PB10), [48](533412_1_En_3_Chapter.xhtml#PB48)\n\nComputational linguists[49](533412_1_En_3_Chapter.xhtml#PB49)\n\nComputer-to-computer interaction (CCI)[149](533412_1_En_7_Chapter.xhtml#PB149)\n\nConceptual dependency (CD)[99](533412_1_En_5_Chapter.xhtml#PB99)\n\nConceptual dependency diagram (CDD)[98](533412_1_En_5_Chapter.xhtml#PB98),\n[99](533412_1_En_5_Chapter.xhtml#PB99)\n\nConjunctions[43](533412_1_En_3_Chapter.xhtml#PB43),\n[51](533412_1_En_3_Chapter.xhtml#PB51)\u2013[54](533412_1_En_3_Chapter.xhtml#PB54)\n\nConnectives[108](533412_1_En_5_Chapter.xhtml#PB108)\n\nConstituent[70](533412_1_En_4_Chapter.xhtml#PB70)\u2013[74](533412_1_En_4_Chapter.xhtml#PB74),\n[76](533412_1_En_4_Chapter.xhtml#PB76),\n[77](533412_1_En_4_Chapter.xhtml#PB77),\n[85](533412_1_En_4_Chapter.xhtml#PB85),\n[88](533412_1_En_4_Chapter.xhtml#PB88), [91](533412_1_En_4_Chapter.xhtml#PB91)\n\nConstituent Likelihood Automatic Word-tagging System\n(CLAWS)[56](533412_1_En_3_Chapter.xhtml#PB56)\n\nContext-free grammar (CFG)[67](533412_1_En_4_Chapter.xhtml#PB67),\n[76](533412_1_En_4_Chapter.xhtml#PB76),\n[77](533412_1_En_4_Chapter.xhtml#PB77),\n[87](533412_1_En_4_Chapter.xhtml#PB87), [93](533412_1_En_4_Chapter.xhtml#PB93)\n\nContext-free language\n(CFL)[76](533412_1_En_4_Chapter.xhtml#PB76)\u2013[77](533412_1_En_4_Chapter.xhtml#PB77)\n\nConversational artificial\nintelligence[402](533412_1_En_16_Chapter.xhtml#PB402)\n\nConvolutional neural networks (CNNs)[170](533412_1_En_7_Chapter.xhtml#PB170)\n\nCoordinating conjunctions[52](533412_1_En_3_Chapter.xhtml#PB52)\n\nCoreference[150](533412_1_En_7_Chapter.xhtml#PB150)\u2013[153](533412_1_En_7_Chapter.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "459179b4-bab5-454d-8a21-c47c762503ec": {"__data__": {"id_": "459179b4-bab5-454d-8a21-c47c762503ec", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eaafd9d7-add8-438d-8f95-8311dd98b3b6", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "d8d7fa570bc01012a920476b1a6337d599db000d1015a166f684a8cbdaa491d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68197fa3-990a-4c8c-82a2-3c2d3628c1dd", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8693a567921841eb973c98293c78ed78224eb88ad047d9851acbd17f493aa88a", "class_name": "RelatedNodeInfo"}}, "hash": "c717054e4eae91169be4fe519388d009076f92390d493a9b6cb0e17bd26330fb", "text": "xhtml#PB87), [93](533412_1_En_4_Chapter.xhtml#PB93)\n\nContext-free language\n(CFL)[76](533412_1_En_4_Chapter.xhtml#PB76)\u2013[77](533412_1_En_4_Chapter.xhtml#PB77)\n\nConversational artificial\nintelligence[402](533412_1_En_16_Chapter.xhtml#PB402)\n\nConvolutional neural networks (CNNs)[170](533412_1_En_7_Chapter.xhtml#PB170)\n\nCoordinating conjunctions[52](533412_1_En_3_Chapter.xhtml#PB52)\n\nCoreference[150](533412_1_En_7_Chapter.xhtml#PB150)\u2013[153](533412_1_En_7_Chapter.xhtml#PB153),\n[159](533412_1_En_7_Chapter.xhtml#PB159),\n[163](533412_1_En_7_Chapter.xhtml#PB163)\u2013[170](533412_1_En_7_Chapter.xhtml#PB170)\n\nCoreference resolution[150](533412_1_En_7_Chapter.xhtml#PB150),\n[151](533412_1_En_7_Chapter.xhtml#PB151),\n[163](533412_1_En_7_Chapter.xhtml#PB163)\n\nCornell Large Movie Conversation\nDataset[401](533412_1_En_16_Chapter.xhtml#PB401)\n\nCornell Movie Corpus archive[404](533412_1_En_16_Chapter.xhtml#PB404),\n[405](533412_1_En_16_Chapter.xhtml#PB405)\n\nCorpus[19](533412_1_En_2_Chapter.xhtml#PB19),\n[21](533412_1_En_2_Chapter.xhtml#PB21)\u2013[25](533412_1_En_2_Chapter.xhtml#PB25),\n[27](533412_1_En_2_Chapter.xhtml#PB27),\n[29](533412_1_En_2_Chapter.xhtml#PB29)\u2013[31](533412_1_En_2_Chapter.xhtml#PB31),\n[33](533412_1_En_2_Chapter.xhtml#PB33),\n[36](533412_1_En_2_Chapter.xhtml#PB36),\n[38](533412_1_En_2_Chapter.xhtml#PB38)\u2013[41](533412_1_En_2_Chapter.xhtml#PB41),\n[44](533412_1_En_3_Chapter.xhtml#PB44),\n[45](533412_1_En_3_Chapter.xhtml#PB45),\n[51](533412_1_En_3_Chapter.xhtml#PB51),\n[56](533412_1_En_3_Chapter.xhtml#PB56),\n[57](533412_1_En_3_Chapter.xhtml#PB57),\n[59](533412_1_En_3_Chapter.xhtml#PB59),\n[60](533412_1_En_3_Chapter.xhtml#PB60),\n[63](533412_1_En_3_Chapter.xhtml#PB63),\n[244](533412_1_En_10_Chapter.xhtml#PB244), 251, 252, 254\u2013255, 259, 262, 264\n\nCortana[402](533412_1_En_16_Chapter.xhtml#PB402), 431\n\nCosine similarity327\u2013328\n\nD\n\nDecoder[188](533412_1_En_8_Chapter.xhtml#PB188)\n\nDeep learning[10](533412_1_En_1_Chapter.xhtml#PB10)\n\nDeepQA[9](533412_1_En_1_Chapter.xhtml#PB9)\n\nDekang Lin method[135](533412_1_En_6_Chapter.xhtml#PB135)\n\nDeterminers[43](533412_1_En_3_Chapter.xhtml#PB43),\n[46](533412_1_En_3_Chapter.xhtml#PB46), [51](533412_1_En_3_Chapter.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68197fa3-990a-4c8c-82a2-3c2d3628c1dd": {"__data__": {"id_": "68197fa3-990a-4c8c-82a2-3c2d3628c1dd", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "459179b4-bab5-454d-8a21-c47c762503ec", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "c717054e4eae91169be4fe519388d009076f92390d493a9b6cb0e17bd26330fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85ba0f77-435d-4b24-b3de-c2b44e76e957", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f6387cbafca83b97cfcfaea80d11a26cf5da7895ab6a01108683ff13460a44f8", "class_name": "RelatedNodeInfo"}}, "hash": "8693a567921841eb973c98293c78ed78224eb88ad047d9851acbd17f493aa88a", "text": "252, 254\u2013255, 259, 262, 264\n\nCortana[402](533412_1_En_16_Chapter.xhtml#PB402), 431\n\nCosine similarity327\u2013328\n\nD\n\nDecoder[188](533412_1_En_8_Chapter.xhtml#PB188)\n\nDeep learning[10](533412_1_En_1_Chapter.xhtml#PB10)\n\nDeepQA[9](533412_1_En_1_Chapter.xhtml#PB9)\n\nDekang Lin method[135](533412_1_En_6_Chapter.xhtml#PB135)\n\nDeterminers[43](533412_1_En_3_Chapter.xhtml#PB43),\n[46](533412_1_En_3_Chapter.xhtml#PB46), [51](533412_1_En_3_Chapter.xhtml#PB51)\n\nDiscourse[48](533412_1_En_3_Chapter.xhtml#PB48),\n[95](533412_1_En_5_Chapter.xhtml#PB95),\n[97](533412_1_En_5_Chapter.xhtml#PB97),\n[123](533412_1_En_6_Chapter.xhtml#PB123),\n[124](533412_1_En_6_Chapter.xhtml#PB124),\n[149](533412_1_En_7_Chapter.xhtml#PB149),\n[151](533412_1_En_7_Chapter.xhtml#PB151),\n[154](533412_1_En_7_Chapter.xhtml#PB154),\n[155](533412_1_En_7_Chapter.xhtml#PB155),\n[158](533412_1_En_7_Chapter.xhtml#PB158)\u2013[162](533412_1_En_7_Chapter.xhtml#PB162),\n[164](533412_1_En_7_Chapter.xhtml#PB164),\n[166](533412_1_En_7_Chapter.xhtml#PB166),\n[167](533412_1_En_7_Chapter.xhtml#PB167),\n[171](533412_1_En_7_Chapter.xhtml#PB171)\n\nDiscourse markers[211](533412_1_En_9_Chapter.xhtml#PB211)\n\nDiscourse segmentation[154](533412_1_En_7_Chapter.xhtml#PB154),\n[208](533412_1_En_9_Chapter.xhtml#PB208)\u2013[212](533412_1_En_9_Chapter.xhtml#PB212),\n[235](533412_1_En_9_Chapter.xhtml#PB235)\n\nDispersion253\u2013255\n\nDistributional models[137](533412_1_En_6_Chapter.xhtml#PB137)\n\nDocument-likelihood[207](533412_1_En_9_Chapter.xhtml#PB207),\n[208](533412_1_En_9_Chapter.xhtml#PB208)\n\nDuer[402](533412_1_En_16_Chapter.xhtml#PB402), 431\n\nE\n\nEmbedding words363\u2013364\n\nEncoder[188](533412_1_En_8_Chapter.xhtml#PB188)\u2013[192](533412_1_En_8_Chapter.xhtml#PB192),\n[373](533412_1_En_15_Chapter.xhtml#PB373),\n[375](533412_1_En_15_Chapter.xhtml#PB375)\u2013[377](533412_1_En_15_Chapter.xhtml#PB377)\n\nExtractive Text Summary (ETS)[212](533412_1_En_9_Chapter.xhtml#PB212)\n\nExtrinsic evaluation[35](533412_1_En_2_Chapter.xhtml#PB35)\n\nF\n\nFastText[320](533412_1_En_13_Chapter.xhtml#PB320)\n\nFeature-based model[219](533412_1_En_9_Chapter.xhtml#PB219)\n\nFeedforward neural network (FNN)[180](533412_1_En_8_Chapter.xhtml#PB180)\n\n_Fillmore\u2019s Case Roles Theory_\n\nFillmore\u2019s\ntheory[103](533412_1_En_5_Chapter.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85ba0f77-435d-4b24-b3de-c2b44e76e957": {"__data__": {"id_": "85ba0f77-435d-4b24-b3de-c2b44e76e957", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68197fa3-990a-4c8c-82a2-3c2d3628c1dd", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8693a567921841eb973c98293c78ed78224eb88ad047d9851acbd17f493aa88a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9f9093f-ee62-4bb0-8eec-5d493daf9c56", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "99e4240f04fbbdc7376d7d396fe1d099ef6c973419e92eaabde9f8e94476cf8c", "class_name": "RelatedNodeInfo"}}, "hash": "f6387cbafca83b97cfcfaea80d11a26cf5da7895ab6a01108683ff13460a44f8", "text": "xhtml#PB373),\n[375](533412_1_En_15_Chapter.xhtml#PB375)\u2013[377](533412_1_En_15_Chapter.xhtml#PB377)\n\nExtractive Text Summary (ETS)[212](533412_1_En_9_Chapter.xhtml#PB212)\n\nExtrinsic evaluation[35](533412_1_En_2_Chapter.xhtml#PB35)\n\nF\n\nFastText[320](533412_1_En_13_Chapter.xhtml#PB320)\n\nFeature-based model[219](533412_1_En_9_Chapter.xhtml#PB219)\n\nFeedforward neural network (FNN)[180](533412_1_En_8_Chapter.xhtml#PB180)\n\n_Fillmore\u2019s Case Roles Theory_\n\nFillmore\u2019s\ntheory[103](533412_1_En_5_Chapter.xhtml#PB103)\u2013[107](533412_1_En_5_Chapter.xhtml#PB107),\n[112](533412_1_En_5_Chapter.xhtml#PB112)\n\nFirst-order predicate calculus (FOPC)[98](533412_1_En_5_Chapter.xhtml#PB98),\n[100](533412_1_En_5_Chapter.xhtml#PB100),\n[102](533412_1_En_5_Chapter.xhtml#PB102),\n[103](533412_1_En_5_Chapter.xhtml#PB103),\n[107](533412_1_En_5_Chapter.xhtml#PB107)\u2013[113](533412_1_En_5_Chapter.xhtml#PB113)\n\nFrame-based\nrepresentation[98](533412_1_En_5_Chapter.xhtml#PB98)\u2013[100](533412_1_En_5_Chapter.xhtml#PB100)\n\nG\n\nGate recurrent unit\n(GRU)[185](533412_1_En_8_Chapter.xhtml#PB185)\u2013[187](533412_1_En_8_Chapter.xhtml#PB187),\n[196](533412_1_En_8_Chapter.xhtml#PB196)\n\nGeneric summarization systems[217](533412_1_En_9_Chapter.xhtml#PB217)\n\nGlove vectors[320](533412_1_En_13_Chapter.xhtml#PB320)\n\nGood Turing (GT) Smoothing[36](533412_1_En_2_Chapter.xhtml#PB36),\n[40](533412_1_En_2_Chapter.xhtml#PB40)\n\nGPT[226](533412_1_En_9_Chapter.xhtml#PB226)\u2013228,\n[235](533412_1_En_9_Chapter.xhtml#PB235)\n\nGrammar-based\nmethod[221](533412_1_En_9_Chapter.xhtml#PB221)\u2013[223](533412_1_En_9_Chapter.xhtml#PB223)\n\nH\n\nHapaxes262, 263\n\nHearst\u2019s\nTextTiling[155](533412_1_En_7_Chapter.xhtml#PB155)\u2013[157](533412_1_En_7_Chapter.xhtml#PB157),\n[171](533412_1_En_7_Chapter.xhtml#PB171)\n\nHeterogeneous transfer learning[177](533412_1_En_8_Chapter.xhtml#PB177)\n\nHMM Tagger[60](533412_1_En_3_Chapter.xhtml#PB60)\n\nHobbs\nalgorithm[163](533412_1_En_7_Chapter.xhtml#PB163)\u2013[166](533412_1_En_7_Chapter.xhtml#PB166)\n\nHobbs, J.R.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9f9093f-ee62-4bb0-8eec-5d493daf9c56": {"__data__": {"id_": "b9f9093f-ee62-4bb0-8eec-5d493daf9c56", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85ba0f77-435d-4b24-b3de-c2b44e76e957", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "f6387cbafca83b97cfcfaea80d11a26cf5da7895ab6a01108683ff13460a44f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f789b1f-d64c-40ae-ab06-de445014722d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "c2b78565431e0e209ad71bc5e2cd661030c8d5cafa18f8e8afea61e52933249e", "class_name": "RelatedNodeInfo"}}, "hash": "99e4240f04fbbdc7376d7d396fe1d099ef6c973419e92eaabde9f8e94476cf8c", "text": "xhtml#PB223)\n\nH\n\nHapaxes262, 263\n\nHearst\u2019s\nTextTiling[155](533412_1_En_7_Chapter.xhtml#PB155)\u2013[157](533412_1_En_7_Chapter.xhtml#PB157),\n[171](533412_1_En_7_Chapter.xhtml#PB171)\n\nHeterogeneous transfer learning[177](533412_1_En_8_Chapter.xhtml#PB177)\n\nHMM Tagger[60](533412_1_En_3_Chapter.xhtml#PB60)\n\nHobbs\nalgorithm[163](533412_1_En_7_Chapter.xhtml#PB163)\u2013[166](533412_1_En_7_Chapter.xhtml#PB166)\n\nHobbs, J.R.[159](533412_1_En_7_Chapter.xhtml#PB159),\n[163](533412_1_En_7_Chapter.xhtml#PB163)\n\nHomogeneous transfer learning[177](533412_1_En_8_Chapter.xhtml#PB177)\n\nHomographs[119](533412_1_En_6_Chapter.xhtml#PB119),\n[124](533412_1_En_6_Chapter.xhtml#PB124)\n\nHomonymy[119](533412_1_En_6_Chapter.xhtml#PB119)\u2013[120](533412_1_En_6_Chapter.xhtml#PB120)\n\nHomophones[119](533412_1_En_6_Chapter.xhtml#PB119)\n\nHopfield, J.[9](533412_1_En_1_Chapter.xhtml#PB9)\n\nHopfield Network[9](533412_1_En_1_Chapter.xhtml#PB9)\n\nHuggingFace[373](533412_1_En_15_Chapter.xhtml#PB373),\n[382](533412_1_En_15_Chapter.xhtml#PB382)\u2013[385](533412_1_En_15_Chapter.xhtml#PB385),\n[390](533412_1_En_15_Chapter.xhtml#PB390),\n[393](533412_1_En_15_Chapter.xhtml#PB393),\n[394](533412_1_En_15_Chapter.xhtml#PB394)\n\nHuman\u2013computer interaction (HCI)[10](533412_1_En_1_Chapter.xhtml#PB10)\n\nHuman language ambiguity7\u2013[8](533412_1_En_1_Chapter.xhtml#PB8)\n\nHuman-to-computer interaction (HCI)[149](533412_1_En_7_Chapter.xhtml#PB149)\n\nHybrid POS tagging[45](533412_1_En_3_Chapter.xhtml#PB45)\n\nHypernymy[122](533412_1_En_6_Chapter.xhtml#PB122),\n[129](533412_1_En_6_Chapter.xhtml#PB129)\n\nHyponymy[122](533412_1_En_6_Chapter.xhtml#PB122),\n[129](533412_1_En_6_Chapter.xhtml#PB129)\n\nI\n\nIMDB340, 350\u2013352, 355, 365, 369, 371\n\nInference[102](533412_1_En_5_Chapter.xhtml#PB102)\u2013[103](533412_1_En_5_Chapter.xhtml#PB103),\n[111](533412_1_En_5_Chapter.xhtml#PB111),\n[112](533412_1_En_5_Chapter.xhtml#PB112)\n\nInflection[43](533412_1_En_3_Chapter.xhtml#PB43),\n[58](533412_1_En_3_Chapter.xhtml#PB58)\n\nInformation content\nsimilarity[134](533412_1_En_6_Chapter.xhtml#PB134)\u2013[135](533412_1_En_6_Chapter.xhtml#PB135)\n\nInformation extraction (IE)[15](533412_1_En_1_Chapter.xhtml#PB15), 276, 283\n\nInformation retrieval (IR)[15](533412_1_En_1_Chapter.xhtml#PB15),\n[199](533412_1_En_9_Chapter.xhtml#PB199)\u2013[212](533412_1_En_9_Chapter.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f789b1f-d64c-40ae-ab06-de445014722d": {"__data__": {"id_": "3f789b1f-d64c-40ae-ab06-de445014722d", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9f9093f-ee62-4bb0-8eec-5d493daf9c56", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "99e4240f04fbbdc7376d7d396fe1d099ef6c973419e92eaabde9f8e94476cf8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c2648ba-8874-49a5-a8c9-bb2f3b979370", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3eff48f637ca9c57049169c81f1e1f9ad1f53bcdfdf88f40ceff8c8cc0c91fa5", "class_name": "RelatedNodeInfo"}}, "hash": "c2b78565431e0e209ad71bc5e2cd661030c8d5cafa18f8e8afea61e52933249e", "text": "[112](533412_1_En_5_Chapter.xhtml#PB112)\n\nInflection[43](533412_1_En_3_Chapter.xhtml#PB43),\n[58](533412_1_En_3_Chapter.xhtml#PB58)\n\nInformation content\nsimilarity[134](533412_1_En_6_Chapter.xhtml#PB134)\u2013[135](533412_1_En_6_Chapter.xhtml#PB135)\n\nInformation extraction (IE)[15](533412_1_En_1_Chapter.xhtml#PB15), 276, 283\n\nInformation retrieval (IR)[15](533412_1_En_1_Chapter.xhtml#PB15),\n[199](533412_1_En_9_Chapter.xhtml#PB199)\u2013[212](533412_1_En_9_Chapter.xhtml#PB212),\n[226](533412_1_En_9_Chapter.xhtml#PB226)\u2013228,\n[235](533412_1_En_9_Chapter.xhtml#PB235), 276\n\nInterjections[43](533412_1_En_3_Chapter.xhtml#PB43),\n[50](533412_1_En_3_Chapter.xhtml#PB50)\n\nInverted document frequency (IDF)[200](533412_1_En_9_Chapter.xhtml#PB200)\n\nJ\n\nJupyter Notebook[244](533412_1_En_10_Chapter.xhtml#PB244)\n\nK\n\nKaggle351\n\nKBQA[227](533412_1_En_9_Chapter.xhtml#PB227), 228\n\nKeras[336](533412_1_En_14_Chapter.xhtml#PB336), 357\u2013371,\n[401](533412_1_En_16_Chapter.xhtml#PB401),\n[404](533412_1_En_16_Chapter.xhtml#PB404), 415, 418, 420, 422, 424, 430, 431\n\nKnowledge acquisition and inferencing\n(KAI)[11](533412_1_En_1_Chapter.xhtml#PB11)\n\nKnowledge-based (KB)[125](533412_1_En_6_Chapter.xhtml#PB125)\n\nL\n\nLanguage detection[336](533412_1_En_14_Chapter.xhtml#PB336)\n\nLanguage model[21](533412_1_En_2_Chapter.xhtml#PB21),\n[24](533412_1_En_2_Chapter.xhtml#PB24),\n[26](533412_1_En_2_Chapter.xhtml#PB26),\n[34](533412_1_En_2_Chapter.xhtml#PB34), [40](533412_1_En_2_Chapter.xhtml#PB40)\n\nLanguage model evaluation\n(LME)[34](533412_1_En_2_Chapter.xhtml#PB34)\u2013[40](533412_1_En_2_Chapter.xhtml#PB40)\n\nLanguage\nmodeling[24](533412_1_En_2_Chapter.xhtml#PB24)\u2013[25](533412_1_En_2_Chapter.xhtml#PB25)\n\nLaplace (Add-one)\nSmoothing[36](533412_1_En_2_Chapter.xhtml#PB36)\u2013[38](533412_1_En_2_Chapter.xhtml#PB38)\n\nLarge Movie Reviews Dataset351\n\nLatent Semantic Analysis (LSA)[221](533412_1_En_9_Chapter.xhtml#PB221)\n\nLatent Semantic\nIndexing[207](533412_1_En_9_Chapter.xhtml#PB207)\u2013[208](533412_1_En_9_Chapter.xhtml#PB208)\n\nLemma[23](533412_1_En_2_Chapter.xhtml#PB23)\n\nLesk Algorithm[136](533412_1_En_6_Chapter.xhtml#PB136)\n\nLexical ambiguity7, [123](533412_1_En_6_Chapter.xhtml#PB123)\n\nLexical analysis[75](533412_1_En_4_Chapter.xhtml#PB75)\n\nLexical dispersion plot253\u2013255\n\nLexical diversity258\u2013259\n\nLexicalized\nparsing[91](533412_1_En_4_Chapter.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c2648ba-8874-49a5-a8c9-bb2f3b979370": {"__data__": {"id_": "1c2648ba-8874-49a5-a8c9-bb2f3b979370", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f789b1f-d64c-40ae-ab06-de445014722d", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "c2b78565431e0e209ad71bc5e2cd661030c8d5cafa18f8e8afea61e52933249e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7fbf2474-06e2-45ca-b405-919ddf7f52d6", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "498aac97c427ad70e5890a6c1756c63d877fd84429ab45bb946a21abd093101f", "class_name": "RelatedNodeInfo"}}, "hash": "3eff48f637ca9c57049169c81f1e1f9ad1f53bcdfdf88f40ceff8c8cc0c91fa5", "text": "xhtml#PB221)\n\nLatent Semantic\nIndexing[207](533412_1_En_9_Chapter.xhtml#PB207)\u2013[208](533412_1_En_9_Chapter.xhtml#PB208)\n\nLemma[23](533412_1_En_2_Chapter.xhtml#PB23)\n\nLesk Algorithm[136](533412_1_En_6_Chapter.xhtml#PB136)\n\nLexical ambiguity7, [123](533412_1_En_6_Chapter.xhtml#PB123)\n\nLexical analysis[75](533412_1_En_4_Chapter.xhtml#PB75)\n\nLexical dispersion plot253\u2013255\n\nLexical diversity258\u2013259\n\nLexicalized\nparsing[91](533412_1_En_4_Chapter.xhtml#PB91)\u2013[93](533412_1_En_4_Chapter.xhtml#PB93)\n\nLexical probability[92](533412_1_En_4_Chapter.xhtml#PB92),\n[93](533412_1_En_4_Chapter.xhtml#PB93)\n\nLexical semantic analysis[117](533412_1_En_6_Chapter.xhtml#PB117)\n\nLexical semantics[117](533412_1_En_6_Chapter.xhtml#PB117),\n[119](533412_1_En_6_Chapter.xhtml#PB119),\n[145](533412_1_En_6_Chapter.xhtml#PB145)\n\nLexicology[6](533412_1_En_1_Chapter.xhtml#PB6)\n\nLexicon[75](533412_1_En_4_Chapter.xhtml#PB75),\n[78](533412_1_En_4_Chapter.xhtml#PB78),\n[80](533412_1_En_4_Chapter.xhtml#PB80), [91](533412_1_En_4_Chapter.xhtml#PB91)\n\nLinguistic levels[6](533412_1_En_1_Chapter.xhtml#PB6), 7\n\nLog-linear model[170](533412_1_En_7_Chapter.xhtml#PB170)\n\nLong short-term memory (LSTM)[10](533412_1_En_1_Chapter.xhtml#PB10),\n[170](533412_1_En_7_Chapter.xhtml#PB170),\n[183](533412_1_En_8_Chapter.xhtml#PB183)\u2013[186](533412_1_En_8_Chapter.xhtml#PB186),\n[188](533412_1_En_8_Chapter.xhtml#PB188),\n[196](533412_1_En_8_Chapter.xhtml#PB196),\n[224](533412_1_En_9_Chapter.xhtml#PB224),\n[226](533412_1_En_9_Chapter.xhtml#PB226),\n[230](533412_1_En_9_Chapter.xhtml#PB230),\n[335](533412_1_En_14_Chapter.xhtml#PB335)\u2013371,\n[374](533412_1_En_15_Chapter.xhtml#PB374),\n[375](533412_1_En_15_Chapter.xhtml#PB375),\n[388](533412_1_En_15_Chapter.xhtml#PB388)\n\nLuhn\u2019s Algorithm[219](533412_1_En_9_Chapter.xhtml#PB219)\n\nM\n\nMachine learning (ML)\nmethod[169](533412_1_En_7_Chapter.xhtml#PB169)\u2013[170](533412_1_En_7_Chapter.xhtml#PB170)\n\nMachine translation (MT)[8](533412_1_En_1_Chapter.xhtml#PB8),\n[14](533412_1_En_1_Chapter.xhtml#PB14),\n[19](533412_1_En_2_Chapter.xhtml#PB19), [22](533412_1_En_2_Chapter.xhtml#PB22)\n\nMarkov\nchain[25](533412_1_En_2_Chapter.xhtml#PB25)\u2013[27](533412_1_En_2_Chapter.xhtml#PB27),\n[40](533412_1_En_2_Chapter.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7fbf2474-06e2-45ca-b405-919ddf7f52d6": {"__data__": {"id_": "7fbf2474-06e2-45ca-b405-919ddf7f52d6", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c2648ba-8874-49a5-a8c9-bb2f3b979370", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "3eff48f637ca9c57049169c81f1e1f9ad1f53bcdfdf88f40ceff8c8cc0c91fa5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ee13276-17e9-4acd-94eb-1b9fb5dce0d2", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "14b36f982bf7a83eb0d4ed136cc9ca54ad36cfd81c290cdbe709dfc2a3ba1e59", "class_name": "RelatedNodeInfo"}}, "hash": "498aac97c427ad70e5890a6c1756c63d877fd84429ab45bb946a21abd093101f", "text": "xhtml#PB169)\u2013[170](533412_1_En_7_Chapter.xhtml#PB170)\n\nMachine translation (MT)[8](533412_1_En_1_Chapter.xhtml#PB8),\n[14](533412_1_En_1_Chapter.xhtml#PB14),\n[19](533412_1_En_2_Chapter.xhtml#PB19), [22](533412_1_En_2_Chapter.xhtml#PB22)\n\nMarkov\nchain[25](533412_1_En_2_Chapter.xhtml#PB25)\u2013[27](533412_1_En_2_Chapter.xhtml#PB27),\n[40](533412_1_En_2_Chapter.xhtml#PB40)\n\nMaximum entropy Markov model (MEMM)[60](533412_1_En_3_Chapter.xhtml#PB60)\n\nMaximum likelihood estimates (MLE)[30](533412_1_En_2_Chapter.xhtml#PB30)\n\nMeaning\nrepresentations[95](533412_1_En_5_Chapter.xhtml#PB95)\u2013[98](533412_1_En_5_Chapter.xhtml#PB98),\n[100](533412_1_En_5_Chapter.xhtml#PB100)\u2013[103](533412_1_En_5_Chapter.xhtml#PB103),\n[110](533412_1_En_5_Chapter.xhtml#PB110),\n[112](533412_1_En_5_Chapter.xhtml#PB112),\n[113](533412_1_En_5_Chapter.xhtml#PB113)\n\nMeSH[130](533412_1_En_6_Chapter.xhtml#PB130)\u2013[131](533412_1_En_6_Chapter.xhtml#PB131)\n\nMetaphor[120](533412_1_En_6_Chapter.xhtml#PB120)\n\nMetonymy[120](533412_1_En_6_Chapter.xhtml#PB120)\n\nMinsky, M.[99](533412_1_En_5_Chapter.xhtml#PB99)\n\nModal verb[54](533412_1_En_3_Chapter.xhtml#PB54)\n\nMorphological parsing[79](533412_1_En_4_Chapter.xhtml#PB79)\n\nMorphology[6](533412_1_En_1_Chapter.xhtml#PB6)\n\nMorphology analysis[96](533412_1_En_5_Chapter.xhtml#PB96)\n\nMovie comments351\n\nMulti-head-attention (MHAttention)[191](533412_1_En_8_Chapter.xhtml#PB191),\n[376](533412_1_En_15_Chapter.xhtml#PB376), 411\u2013412\n\nMultiple document\nsummarization[217](533412_1_En_9_Chapter.xhtml#PB217)\u2013[224](533412_1_En_9_Chapter.xhtml#PB224)\n\nN\n\nName entity recognition (NER)[227](533412_1_En_9_Chapter.xhtml#PB227), 276\n\nNatural language generation (NLG)[11](533412_1_En_1_Chapter.xhtml#PB11)\n\nNatural language processing[3](533412_1_En_1_Chapter.xhtml#PB3)\u201316\n\nNatural language toolkit (NLTK)[243](533412_1_En_10_Chapter.xhtml#PB243)\u2013264,\n[267](533412_1_En_11_Chapter.xhtml#PB267)\u2013311,\n[401](533412_1_En_16_Chapter.xhtml#PB401)\n\nNatural language understanding\n(NLU)[11](533412_1_En_1_Chapter.xhtml#PB11)\u2013[13](533412_1_En_1_Chapter.xhtml#PB13),\n[48](533412_1_En_3_Chapter.xhtml#PB48),\n[65](533412_1_En_3_Chapter.xhtml#PB65),\n[70](533412_1_En_4_Chapter.xhtml#PB70), 276\n\nNext sentence prediction (NSP)[193](533412_1_En_8_Chapter.xhtml#PB193),\n[195](533412_1_En_8_Chapter.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ee13276-17e9-4acd-94eb-1b9fb5dce0d2": {"__data__": {"id_": "7ee13276-17e9-4acd-94eb-1b9fb5dce0d2", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7fbf2474-06e2-45ca-b405-919ddf7f52d6", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "498aac97c427ad70e5890a6c1756c63d877fd84429ab45bb946a21abd093101f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e9236bb-38f7-4bc8-b0f6-f1aabb261407", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "5615c9a556038dd485c0721a5c2f4f2495bf0c31e1722cb23eda508d77190a33", "class_name": "RelatedNodeInfo"}}, "hash": "14b36f982bf7a83eb0d4ed136cc9ca54ad36cfd81c290cdbe709dfc2a3ba1e59", "text": "[267](533412_1_En_11_Chapter.xhtml#PB267)\u2013311,\n[401](533412_1_En_16_Chapter.xhtml#PB401)\n\nNatural language understanding\n(NLU)[11](533412_1_En_1_Chapter.xhtml#PB11)\u2013[13](533412_1_En_1_Chapter.xhtml#PB13),\n[48](533412_1_En_3_Chapter.xhtml#PB48),\n[65](533412_1_En_3_Chapter.xhtml#PB65),\n[70](533412_1_En_4_Chapter.xhtml#PB70), 276\n\nNext sentence prediction (NSP)[193](533412_1_En_8_Chapter.xhtml#PB193),\n[195](533412_1_En_8_Chapter.xhtml#PB195)\n\nN-gram[21](533412_1_En_2_Chapter.xhtml#PB21),\n[22](533412_1_En_2_Chapter.xhtml#PB22),\n[26](533412_1_En_2_Chapter.xhtml#PB26),\n[27](533412_1_En_2_Chapter.xhtml#PB27),\n[29](533412_1_En_2_Chapter.xhtml#PB29)\u2013[33](533412_1_En_2_Chapter.xhtml#PB33),\n[35](533412_1_En_2_Chapter.xhtml#PB35),\n[36](533412_1_En_2_Chapter.xhtml#PB36),\n[38](533412_1_En_2_Chapter.xhtml#PB38)\u2013[41](533412_1_En_2_Chapter.xhtml#PB41),\n[46](533412_1_En_3_Chapter.xhtml#PB46),\n[47](533412_1_En_3_Chapter.xhtml#PB47),\n[58](533412_1_En_3_Chapter.xhtml#PB58),\n[60](533412_1_En_3_Chapter.xhtml#PB60),\n[86](533412_1_En_4_Chapter.xhtml#PB86),\n[87](533412_1_En_4_Chapter.xhtml#PB87),\n[90](533412_1_En_4_Chapter.xhtml#PB90),\n[92](533412_1_En_4_Chapter.xhtml#PB92),\n[211](533412_1_En_9_Chapter.xhtml#PB211),\n[267](533412_1_En_11_Chapter.xhtml#PB267),\n[268](533412_1_En_11_Chapter.xhtml#PB268), 270, 271, 273\n\nNoun-phrase (NP)[72](533412_1_En_4_Chapter.xhtml#PB72)\n\nNouns[43](533412_1_En_3_Chapter.xhtml#PB43),\n[44](533412_1_En_3_Chapter.xhtml#PB44),\n[49](533412_1_En_3_Chapter.xhtml#PB49),\n[51](533412_1_En_3_Chapter.xhtml#PB51),\n[53](533412_1_En_3_Chapter.xhtml#PB53), [56](533412_1_En_3_Chapter.xhtml#PB56)\n\nP\n\nParsing[67](533412_1_En_4_Chapter.xhtml#PB67)\u2013[94](533412_1_En_4_Chapter.xhtml#PB94),\n[268](533412_1_En_11_Chapter.xhtml#PB268)\n\nPart-of-Speech\n(POS)[43](533412_1_En_3_Chapter.xhtml#PB43)\u2013[65](533412_1_En_3_Chapter.xhtml#PB65),\n[67](533412_1_En_4_Chapter.xhtml#PB67)\n\nPath-based\nsimilarity[132](533412_1_En_6_Chapter.xhtml#PB132)\u2013[134](533412_1_En_6_Chapter.xhtml#PB134),\n[145](533412_1_En_6_Chapter.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e9236bb-38f7-4bc8-b0f6-f1aabb261407": {"__data__": {"id_": "1e9236bb-38f7-4bc8-b0f6-f1aabb261407", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ee13276-17e9-4acd-94eb-1b9fb5dce0d2", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "14b36f982bf7a83eb0d4ed136cc9ca54ad36cfd81c290cdbe709dfc2a3ba1e59", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3545c355-d643-4a8e-a499-c147943664d1", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8ba929ebca8ad79ed6323b6f07b17e7b41bfb3e8d0e4312cb455296709a35419", "class_name": "RelatedNodeInfo"}}, "hash": "5615c9a556038dd485c0721a5c2f4f2495bf0c31e1722cb23eda508d77190a33", "text": "xhtml#PB67)\u2013[94](533412_1_En_4_Chapter.xhtml#PB94),\n[268](533412_1_En_11_Chapter.xhtml#PB268)\n\nPart-of-Speech\n(POS)[43](533412_1_En_3_Chapter.xhtml#PB43)\u2013[65](533412_1_En_3_Chapter.xhtml#PB65),\n[67](533412_1_En_4_Chapter.xhtml#PB67)\n\nPath-based\nsimilarity[132](533412_1_En_6_Chapter.xhtml#PB132)\u2013[134](533412_1_En_6_Chapter.xhtml#PB134),\n[145](533412_1_En_6_Chapter.xhtml#PB145)\n\nPenn Treebank (PTB)[45](533412_1_En_3_Chapter.xhtml#PB45),\n[46](533412_1_En_3_Chapter.xhtml#PB46)\n\nPerplexity\n(PP)[34](533412_1_En_2_Chapter.xhtml#PB34)\u2013[35](533412_1_En_2_Chapter.xhtml#PB35),\n[41](533412_1_En_2_Chapter.xhtml#PB41)\n\nPhonetics[6](533412_1_En_1_Chapter.xhtml#PB6)\n\nPhonological parsing[79](533412_1_En_4_Chapter.xhtml#PB79)\n\nPointer-Generator Networks[224](533412_1_En_9_Chapter.xhtml#PB224)\n\nPoint-wise Mutual Information (PMI)[139](533412_1_En_6_Chapter.xhtml#PB139)\n\nPolysemy[120](533412_1_En_6_Chapter.xhtml#PB120),\n[129](533412_1_En_6_Chapter.xhtml#PB129)\n\nPorter Stemmer[285](533412_1_En_12_Chapter.xhtml#PB285), 289\u2013292\n\nPositional encoding189\u2013[190](533412_1_En_8_Chapter.xhtml#PB190), 413, 416\n\nPositive Point-wise Mutual Information\n(PPMI)[140](533412_1_En_6_Chapter.xhtml#PB140)\u2013[142](533412_1_En_6_Chapter.xhtml#PB142),\n[144](533412_1_En_6_Chapter.xhtml#PB144)\u2013[146](533412_1_En_6_Chapter.xhtml#PB146)\n\nPOS tagger[285](533412_1_En_12_Chapter.xhtml#PB285), 304\u2013306, 308\u2013310\n\nPOS tagging[44](533412_1_En_3_Chapter.xhtml#PB44),\n[45](533412_1_En_3_Chapter.xhtml#PB45),\n[47](533412_1_En_3_Chapter.xhtml#PB47)\u2013[49](533412_1_En_3_Chapter.xhtml#PB49),\n[58](533412_1_En_3_Chapter.xhtml#PB58),\n[60](533412_1_En_3_Chapter.xhtml#PB60),\n[64](533412_1_En_3_Chapter.xhtml#PB64),\n[65](533412_1_En_3_Chapter.xhtml#PB65),\n[124](533412_1_En_6_Chapter.xhtml#PB124),\n[244](533412_1_En_10_Chapter.xhtml#PB244),\n[285](533412_1_En_12_Chapter.xhtml#PB285)\u2013311\n\nPOS Tagset301, 302\n\nPragmatic[124](533412_1_En_6_Chapter.xhtml#PB124)\n\nPragmatic ambiguity[8](533412_1_En_1_Chapter.xhtml#PB8)\n\nPragmatic analysis[12](533412_1_En_1_Chapter.xhtml#PB12),\n[13](533412_1_En_1_Chapter.xhtml#PB13), 16\n\nPragmatic meaning[95](533412_1_En_5_Chapter.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3545c355-d643-4a8e-a499-c147943664d1": {"__data__": {"id_": "3545c355-d643-4a8e-a499-c147943664d1", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e9236bb-38f7-4bc8-b0f6-f1aabb261407", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "5615c9a556038dd485c0721a5c2f4f2495bf0c31e1722cb23eda508d77190a33", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "835c2031-d9f2-4215-8457-261c016a8877", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "2ceb9e5684e8d25fb6fa855b668126014a3aed0f27898b8c2647fd8e6e6e11fb", "class_name": "RelatedNodeInfo"}}, "hash": "8ba929ebca8ad79ed6323b6f07b17e7b41bfb3e8d0e4312cb455296709a35419", "text": "xhtml#PB65),\n[124](533412_1_En_6_Chapter.xhtml#PB124),\n[244](533412_1_En_10_Chapter.xhtml#PB244),\n[285](533412_1_En_12_Chapter.xhtml#PB285)\u2013311\n\nPOS Tagset301, 302\n\nPragmatic[124](533412_1_En_6_Chapter.xhtml#PB124)\n\nPragmatic ambiguity[8](533412_1_En_1_Chapter.xhtml#PB8)\n\nPragmatic analysis[12](533412_1_En_1_Chapter.xhtml#PB12),\n[13](533412_1_En_1_Chapter.xhtml#PB13), 16\n\nPragmatic meaning[95](533412_1_En_5_Chapter.xhtml#PB95)\n\nPragmatics analysis[149](533412_1_En_7_Chapter.xhtml#PB149)\n\nPredicates[108](533412_1_En_5_Chapter.xhtml#PB108),\n[109](533412_1_En_5_Chapter.xhtml#PB109)\n\nPrepositions[51](533412_1_En_3_Chapter.xhtml#PB51)\u2013[52](533412_1_En_3_Chapter.xhtml#PB52)\n\nProbabilistic context-free grammar\n(PCFG)[87](533412_1_En_4_Chapter.xhtml#PB87),\n[88](533412_1_En_4_Chapter.xhtml#PB88),\n[90](533412_1_En_4_Chapter.xhtml#PB90)\u2013[91](533412_1_En_4_Chapter.xhtml#PB91),\n[94](533412_1_En_4_Chapter.xhtml#PB94)\n\nProbabilistic Ranking Principle\n(PRP)[202](533412_1_En_9_Chapter.xhtml#PB202)\u2013[207](533412_1_En_9_Chapter.xhtml#PB207)\n\nPronouns[43](533412_1_En_3_Chapter.xhtml#PB43),\n[51](533412_1_En_3_Chapter.xhtml#PB51),\n[53](533412_1_En_3_Chapter.xhtml#PB53), [55](533412_1_En_3_Chapter.xhtml#PB55)\n\nPTB Tagset[285](533412_1_En_12_Chapter.xhtml#PB285), 302\n\nQ\n\nQ&A chatbots[19](533412_1_En_2_Chapter.xhtml#PB19)\n\nQA systems16, [224](533412_1_En_9_Chapter.xhtml#PB224),\n[225](533412_1_En_9_Chapter.xhtml#PB225),\n[227](533412_1_En_9_Chapter.xhtml#PB227)\u2013[229](533412_1_En_9_Chapter.xhtml#PB229),\n[231](533412_1_En_9_Chapter.xhtml#PB231),\n[233](533412_1_En_9_Chapter.xhtml#PB233),\n[235](533412_1_En_9_Chapter.xhtml#PB235)\n\nQuadrigram[22](533412_1_En_2_Chapter.xhtml#PB22),\n[33](533412_1_En_2_Chapter.xhtml#PB33), 269, 275\n\nQuantifiers[108](533412_1_En_5_Chapter.xhtml#PB108)\n\nQuery-focused summarization (QFS)\nsystems[215](533412_1_En_9_Chapter.xhtml#PB215)\u2013[216](533412_1_En_9_Chapter.xhtml#PB216)\n\nQuery-\nlikelihood[207](533412_1_En_9_Chapter.xhtml#PB207)\u2013[208](533412_1_En_9_Chapter.xhtml#PB208)\n\nR\n\nRecurrent neural network (RNN)[170](533412_1_En_7_Chapter.xhtml#PB170),\n[180](533412_1_En_8_Chapter.xhtml#PB180)\u2013[188](533412_1_En_8_Chapter.xhtml#PB188),", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "835c2031-d9f2-4215-8457-261c016a8877": {"__data__": {"id_": "835c2031-d9f2-4215-8457-261c016a8877", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3545c355-d643-4a8e-a499-c147943664d1", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "8ba929ebca8ad79ed6323b6f07b17e7b41bfb3e8d0e4312cb455296709a35419", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51211326-f076-4aa1-bd03-8613f055845b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "094e499917ce69c9c3c222a4d76c2d9b07986a3bf0bb291ed05b354203b40311", "class_name": "RelatedNodeInfo"}}, "hash": "2ceb9e5684e8d25fb6fa855b668126014a3aed0f27898b8c2647fd8e6e6e11fb", "text": "xhtml#PB33), 269, 275\n\nQuantifiers[108](533412_1_En_5_Chapter.xhtml#PB108)\n\nQuery-focused summarization (QFS)\nsystems[215](533412_1_En_9_Chapter.xhtml#PB215)\u2013[216](533412_1_En_9_Chapter.xhtml#PB216)\n\nQuery-\nlikelihood[207](533412_1_En_9_Chapter.xhtml#PB207)\u2013[208](533412_1_En_9_Chapter.xhtml#PB208)\n\nR\n\nRecurrent neural network (RNN)[170](533412_1_En_7_Chapter.xhtml#PB170),\n[180](533412_1_En_8_Chapter.xhtml#PB180)\u2013[188](533412_1_En_8_Chapter.xhtml#PB188),\n[196](533412_1_En_8_Chapter.xhtml#PB196),\n[373](533412_1_En_15_Chapter.xhtml#PB373)\n\nReferring expression (RE)[159](533412_1_En_7_Chapter.xhtml#PB159),\n[161](533412_1_En_7_Chapter.xhtml#PB161)\n\nRegular language (RL)[76](533412_1_En_4_Chapter.xhtml#PB76)\n\nResnik method[135](533412_1_En_6_Chapter.xhtml#PB135),\n[145](533412_1_En_6_Chapter.xhtml#PB145)\n\nRhetorical structure theory (RST)[158](533412_1_En_7_Chapter.xhtml#PB158),\n[209](533412_1_En_9_Chapter.xhtml#PB209),\n[211](533412_1_En_9_Chapter.xhtml#PB211)\n\nRule-based POS tagging[45](533412_1_En_3_Chapter.xhtml#PB45)\n\nRule-based QA systems[227](533412_1_En_9_Chapter.xhtml#PB227)\n\nS\n\nSelectional restrictions[107](533412_1_En_5_Chapter.xhtml#PB107)\n\nSelf-attention[190](533412_1_En_8_Chapter.xhtml#PB190), 410\n\nSelf-attention mechanism[227](533412_1_En_9_Chapter.xhtml#PB227)\n\nSemantic ambiguity7\n\nSemantic analysis[12](533412_1_En_1_Chapter.xhtml#PB12),\n[13](533412_1_En_1_Chapter.xhtml#PB13),\n[244](533412_1_En_10_Chapter.xhtml#PB244),\n[268](533412_1_En_11_Chapter.xhtml#PB268),\n[313](533412_1_En_13_Chapter.xhtml#PB313)\u2013333\n\nSemantic categorization330\n\nSemantic level[6](533412_1_En_1_Chapter.xhtml#PB6)\n\nSemantic meaning[95](533412_1_En_5_Chapter.xhtml#PB95)\n\nSemantic\nnetworks[98](533412_1_En_5_Chapter.xhtml#PB98)\u2013[99](533412_1_En_5_Chapter.xhtml#PB99)\n\nSemantic processing[97](533412_1_En_5_Chapter.xhtml#PB97)\n\nSemantics[48](533412_1_En_3_Chapter.xhtml#PB48),\n[95](533412_1_En_5_Chapter.xhtml#PB95),\n[107](533412_1_En_5_Chapter.xhtml#PB107),\n[108](533412_1_En_5_Chapter.xhtml#PB108),\n[111](533412_1_En_5_Chapter.xhtml#PB111)\n\nSemantic similarity[320](533412_1_En_13_Chapter.xhtml#PB320), 323, 326\u2013333\n\nSemi-supervised methods[125](533412_1_En_6_Chapter.xhtml#PB125)\n\nSentiment analysis[15](533412_1_En_1_Chapter.xhtml#PB15),\n[337](533412_1_En_14_Chapter.xhtml#PB337),\n[338](533412_1_En_14_Chapter.xhtml#PB338),", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51211326-f076-4aa1-bd03-8613f055845b": {"__data__": {"id_": "51211326-f076-4aa1-bd03-8613f055845b", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "835c2031-d9f2-4215-8457-261c016a8877", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "2ceb9e5684e8d25fb6fa855b668126014a3aed0f27898b8c2647fd8e6e6e11fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f626da33-7ec5-4054-b01b-6ca8d9d382e4", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "7b93813797c6b1081109a396d996dd875fa500722db284158dfd3306576dfe39", "class_name": "RelatedNodeInfo"}}, "hash": "094e499917ce69c9c3c222a4d76c2d9b07986a3bf0bb291ed05b354203b40311", "text": "xhtml#PB95),\n[107](533412_1_En_5_Chapter.xhtml#PB107),\n[108](533412_1_En_5_Chapter.xhtml#PB108),\n[111](533412_1_En_5_Chapter.xhtml#PB111)\n\nSemantic similarity[320](533412_1_En_13_Chapter.xhtml#PB320), 323, 326\u2013333\n\nSemi-supervised methods[125](533412_1_En_6_Chapter.xhtml#PB125)\n\nSentiment analysis[15](533412_1_En_1_Chapter.xhtml#PB15),\n[337](533412_1_En_14_Chapter.xhtml#PB337),\n[338](533412_1_En_14_Chapter.xhtml#PB338),\n[401](533412_1_En_16_Chapter.xhtml#PB401)\n\nShakespeare, W.[31](533412_1_En_2_Chapter.xhtml#PB31)\n\nShannon\u2019s method[31](533412_1_En_2_Chapter.xhtml#PB31),\n[32](533412_1_En_2_Chapter.xhtml#PB32)\n\nSherlock\nHolmes[27](533412_1_En_2_Chapter.xhtml#PB27)\u2013[31](533412_1_En_2_Chapter.xhtml#PB31),\n[33](533412_1_En_2_Chapter.xhtml#PB33),\n[35](533412_1_En_2_Chapter.xhtml#PB35),\n[37](533412_1_En_2_Chapter.xhtml#PB37),\n[38](533412_1_En_2_Chapter.xhtml#PB38),\n[40](533412_1_En_2_Chapter.xhtml#PB40),\n[41](533412_1_En_2_Chapter.xhtml#PB41),\n[247](533412_1_En_10_Chapter.xhtml#PB247), 249, 251, 256, 257, 262, 263,\n[268](533412_1_En_11_Chapter.xhtml#PB268), 270, 271, 275, 278\u2013281\n\nSingle and multiple document\nsummarization[217](533412_1_En_9_Chapter.xhtml#PB217)\n\nSmoothing[141](533412_1_En_6_Chapter.xhtml#PB141)\u2013[143](533412_1_En_6_Chapter.xhtml#PB143)\n\nSmoothing techniques[36](533412_1_En_2_Chapter.xhtml#PB36)\n\nSnowball Stemmer[285](533412_1_En_12_Chapter.xhtml#PB285), 289, 291\u2013292\n\nSpaCy[243](533412_1_En_10_Chapter.xhtml#PB243),\n[267](533412_1_En_11_Chapter.xhtml#PB267), 276\u2013284,\n[313](533412_1_En_13_Chapter.xhtml#PB313)\u2013333,\n[335](533412_1_En_14_Chapter.xhtml#PB335)\u2013371,\n[373](533412_1_En_15_Chapter.xhtml#PB373)\u2013398,\n[401](533412_1_En_16_Chapter.xhtml#PB401)\n\nSpeech recognition[10](533412_1_En_1_Chapter.xhtml#PB10),\n[12](533412_1_En_1_Chapter.xhtml#PB12),\n[13](533412_1_En_1_Chapter.xhtml#PB13),\n[268](533412_1_En_11_Chapter.xhtml#PB268)\n\nStem[23](533412_1_En_2_Chapter.xhtml#PB23)\n\nStemming[244](533412_1_En_10_Chapter.xhtml#PB244),\n[285](533412_1_En_12_Chapter.xhtml#PB285), 288, 289, 301\n\nStochastic POS tagging[45](533412_1_En_3_Chapter.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f626da33-7ec5-4054-b01b-6ca8d9d382e4": {"__data__": {"id_": "f626da33-7ec5-4054-b01b-6ca8d9d382e4", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51211326-f076-4aa1-bd03-8613f055845b", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "094e499917ce69c9c3c222a4d76c2d9b07986a3bf0bb291ed05b354203b40311", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b5ed22e-7e91-4de6-b81c-7920d97b76ce", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "fd70d7aef5cf3e016ecbd055bb0674f97d98a083e07654144a70fe78f1760a8c", "class_name": "RelatedNodeInfo"}}, "hash": "7b93813797c6b1081109a396d996dd875fa500722db284158dfd3306576dfe39", "text": "[401](533412_1_En_16_Chapter.xhtml#PB401)\n\nSpeech recognition[10](533412_1_En_1_Chapter.xhtml#PB10),\n[12](533412_1_En_1_Chapter.xhtml#PB12),\n[13](533412_1_En_1_Chapter.xhtml#PB13),\n[268](533412_1_En_11_Chapter.xhtml#PB268)\n\nStem[23](533412_1_En_2_Chapter.xhtml#PB23)\n\nStemming[244](533412_1_En_10_Chapter.xhtml#PB244),\n[285](533412_1_En_12_Chapter.xhtml#PB285), 288, 289, 301\n\nStochastic POS tagging[45](533412_1_En_3_Chapter.xhtml#PB45)\n\nStop-words292\n\nSubject\u2013Predicate\u2013Object (SPO)[227](533412_1_En_9_Chapter.xhtml#PB227)\n\nSummaRuNNer[219](533412_1_En_9_Chapter.xhtml#PB219),\n[221](533412_1_En_9_Chapter.xhtml#PB221)\n\nSupervised discourse segmentation[158](533412_1_En_7_Chapter.xhtml#PB158)\n\nSupervised learning (SL)[125](533412_1_En_6_Chapter.xhtml#PB125)\n\nSVD model[208](533412_1_En_9_Chapter.xhtml#PB208)\n\nSymbolic representations[118](533412_1_En_6_Chapter.xhtml#PB118)\n\nSynonyms[121](533412_1_En_6_Chapter.xhtml#PB121)\n\nSynsets[126](533412_1_En_6_Chapter.xhtml#PB126),\n[145](533412_1_En_6_Chapter.xhtml#PB145)\n\nSyntactic ambiguity7\n\nSyntactic levels[6](533412_1_En_1_Chapter.xhtml#PB6)\n\nSyntactic parsing[79](533412_1_En_4_Chapter.xhtml#PB79),\n[80](533412_1_En_4_Chapter.xhtml#PB80)\n\nSyntactic rules[68](533412_1_En_4_Chapter.xhtml#PB68)\n\nSyntax[48](533412_1_En_3_Chapter.xhtml#PB48),\n[67](533412_1_En_4_Chapter.xhtml#PB67)\u2013[94](533412_1_En_4_Chapter.xhtml#PB94)\n\nSyntax analysis[13](533412_1_En_1_Chapter.xhtml#PB13),\n[67](533412_1_En_4_Chapter.xhtml#PB67)\n\nT\n\nTaggers\nevaluations[63](533412_1_En_3_Chapter.xhtml#PB63)\u2013[64](533412_1_En_3_Chapter.xhtml#PB64)\n\nTagging[43](533412_1_En_3_Chapter.xhtml#PB43)\u2013[65](533412_1_En_3_Chapter.xhtml#PB65),\n[335](533412_1_En_14_Chapter.xhtml#PB335),\n[336](533412_1_En_14_Chapter.xhtml#PB336), 358\n\nTag sequence frequency[60](533412_1_En_3_Chapter.xhtml#PB60)\n\nTagset[56](533412_1_En_3_Chapter.xhtml#PB56)\u2013[58](533412_1_En_3_Chapter.xhtml#PB58)\n\nTaskmaster430\n\nTensorFlow[316](533412_1_En_13_Chapter.xhtml#PB316),\n[335](533412_1_En_14_Chapter.xhtml#PB335),\n[336](533412_1_En_14_Chapter.xhtml#PB336), 357, 358, 367,\n[373](533412_1_En_15_Chapter.xhtml#PB373)\u2013398,\n[401](533412_1_En_16_Chapter.xhtml#PB401)\u2013431\n\nTerm-context matrix[139](533412_1_En_6_Chapter.xhtml#PB139),", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b5ed22e-7e91-4de6-b81c-7920d97b76ce": {"__data__": {"id_": "6b5ed22e-7e91-4de6-b81c-7920d97b76ce", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f626da33-7ec5-4054-b01b-6ca8d9d382e4", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "7b93813797c6b1081109a396d996dd875fa500722db284158dfd3306576dfe39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4576772d-15c7-4487-931c-e2febd229cb2", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "25fb2f6b1f662af302a6c14e0290951ab5f7a168745b505843fd0f01c7ef69e7", "class_name": "RelatedNodeInfo"}}, "hash": "fd70d7aef5cf3e016ecbd055bb0674f97d98a083e07654144a70fe78f1760a8c", "text": "xhtml#PB60)\n\nTagset[56](533412_1_En_3_Chapter.xhtml#PB56)\u2013[58](533412_1_En_3_Chapter.xhtml#PB58)\n\nTaskmaster430\n\nTensorFlow[316](533412_1_En_13_Chapter.xhtml#PB316),\n[335](533412_1_En_14_Chapter.xhtml#PB335),\n[336](533412_1_En_14_Chapter.xhtml#PB336), 357, 358, 367,\n[373](533412_1_En_15_Chapter.xhtml#PB373)\u2013398,\n[401](533412_1_En_16_Chapter.xhtml#PB401)\u2013431\n\nTerm-context matrix[139](533412_1_En_6_Chapter.xhtml#PB139),\n[141](533412_1_En_6_Chapter.xhtml#PB141),\n[145](533412_1_En_6_Chapter.xhtml#PB145)\n\nTerm Distribution Models[207](533412_1_En_9_Chapter.xhtml#PB207)\n\nTerm-document\nmatrix[137](533412_1_En_6_Chapter.xhtml#PB137)\u2013[139](533412_1_En_6_Chapter.xhtml#PB139)\n\nTerm-frequency (TF)[200](533412_1_En_9_Chapter.xhtml#PB200)\n\nText analysis[243](533412_1_En_10_Chapter.xhtml#PB243), 249, 253, 257, 259,\n296\u2013299\n\nTextCategorizer[335](533412_1_En_14_Chapter.xhtml#PB335),\n[338](533412_1_En_14_Chapter.xhtml#PB338)\u2013343, 345, 347\u2013351, 355, 356\n\nText classification[244](533412_1_En_10_Chapter.xhtml#PB244),\n[335](533412_1_En_14_Chapter.xhtml#PB335)\u2013371\n\nText\ncoherent[158](533412_1_En_7_Chapter.xhtml#PB158)\u2013[159](533412_1_En_7_Chapter.xhtml#PB159)\n\nText processing248\u2013249, 252\n\nText summarization\n(TS)[212](533412_1_En_9_Chapter.xhtml#PB212)\u2013[224](533412_1_En_9_Chapter.xhtml#PB224),\n[235](533412_1_En_9_Chapter.xhtml#PB235)\n\nTextTeaser[219](533412_1_En_9_Chapter.xhtml#PB219)\n\nTextTiling[208](533412_1_En_9_Chapter.xhtml#PB208)\n\nThesaurus[130](533412_1_En_6_Chapter.xhtml#PB130)\u2013[132](533412_1_En_6_Chapter.xhtml#PB132)\n\nTL system[235](533412_1_En_9_Chapter.xhtml#PB235)\n\nTokenization[243](533412_1_En_10_Chapter.xhtml#PB243),\n[244](533412_1_En_10_Chapter.xhtml#PB244), 255\u2013259,\n[267](533412_1_En_11_Chapter.xhtml#PB267)\u2013288, 301\n\nTokens[23](533412_1_En_2_Chapter.xhtml#PB23)\n\nTop-down\nparser[80](533412_1_En_4_Chapter.xhtml#PB80)\u2013[82](533412_1_En_4_Chapter.xhtml#PB82)\n\nTopic generation[336](533412_1_En_14_Chapter.xhtml#PB336)\n\nTransfer learning\n(TL)[175](533412_1_En_8_Chapter.xhtml#PB175)\u2013[196](533412_1_En_8_Chapter.xhtml#PB196)\n\nTransferTransfo Conversational\nAgents[234](533412_1_En_9_Chapter.xhtml#PB234)\u2013[235](533412_1_En_9_Chapter.xhtml#PB235)\n\nTransformation-based learning\n(TBL)[61](533412_1_En_3_Chapter.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4576772d-15c7-4487-931c-e2febd229cb2": {"__data__": {"id_": "4576772d-15c7-4487-931c-e2febd229cb2", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b5ed22e-7e91-4de6-b81c-7920d97b76ce", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "fd70d7aef5cf3e016ecbd055bb0674f97d98a083e07654144a70fe78f1760a8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5ddcd3f-5221-4f7c-8ff9-a33d5231e6a2", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9b0c7c50c3bf877e4660f99ad00bbe6ce3bfb57cbd5757e2073f5e49e19b00d8", "class_name": "RelatedNodeInfo"}}, "hash": "25fb2f6b1f662af302a6c14e0290951ab5f7a168745b505843fd0f01c7ef69e7", "text": "xhtml#PB23)\n\nTop-down\nparser[80](533412_1_En_4_Chapter.xhtml#PB80)\u2013[82](533412_1_En_4_Chapter.xhtml#PB82)\n\nTopic generation[336](533412_1_En_14_Chapter.xhtml#PB336)\n\nTransfer learning\n(TL)[175](533412_1_En_8_Chapter.xhtml#PB175)\u2013[196](533412_1_En_8_Chapter.xhtml#PB196)\n\nTransferTransfo Conversational\nAgents[234](533412_1_En_9_Chapter.xhtml#PB234)\u2013[235](533412_1_En_9_Chapter.xhtml#PB235)\n\nTransformation-based learning\n(TBL)[61](533412_1_En_3_Chapter.xhtml#PB61)\u2013[62](533412_1_En_3_Chapter.xhtml#PB62),\n[65](533412_1_En_3_Chapter.xhtml#PB65)\n\nTransformers[170](533412_1_En_7_Chapter.xhtml#PB170),\n[175](533412_1_En_8_Chapter.xhtml#PB175)\u2013[196](533412_1_En_8_Chapter.xhtml#PB196),\n[243](533412_1_En_10_Chapter.xhtml#PB243),\n[373](533412_1_En_15_Chapter.xhtml#PB373)\u2013398,\n[401](533412_1_En_16_Chapter.xhtml#PB401)\u2013431\n\nTrigrams[21](533412_1_En_2_Chapter.xhtml#PB21),\n[27](533412_1_En_2_Chapter.xhtml#PB27),\n[32](533412_1_En_2_Chapter.xhtml#PB32), 269\n\nTuring, A.[5](533412_1_En_1_Chapter.xhtml#PB5),\n[8](533412_1_En_1_Chapter.xhtml#PB8)\n\nTuring Test[5](533412_1_En_1_Chapter.xhtml#PB5)\n\nTwo-tower model[202](533412_1_En_9_Chapter.xhtml#PB202)\n\nU\n\nUnigram[21](533412_1_En_2_Chapter.xhtml#PB21),\n[27](533412_1_En_2_Chapter.xhtml#PB27),\n[28](533412_1_En_2_Chapter.xhtml#PB28), [32](533412_1_En_2_Chapter.xhtml#PB32)\n\nUnsupervised discourse\nsegmentation[154](533412_1_En_7_Chapter.xhtml#PB154)\u2013[155](533412_1_En_7_Chapter.xhtml#PB155)\n\nUtterance[23](533412_1_En_2_Chapter.xhtml#PB23)\n\nV\n\nVagueness[100](533412_1_En_5_Chapter.xhtml#PB100),\n[101](533412_1_En_5_Chapter.xhtml#PB101)\n\nVectorial representation[118](533412_1_En_6_Chapter.xhtml#PB118)\n\nVector space\nmodel[200](533412_1_En_9_Chapter.xhtml#PB200)\u2013[204](533412_1_En_9_Chapter.xhtml#PB204)\n\nVerb-phrase\n(VP)[72](533412_1_En_4_Chapter.xhtml#PB72)\u2013[73](533412_1_En_4_Chapter.xhtml#PB73)\n\nVerbs[43](533412_1_En_3_Chapter.xhtml#PB43),\n[44](533412_1_En_3_Chapter.xhtml#PB44),\n[47](533412_1_En_3_Chapter.xhtml#PB47),\n[53](533412_1_En_3_Chapter.xhtml#PB53)\u2013[56](533412_1_En_3_Chapter.xhtml#PB56),\n[59](533412_1_En_3_Chapter.xhtml#PB59), [64](533412_1_En_3_Chapter.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5ddcd3f-5221-4f7c-8ff9-a33d5231e6a2": {"__data__": {"id_": "d5ddcd3f-5221-4f7c-8ff9-a33d5231e6a2", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4576772d-15c7-4487-931c-e2febd229cb2", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "25fb2f6b1f662af302a6c14e0290951ab5f7a168745b505843fd0f01c7ef69e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "854f739d-2def-421e-a9eb-10b30f5fcf28", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "115fc771cbabfc4616eafa7c88959f4a807ded29f2b4e566210d68c25b7135b5", "class_name": "RelatedNodeInfo"}}, "hash": "9b0c7c50c3bf877e4660f99ad00bbe6ce3bfb57cbd5757e2073f5e49e19b00d8", "text": "xhtml#PB204)\n\nVerb-phrase\n(VP)[72](533412_1_En_4_Chapter.xhtml#PB72)\u2013[73](533412_1_En_4_Chapter.xhtml#PB73)\n\nVerbs[43](533412_1_En_3_Chapter.xhtml#PB43),\n[44](533412_1_En_3_Chapter.xhtml#PB44),\n[47](533412_1_En_3_Chapter.xhtml#PB47),\n[53](533412_1_En_3_Chapter.xhtml#PB53)\u2013[56](533412_1_En_3_Chapter.xhtml#PB56),\n[59](533412_1_En_3_Chapter.xhtml#PB59), [64](533412_1_En_3_Chapter.xhtml#PB64)\n\nVerifiability[100](533412_1_En_5_Chapter.xhtml#PB100)\n\nVP\nsubcategorization[74](533412_1_En_4_Chapter.xhtml#PB74)\u2013[75](533412_1_En_4_Chapter.xhtml#PB75)\n\nW\n\nWake word[403](533412_1_En_16_Chapter.xhtml#PB403)\n\nWide&Deep model[202](533412_1_En_9_Chapter.xhtml#PB202),\n[205](533412_1_En_9_Chapter.xhtml#PB205)\n\nWordCloud[285](533412_1_En_12_Chapter.xhtml#PB285), 299\u2013301\n\nWord form[23](533412_1_En_2_Chapter.xhtml#PB23)\n\nWord frequency[60](533412_1_En_3_Chapter.xhtml#PB60)\n\nWordNet[123](533412_1_En_6_Chapter.xhtml#PB123),\n[126](533412_1_En_6_Chapter.xhtml#PB126)\u2013[130](533412_1_En_6_Chapter.xhtml#PB130),\n[145](533412_1_En_6_Chapter.xhtml#PB145)\n\nWord prediction[19](533412_1_En_2_Chapter.xhtml#PB19),\n[20](533412_1_En_2_Chapter.xhtml#PB20)\n\nWord sense[118](533412_1_En_6_Chapter.xhtml#PB118),\n[123](533412_1_En_6_Chapter.xhtml#PB123)\n\nWord sense disambiguation\n(WSD)[123](533412_1_En_6_Chapter.xhtml#PB123)\u2013[126](533412_1_En_6_Chapter.xhtml#PB126),\n[145](533412_1_En_6_Chapter.xhtml#PB145)\n\nWord tokenization[244](533412_1_En_10_Chapter.xhtml#PB244)\n\nWord usage frequency259\n\nWord2vec[314](533412_1_En_13_Chapter.xhtml#PB314),\n[320](533412_1_En_13_Chapter.xhtml#PB320)\n\nWord vectors[137](533412_1_En_6_Chapter.xhtml#PB137),\n[313](533412_1_En_13_Chapter.xhtml#PB313),\n[314](533412_1_En_13_Chapter.xhtml#PB314),\n317\u2013[320](533412_1_En_13_Chapter.xhtml#PB320), 326\n\nWorld knowledge[116](533412_1_En_6_Chapter.xhtml#PB116),\n[124](533412_1_En_6_Chapter.xhtml#PB124)\n\nX\n\nXiao Ice QA\nSystem[232](533412_1_En_9_Chapter.xhtml#PB232)\u2013[234](533412_1_En_9_Chapter.xhtml#PB234)\n\nXiaowei[402](533412_1_En_16_Chapter.xhtml#PB402), 431\n\nZ\n\nZeugma\ntest[120](533412_1_En_6_Chapter.xhtml#PB120)\u2013[121](533412_1_En_6_Chapter.xhtml#PB121),\n[145](533412_1_En_6_Chapter.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "854f739d-2def-421e-a9eb-10b30f5fcf28": {"__data__": {"id_": "854f739d-2def-421e-a9eb-10b30f5fcf28", "embedding": null, "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c2ec5ed-64e5-4db4-8b41-392dfddc7774", "node_type": "4", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "446dcb824bffa8c1a0673b7110cd1d668e0bb125d4c5632e40f0d5d59480f3f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5ddcd3f-5221-4f7c-8ff9-a33d5231e6a2", "node_type": "1", "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}, "hash": "9b0c7c50c3bf877e4660f99ad00bbe6ce3bfb57cbd5757e2073f5e49e19b00d8", "class_name": "RelatedNodeInfo"}}, "hash": "115fc771cbabfc4616eafa7c88959f4a807ded29f2b4e566210d68c25b7135b5", "text": "xhtml#PB320), 326\n\nWorld knowledge[116](533412_1_En_6_Chapter.xhtml#PB116),\n[124](533412_1_En_6_Chapter.xhtml#PB124)\n\nX\n\nXiao Ice QA\nSystem[232](533412_1_En_9_Chapter.xhtml#PB232)\u2013[234](533412_1_En_9_Chapter.xhtml#PB234)\n\nXiaowei[402](533412_1_En_16_Chapter.xhtml#PB402), 431\n\nZ\n\nZeugma\ntest[120](533412_1_En_6_Chapter.xhtml#PB120)\u2013[121](533412_1_En_6_Chapter.xhtml#PB121),\n[145](533412_1_En_6_Chapter.xhtml#PB145)\n\nZipf\u2019s law[36](533412_1_En_2_Chapter.xhtml#PB36)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"2c2ec5ed-64e5-4db4-8b41-392dfddc7774": {"node_ids": ["ce0b9e05-f4ba-4c97-a8e3-1ecf42aac77f", "75d4fb0b-08fb-40eb-b50d-b1260a6592f1", "6ed7accd-39b3-47c9-9f7a-dc8ceb9e62bc", "0724a624-7669-4eb7-bf18-7591e9632c85", "d7f6502a-55e0-40ad-937f-c05a86f9eacd", "b466d2f2-0f85-47f9-89d7-27a9d64af5f5", "d41a7326-c21f-46f0-8d47-9dd42dfaef09", "99b6da0f-83f7-4222-a681-9d783aeff388", "ab4675ba-d576-4f01-a59d-e12f9a7439ec", "a133bbfb-864e-42cf-93c4-692cc1a8b49a", "f82c496f-4376-4a62-b2de-0b496e690569", "423d1d2f-8018-4f56-888f-bc984448017c", "5c218b3c-c3ac-443d-8ff5-38b7f8aad1b8", "3b40620f-c940-49b5-8d5d-7267ee87459c", "48c29261-138b-4f7e-ae82-e0d4871ad9e7", "f9815be4-bcae-4703-8b18-69079213728e", "d70c0638-6e54-4009-a63d-5d033d34e276", "a191cbfb-4c98-4e78-975f-89cfec9078fd", "c4eb8364-7b9d-4ccf-a75e-5de32553ee2d", "c6620f44-b87c-4e9e-b972-a6ce35447511", "c545670a-68aa-4696-801a-f6fe38eb4075", "60f7b2bf-4ef4-480e-83a1-3c5a44c04f43", "59473d92-bda3-4a69-bbcf-5e5ff1f154c4", "5642e195-1c2e-4a22-8de9-389dbfea0e1f", "8afa1e4c-4311-43c7-903c-49de079b1671", "d031bc19-0a0b-4a01-a903-5a706e12bfce", "0f33a452-5b20-49d4-a598-ae18401fc514", "5daa7103-08f4-468b-bcc3-59fb4d83307f", "700aebbd-f2ee-48f1-bd3c-2a6f4dba2e6f", "0463471e-72fe-4443-bb7a-edcc7115419b", "4fa797d7-de8f-47a7-98c9-88ecb074fbc1", "4f5fcfa4-ca5b-46a6-85a9-8e527600a09e", "a4efa5c2-b683-46cd-b9dd-b0167b13e735", "0c9dbe0c-27f1-4b65-9822-836a0c505fae", "02735595-f141-4f9b-86dc-363c92f33548", "20d76e62-0b68-4554-8dd6-67cd64ee446e", "7752d95b-e048-4ea9-abae-9126d0c3e226", "5faf170a-4714-4769-a44b-73a42b2af98d", "8df08d6c-1ec8-464b-8902-fede8a6e9662", "aa84c709-a13e-4950-bfa0-a6fc091396eb", "4ac1f787-e014-40cf-b7fc-221d16d412d9", "219d9685-fedd-49fb-ae25-91b470521aa2", "5df39e9a-5fdd-4eee-92a9-bbf3fe9802b2", "fb24882b-de0f-47d5-a14a-89930bfb4952", "c758939b-e72c-41b7-a48e-63e7efdee069", "6121ac62-6174-4244-825e-4f6c29e43a00", "732f037c-2f31-4f4b-911a-6aece2dd565d", "b5e7e4a3-93e3-4d4b-a375-555a6edef2df", "60271cf2-42b6-4201-8fa2-5900ba62713b", "4b02307a-749a-448b-9fee-a287c51518f9", "24b2c9d8-9a7b-439c-9e3d-0881b3d444f5", "a09e5836-dc3d-4176-9c21-778a391a9e6f", "dd6ea1ce-514a-451a-8d45-df160c663ea3", "528afe42-98b6-4296-ab83-b61d8db1d54e", "a1122825-051d-422c-b717-4b077a1677fb", "535d0953-a27d-4542-a114-6c06ac41d09e", "6791e156-fcca-482f-bbe3-fbd41a9e2804", "53073738-ab94-4899-b213-31eb2e3023bc", "925f9454-708c-499d-86b4-96d54525be28", "e667b938-5533-4206-999c-036a1610e504", "d72652b4-ad63-4c20-8710-4ee8a46e01b6", "e88ea21d-5360-467f-892d-14ef3b4ff6db", "ca46d90a-6a0a-4054-b8bd-b909a3001165", "15890fde-4b03-4090-9ed1-7191f72f00c3", "7b13915e-5495-4ff4-904a-49ef7b277270", "781e5e69-16ec-459e-aaac-5752fa3a22f4", "84822d53-e37d-4927-ae72-dee3c757a1f8", "bf61401b-767e-46c8-a081-d3eef3139e70", "0076e169-797d-4bf7-820b-b71ddd3cc632", "344ec229-3569-4665-a445-eb81a1513958", "a2cd35ce-3af4-468c-af3d-b58d6778b03a", "b8f116ff-73b1-43df-a4a2-230fa30df2b7", "e6eb8953-5820-4348-92fc-6b2374b0eba3", "13e765d8-be27-4c9e-976d-ba30c50de3a7", "2797a072-f1ee-4430-90d2-be5174db837a", "80576584-c57c-4e47-895e-33f0c5f9e477", "ddc99327-8512-4467-8f63-1f1df4c7e861", "b0f39855-071a-49bc-b7f8-48e96c142da8", "668ce8c6-232e-4c86-8eb6-45ce8ec0a1a4", "d319b514-73cb-4d7c-b438-4f376d791c26", "05178036-8e53-443a-b2d0-9d58ed583caa", "5501ae58-814e-4f3e-9e74-1be94f455d12", "77a515e5-0eb4-4703-ac62-7ce0f899bb9d", "32043cd8-4f1b-40aa-a915-b20b24cc58b7", "cc5dba57-4dba-4185-9e02-e133ce92c8ba", "331462d8-74f8-42ee-a2d4-c82b8b5f6eb8", "379795ad-cd6e-4919-ae94-e1e957728d85", "723d2a3a-1b1e-4e99-a1b6-6225edfe33bf", "c94e6ca9-20cf-4a22-abd0-227220f8ba47", "521cdbc0-f093-4182-9fb7-675d51d8bbe8", "fee58545-512c-4ca6-aeac-c39e423a6d64", "96229776-9ca8-4693-8b5c-f7a5eafcadef", "35e62314-07b0-4bd4-ac68-c6a728a21e58", "632c0d18-5b4c-461d-9d75-73d9a50db254", "f48a3feb-0a05-4043-9d55-fe9e834590c8", "5ac5d598-6a86-4d48-98da-d1eb787a8069", "74c9dd41-6daa-4924-bf7a-0b5abcd0a46f", "1f1080d4-7827-47b9-97ac-cc0014658a6a", "ab11e74c-f3ec-4c74-b401-5c065c3ad6a1", "cbfcb61f-bd1b-480c-9294-75cd9ae2720b", "b1c2e3c2-53e3-4f2b-8b70-5a2c96ff3ea2", "a1a8a867-b916-4d1c-8344-83fc7f136d9f", "0f4bca3b-8a2d-4b46-9e66-0ab40c3892e2", "b030ab3e-69b3-4c2e-9f0a-39541bf80cf0", "1f4b4260-f0a6-44f2-8ec5-796514392f74", "98ec463e-00aa-4b21-acec-c61ca61a2547", "542e3e7e-e3ca-4632-8700-4e602e99d875", "290e2dc9-d928-4cb4-a952-a5717c1a52f9", "3d09852a-b4f6-41e0-a385-15ab77b3aa46", "471a4722-e519-4537-8b54-73e79cab831e", "3a5fdbae-d8f0-49ed-ad88-651a98d525d1", "1b19fce5-054a-4767-be9c-c74e8662ee2b", "9a8933ee-8098-405b-ae91-184e3ba43068", "630f713f-df93-4c00-9741-54633ff17775", "2d607e9a-32d4-4498-a209-81f17a445317", "2a325d46-66cf-4c0e-bc5b-d24307d91452", "cb765245-f3ad-4357-9964-f2f027ba17f4", "b3dd270c-7cb5-43ae-98e5-689dbb7629b6", "a8d803e0-c547-4d0c-ac84-b50dcbd3721d", "48bb1a59-e55c-44ea-b0a0-d0e02c94b641", "63a4bd6d-e2f5-4fb9-933b-39662c7c812c", "6aec1e8b-3df6-4662-92ea-3828d287524e", "cbc92233-4a9c-4d7d-813f-dd94a2fe8f04", "dc6b2c27-0fde-4321-8ceb-45f8d5221ab8", "8dd7916e-c09d-4190-a57e-8dd1f2eeef37", "5b9692f0-02f5-4fd7-8ec0-6321c3b7f203", "828b251c-7fc7-4b90-9ccd-d9e0900f2061", "1a068b35-98d5-43e2-9717-a01bfd64426c", "965acd25-5015-4688-a3e9-d37f06a5a82a", "77ad28f1-2c3c-4dd4-b3ac-b673719a7a46", "641a4acd-647f-4461-9c3b-665b15f08f74", "5d2ddf35-1b06-46dc-8939-020e2683c7ea", "de394b31-815e-41b6-866b-4254de6f1fcb", "8c4c6450-2aad-447c-aa48-7db5816d05fc", "f54b124a-b23b-48c8-bc72-74d818994e78", "3d951d46-71ee-4450-81a8-249a3d1d379c", "4c10f578-9617-4ff9-a27e-0c9f487a398c", "0417ac08-df82-4822-81bc-4c0b75fcb71b", "5c34d5d6-e083-4f0a-bd38-a4f1a55402bd", "3f296ff5-5e78-4b0c-85fe-d2f0aa9dfd36", "f5b45c0c-780a-4c07-9329-57e41c1e3700", "80e5d739-3b00-4259-9797-6d2278fbfb14", "e9a8004a-abac-48c4-b129-a20461b0a3e8", "63e2e665-c350-4422-a3ef-cc5fc2712a93", "9ec573f0-4669-4a2c-acac-0a589bb21e2a", "e6a8a432-83e6-41f8-9083-1f5431eff637", "40eb9039-c2f3-4d30-9b7d-7a8ef047469d", "c35795ee-c5d2-473e-8726-ccc51ee9d9eb", "7c4e0ddc-12fe-4500-b878-b6b87efed231", "e3a8ed41-5133-4ae7-923d-a565af99fa7e", "6a4d1058-b028-41b4-98ad-72370fe1093b", "b740b58a-71cd-4b92-bd5a-b07e2b25dc70", "12bba127-6227-417f-a8a8-0b71e26447df", "9d09f8ab-0693-484b-9233-805f358333f9", "2792e8e0-2019-403f-9636-67e2cc02a0cc", "1fbf18af-77b0-48ad-a125-081ab07d30ce", "623f002c-a899-46c5-9873-74a19770c592", "ffda6ceb-fd17-4d46-a30c-fd937a1d9d9d", "fbcf7042-6fcb-4267-8a07-f3e784a867af", "a5b7195c-ce65-4f18-9596-aa26e8a7dadf", "fb6798d7-574f-47f4-a964-fbe2cf227d40", "e1942889-c301-4630-a8e0-b56f5bacfa05", "43a14e95-5be9-433c-be93-d04baad41ab6", "21d6d0ae-27af-43d5-946e-648883050065", "0c4d99ed-5036-40e9-af5b-37117135d1ba", "2f5f9482-81fa-496d-aa5d-a4b501471495", "64827f10-25d6-4752-9e3e-9ffbffd3dfca", "30820258-a51a-4ed7-9749-4f0e55a08c77", "90a10781-a82b-4a14-a197-24cc7a5bcd76", "2738c963-3b32-45a6-b316-65230187332d", "72d52d59-bb01-48dc-8a54-3dfbc699be5d", "0a3484a5-3d67-485f-b143-70f5f1aaa459", "26ffa5ee-477a-45b9-81e6-397321ab72ad", "ebcec406-cfa4-4318-bf26-b41ff5f72485", "4f90492d-4f3d-453b-ba4b-d46faef3e44c", "b367f9e5-a1b5-4174-9aa3-b7fce1d27b22", "f5036856-90ce-4a0e-a95b-e49576288d61", "b3411dae-27c7-4320-b6e1-9e4139c1c9eb", "f6c17a3f-c81a-47d8-b715-5e99fe7593fc", "917ce3d2-de34-4533-82ae-de43e0475dee", "b9b0b7a8-cf13-4756-8a80-5fa4be811099", "ba741414-e089-4eaa-af02-1bf9ebf40169", "b566f661-0271-4a9e-a219-ec657392410b", "ef3464ca-117d-46a4-bf59-338ef3972f22", "25384309-fcc4-4142-ba43-67d35e18ace8", "772737cd-f6f7-48b7-b804-c01878343b78", "d6c2b407-585b-40b9-af57-89741193073e", "840bfb23-2e93-4441-ab43-3d430280a18f", "e73ae97e-5eb8-48f0-9863-d29a7b4bd638", "071796b2-c690-4e5e-8f66-52dc4a24a140", "9c0fb3e8-b37d-4716-b3e7-e26f964ebf7b", "c508c36d-07c2-433a-a0ad-8027f870e8bf", "c1887ceb-8bfb-49e5-a4a1-70c459e1746c", "96a04c58-6f55-4692-8422-a08b421f9430", "6fd7a78e-6f18-42f2-9984-401de4bf5d57", "f59c3cd3-9d2a-4bed-a8a8-a22915fca3f3", "fb922417-c0b2-469f-bcc9-8efcdb34f34e", "baa8a43a-847e-4883-bdfe-56562826cd5b", "19d154ae-7244-4933-a2f3-9899d84f5c54", "9417db7d-1e88-4534-a6bf-e395abd7b616", "60b11cc4-41a6-4d86-99a4-afa86401c63e", "076e0f0d-2a58-4075-8205-64d34161176d", "2e49791e-be80-48bc-9abe-94fb78056f86", "d22bbedb-a982-43e1-b839-29ea388c95c4", "840a3c47-d0d1-4be7-9aa3-3601eb0ba40a", "b76ef6ff-ef7e-4a70-9a93-53418a22ce7d", "b091f2f0-8541-4b80-80bf-f5fceb61406c", "aaa04e9b-1b9f-43aa-a323-56f1acbf036c", "b4d84669-bde2-45ff-9867-3fd174a84d14", "6b4150c2-c8a7-402a-a2c7-a8e9957c668e", "0b0f522d-72d1-4a39-9d3d-9440cf5aa039", "0a7ad520-810c-4d3b-8402-6e2ac6363ea9", "7ae0b6fe-524f-48a1-82bc-dc3bfe0bf78e", "f9bd7c0a-1d77-4f66-9b3c-a6ae4d37a1a7", "b7fde8df-6e12-4ab4-a76e-d8c7542e9e55", "a65a646e-257f-4ca4-b433-2edb6a9c0e25", "8474ecaf-d19d-418f-bbb1-6c70fdf3e2ab", "5cae8a8e-2461-41ed-9f4d-84ecb323a469", "126abbbc-585d-4278-a6f3-61a1593773fb", "9d22509a-fcdd-4cab-b86d-a30672628857", "a18b872d-4a23-4fa1-8218-7953940e6511", "e691f74a-8eff-4238-91f5-2bffa27b16b4", "ecc61760-3573-46d9-a2c9-34ddd81a791b", "ea07a436-ffb3-4aad-be18-c36ad5aa6358", "d26df7cd-905e-4ad8-8cf5-44c2b04b7aa3", "47cd5ba4-2c63-4b7f-8ae2-4ba862ec3bba", "52724a04-e86c-42bf-89f2-6a0328353ef8", "b6f406bf-c6b3-474a-b5c6-4771d39de426", "ecde0cd6-14a7-45d4-ae68-2f425caae914", "07a57000-bdad-4b8f-a3bb-6ad10a2ed640", "6de95ad1-f70c-4504-a3cb-daab98c3d4fb", "414fc246-cf1f-4387-b9e9-50a59269df94", "332f11eb-a03f-49f0-b0c2-57e31cb8579f", "52270854-9553-4699-a114-1c433b7bb5da", "1dc3c49a-c099-48bc-b29e-7c863951889d", "fc4d2b0b-f3fe-4d60-be8f-d310998632cb", "9da16b3f-2b5c-4828-b597-86854a258b89", "22cdf90a-2a95-47c0-b3c8-f12f7396ab15", "b278d73c-a50e-446c-8179-937d59970111", "de74b1b6-6916-48f3-8b77-0b5c23d44a26", "e1433fcb-417c-42b0-9d33-480713fb5bdc", "ba0b9c0b-76ae-4e1b-b98b-2c5956f89abe", "d976fc42-f398-47cd-a442-46abf33022a6", "753d27f6-feb2-4496-9ae7-0b1726d4670c", "a763ca33-232b-4b6c-9631-3c007080beb4", "550dc4b8-d885-43d0-bef9-696742bf0089", "08a73a94-cbec-4e2f-892b-6e676479116d", "2df5ffbc-5c79-4aa6-bd66-f7cc6a2e5967", "b6479543-c9e2-4ed0-94ec-050bffee8255", "afca20e9-5413-48ad-b52c-684eb8684299", "e2a94554-90db-4b7e-9cd4-504d5e21f9e5", "e49cfda0-7847-472d-9224-298a38f7a643", "8354677e-2e80-4a7c-a595-25d11f5012ef", "7a24a74b-898f-459c-9918-0d3c9355dc17", "6c57e169-5f39-4e06-ac8e-ff56b0d1c1fd", "62655d5a-6d9b-4984-acab-29528a6aec7f", "38160117-0ac5-400c-a8f4-7ddd30499712", "5a0e182a-f626-4801-87d7-5ad200c2439b", "4e019c0b-8b74-4e5e-b92c-05959c245376", "d5384145-7a6a-47c4-a092-31ab4d25dcea", "a4b071c0-100f-442a-a76f-8e521b021b15", "7e2a0177-3005-4abd-b7a8-f56792e2dfa5", "71c42d16-407f-4a27-b78c-6d9300942be4", "f1a7e625-d391-4b87-9d1a-f1e1efb00005", "fd8660e1-811b-4090-bf77-608d17849b66", "46f079f5-0da9-4c9d-9f7a-b37d9b3762f8", "b04af3bf-0277-41a5-823b-d60bd723bc33", "ec77d0b1-8f3c-4f00-9230-e6b127533479", "81313276-cf58-464d-9b71-d5eba1a3fba1", "b9029a78-c1dc-4828-bfee-7900a3378184", "88fc90ea-8ffc-466f-90c0-fc10e285d819", "98f79b34-99f6-42d4-a905-f611eb0ec699", "6210a687-7ffb-478e-9cd9-5c68fc6c3f2a", "2807e937-7ce2-4d9a-a8db-66b5af9d3cbd", "f200de91-2ef5-48bc-8ba8-e374b3f44f01", "93aa9ea2-be05-4aeb-9c07-be8880329809", "19344792-e779-4473-89f4-e6ac9074a9df", "2a02f0b2-75af-49a8-bf12-a4e8b039e072", "ec48e832-eb08-4475-a762-65bd6f32d68b", "5801ae0e-e6f1-4b82-a4e5-7ade293997bb", "46407f5c-13b7-48b6-b3d1-0916776cb806", "45a74346-fb00-4e7a-98ed-4d61bd6ba7f0", "ed15d0cb-37d6-4221-aa62-c438fb0baf9b", "6cc2bfa2-a695-4bd9-aadf-9bfea6e71547", "e6f043b5-6131-4d29-8d23-5c00d56e2ae8", "4e79feec-40be-4184-837d-355b8ea6d3ef", "a2f59b53-2c62-45bc-a8a8-deb071cbefb4", "e0bab5e1-8d68-4a58-9bd8-3a514e56ffb0", "cd5c76d0-e34e-4835-9d4d-0ffdb141d1e4", "8e971391-34b4-417f-b177-0719365b2a88", "20b2460b-100f-4f1d-9af9-05304a0cc4e0", "89f70779-3fed-4981-affc-2c3464eaa30c", "5eeb2ea6-00e1-4972-9b01-03bbcb378058", "b37e3fcc-fd8a-446c-b8ec-1a44486c78f6", "d4cf1b79-17a9-4d8d-889b-d16e071b8af3", "62056447-7355-4d55-b8ce-1554f19bdc92", "a97a763b-9acb-4697-9a32-d91c65fdeb2b", "4e79d1a1-6aca-4766-9adc-0ca5b90eb143", "bee044a8-00b9-49fc-a02e-553193091c9b", "c7710e3c-927f-468f-a50b-bfa09014a3eb", "a3e2c50f-7e64-455a-83e3-9a4d49b57115", "bbe4f542-7fd0-42fb-9be4-cb5001c79bba", "a5c32338-016e-4986-9a9e-17a9c04475ea", "dd6b3eaf-6a94-4ccc-8a83-6d4c32083257", "5bcdf5e3-0748-48d9-85bb-1b75a76ab3fa", "d692038f-f6ca-42c0-bac5-79bad48a0640", "eaafd9d7-add8-438d-8f95-8311dd98b3b6", "459179b4-bab5-454d-8a21-c47c762503ec", "68197fa3-990a-4c8c-82a2-3c2d3628c1dd", "85ba0f77-435d-4b24-b3de-c2b44e76e957", "b9f9093f-ee62-4bb0-8eec-5d493daf9c56", "3f789b1f-d64c-40ae-ab06-de445014722d", "1c2648ba-8874-49a5-a8c9-bb2f3b979370", "7fbf2474-06e2-45ca-b405-919ddf7f52d6", "7ee13276-17e9-4acd-94eb-1b9fb5dce0d2", "1e9236bb-38f7-4bc8-b0f6-f1aabb261407", "3545c355-d643-4a8e-a499-c147943664d1", "835c2031-d9f2-4215-8457-261c016a8877", "51211326-f076-4aa1-bd03-8613f055845b", "f626da33-7ec5-4054-b01b-6ca8d9d382e4", "6b5ed22e-7e91-4de6-b81c-7920d97b76ce", "4576772d-15c7-4487-931c-e2febd229cb2", "d5ddcd3f-5221-4f7c-8ff9-a33d5231e6a2", "854f739d-2def-421e-a9eb-10b30f5fcf28"], "metadata": {"file_path": "nlp-textbook/Natural Language Processing - A Textbook With Python Implementation.epub", "file_name": "Natural Language Processing - A Textbook With Python Implementation.epub", "file_type": "application/epub+zip", "file_size": 63714107, "creation_date": "2023-11-17", "last_modified_date": "2023-11-16", "last_accessed_date": "2023-11-17"}}}}